I0421 20:13:59.449364      22 e2e.go:126] Starting e2e run "3e989928-34dd-48c8-8a06-ed2a4ab89908" on Ginkgo node 1
Apr 21 20:13:59.463: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1682108039 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Apr 21 20:13:59.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:13:59.580: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr 21 20:13:59.589: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 21 20:13:59.599: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 21 20:13:59.599: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Apr 21 20:13:59.599: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 21 20:13:59.601: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
Apr 21 20:13:59.601: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 21 20:13:59.601: INFO: e2e test version: v1.26.3
Apr 21 20:13:59.602: INFO: kube-apiserver version: v1.26.3
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Apr 21 20:13:59.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:13:59.604: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.025 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Apr 21 20:13:59.579: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:13:59.580: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Apr 21 20:13:59.589: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Apr 21 20:13:59.599: INFO: 10 / 10 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Apr 21 20:13:59.599: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
    Apr 21 20:13:59.599: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Apr 21 20:13:59.601: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
    Apr 21 20:13:59.601: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Apr 21 20:13:59.601: INFO: e2e test version: v1.26.3
    Apr 21 20:13:59.602: INFO: kube-apiserver version: v1.26.3
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Apr 21 20:13:59.602: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:13:59.604: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:13:59.626
Apr 21 20:13:59.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename conformance-tests 04/21/23 20:13:59.627
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:13:59.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:13:59.637
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 04/21/23 20:13:59.639
Apr 21 20:13:59.639: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Apr 21 20:13:59.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-4466" for this suite. 04/21/23 20:13:59.643
------------------------------
â€¢ [0.020 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:13:59.626
    Apr 21 20:13:59.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename conformance-tests 04/21/23 20:13:59.627
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:13:59.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:13:59.637
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 04/21/23 20:13:59.639
    Apr 21 20:13:59.639: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:13:59.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-4466" for this suite. 04/21/23 20:13:59.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:13:59.647
Apr 21 20:13:59.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename statefulset 04/21/23 20:13:59.648
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:13:59.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:13:59.655
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8990 04/21/23 20:13:59.657
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 04/21/23 20:13:59.66
Apr 21 20:13:59.668: INFO: Found 0 stateful pods, waiting for 3
Apr 21 20:14:09.671: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 20:14:09.671: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 20:14:09.671: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/21/23 20:14:09.675
Apr 21 20:14:09.692: INFO: Updating stateful set ss2
STEP: Creating a new revision 04/21/23 20:14:09.692
STEP: Not applying an update when the partition is greater than the number of replicas 04/21/23 20:14:19.7
STEP: Performing a canary update 04/21/23 20:14:19.7
Apr 21 20:14:19.717: INFO: Updating stateful set ss2
Apr 21 20:14:19.720: INFO: Waiting for Pod statefulset-8990/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 04/21/23 20:14:29.725
Apr 21 20:14:29.834: INFO: Found 2 stateful pods, waiting for 3
Apr 21 20:14:39.838: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 20:14:39.838: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 20:14:39.838: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 04/21/23 20:14:39.841
Apr 21 20:14:39.858: INFO: Updating stateful set ss2
Apr 21 20:14:39.861: INFO: Waiting for Pod statefulset-8990/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Apr 21 20:14:49.882: INFO: Updating stateful set ss2
Apr 21 20:14:49.886: INFO: Waiting for StatefulSet statefulset-8990/ss2 to complete update
Apr 21 20:14:49.886: INFO: Waiting for Pod statefulset-8990/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 21 20:14:59.890: INFO: Deleting all statefulset in ns statefulset-8990
Apr 21 20:14:59.892: INFO: Scaling statefulset ss2 to 0
Apr 21 20:15:09.901: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 20:15:09.903: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:15:09.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8990" for this suite. 04/21/23 20:15:09.912
------------------------------
â€¢ [SLOW TEST] [70.268 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:13:59.647
    Apr 21 20:13:59.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename statefulset 04/21/23 20:13:59.648
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:13:59.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:13:59.655
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8990 04/21/23 20:13:59.657
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 04/21/23 20:13:59.66
    Apr 21 20:13:59.668: INFO: Found 0 stateful pods, waiting for 3
    Apr 21 20:14:09.671: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 20:14:09.671: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 20:14:09.671: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/21/23 20:14:09.675
    Apr 21 20:14:09.692: INFO: Updating stateful set ss2
    STEP: Creating a new revision 04/21/23 20:14:09.692
    STEP: Not applying an update when the partition is greater than the number of replicas 04/21/23 20:14:19.7
    STEP: Performing a canary update 04/21/23 20:14:19.7
    Apr 21 20:14:19.717: INFO: Updating stateful set ss2
    Apr 21 20:14:19.720: INFO: Waiting for Pod statefulset-8990/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 04/21/23 20:14:29.725
    Apr 21 20:14:29.834: INFO: Found 2 stateful pods, waiting for 3
    Apr 21 20:14:39.838: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 20:14:39.838: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 20:14:39.838: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 04/21/23 20:14:39.841
    Apr 21 20:14:39.858: INFO: Updating stateful set ss2
    Apr 21 20:14:39.861: INFO: Waiting for Pod statefulset-8990/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Apr 21 20:14:49.882: INFO: Updating stateful set ss2
    Apr 21 20:14:49.886: INFO: Waiting for StatefulSet statefulset-8990/ss2 to complete update
    Apr 21 20:14:49.886: INFO: Waiting for Pod statefulset-8990/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 21 20:14:59.890: INFO: Deleting all statefulset in ns statefulset-8990
    Apr 21 20:14:59.892: INFO: Scaling statefulset ss2 to 0
    Apr 21 20:15:09.901: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 20:15:09.903: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:15:09.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8990" for this suite. 04/21/23 20:15:09.912
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:15:09.916
Apr 21 20:15:09.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:15:09.916
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:09.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:09.926
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:15:09.927
Apr 21 20:15:09.933: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401" in namespace "projected-3669" to be "Succeeded or Failed"
Apr 21 20:15:09.934: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401": Phase="Pending", Reason="", readiness=false. Elapsed: 1.391563ms
Apr 21 20:15:11.937: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004515555s
Apr 21 20:15:13.938: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00573282s
Apr 21 20:15:15.938: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005100067s
STEP: Saw pod success 04/21/23 20:15:15.938
Apr 21 20:15:15.938: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401" satisfied condition "Succeeded or Failed"
Apr 21 20:15:15.940: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401 container client-container: <nil>
STEP: delete the pod 04/21/23 20:15:15.952
Apr 21 20:15:15.960: INFO: Waiting for pod downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401 to disappear
Apr 21 20:15:15.961: INFO: Pod downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 20:15:15.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3669" for this suite. 04/21/23 20:15:15.964
------------------------------
â€¢ [SLOW TEST] [6.052 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:15:09.916
    Apr 21 20:15:09.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:15:09.916
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:09.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:09.926
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:15:09.927
    Apr 21 20:15:09.933: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401" in namespace "projected-3669" to be "Succeeded or Failed"
    Apr 21 20:15:09.934: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401": Phase="Pending", Reason="", readiness=false. Elapsed: 1.391563ms
    Apr 21 20:15:11.937: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004515555s
    Apr 21 20:15:13.938: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00573282s
    Apr 21 20:15:15.938: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005100067s
    STEP: Saw pod success 04/21/23 20:15:15.938
    Apr 21 20:15:15.938: INFO: Pod "downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401" satisfied condition "Succeeded or Failed"
    Apr 21 20:15:15.940: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401 container client-container: <nil>
    STEP: delete the pod 04/21/23 20:15:15.952
    Apr 21 20:15:15.960: INFO: Waiting for pod downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401 to disappear
    Apr 21 20:15:15.961: INFO: Pod downwardapi-volume-9543d171-b558-4afe-8396-772d7fd98401 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:15:15.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3669" for this suite. 04/21/23 20:15:15.964
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:15:15.97
Apr 21 20:15:15.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 20:15:15.97
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:15.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:15.979
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 04/21/23 20:15:15.981
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 20:15:15.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7289" for this suite. 04/21/23 20:15:15.985
------------------------------
â€¢ [0.018 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:15:15.97
    Apr 21 20:15:15.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 20:15:15.97
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:15.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:15.979
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 04/21/23 20:15:15.981
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:15:15.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7289" for this suite. 04/21/23 20:15:15.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:15:15.988
Apr 21 20:15:15.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:15:15.989
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:15.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:15.998
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:15:16
Apr 21 20:15:16.006: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e" in namespace "projected-3307" to be "Succeeded or Failed"
Apr 21 20:15:16.008: INFO: Pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348773ms
Apr 21 20:15:18.011: INFO: Pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005534137s
Apr 21 20:15:20.011: INFO: Pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005090177s
STEP: Saw pod success 04/21/23 20:15:20.011
Apr 21 20:15:20.011: INFO: Pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e" satisfied condition "Succeeded or Failed"
Apr 21 20:15:20.013: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e container client-container: <nil>
STEP: delete the pod 04/21/23 20:15:20.018
Apr 21 20:15:20.026: INFO: Waiting for pod downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e to disappear
Apr 21 20:15:20.028: INFO: Pod downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 20:15:20.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3307" for this suite. 04/21/23 20:15:20.03
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:15:15.988
    Apr 21 20:15:15.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:15:15.989
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:15.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:15.998
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:15:16
    Apr 21 20:15:16.006: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e" in namespace "projected-3307" to be "Succeeded or Failed"
    Apr 21 20:15:16.008: INFO: Pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348773ms
    Apr 21 20:15:18.011: INFO: Pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005534137s
    Apr 21 20:15:20.011: INFO: Pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005090177s
    STEP: Saw pod success 04/21/23 20:15:20.011
    Apr 21 20:15:20.011: INFO: Pod "downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e" satisfied condition "Succeeded or Failed"
    Apr 21 20:15:20.013: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e container client-container: <nil>
    STEP: delete the pod 04/21/23 20:15:20.018
    Apr 21 20:15:20.026: INFO: Waiting for pod downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e to disappear
    Apr 21 20:15:20.028: INFO: Pod downwardapi-volume-8a2caef5-8200-4290-82ca-34089689372e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:15:20.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3307" for this suite. 04/21/23 20:15:20.03
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:15:20.033
Apr 21 20:15:20.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename security-context-test 04/21/23 20:15:20.034
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:20.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:20.042
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Apr 21 20:15:20.048: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f" in namespace "security-context-test-801" to be "Succeeded or Failed"
Apr 21 20:15:20.049: INFO: Pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.474326ms
Apr 21 20:15:22.052: INFO: Pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003806989s
Apr 21 20:15:24.052: INFO: Pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003827581s
Apr 21 20:15:24.052: INFO: Pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f" satisfied condition "Succeeded or Failed"
Apr 21 20:15:24.057: INFO: Got logs for pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 21 20:15:24.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-801" for this suite. 04/21/23 20:15:24.059
------------------------------
â€¢ [4.031 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:15:20.033
    Apr 21 20:15:20.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename security-context-test 04/21/23 20:15:20.034
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:20.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:20.042
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Apr 21 20:15:20.048: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f" in namespace "security-context-test-801" to be "Succeeded or Failed"
    Apr 21 20:15:20.049: INFO: Pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.474326ms
    Apr 21 20:15:22.052: INFO: Pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003806989s
    Apr 21 20:15:24.052: INFO: Pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003827581s
    Apr 21 20:15:24.052: INFO: Pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f" satisfied condition "Succeeded or Failed"
    Apr 21 20:15:24.057: INFO: Got logs for pod "busybox-privileged-false-ea172e26-a784-4fb5-ab97-b45e08ed4e8f": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:15:24.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-801" for this suite. 04/21/23 20:15:24.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:15:24.064
Apr 21 20:15:24.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename certificates 04/21/23 20:15:24.065
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:24.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:24.073
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 04/21/23 20:15:24.508
STEP: getting /apis/certificates.k8s.io 04/21/23 20:15:24.51
STEP: getting /apis/certificates.k8s.io/v1 04/21/23 20:15:24.511
STEP: creating 04/21/23 20:15:24.511
STEP: getting 04/21/23 20:15:24.523
STEP: listing 04/21/23 20:15:24.524
STEP: watching 04/21/23 20:15:24.526
Apr 21 20:15:24.526: INFO: starting watch
STEP: patching 04/21/23 20:15:24.527
STEP: updating 04/21/23 20:15:24.532
Apr 21 20:15:24.537: INFO: waiting for watch events with expected annotations
Apr 21 20:15:24.537: INFO: saw patched and updated annotations
STEP: getting /approval 04/21/23 20:15:24.537
STEP: patching /approval 04/21/23 20:15:24.538
STEP: updating /approval 04/21/23 20:15:24.541
STEP: getting /status 04/21/23 20:15:24.546
STEP: patching /status 04/21/23 20:15:24.547
STEP: updating /status 04/21/23 20:15:24.552
STEP: deleting 04/21/23 20:15:24.561
STEP: deleting a collection 04/21/23 20:15:24.577
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:15:24.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-9620" for this suite. 04/21/23 20:15:24.641
------------------------------
â€¢ [0.580 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:15:24.064
    Apr 21 20:15:24.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename certificates 04/21/23 20:15:24.065
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:24.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:24.073
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 04/21/23 20:15:24.508
    STEP: getting /apis/certificates.k8s.io 04/21/23 20:15:24.51
    STEP: getting /apis/certificates.k8s.io/v1 04/21/23 20:15:24.511
    STEP: creating 04/21/23 20:15:24.511
    STEP: getting 04/21/23 20:15:24.523
    STEP: listing 04/21/23 20:15:24.524
    STEP: watching 04/21/23 20:15:24.526
    Apr 21 20:15:24.526: INFO: starting watch
    STEP: patching 04/21/23 20:15:24.527
    STEP: updating 04/21/23 20:15:24.532
    Apr 21 20:15:24.537: INFO: waiting for watch events with expected annotations
    Apr 21 20:15:24.537: INFO: saw patched and updated annotations
    STEP: getting /approval 04/21/23 20:15:24.537
    STEP: patching /approval 04/21/23 20:15:24.538
    STEP: updating /approval 04/21/23 20:15:24.541
    STEP: getting /status 04/21/23 20:15:24.546
    STEP: patching /status 04/21/23 20:15:24.547
    STEP: updating /status 04/21/23 20:15:24.552
    STEP: deleting 04/21/23 20:15:24.561
    STEP: deleting a collection 04/21/23 20:15:24.577
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:15:24.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-9620" for this suite. 04/21/23 20:15:24.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:15:24.645
Apr 21 20:15:24.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename init-container 04/21/23 20:15:24.646
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:24.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:24.654
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 04/21/23 20:15:24.655
Apr 21 20:15:24.655: INFO: PodSpec: initContainers in spec.initContainers
Apr 21 20:16:08.904: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-da763b6d-c1bf-4d47-a99b-1686b0d5592f", GenerateName:"", Namespace:"init-container-9097", SelfLink:"", UID:"d431a74a-b538-4c0b-9999-9dc71e978c63", ResourceVersion:"1143", Generation:0, CreationTimestamp:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"655801501"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012b0618), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 21, 20, 16, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012b0648), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-klf5r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00128b1a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-klf5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-klf5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-klf5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00038c9d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8sconformance-m02", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00021cd90), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00038cc70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00038cd60)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00038cd68), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00038cd6c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012c6140), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.49.3", PodIP:"10.244.1.13", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.13"}}, StartTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00021ce70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00021cee0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"docker-pullable://registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"docker://953fd1de5b59e044c82ea46e700e330a1ff8cd37d0d3d1efba4f32e14e0eb2bd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00128b220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00128b200), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00038d0af)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:16:08.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-9097" for this suite. 04/21/23 20:16:08.907
------------------------------
â€¢ [SLOW TEST] [44.266 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:15:24.645
    Apr 21 20:15:24.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename init-container 04/21/23 20:15:24.646
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:15:24.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:15:24.654
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 04/21/23 20:15:24.655
    Apr 21 20:15:24.655: INFO: PodSpec: initContainers in spec.initContainers
    Apr 21 20:16:08.904: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-da763b6d-c1bf-4d47-a99b-1686b0d5592f", GenerateName:"", Namespace:"init-container-9097", SelfLink:"", UID:"d431a74a-b538-4c0b-9999-9dc71e978c63", ResourceVersion:"1143", Generation:0, CreationTimestamp:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"655801501"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012b0618), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 21, 20, 16, 8, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0012b0648), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-klf5r", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00128b1a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-klf5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-klf5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-klf5r", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00038c9d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8sconformance-m02", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00021cd90), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00038cc70)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00038cd60)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00038cd68), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00038cd6c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0012c6140), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.49.3", PodIP:"10.244.1.13", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.13"}}, StartTime:time.Date(2023, time.April, 21, 20, 15, 24, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00021ce70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00021cee0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"docker-pullable://registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"docker://953fd1de5b59e044c82ea46e700e330a1ff8cd37d0d3d1efba4f32e14e0eb2bd", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00128b220), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00128b200), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00038d0af)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:16:08.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-9097" for this suite. 04/21/23 20:16:08.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:16:08.912
Apr 21 20:16:08.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replication-controller 04/21/23 20:16:08.912
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:08.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:08.922
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65 04/21/23 20:16:08.924
Apr 21 20:16:08.929: INFO: Pod name my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65: Found 0 pods out of 1
Apr 21 20:16:13.931: INFO: Pod name my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65: Found 1 pods out of 1
Apr 21 20:16:13.931: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65" are running
Apr 21 20:16:13.931: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s" in namespace "replication-controller-6122" to be "running"
Apr 21 20:16:13.933: INFO: Pod "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s": Phase="Running", Reason="", readiness=true. Elapsed: 1.652954ms
Apr 21 20:16:13.933: INFO: Pod "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s" satisfied condition "running"
Apr 21 20:16:13.933: INFO: Pod "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 20:16:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 20:16:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 20:16:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 20:16:08 +0000 UTC Reason: Message:}])
Apr 21 20:16:13.933: INFO: Trying to dial the pod
Apr 21 20:16:18.940: INFO: Controller my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65: Got expected result from replica 1 [my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s]: "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 21 20:16:18.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6122" for this suite. 04/21/23 20:16:18.942
------------------------------
â€¢ [SLOW TEST] [10.034 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:16:08.912
    Apr 21 20:16:08.912: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replication-controller 04/21/23 20:16:08.912
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:08.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:08.922
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65 04/21/23 20:16:08.924
    Apr 21 20:16:08.929: INFO: Pod name my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65: Found 0 pods out of 1
    Apr 21 20:16:13.931: INFO: Pod name my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65: Found 1 pods out of 1
    Apr 21 20:16:13.931: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65" are running
    Apr 21 20:16:13.931: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s" in namespace "replication-controller-6122" to be "running"
    Apr 21 20:16:13.933: INFO: Pod "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s": Phase="Running", Reason="", readiness=true. Elapsed: 1.652954ms
    Apr 21 20:16:13.933: INFO: Pod "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s" satisfied condition "running"
    Apr 21 20:16:13.933: INFO: Pod "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 20:16:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 20:16:09 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 20:16:09 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 20:16:08 +0000 UTC Reason: Message:}])
    Apr 21 20:16:13.933: INFO: Trying to dial the pod
    Apr 21 20:16:18.940: INFO: Controller my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65: Got expected result from replica 1 [my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s]: "my-hostname-basic-a8eca27b-a9ac-42a6-8059-ad4b5edfea65-fsl4s", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:16:18.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6122" for this suite. 04/21/23 20:16:18.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:16:18.946
Apr 21 20:16:18.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 20:16:18.947
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:18.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:18.957
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 04/21/23 20:16:18.958
Apr 21 20:16:18.964: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac" in namespace "emptydir-8116" to be "running"
Apr 21 20:16:18.965: INFO: Pod "pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46539ms
Apr 21 20:16:20.968: INFO: Pod "pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac": Phase="Running", Reason="", readiness=false. Elapsed: 2.004650561s
Apr 21 20:16:20.968: INFO: Pod "pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac" satisfied condition "running"
STEP: Reading file content from the nginx-container 04/21/23 20:16:20.968
Apr 21 20:16:20.969: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8116 PodName:pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:16:20.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:16:20.969: INFO: ExecWithOptions: Clientset creation
Apr 21 20:16:20.969: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8116/pods/pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Apr 21 20:16:21.044: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 20:16:21.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8116" for this suite. 04/21/23 20:16:21.047
------------------------------
â€¢ [2.104 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:16:18.946
    Apr 21 20:16:18.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 20:16:18.947
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:18.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:18.957
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 04/21/23 20:16:18.958
    Apr 21 20:16:18.964: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac" in namespace "emptydir-8116" to be "running"
    Apr 21 20:16:18.965: INFO: Pod "pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46539ms
    Apr 21 20:16:20.968: INFO: Pod "pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac": Phase="Running", Reason="", readiness=false. Elapsed: 2.004650561s
    Apr 21 20:16:20.968: INFO: Pod "pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac" satisfied condition "running"
    STEP: Reading file content from the nginx-container 04/21/23 20:16:20.968
    Apr 21 20:16:20.969: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8116 PodName:pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:16:20.969: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:16:20.969: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:16:20.969: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8116/pods/pod-sharedvolume-8c11d203-9512-4e27-ae9c-b6b5e95eadac/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Apr 21 20:16:21.044: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:16:21.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8116" for this suite. 04/21/23 20:16:21.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:16:21.051
Apr 21 20:16:21.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-runtime 04/21/23 20:16:21.052
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:21.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:21.062
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 04/21/23 20:16:21.063
STEP: wait for the container to reach Failed 04/21/23 20:16:21.067
STEP: get the container status 04/21/23 20:16:24.076
STEP: the container should be terminated 04/21/23 20:16:24.078
STEP: the termination message should be set 04/21/23 20:16:24.078
Apr 21 20:16:24.078: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 04/21/23 20:16:24.078
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 21 20:16:24.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7613" for this suite. 04/21/23 20:16:24.087
------------------------------
â€¢ [3.039 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:16:21.051
    Apr 21 20:16:21.051: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-runtime 04/21/23 20:16:21.052
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:21.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:21.062
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 04/21/23 20:16:21.063
    STEP: wait for the container to reach Failed 04/21/23 20:16:21.067
    STEP: get the container status 04/21/23 20:16:24.076
    STEP: the container should be terminated 04/21/23 20:16:24.078
    STEP: the termination message should be set 04/21/23 20:16:24.078
    Apr 21 20:16:24.078: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 04/21/23 20:16:24.078
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:16:24.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7613" for this suite. 04/21/23 20:16:24.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:16:24.091
Apr 21 20:16:24.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:16:24.091
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:24.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:24.099
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:16:24.108
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:16:24.588
STEP: Deploying the webhook pod 04/21/23 20:16:24.592
STEP: Wait for the deployment to be ready 04/21/23 20:16:24.599
Apr 21 20:16:24.605: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 20:16:26.611
STEP: Verifying the service has paired with the endpoint 04/21/23 20:16:26.619
Apr 21 20:16:27.619: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Apr 21 20:16:27.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7333-crds.webhook.example.com via the AdmissionRegistration API 04/21/23 20:16:28.131
STEP: Creating a custom resource while v1 is storage version 04/21/23 20:16:28.144
STEP: Patching Custom Resource Definition to set v2 as storage 04/21/23 20:16:30.181
STEP: Patching the custom resource while v2 is storage version 04/21/23 20:16:30.193
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:16:30.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-730" for this suite. 04/21/23 20:16:30.762
STEP: Destroying namespace "webhook-730-markers" for this suite. 04/21/23 20:16:30.767
------------------------------
â€¢ [SLOW TEST] [6.681 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:16:24.091
    Apr 21 20:16:24.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:16:24.091
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:24.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:24.099
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:16:24.108
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:16:24.588
    STEP: Deploying the webhook pod 04/21/23 20:16:24.592
    STEP: Wait for the deployment to be ready 04/21/23 20:16:24.599
    Apr 21 20:16:24.605: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 20:16:26.611
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:16:26.619
    Apr 21 20:16:27.619: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Apr 21 20:16:27.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7333-crds.webhook.example.com via the AdmissionRegistration API 04/21/23 20:16:28.131
    STEP: Creating a custom resource while v1 is storage version 04/21/23 20:16:28.144
    STEP: Patching Custom Resource Definition to set v2 as storage 04/21/23 20:16:30.181
    STEP: Patching the custom resource while v2 is storage version 04/21/23 20:16:30.193
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:16:30.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-730" for this suite. 04/21/23 20:16:30.762
    STEP: Destroying namespace "webhook-730-markers" for this suite. 04/21/23 20:16:30.767
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:16:30.772
Apr 21 20:16:30.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 20:16:30.774
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:30.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:30.784
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 04/21/23 20:16:30.786
Apr 21 20:16:30.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: mark a version not serverd 04/21/23 20:16:34.037
STEP: check the unserved version gets removed 04/21/23 20:16:34.049
STEP: check the other version is not changed 04/21/23 20:16:34.839
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:16:37.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-4897" for this suite. 04/21/23 20:16:37.443
------------------------------
â€¢ [SLOW TEST] [6.675 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:16:30.772
    Apr 21 20:16:30.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 20:16:30.774
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:30.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:30.784
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 04/21/23 20:16:30.786
    Apr 21 20:16:30.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: mark a version not serverd 04/21/23 20:16:34.037
    STEP: check the unserved version gets removed 04/21/23 20:16:34.049
    STEP: check the other version is not changed 04/21/23 20:16:34.839
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:16:37.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-4897" for this suite. 04/21/23 20:16:37.443
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:16:37.448
Apr 21 20:16:37.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename namespaces 04/21/23 20:16:37.449
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:37.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:37.456
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 04/21/23 20:16:37.458
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:37.466
STEP: Creating a pod in the namespace 04/21/23 20:16:37.467
STEP: Waiting for the pod to have running status 04/21/23 20:16:37.471
Apr 21 20:16:37.471: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7323" to be "running"
Apr 21 20:16:37.473: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.292344ms
Apr 21 20:16:39.475: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003556439s
Apr 21 20:16:39.475: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 04/21/23 20:16:39.475
STEP: Waiting for the namespace to be removed. 04/21/23 20:16:39.478
STEP: Recreating the namespace 04/21/23 20:16:50.481
STEP: Verifying there are no pods in the namespace 04/21/23 20:16:50.49
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:16:50.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6725" for this suite. 04/21/23 20:16:50.493
STEP: Destroying namespace "nsdeletetest-7323" for this suite. 04/21/23 20:16:50.496
Apr 21 20:16:50.497: INFO: Namespace nsdeletetest-7323 was already deleted
STEP: Destroying namespace "nsdeletetest-5189" for this suite. 04/21/23 20:16:50.497
------------------------------
â€¢ [SLOW TEST] [13.053 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:16:37.448
    Apr 21 20:16:37.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename namespaces 04/21/23 20:16:37.449
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:37.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:37.456
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 04/21/23 20:16:37.458
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:37.466
    STEP: Creating a pod in the namespace 04/21/23 20:16:37.467
    STEP: Waiting for the pod to have running status 04/21/23 20:16:37.471
    Apr 21 20:16:37.471: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-7323" to be "running"
    Apr 21 20:16:37.473: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.292344ms
    Apr 21 20:16:39.475: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003556439s
    Apr 21 20:16:39.475: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 04/21/23 20:16:39.475
    STEP: Waiting for the namespace to be removed. 04/21/23 20:16:39.478
    STEP: Recreating the namespace 04/21/23 20:16:50.481
    STEP: Verifying there are no pods in the namespace 04/21/23 20:16:50.49
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:16:50.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6725" for this suite. 04/21/23 20:16:50.493
    STEP: Destroying namespace "nsdeletetest-7323" for this suite. 04/21/23 20:16:50.496
    Apr 21 20:16:50.497: INFO: Namespace nsdeletetest-7323 was already deleted
    STEP: Destroying namespace "nsdeletetest-5189" for this suite. 04/21/23 20:16:50.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:16:50.501
Apr 21 20:16:50.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-probe 04/21/23 20:16:50.502
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:50.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:50.509
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-f9aa62e5-f646-466d-9c21-434700275485 in namespace container-probe-9210 04/21/23 20:16:50.511
Apr 21 20:16:50.516: INFO: Waiting up to 5m0s for pod "liveness-f9aa62e5-f646-466d-9c21-434700275485" in namespace "container-probe-9210" to be "not pending"
Apr 21 20:16:50.517: INFO: Pod "liveness-f9aa62e5-f646-466d-9c21-434700275485": Phase="Pending", Reason="", readiness=false. Elapsed: 1.434382ms
Apr 21 20:16:52.520: INFO: Pod "liveness-f9aa62e5-f646-466d-9c21-434700275485": Phase="Running", Reason="", readiness=true. Elapsed: 2.003745999s
Apr 21 20:16:52.520: INFO: Pod "liveness-f9aa62e5-f646-466d-9c21-434700275485" satisfied condition "not pending"
Apr 21 20:16:52.520: INFO: Started pod liveness-f9aa62e5-f646-466d-9c21-434700275485 in namespace container-probe-9210
STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 20:16:52.52
Apr 21 20:16:52.522: INFO: Initial restart count of pod liveness-f9aa62e5-f646-466d-9c21-434700275485 is 0
Apr 21 20:17:12.551: INFO: Restart count of pod container-probe-9210/liveness-f9aa62e5-f646-466d-9c21-434700275485 is now 1 (20.02960754s elapsed)
STEP: deleting the pod 04/21/23 20:17:12.551
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 21 20:17:12.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9210" for this suite. 04/21/23 20:17:12.559
------------------------------
â€¢ [SLOW TEST] [22.062 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:16:50.501
    Apr 21 20:16:50.502: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-probe 04/21/23 20:16:50.502
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:16:50.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:16:50.509
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-f9aa62e5-f646-466d-9c21-434700275485 in namespace container-probe-9210 04/21/23 20:16:50.511
    Apr 21 20:16:50.516: INFO: Waiting up to 5m0s for pod "liveness-f9aa62e5-f646-466d-9c21-434700275485" in namespace "container-probe-9210" to be "not pending"
    Apr 21 20:16:50.517: INFO: Pod "liveness-f9aa62e5-f646-466d-9c21-434700275485": Phase="Pending", Reason="", readiness=false. Elapsed: 1.434382ms
    Apr 21 20:16:52.520: INFO: Pod "liveness-f9aa62e5-f646-466d-9c21-434700275485": Phase="Running", Reason="", readiness=true. Elapsed: 2.003745999s
    Apr 21 20:16:52.520: INFO: Pod "liveness-f9aa62e5-f646-466d-9c21-434700275485" satisfied condition "not pending"
    Apr 21 20:16:52.520: INFO: Started pod liveness-f9aa62e5-f646-466d-9c21-434700275485 in namespace container-probe-9210
    STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 20:16:52.52
    Apr 21 20:16:52.522: INFO: Initial restart count of pod liveness-f9aa62e5-f646-466d-9c21-434700275485 is 0
    Apr 21 20:17:12.551: INFO: Restart count of pod container-probe-9210/liveness-f9aa62e5-f646-466d-9c21-434700275485 is now 1 (20.02960754s elapsed)
    STEP: deleting the pod 04/21/23 20:17:12.551
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:17:12.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9210" for this suite. 04/21/23 20:17:12.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:17:12.565
Apr 21 20:17:12.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:17:12.566
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:12.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:12.575
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 04/21/23 20:17:12.576
Apr 21 20:17:12.582: INFO: Waiting up to 5m0s for pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c" in namespace "projected-1362" to be "running and ready"
Apr 21 20:17:12.583: INFO: Pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.515915ms
Apr 21 20:17:12.583: INFO: The phase of Pod labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:17:14.586: INFO: Pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.003730378s
Apr 21 20:17:14.586: INFO: The phase of Pod labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c is Running (Ready = true)
Apr 21 20:17:14.586: INFO: Pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c" satisfied condition "running and ready"
Apr 21 20:17:15.106: INFO: Successfully updated pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 20:17:19.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1362" for this suite. 04/21/23 20:17:19.126
------------------------------
â€¢ [SLOW TEST] [6.565 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:17:12.565
    Apr 21 20:17:12.565: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:17:12.566
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:12.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:12.575
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 04/21/23 20:17:12.576
    Apr 21 20:17:12.582: INFO: Waiting up to 5m0s for pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c" in namespace "projected-1362" to be "running and ready"
    Apr 21 20:17:12.583: INFO: Pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.515915ms
    Apr 21 20:17:12.583: INFO: The phase of Pod labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:17:14.586: INFO: Pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.003730378s
    Apr 21 20:17:14.586: INFO: The phase of Pod labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c is Running (Ready = true)
    Apr 21 20:17:14.586: INFO: Pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c" satisfied condition "running and ready"
    Apr 21 20:17:15.106: INFO: Successfully updated pod "labelsupdatef94adc54-786b-4a19-90e4-4db5bf001c5c"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:17:19.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1362" for this suite. 04/21/23 20:17:19.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:17:19.131
Apr 21 20:17:19.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename statefulset 04/21/23 20:17:19.132
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:19.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:19.141
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4741 04/21/23 20:17:19.143
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-4741 04/21/23 20:17:19.147
Apr 21 20:17:19.152: INFO: Found 0 stateful pods, waiting for 1
Apr 21 20:17:29.155: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 04/21/23 20:17:29.158
STEP: Getting /status 04/21/23 20:17:29.165
Apr 21 20:17:29.168: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 04/21/23 20:17:29.168
Apr 21 20:17:29.175: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 04/21/23 20:17:29.175
Apr 21 20:17:29.176: INFO: Observed &StatefulSet event: ADDED
Apr 21 20:17:29.176: INFO: Found Statefulset ss in namespace statefulset-4741 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 21 20:17:29.176: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 04/21/23 20:17:29.176
Apr 21 20:17:29.176: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 21 20:17:29.181: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 04/21/23 20:17:29.181
Apr 21 20:17:29.182: INFO: Observed &StatefulSet event: ADDED
Apr 21 20:17:29.182: INFO: Observed Statefulset ss in namespace statefulset-4741 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 21 20:17:29.182: INFO: Observed &StatefulSet event: MODIFIED
Apr 21 20:17:29.182: INFO: Found Statefulset ss in namespace statefulset-4741 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 21 20:17:29.182: INFO: Deleting all statefulset in ns statefulset-4741
Apr 21 20:17:29.183: INFO: Scaling statefulset ss to 0
Apr 21 20:17:39.198: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 20:17:39.199: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:17:39.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4741" for this suite. 04/21/23 20:17:39.208
------------------------------
â€¢ [SLOW TEST] [20.080 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:17:19.131
    Apr 21 20:17:19.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename statefulset 04/21/23 20:17:19.132
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:19.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:19.141
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4741 04/21/23 20:17:19.143
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-4741 04/21/23 20:17:19.147
    Apr 21 20:17:19.152: INFO: Found 0 stateful pods, waiting for 1
    Apr 21 20:17:29.155: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 04/21/23 20:17:29.158
    STEP: Getting /status 04/21/23 20:17:29.165
    Apr 21 20:17:29.168: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 04/21/23 20:17:29.168
    Apr 21 20:17:29.175: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 04/21/23 20:17:29.175
    Apr 21 20:17:29.176: INFO: Observed &StatefulSet event: ADDED
    Apr 21 20:17:29.176: INFO: Found Statefulset ss in namespace statefulset-4741 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 21 20:17:29.176: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 04/21/23 20:17:29.176
    Apr 21 20:17:29.176: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr 21 20:17:29.181: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 04/21/23 20:17:29.181
    Apr 21 20:17:29.182: INFO: Observed &StatefulSet event: ADDED
    Apr 21 20:17:29.182: INFO: Observed Statefulset ss in namespace statefulset-4741 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 21 20:17:29.182: INFO: Observed &StatefulSet event: MODIFIED
    Apr 21 20:17:29.182: INFO: Found Statefulset ss in namespace statefulset-4741 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 21 20:17:29.182: INFO: Deleting all statefulset in ns statefulset-4741
    Apr 21 20:17:29.183: INFO: Scaling statefulset ss to 0
    Apr 21 20:17:39.198: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 20:17:39.199: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:17:39.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4741" for this suite. 04/21/23 20:17:39.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:17:39.213
Apr 21 20:17:39.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:17:39.213
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:39.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:39.223
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:17:43.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8723" for this suite. 04/21/23 20:17:43.236
------------------------------
â€¢ [4.027 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:17:39.213
    Apr 21 20:17:39.213: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:17:39.213
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:39.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:39.223
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:17:43.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8723" for this suite. 04/21/23 20:17:43.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:17:43.241
Apr 21 20:17:43.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 20:17:43.241
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:43.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:43.25
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-149 04/21/23 20:17:43.252
STEP: creating service affinity-nodeport in namespace services-149 04/21/23 20:17:43.252
STEP: creating replication controller affinity-nodeport in namespace services-149 04/21/23 20:17:43.26
I0421 20:17:43.265505      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-149, replica count: 3
I0421 20:17:46.316603      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0421 20:17:49.316957      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 21 20:17:49.323: INFO: Creating new exec pod
Apr 21 20:17:49.326: INFO: Waiting up to 5m0s for pod "execpod-affinityq48kd" in namespace "services-149" to be "running"
Apr 21 20:17:49.328: INFO: Pod "execpod-affinityq48kd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577563ms
Apr 21 20:17:51.330: INFO: Pod "execpod-affinityq48kd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00392666s
Apr 21 20:17:51.330: INFO: Pod "execpod-affinityq48kd" satisfied condition "running"
Apr 21 20:17:52.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Apr 21 20:17:52.472: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr 21 20:17:52.472: INFO: stdout: ""
Apr 21 20:17:52.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c nc -v -z -w 2 10.97.191.151 80'
Apr 21 20:17:52.596: INFO: stderr: "+ nc -v -z -w 2 10.97.191.151 80\nConnection to 10.97.191.151 80 port [tcp/http] succeeded!\n"
Apr 21 20:17:52.596: INFO: stdout: ""
Apr 21 20:17:52.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c nc -v -z -w 2 192.168.49.2 31337'
Apr 21 20:17:52.723: INFO: stderr: "+ nc -v -z -w 2 192.168.49.2 31337\nConnection to 192.168.49.2 31337 port [tcp/*] succeeded!\n"
Apr 21 20:17:52.723: INFO: stdout: ""
Apr 21 20:17:52.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c nc -v -z -w 2 192.168.49.3 31337'
Apr 21 20:17:52.838: INFO: stderr: "+ nc -v -z -w 2 192.168.49.3 31337\nConnection to 192.168.49.3 31337 port [tcp/*] succeeded!\n"
Apr 21 20:17:52.838: INFO: stdout: ""
Apr 21 20:17:52.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:31337/ ; done'
Apr 21 20:17:53.006: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n"
Apr 21 20:17:53.006: INFO: stdout: "\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z"
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
Apr 21 20:17:53.006: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-149, will wait for the garbage collector to delete the pods 04/21/23 20:17:53.015
Apr 21 20:17:53.077: INFO: Deleting ReplicationController affinity-nodeport took: 10.193693ms
Apr 21 20:17:53.177: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.340361ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 20:17:55.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-149" for this suite. 04/21/23 20:17:55.196
------------------------------
â€¢ [SLOW TEST] [11.958 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:17:43.241
    Apr 21 20:17:43.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 20:17:43.241
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:43.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:43.25
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-149 04/21/23 20:17:43.252
    STEP: creating service affinity-nodeport in namespace services-149 04/21/23 20:17:43.252
    STEP: creating replication controller affinity-nodeport in namespace services-149 04/21/23 20:17:43.26
    I0421 20:17:43.265505      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-149, replica count: 3
    I0421 20:17:46.316603      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0421 20:17:49.316957      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 21 20:17:49.323: INFO: Creating new exec pod
    Apr 21 20:17:49.326: INFO: Waiting up to 5m0s for pod "execpod-affinityq48kd" in namespace "services-149" to be "running"
    Apr 21 20:17:49.328: INFO: Pod "execpod-affinityq48kd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.577563ms
    Apr 21 20:17:51.330: INFO: Pod "execpod-affinityq48kd": Phase="Running", Reason="", readiness=true. Elapsed: 2.00392666s
    Apr 21 20:17:51.330: INFO: Pod "execpod-affinityq48kd" satisfied condition "running"
    Apr 21 20:17:52.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Apr 21 20:17:52.472: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Apr 21 20:17:52.472: INFO: stdout: ""
    Apr 21 20:17:52.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c nc -v -z -w 2 10.97.191.151 80'
    Apr 21 20:17:52.596: INFO: stderr: "+ nc -v -z -w 2 10.97.191.151 80\nConnection to 10.97.191.151 80 port [tcp/http] succeeded!\n"
    Apr 21 20:17:52.596: INFO: stdout: ""
    Apr 21 20:17:52.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c nc -v -z -w 2 192.168.49.2 31337'
    Apr 21 20:17:52.723: INFO: stderr: "+ nc -v -z -w 2 192.168.49.2 31337\nConnection to 192.168.49.2 31337 port [tcp/*] succeeded!\n"
    Apr 21 20:17:52.723: INFO: stdout: ""
    Apr 21 20:17:52.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c nc -v -z -w 2 192.168.49.3 31337'
    Apr 21 20:17:52.838: INFO: stderr: "+ nc -v -z -w 2 192.168.49.3 31337\nConnection to 192.168.49.3 31337 port [tcp/*] succeeded!\n"
    Apr 21 20:17:52.838: INFO: stdout: ""
    Apr 21 20:17:52.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-149 exec execpod-affinityq48kd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:31337/ ; done'
    Apr 21 20:17:53.006: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31337/\n"
    Apr 21 20:17:53.006: INFO: stdout: "\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z\naffinity-nodeport-kp29z"
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Received response from host: affinity-nodeport-kp29z
    Apr 21 20:17:53.006: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-149, will wait for the garbage collector to delete the pods 04/21/23 20:17:53.015
    Apr 21 20:17:53.077: INFO: Deleting ReplicationController affinity-nodeport took: 10.193693ms
    Apr 21 20:17:53.177: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.340361ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:17:55.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-149" for this suite. 04/21/23 20:17:55.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:17:55.201
Apr 21 20:17:55.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename daemonsets 04/21/23 20:17:55.201
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:55.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:55.21
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 04/21/23 20:17:55.22
STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 20:17:55.223
Apr 21 20:17:55.227: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 20:17:55.227: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 20:17:56.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 20:17:56.231: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 20:17:57.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 21 20:17:57.231: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 04/21/23 20:17:57.233
Apr 21 20:17:57.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 20:17:57.245: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 20:17:58.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 21 20:17:58.251: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 04/21/23 20:17:58.251
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/21/23 20:17:58.254
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9582, will wait for the garbage collector to delete the pods 04/21/23 20:17:58.254
Apr 21 20:17:58.310: INFO: Deleting DaemonSet.extensions daemon-set took: 3.495981ms
Apr 21 20:17:58.410: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.514353ms
Apr 21 20:18:01.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 20:18:01.212: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 21 20:18:01.214: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1801"},"items":null}

Apr 21 20:18:01.216: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1801"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:18:01.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9582" for this suite. 04/21/23 20:18:01.222
------------------------------
â€¢ [SLOW TEST] [6.025 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:17:55.201
    Apr 21 20:17:55.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename daemonsets 04/21/23 20:17:55.201
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:17:55.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:17:55.21
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 04/21/23 20:17:55.22
    STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 20:17:55.223
    Apr 21 20:17:55.227: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 20:17:55.227: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 20:17:56.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 20:17:56.231: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 20:17:57.231: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 21 20:17:57.231: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 04/21/23 20:17:57.233
    Apr 21 20:17:57.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 20:17:57.245: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 20:17:58.251: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 21 20:17:58.251: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 04/21/23 20:17:58.251
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/21/23 20:17:58.254
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9582, will wait for the garbage collector to delete the pods 04/21/23 20:17:58.254
    Apr 21 20:17:58.310: INFO: Deleting DaemonSet.extensions daemon-set took: 3.495981ms
    Apr 21 20:17:58.410: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.514353ms
    Apr 21 20:18:01.212: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 20:18:01.212: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 21 20:18:01.214: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1801"},"items":null}

    Apr 21 20:18:01.216: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1801"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:18:01.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9582" for this suite. 04/21/23 20:18:01.222
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:18:01.226
Apr 21 20:18:01.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename security-context-test 04/21/23 20:18:01.226
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:01.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:01.236
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Apr 21 20:18:01.243: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f" in namespace "security-context-test-6409" to be "Succeeded or Failed"
Apr 21 20:18:01.245: INFO: Pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.561455ms
Apr 21 20:18:03.247: INFO: Pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00411234s
Apr 21 20:18:05.247: INFO: Pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004050026s
Apr 21 20:18:05.247: INFO: Pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 21 20:18:05.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6409" for this suite. 04/21/23 20:18:05.254
------------------------------
â€¢ [4.033 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:18:01.226
    Apr 21 20:18:01.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename security-context-test 04/21/23 20:18:01.226
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:01.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:01.236
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Apr 21 20:18:01.243: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f" in namespace "security-context-test-6409" to be "Succeeded or Failed"
    Apr 21 20:18:01.245: INFO: Pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.561455ms
    Apr 21 20:18:03.247: INFO: Pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00411234s
    Apr 21 20:18:05.247: INFO: Pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004050026s
    Apr 21 20:18:05.247: INFO: Pod "alpine-nnp-false-71c33518-f4ee-4e56-8ed9-8aa035c2c17f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:18:05.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6409" for this suite. 04/21/23 20:18:05.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:18:05.259
Apr 21 20:18:05.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename endpointslice 04/21/23 20:18:05.26
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:05.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:05.268
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 21 20:18:07.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-8812" for this suite. 04/21/23 20:18:07.305
------------------------------
â€¢ [2.049 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:18:05.259
    Apr 21 20:18:05.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename endpointslice 04/21/23 20:18:05.26
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:05.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:05.268
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:18:07.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-8812" for this suite. 04/21/23 20:18:07.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:18:07.309
Apr 21 20:18:07.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 20:18:07.31
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:07.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:07.32
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-2ad1b57d-492a-4d64-a416-752a5c284fb3 04/21/23 20:18:07.322
STEP: Creating a pod to test consume configMaps 04/21/23 20:18:07.325
Apr 21 20:18:07.331: INFO: Waiting up to 5m0s for pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba" in namespace "configmap-2838" to be "Succeeded or Failed"
Apr 21 20:18:07.332: INFO: Pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.725867ms
Apr 21 20:18:09.334: INFO: Pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003854907s
Apr 21 20:18:11.335: INFO: Pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004631588s
STEP: Saw pod success 04/21/23 20:18:11.335
Apr 21 20:18:11.335: INFO: Pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba" satisfied condition "Succeeded or Failed"
Apr 21 20:18:11.337: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba container agnhost-container: <nil>
STEP: delete the pod 04/21/23 20:18:11.342
Apr 21 20:18:11.347: INFO: Waiting for pod pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba to disappear
Apr 21 20:18:11.349: INFO: Pod pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:18:11.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2838" for this suite. 04/21/23 20:18:11.351
------------------------------
â€¢ [4.045 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:18:07.309
    Apr 21 20:18:07.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 20:18:07.31
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:07.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:07.32
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-2ad1b57d-492a-4d64-a416-752a5c284fb3 04/21/23 20:18:07.322
    STEP: Creating a pod to test consume configMaps 04/21/23 20:18:07.325
    Apr 21 20:18:07.331: INFO: Waiting up to 5m0s for pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba" in namespace "configmap-2838" to be "Succeeded or Failed"
    Apr 21 20:18:07.332: INFO: Pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba": Phase="Pending", Reason="", readiness=false. Elapsed: 1.725867ms
    Apr 21 20:18:09.334: INFO: Pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003854907s
    Apr 21 20:18:11.335: INFO: Pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004631588s
    STEP: Saw pod success 04/21/23 20:18:11.335
    Apr 21 20:18:11.335: INFO: Pod "pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba" satisfied condition "Succeeded or Failed"
    Apr 21 20:18:11.337: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 20:18:11.342
    Apr 21 20:18:11.347: INFO: Waiting for pod pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba to disappear
    Apr 21 20:18:11.349: INFO: Pod pod-configmaps-908d1d49-20e6-4992-890d-e245916771ba no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:18:11.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2838" for this suite. 04/21/23 20:18:11.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:18:11.356
Apr 21 20:18:11.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 20:18:11.356
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:11.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:11.364
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 04/21/23 20:18:11.365
Apr 21 20:18:11.370: INFO: Waiting up to 5m0s for pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8" in namespace "downward-api-7570" to be "running and ready"
Apr 21 20:18:11.372: INFO: Pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.376458ms
Apr 21 20:18:11.372: INFO: The phase of Pod annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:18:13.374: INFO: Pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.003749896s
Apr 21 20:18:13.374: INFO: The phase of Pod annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8 is Running (Ready = true)
Apr 21 20:18:13.374: INFO: Pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8" satisfied condition "running and ready"
Apr 21 20:18:13.889: INFO: Successfully updated pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 20:18:17.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7570" for this suite. 04/21/23 20:18:17.909
------------------------------
â€¢ [SLOW TEST] [6.557 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:18:11.356
    Apr 21 20:18:11.356: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 20:18:11.356
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:11.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:11.364
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 04/21/23 20:18:11.365
    Apr 21 20:18:11.370: INFO: Waiting up to 5m0s for pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8" in namespace "downward-api-7570" to be "running and ready"
    Apr 21 20:18:11.372: INFO: Pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.376458ms
    Apr 21 20:18:11.372: INFO: The phase of Pod annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:18:13.374: INFO: Pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.003749896s
    Apr 21 20:18:13.374: INFO: The phase of Pod annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8 is Running (Ready = true)
    Apr 21 20:18:13.374: INFO: Pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8" satisfied condition "running and ready"
    Apr 21 20:18:13.889: INFO: Successfully updated pod "annotationupdate49e2e29d-ed7d-4b02-9ad4-29d5e023e1e8"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:18:17.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7570" for this suite. 04/21/23 20:18:17.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:18:17.914
Apr 21 20:18:17.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename cronjob 04/21/23 20:18:17.915
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:17.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:17.925
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 04/21/23 20:18:17.927
STEP: Ensuring a job is scheduled 04/21/23 20:18:17.93
STEP: Ensuring exactly one is scheduled 04/21/23 20:19:01.934
STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/21/23 20:19:01.936
STEP: Ensuring no more jobs are scheduled 04/21/23 20:19:01.938
STEP: Removing cronjob 04/21/23 20:24:01.942
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:01.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5602" for this suite. 04/21/23 20:24:01.948
------------------------------
â€¢ [SLOW TEST] [344.039 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:18:17.914
    Apr 21 20:18:17.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename cronjob 04/21/23 20:18:17.915
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:18:17.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:18:17.925
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 04/21/23 20:18:17.927
    STEP: Ensuring a job is scheduled 04/21/23 20:18:17.93
    STEP: Ensuring exactly one is scheduled 04/21/23 20:19:01.934
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/21/23 20:19:01.936
    STEP: Ensuring no more jobs are scheduled 04/21/23 20:19:01.938
    STEP: Removing cronjob 04/21/23 20:24:01.942
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:01.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5602" for this suite. 04/21/23 20:24:01.948
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:01.953
Apr 21 20:24:01.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 20:24:01.954
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:01.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:01.963
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/21/23 20:24:01.965
Apr 21 20:24:01.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1369 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr 21 20:24:02.023: INFO: stderr: ""
Apr 21 20:24:02.023: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 04/21/23 20:24:02.023
Apr 21 20:24:02.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1369 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Apr 21 20:24:02.468: INFO: stderr: ""
Apr 21 20:24:02.468: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/21/23 20:24:02.468
Apr 21 20:24:02.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1369 delete pods e2e-test-httpd-pod'
Apr 21 20:24:04.232: INFO: stderr: ""
Apr 21 20:24:04.232: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:04.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1369" for this suite. 04/21/23 20:24:04.234
------------------------------
â€¢ [2.284 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:01.953
    Apr 21 20:24:01.953: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 20:24:01.954
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:01.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:01.963
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/21/23 20:24:01.965
    Apr 21 20:24:01.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1369 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Apr 21 20:24:02.023: INFO: stderr: ""
    Apr 21 20:24:02.023: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 04/21/23 20:24:02.023
    Apr 21 20:24:02.023: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1369 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Apr 21 20:24:02.468: INFO: stderr: ""
    Apr 21 20:24:02.468: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/21/23 20:24:02.468
    Apr 21 20:24:02.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1369 delete pods e2e-test-httpd-pod'
    Apr 21 20:24:04.232: INFO: stderr: ""
    Apr 21 20:24:04.232: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:04.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1369" for this suite. 04/21/23 20:24:04.234
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:04.238
Apr 21 20:24:04.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename podtemplate 04/21/23 20:24:04.239
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:04.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:04.248
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:04.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-860" for this suite. 04/21/23 20:24:04.267
------------------------------
â€¢ [0.032 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:04.238
    Apr 21 20:24:04.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename podtemplate 04/21/23 20:24:04.239
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:04.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:04.248
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:04.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-860" for this suite. 04/21/23 20:24:04.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:04.27
Apr 21 20:24:04.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-runtime 04/21/23 20:24:04.271
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:04.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:04.278
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 04/21/23 20:24:04.28
STEP: wait for the container to reach Succeeded 04/21/23 20:24:04.284
STEP: get the container status 04/21/23 20:24:07.294
STEP: the container should be terminated 04/21/23 20:24:07.295
STEP: the termination message should be set 04/21/23 20:24:07.296
Apr 21 20:24:07.296: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 04/21/23 20:24:07.296
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:07.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7450" for this suite. 04/21/23 20:24:07.305
------------------------------
â€¢ [3.038 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:04.27
    Apr 21 20:24:04.270: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-runtime 04/21/23 20:24:04.271
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:04.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:04.278
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 04/21/23 20:24:04.28
    STEP: wait for the container to reach Succeeded 04/21/23 20:24:04.284
    STEP: get the container status 04/21/23 20:24:07.294
    STEP: the container should be terminated 04/21/23 20:24:07.295
    STEP: the termination message should be set 04/21/23 20:24:07.296
    Apr 21 20:24:07.296: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 04/21/23 20:24:07.296
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:07.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7450" for this suite. 04/21/23 20:24:07.305
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:07.309
Apr 21 20:24:07.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename runtimeclass 04/21/23 20:24:07.309
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:07.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:07.318
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:07.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1538" for this suite. 04/21/23 20:24:07.325
------------------------------
â€¢ [0.019 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:07.309
    Apr 21 20:24:07.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename runtimeclass 04/21/23 20:24:07.309
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:07.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:07.318
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:07.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1538" for this suite. 04/21/23 20:24:07.325
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:07.328
Apr 21 20:24:07.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 20:24:07.329
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:07.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:07.336
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/21/23 20:24:07.338
Apr 21 20:24:07.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5459 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Apr 21 20:24:07.397: INFO: stderr: ""
Apr 21 20:24:07.397: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 04/21/23 20:24:07.397
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Apr 21 20:24:07.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5459 delete pods e2e-test-httpd-pod'
Apr 21 20:24:09.291: INFO: stderr: ""
Apr 21 20:24:09.291: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:09.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5459" for this suite. 04/21/23 20:24:09.293
------------------------------
â€¢ [1.968 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:07.328
    Apr 21 20:24:07.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 20:24:07.329
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:07.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:07.336
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/21/23 20:24:07.338
    Apr 21 20:24:07.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5459 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Apr 21 20:24:07.397: INFO: stderr: ""
    Apr 21 20:24:07.397: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 04/21/23 20:24:07.397
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Apr 21 20:24:07.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5459 delete pods e2e-test-httpd-pod'
    Apr 21 20:24:09.291: INFO: stderr: ""
    Apr 21 20:24:09.291: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:09.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5459" for this suite. 04/21/23 20:24:09.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:09.296
Apr 21 20:24:09.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 20:24:09.297
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:09.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:09.308
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Apr 21 20:24:09.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: creating the pod 04/21/23 20:24:09.333
STEP: submitting the pod to kubernetes 04/21/23 20:24:09.333
Apr 21 20:24:09.338: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19" in namespace "pods-952" to be "running and ready"
Apr 21 20:24:09.340: INFO: Pod "pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581999ms
Apr 21 20:24:09.340: INFO: The phase of Pod pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:24:11.343: INFO: Pod "pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19": Phase="Running", Reason="", readiness=true. Elapsed: 2.004983915s
Apr 21 20:24:11.343: INFO: The phase of Pod pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19 is Running (Ready = true)
Apr 21 20:24:11.343: INFO: Pod "pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:11.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-952" for this suite. 04/21/23 20:24:11.361
------------------------------
â€¢ [2.068 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:09.296
    Apr 21 20:24:09.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 20:24:09.297
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:09.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:09.308
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Apr 21 20:24:09.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: creating the pod 04/21/23 20:24:09.333
    STEP: submitting the pod to kubernetes 04/21/23 20:24:09.333
    Apr 21 20:24:09.338: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19" in namespace "pods-952" to be "running and ready"
    Apr 21 20:24:09.340: INFO: Pod "pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19": Phase="Pending", Reason="", readiness=false. Elapsed: 1.581999ms
    Apr 21 20:24:09.340: INFO: The phase of Pod pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:24:11.343: INFO: Pod "pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19": Phase="Running", Reason="", readiness=true. Elapsed: 2.004983915s
    Apr 21 20:24:11.343: INFO: The phase of Pod pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19 is Running (Ready = true)
    Apr 21 20:24:11.343: INFO: Pod "pod-logs-websocket-23895431-d0d8-487a-bcbb-d50c30c2af19" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:11.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-952" for this suite. 04/21/23 20:24:11.361
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:11.365
Apr 21 20:24:11.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 20:24:11.366
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:11.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:11.376
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 04/21/23 20:24:11.378
Apr 21 20:24:11.382: INFO: Waiting up to 5m0s for pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f" in namespace "emptydir-7208" to be "Succeeded or Failed"
Apr 21 20:24:11.384: INFO: Pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.483168ms
Apr 21 20:24:13.387: INFO: Pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004734299s
Apr 21 20:24:15.386: INFO: Pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004150372s
STEP: Saw pod success 04/21/23 20:24:15.386
Apr 21 20:24:15.387: INFO: Pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f" satisfied condition "Succeeded or Failed"
Apr 21 20:24:15.388: INFO: Trying to get logs from node k8sconformance-m02 pod pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f container test-container: <nil>
STEP: delete the pod 04/21/23 20:24:15.393
Apr 21 20:24:15.401: INFO: Waiting for pod pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f to disappear
Apr 21 20:24:15.403: INFO: Pod pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:15.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7208" for this suite. 04/21/23 20:24:15.405
------------------------------
â€¢ [4.043 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:11.365
    Apr 21 20:24:11.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 20:24:11.366
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:11.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:11.376
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 04/21/23 20:24:11.378
    Apr 21 20:24:11.382: INFO: Waiting up to 5m0s for pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f" in namespace "emptydir-7208" to be "Succeeded or Failed"
    Apr 21 20:24:11.384: INFO: Pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.483168ms
    Apr 21 20:24:13.387: INFO: Pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004734299s
    Apr 21 20:24:15.386: INFO: Pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004150372s
    STEP: Saw pod success 04/21/23 20:24:15.386
    Apr 21 20:24:15.387: INFO: Pod "pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f" satisfied condition "Succeeded or Failed"
    Apr 21 20:24:15.388: INFO: Trying to get logs from node k8sconformance-m02 pod pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f container test-container: <nil>
    STEP: delete the pod 04/21/23 20:24:15.393
    Apr 21 20:24:15.401: INFO: Waiting for pod pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f to disappear
    Apr 21 20:24:15.403: INFO: Pod pod-ff9fcd02-8cb6-4b4f-9b73-b573a684b76f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:15.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7208" for this suite. 04/21/23 20:24:15.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:15.408
Apr 21 20:24:15.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename proxy 04/21/23 20:24:15.409
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:15.418
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:15.419
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 04/21/23 20:24:15.428
STEP: creating replication controller proxy-service-nc6xr in namespace proxy-1158 04/21/23 20:24:15.428
I0421 20:24:15.433766      22 runners.go:193] Created replication controller with name: proxy-service-nc6xr, namespace: proxy-1158, replica count: 1
I0421 20:24:16.485405      22 runners.go:193] proxy-service-nc6xr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0421 20:24:17.486143      22 runners.go:193] proxy-service-nc6xr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 21 20:24:17.488: INFO: setup took 2.066855377s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 04/21/23 20:24:17.488
Apr 21 20:24:17.494: INFO: (0) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 5.740781ms)
Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 6.428252ms)
Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 6.510605ms)
Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 6.481783ms)
Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 6.475566ms)
Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 6.575543ms)
Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 7.195879ms)
Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 7.25443ms)
Apr 21 20:24:17.496: INFO: (0) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 7.620328ms)
Apr 21 20:24:17.496: INFO: (0) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 7.762368ms)
Apr 21 20:24:17.496: INFO: (0) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 7.73279ms)
Apr 21 20:24:17.499: INFO: (0) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 11.274959ms)
Apr 21 20:24:17.499: INFO: (0) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 11.194413ms)
Apr 21 20:24:17.499: INFO: (0) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 11.255192ms)
Apr 21 20:24:17.500: INFO: (0) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 11.838519ms)
Apr 21 20:24:17.500: INFO: (0) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 12.072022ms)
Apr 21 20:24:17.503: INFO: (1) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 2.987335ms)
Apr 21 20:24:17.503: INFO: (1) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.11229ms)
Apr 21 20:24:17.503: INFO: (1) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.272087ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.156009ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.209331ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.174106ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.28388ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.230585ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.308444ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.259774ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.769935ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 3.779828ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 3.871103ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.81964ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 3.756758ms)
Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.778367ms)
Apr 21 20:24:17.507: INFO: (2) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.052412ms)
Apr 21 20:24:17.507: INFO: (2) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.030188ms)
Apr 21 20:24:17.507: INFO: (2) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.01817ms)
Apr 21 20:24:17.507: INFO: (2) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.040531ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.328275ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.668557ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.767532ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.742637ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.739916ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.817638ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.966966ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.846841ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 3.887549ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 3.933877ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 3.923703ms)
Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 4.058962ms)
Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 2.852113ms)
Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 2.789168ms)
Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 2.877268ms)
Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 2.849101ms)
Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 2.87084ms)
Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.83544ms)
Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.874826ms)
Apr 21 20:24:17.512: INFO: (3) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.291923ms)
Apr 21 20:24:17.512: INFO: (3) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.298163ms)
Apr 21 20:24:17.512: INFO: (3) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.381157ms)
Apr 21 20:24:17.512: INFO: (3) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.825225ms)
Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.425737ms)
Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.551342ms)
Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.480123ms)
Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.51777ms)
Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.476182ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 2.498096ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.435156ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 2.466965ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 2.567613ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.193614ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.114574ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.144626ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.236991ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 3.403357ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.389417ms)
Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.383653ms)
Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 3.454284ms)
Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 3.480332ms)
Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.450077ms)
Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.502991ms)
Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.489542ms)
Apr 21 20:24:17.519: INFO: (5) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.699222ms)
Apr 21 20:24:17.519: INFO: (5) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 2.70152ms)
Apr 21 20:24:17.519: INFO: (5) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 2.89894ms)
Apr 21 20:24:17.519: INFO: (5) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 2.840867ms)
Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 2.852451ms)
Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 2.932661ms)
Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.315172ms)
Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.342566ms)
Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.409305ms)
Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.467529ms)
Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.792278ms)
Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.467404ms)
Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.455409ms)
Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 4.451681ms)
Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.430941ms)
Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.414238ms)
Apr 21 20:24:17.523: INFO: (6) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 1.987629ms)
Apr 21 20:24:17.524: INFO: (6) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.963853ms)
Apr 21 20:24:17.524: INFO: (6) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.148247ms)
Apr 21 20:24:17.524: INFO: (6) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.249091ms)
Apr 21 20:24:17.524: INFO: (6) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.22393ms)
Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.147159ms)
Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.550603ms)
Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.538866ms)
Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.601725ms)
Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.586789ms)
Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.657788ms)
Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.681753ms)
Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.570068ms)
Apr 21 20:24:17.526: INFO: (6) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.662312ms)
Apr 21 20:24:17.526: INFO: (6) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.646004ms)
Apr 21 20:24:17.526: INFO: (6) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.655114ms)
Apr 21 20:24:17.528: INFO: (7) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 1.913403ms)
Apr 21 20:24:17.528: INFO: (7) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 1.912956ms)
Apr 21 20:24:17.528: INFO: (7) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 1.859024ms)
Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.178704ms)
Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.119194ms)
Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.164708ms)
Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.267898ms)
Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.503676ms)
Apr 21 20:24:17.530: INFO: (7) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.472253ms)
Apr 21 20:24:17.530: INFO: (7) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.565501ms)
Apr 21 20:24:17.530: INFO: (7) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 3.63499ms)
Apr 21 20:24:17.530: INFO: (7) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.960326ms)
Apr 21 20:24:17.532: INFO: (7) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.777474ms)
Apr 21 20:24:17.532: INFO: (7) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 5.818578ms)
Apr 21 20:24:17.532: INFO: (7) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.823226ms)
Apr 21 20:24:17.532: INFO: (7) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.824492ms)
Apr 21 20:24:17.535: INFO: (8) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.253245ms)
Apr 21 20:24:17.535: INFO: (8) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.188009ms)
Apr 21 20:24:17.535: INFO: (8) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.24239ms)
Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.895197ms)
Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.845428ms)
Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.957041ms)
Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.86156ms)
Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.971627ms)
Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.893905ms)
Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 4.460961ms)
Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.851906ms)
Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 6.125184ms)
Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 6.061946ms)
Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 6.091373ms)
Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 6.128561ms)
Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 6.201725ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.863305ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 4.942777ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.01359ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.882816ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.901521ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.015607ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.070448ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.110995ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 5.093995ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.076073ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 5.024587ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.239488ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.187546ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 5.138744ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.096836ms)
Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.120641ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.340885ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.315834ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 4.383403ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 4.494738ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 4.416093ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.443652ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.395592ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 4.416661ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 4.438784ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.58322ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.481599ms)
Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.580929ms)
Apr 21 20:24:17.549: INFO: (10) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.299002ms)
Apr 21 20:24:17.549: INFO: (10) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.192701ms)
Apr 21 20:24:17.549: INFO: (10) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.198941ms)
Apr 21 20:24:17.549: INFO: (10) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.089085ms)
Apr 21 20:24:17.554: INFO: (11) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.084105ms)
Apr 21 20:24:17.554: INFO: (11) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.050152ms)
Apr 21 20:24:17.554: INFO: (11) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 5.11631ms)
Apr 21 20:24:17.554: INFO: (11) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 5.240351ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 5.615325ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.564215ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 5.57462ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.641604ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.631982ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.622126ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 5.640683ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.680453ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.713574ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.789361ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 6.055494ms)
Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 6.418576ms)
Apr 21 20:24:17.559: INFO: (12) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.107836ms)
Apr 21 20:24:17.559: INFO: (12) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.293972ms)
Apr 21 20:24:17.559: INFO: (12) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.176567ms)
Apr 21 20:24:17.560: INFO: (12) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.04518ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 4.949737ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 4.964259ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.961567ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 5.032182ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.127613ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.172198ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.251924ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.25926ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.03546ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.155514ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.173147ms)
Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.209225ms)
Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.34668ms)
Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.40187ms)
Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.67213ms)
Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 4.455354ms)
Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.622996ms)
Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.720619ms)
Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.981929ms)
Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.428109ms)
Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.624464ms)
Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.446468ms)
Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 4.50538ms)
Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.636545ms)
Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.969319ms)
Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.289649ms)
Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.051231ms)
Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.732919ms)
Apr 21 20:24:17.570: INFO: (14) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.010639ms)
Apr 21 20:24:17.570: INFO: (14) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.870401ms)
Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.818077ms)
Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.779082ms)
Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.75276ms)
Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.802291ms)
Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 5.792481ms)
Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.78135ms)
Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.951094ms)
Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 6.513271ms)
Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 6.515721ms)
Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 6.682918ms)
Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 6.644309ms)
Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 6.69933ms)
Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 6.686259ms)
Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 6.591461ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 60.076547ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 60.457506ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 60.420611ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 60.490633ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 60.531874ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 60.316734ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 60.621126ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 60.921761ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 60.727787ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 60.600699ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 60.207335ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 60.307603ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 60.631243ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 60.685103ms)
Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 60.945779ms)
Apr 21 20:24:17.635: INFO: (15) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 61.555429ms)
Apr 21 20:24:17.637: INFO: (16) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 2.466253ms)
Apr 21 20:24:17.638: INFO: (16) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 2.718958ms)
Apr 21 20:24:17.639: INFO: (16) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.919546ms)
Apr 21 20:24:17.640: INFO: (16) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.554097ms)
Apr 21 20:24:17.640: INFO: (16) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 4.481706ms)
Apr 21 20:24:17.640: INFO: (16) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.525776ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.729364ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.894205ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 5.846543ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 5.769261ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.881251ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 5.735757ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.903325ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 5.789413ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.825253ms)
Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.870267ms)
Apr 21 20:24:17.645: INFO: (17) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.728359ms)
Apr 21 20:24:17.645: INFO: (17) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.876963ms)
Apr 21 20:24:17.645: INFO: (17) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.838339ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 4.562927ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.510289ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.712416ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.713937ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.772146ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 4.781409ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 4.808455ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.90831ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.807171ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 4.952138ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.929034ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.987471ms)
Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.879664ms)
Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.613438ms)
Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.652243ms)
Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 5.626244ms)
Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 5.656767ms)
Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.741619ms)
Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.91737ms)
Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.963239ms)
Apr 21 20:24:17.653: INFO: (18) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 6.762244ms)
Apr 21 20:24:17.653: INFO: (18) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 7.119247ms)
Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 7.825612ms)
Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 7.762079ms)
Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 7.790089ms)
Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 7.808586ms)
Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 7.910696ms)
Apr 21 20:24:17.655: INFO: (18) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 8.924163ms)
Apr 21 20:24:17.655: INFO: (18) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 8.957848ms)
Apr 21 20:24:17.658: INFO: (19) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 2.326925ms)
Apr 21 20:24:17.658: INFO: (19) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 2.720361ms)
Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.289565ms)
Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.321509ms)
Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.285057ms)
Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.797422ms)
Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.772805ms)
Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.233148ms)
Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.396005ms)
Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.327063ms)
Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.536217ms)
Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.611651ms)
Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.780004ms)
Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.972674ms)
Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.050003ms)
Apr 21 20:24:17.661: INFO: (19) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.561033ms)
STEP: deleting ReplicationController proxy-service-nc6xr in namespace proxy-1158, will wait for the garbage collector to delete the pods 04/21/23 20:24:17.661
Apr 21 20:24:17.717: INFO: Deleting ReplicationController proxy-service-nc6xr took: 3.70562ms
Apr 21 20:24:17.818: INFO: Terminating ReplicationController proxy-service-nc6xr pods took: 101.058383ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:20.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-1158" for this suite. 04/21/23 20:24:20.422
------------------------------
â€¢ [SLOW TEST] [5.017 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:15.408
    Apr 21 20:24:15.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename proxy 04/21/23 20:24:15.409
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:15.418
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:15.419
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 04/21/23 20:24:15.428
    STEP: creating replication controller proxy-service-nc6xr in namespace proxy-1158 04/21/23 20:24:15.428
    I0421 20:24:15.433766      22 runners.go:193] Created replication controller with name: proxy-service-nc6xr, namespace: proxy-1158, replica count: 1
    I0421 20:24:16.485405      22 runners.go:193] proxy-service-nc6xr Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0421 20:24:17.486143      22 runners.go:193] proxy-service-nc6xr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 21 20:24:17.488: INFO: setup took 2.066855377s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 04/21/23 20:24:17.488
    Apr 21 20:24:17.494: INFO: (0) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 5.740781ms)
    Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 6.428252ms)
    Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 6.510605ms)
    Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 6.481783ms)
    Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 6.475566ms)
    Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 6.575543ms)
    Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 7.195879ms)
    Apr 21 20:24:17.495: INFO: (0) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 7.25443ms)
    Apr 21 20:24:17.496: INFO: (0) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 7.620328ms)
    Apr 21 20:24:17.496: INFO: (0) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 7.762368ms)
    Apr 21 20:24:17.496: INFO: (0) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 7.73279ms)
    Apr 21 20:24:17.499: INFO: (0) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 11.274959ms)
    Apr 21 20:24:17.499: INFO: (0) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 11.194413ms)
    Apr 21 20:24:17.499: INFO: (0) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 11.255192ms)
    Apr 21 20:24:17.500: INFO: (0) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 11.838519ms)
    Apr 21 20:24:17.500: INFO: (0) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 12.072022ms)
    Apr 21 20:24:17.503: INFO: (1) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 2.987335ms)
    Apr 21 20:24:17.503: INFO: (1) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.11229ms)
    Apr 21 20:24:17.503: INFO: (1) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.272087ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.156009ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.209331ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.174106ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.28388ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.230585ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.308444ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.259774ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.769935ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 3.779828ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 3.871103ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.81964ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 3.756758ms)
    Apr 21 20:24:17.504: INFO: (1) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.778367ms)
    Apr 21 20:24:17.507: INFO: (2) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.052412ms)
    Apr 21 20:24:17.507: INFO: (2) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.030188ms)
    Apr 21 20:24:17.507: INFO: (2) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.01817ms)
    Apr 21 20:24:17.507: INFO: (2) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.040531ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.328275ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.668557ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.767532ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.742637ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.739916ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.817638ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.966966ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.846841ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 3.887549ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 3.933877ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 3.923703ms)
    Apr 21 20:24:17.508: INFO: (2) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 4.058962ms)
    Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 2.852113ms)
    Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 2.789168ms)
    Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 2.877268ms)
    Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 2.849101ms)
    Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 2.87084ms)
    Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.83544ms)
    Apr 21 20:24:17.511: INFO: (3) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.874826ms)
    Apr 21 20:24:17.512: INFO: (3) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.291923ms)
    Apr 21 20:24:17.512: INFO: (3) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.298163ms)
    Apr 21 20:24:17.512: INFO: (3) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.381157ms)
    Apr 21 20:24:17.512: INFO: (3) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.825225ms)
    Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.425737ms)
    Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.551342ms)
    Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.480123ms)
    Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.51777ms)
    Apr 21 20:24:17.513: INFO: (3) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.476182ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 2.498096ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.435156ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 2.466965ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 2.567613ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.193614ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.114574ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.144626ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.236991ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 3.403357ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.389417ms)
    Apr 21 20:24:17.516: INFO: (4) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.383653ms)
    Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 3.454284ms)
    Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 3.480332ms)
    Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.450077ms)
    Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.502991ms)
    Apr 21 20:24:17.517: INFO: (4) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.489542ms)
    Apr 21 20:24:17.519: INFO: (5) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.699222ms)
    Apr 21 20:24:17.519: INFO: (5) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 2.70152ms)
    Apr 21 20:24:17.519: INFO: (5) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 2.89894ms)
    Apr 21 20:24:17.519: INFO: (5) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 2.840867ms)
    Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 2.852451ms)
    Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 2.932661ms)
    Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.315172ms)
    Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.342566ms)
    Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.409305ms)
    Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.467529ms)
    Apr 21 20:24:17.520: INFO: (5) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.792278ms)
    Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.467404ms)
    Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.455409ms)
    Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 4.451681ms)
    Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.430941ms)
    Apr 21 20:24:17.521: INFO: (5) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.414238ms)
    Apr 21 20:24:17.523: INFO: (6) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 1.987629ms)
    Apr 21 20:24:17.524: INFO: (6) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 2.963853ms)
    Apr 21 20:24:17.524: INFO: (6) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.148247ms)
    Apr 21 20:24:17.524: INFO: (6) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.249091ms)
    Apr 21 20:24:17.524: INFO: (6) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 3.22393ms)
    Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.147159ms)
    Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.550603ms)
    Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.538866ms)
    Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.601725ms)
    Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.586789ms)
    Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.657788ms)
    Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 3.681753ms)
    Apr 21 20:24:17.525: INFO: (6) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.570068ms)
    Apr 21 20:24:17.526: INFO: (6) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.662312ms)
    Apr 21 20:24:17.526: INFO: (6) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.646004ms)
    Apr 21 20:24:17.526: INFO: (6) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.655114ms)
    Apr 21 20:24:17.528: INFO: (7) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 1.913403ms)
    Apr 21 20:24:17.528: INFO: (7) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 1.912956ms)
    Apr 21 20:24:17.528: INFO: (7) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 1.859024ms)
    Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.178704ms)
    Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.119194ms)
    Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.164708ms)
    Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.267898ms)
    Apr 21 20:24:17.529: INFO: (7) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 3.503676ms)
    Apr 21 20:24:17.530: INFO: (7) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.472253ms)
    Apr 21 20:24:17.530: INFO: (7) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.565501ms)
    Apr 21 20:24:17.530: INFO: (7) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 3.63499ms)
    Apr 21 20:24:17.530: INFO: (7) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.960326ms)
    Apr 21 20:24:17.532: INFO: (7) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.777474ms)
    Apr 21 20:24:17.532: INFO: (7) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 5.818578ms)
    Apr 21 20:24:17.532: INFO: (7) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.823226ms)
    Apr 21 20:24:17.532: INFO: (7) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.824492ms)
    Apr 21 20:24:17.535: INFO: (8) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.253245ms)
    Apr 21 20:24:17.535: INFO: (8) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.188009ms)
    Apr 21 20:24:17.535: INFO: (8) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.24239ms)
    Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 3.895197ms)
    Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.845428ms)
    Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.957041ms)
    Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.86156ms)
    Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.971627ms)
    Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.893905ms)
    Apr 21 20:24:17.536: INFO: (8) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 4.460961ms)
    Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.851906ms)
    Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 6.125184ms)
    Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 6.061946ms)
    Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 6.091373ms)
    Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 6.128561ms)
    Apr 21 20:24:17.538: INFO: (8) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 6.201725ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.863305ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 4.942777ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.01359ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.882816ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.901521ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.015607ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.070448ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.110995ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 5.093995ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.076073ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 5.024587ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.239488ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.187546ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 5.138744ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.096836ms)
    Apr 21 20:24:17.543: INFO: (9) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.120641ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.340885ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.315834ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 4.383403ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 4.494738ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 4.416093ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.443652ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.395592ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 4.416661ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 4.438784ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.58322ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.481599ms)
    Apr 21 20:24:17.548: INFO: (10) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.580929ms)
    Apr 21 20:24:17.549: INFO: (10) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.299002ms)
    Apr 21 20:24:17.549: INFO: (10) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.192701ms)
    Apr 21 20:24:17.549: INFO: (10) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.198941ms)
    Apr 21 20:24:17.549: INFO: (10) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.089085ms)
    Apr 21 20:24:17.554: INFO: (11) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.084105ms)
    Apr 21 20:24:17.554: INFO: (11) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.050152ms)
    Apr 21 20:24:17.554: INFO: (11) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 5.11631ms)
    Apr 21 20:24:17.554: INFO: (11) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 5.240351ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 5.615325ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.564215ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 5.57462ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.641604ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.631982ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.622126ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 5.640683ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.680453ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.713574ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.789361ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 6.055494ms)
    Apr 21 20:24:17.555: INFO: (11) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 6.418576ms)
    Apr 21 20:24:17.559: INFO: (12) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.107836ms)
    Apr 21 20:24:17.559: INFO: (12) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 3.293972ms)
    Apr 21 20:24:17.559: INFO: (12) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.176567ms)
    Apr 21 20:24:17.560: INFO: (12) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.04518ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 4.949737ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 4.964259ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.961567ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 5.032182ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.127613ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.172198ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.251924ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.25926ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.03546ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.155514ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.173147ms)
    Apr 21 20:24:17.561: INFO: (12) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.209225ms)
    Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.34668ms)
    Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.40187ms)
    Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.67213ms)
    Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 4.455354ms)
    Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.622996ms)
    Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.720619ms)
    Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.981929ms)
    Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.428109ms)
    Apr 21 20:24:17.565: INFO: (13) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.624464ms)
    Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.446468ms)
    Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 4.50538ms)
    Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.636545ms)
    Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.969319ms)
    Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.289649ms)
    Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 5.051231ms)
    Apr 21 20:24:17.566: INFO: (13) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.732919ms)
    Apr 21 20:24:17.570: INFO: (14) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.010639ms)
    Apr 21 20:24:17.570: INFO: (14) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 3.870401ms)
    Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.818077ms)
    Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.779082ms)
    Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.75276ms)
    Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.802291ms)
    Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 5.792481ms)
    Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.78135ms)
    Apr 21 20:24:17.572: INFO: (14) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.951094ms)
    Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 6.513271ms)
    Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 6.515721ms)
    Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 6.682918ms)
    Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 6.644309ms)
    Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 6.69933ms)
    Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 6.686259ms)
    Apr 21 20:24:17.573: INFO: (14) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 6.591461ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 60.076547ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 60.457506ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 60.420611ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 60.490633ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 60.531874ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 60.316734ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 60.621126ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 60.921761ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 60.727787ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 60.600699ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 60.207335ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 60.307603ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 60.631243ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 60.685103ms)
    Apr 21 20:24:17.634: INFO: (15) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 60.945779ms)
    Apr 21 20:24:17.635: INFO: (15) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 61.555429ms)
    Apr 21 20:24:17.637: INFO: (16) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 2.466253ms)
    Apr 21 20:24:17.638: INFO: (16) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 2.718958ms)
    Apr 21 20:24:17.639: INFO: (16) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.919546ms)
    Apr 21 20:24:17.640: INFO: (16) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.554097ms)
    Apr 21 20:24:17.640: INFO: (16) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 4.481706ms)
    Apr 21 20:24:17.640: INFO: (16) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.525776ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.729364ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 5.894205ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 5.846543ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 5.769261ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 5.881251ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 5.735757ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.903325ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 5.789413ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.825253ms)
    Apr 21 20:24:17.641: INFO: (16) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.870267ms)
    Apr 21 20:24:17.645: INFO: (17) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.728359ms)
    Apr 21 20:24:17.645: INFO: (17) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.876963ms)
    Apr 21 20:24:17.645: INFO: (17) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.838339ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 4.562927ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.510289ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.712416ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.713937ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 4.772146ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 4.781409ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 4.808455ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.90831ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.807171ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 4.952138ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.929034ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.987471ms)
    Apr 21 20:24:17.646: INFO: (17) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.879664ms)
    Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 5.613438ms)
    Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.652243ms)
    Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 5.626244ms)
    Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 5.656767ms)
    Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 5.741619ms)
    Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 5.91737ms)
    Apr 21 20:24:17.652: INFO: (18) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 5.963239ms)
    Apr 21 20:24:17.653: INFO: (18) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 6.762244ms)
    Apr 21 20:24:17.653: INFO: (18) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 7.119247ms)
    Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 7.825612ms)
    Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 7.762079ms)
    Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 7.790089ms)
    Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 7.808586ms)
    Apr 21 20:24:17.654: INFO: (18) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 7.910696ms)
    Apr 21 20:24:17.655: INFO: (18) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 8.924163ms)
    Apr 21 20:24:17.655: INFO: (18) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 8.957848ms)
    Apr 21 20:24:17.658: INFO: (19) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:443/proxy/tlsrewritem... (200; 2.326925ms)
    Apr 21 20:24:17.658: INFO: (19) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms/proxy/rewriteme">test</a> (200; 2.720361ms)
    Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 3.289565ms)
    Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">test<... (200; 3.321509ms)
    Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 3.285057ms)
    Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/: <a href="/api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:1080/proxy/rewriteme">... (200; 3.797422ms)
    Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:460/proxy/: tls baz (200; 3.772805ms)
    Apr 21 20:24:17.659: INFO: (19) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname1/proxy/: foo (200; 4.233148ms)
    Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/pods/http:proxy-service-nc6xr-xqzms:162/proxy/: bar (200; 4.396005ms)
    Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname1/proxy/: tls baz (200; 4.327063ms)
    Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/pods/proxy-service-nc6xr-xqzms:160/proxy/: foo (200; 4.536217ms)
    Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname1/proxy/: foo (200; 4.611651ms)
    Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/pods/https:proxy-service-nc6xr-xqzms:462/proxy/: tls qux (200; 4.780004ms)
    Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/services/proxy-service-nc6xr:portname2/proxy/: bar (200; 4.972674ms)
    Apr 21 20:24:17.660: INFO: (19) /api/v1/namespaces/proxy-1158/services/https:proxy-service-nc6xr:tlsportname2/proxy/: tls qux (200; 5.050003ms)
    Apr 21 20:24:17.661: INFO: (19) /api/v1/namespaces/proxy-1158/services/http:proxy-service-nc6xr:portname2/proxy/: bar (200; 5.561033ms)
    STEP: deleting ReplicationController proxy-service-nc6xr in namespace proxy-1158, will wait for the garbage collector to delete the pods 04/21/23 20:24:17.661
    Apr 21 20:24:17.717: INFO: Deleting ReplicationController proxy-service-nc6xr took: 3.70562ms
    Apr 21 20:24:17.818: INFO: Terminating ReplicationController proxy-service-nc6xr pods took: 101.058383ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:20.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-1158" for this suite. 04/21/23 20:24:20.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:20.427
Apr 21 20:24:20.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename endpointslice 04/21/23 20:24:20.428
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:20.435
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:20.437
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 04/21/23 20:24:25.486
STEP: referencing matching pods with named port 04/21/23 20:24:30.491
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 04/21/23 20:24:35.496
STEP: recreating EndpointSlices after they've been deleted 04/21/23 20:24:40.5
Apr 21 20:24:40.511: INFO: EndpointSlice for Service endpointslice-7066/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:50.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7066" for this suite. 04/21/23 20:24:50.519
------------------------------
â€¢ [SLOW TEST] [30.096 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:20.427
    Apr 21 20:24:20.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename endpointslice 04/21/23 20:24:20.428
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:20.435
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:20.437
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 04/21/23 20:24:25.486
    STEP: referencing matching pods with named port 04/21/23 20:24:30.491
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 04/21/23 20:24:35.496
    STEP: recreating EndpointSlices after they've been deleted 04/21/23 20:24:40.5
    Apr 21 20:24:40.511: INFO: EndpointSlice for Service endpointslice-7066/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:50.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7066" for this suite. 04/21/23 20:24:50.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:50.523
Apr 21 20:24:50.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 20:24:50.524
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:50.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:50.533
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:24:50.535
Apr 21 20:24:50.539: INFO: Waiting up to 5m0s for pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22" in namespace "downward-api-6083" to be "Succeeded or Failed"
Apr 21 20:24:50.541: INFO: Pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22": Phase="Pending", Reason="", readiness=false. Elapsed: 1.568267ms
Apr 21 20:24:52.543: INFO: Pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004111637s
Apr 21 20:24:54.543: INFO: Pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004044974s
STEP: Saw pod success 04/21/23 20:24:54.543
Apr 21 20:24:54.544: INFO: Pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22" satisfied condition "Succeeded or Failed"
Apr 21 20:24:54.545: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22 container client-container: <nil>
STEP: delete the pod 04/21/23 20:24:54.55
Apr 21 20:24:54.558: INFO: Waiting for pod downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22 to disappear
Apr 21 20:24:54.560: INFO: Pod downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:54.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6083" for this suite. 04/21/23 20:24:54.562
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:50.523
    Apr 21 20:24:50.524: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 20:24:50.524
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:50.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:50.533
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:24:50.535
    Apr 21 20:24:50.539: INFO: Waiting up to 5m0s for pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22" in namespace "downward-api-6083" to be "Succeeded or Failed"
    Apr 21 20:24:50.541: INFO: Pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22": Phase="Pending", Reason="", readiness=false. Elapsed: 1.568267ms
    Apr 21 20:24:52.543: INFO: Pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004111637s
    Apr 21 20:24:54.543: INFO: Pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004044974s
    STEP: Saw pod success 04/21/23 20:24:54.543
    Apr 21 20:24:54.544: INFO: Pod "downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22" satisfied condition "Succeeded or Failed"
    Apr 21 20:24:54.545: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22 container client-container: <nil>
    STEP: delete the pod 04/21/23 20:24:54.55
    Apr 21 20:24:54.558: INFO: Waiting for pod downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22 to disappear
    Apr 21 20:24:54.560: INFO: Pod downwardapi-volume-411adfdd-ca58-494f-b720-4ad3d2682b22 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:54.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6083" for this suite. 04/21/23 20:24:54.562
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:54.566
Apr 21 20:24:54.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 20:24:54.567
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:54.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:54.576
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 04/21/23 20:24:54.578
Apr 21 20:24:54.582: INFO: Waiting up to 5m0s for pod "pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c" in namespace "pods-9759" to be "running and ready"
Apr 21 20:24:54.584: INFO: Pod "pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.491446ms
Apr 21 20:24:54.584: INFO: The phase of Pod pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:24:56.586: INFO: Pod "pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004078971s
Apr 21 20:24:56.586: INFO: The phase of Pod pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c is Running (Ready = true)
Apr 21 20:24:56.586: INFO: Pod "pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c" satisfied condition "running and ready"
Apr 21 20:24:56.589: INFO: Pod pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c has hostIP: 192.168.49.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 20:24:56.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9759" for this suite. 04/21/23 20:24:56.591
------------------------------
â€¢ [2.028 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:54.566
    Apr 21 20:24:54.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 20:24:54.567
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:54.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:54.576
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 04/21/23 20:24:54.578
    Apr 21 20:24:54.582: INFO: Waiting up to 5m0s for pod "pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c" in namespace "pods-9759" to be "running and ready"
    Apr 21 20:24:54.584: INFO: Pod "pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.491446ms
    Apr 21 20:24:54.584: INFO: The phase of Pod pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:24:56.586: INFO: Pod "pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c": Phase="Running", Reason="", readiness=true. Elapsed: 2.004078971s
    Apr 21 20:24:56.586: INFO: The phase of Pod pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c is Running (Ready = true)
    Apr 21 20:24:56.586: INFO: Pod "pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c" satisfied condition "running and ready"
    Apr 21 20:24:56.589: INFO: Pod pod-hostip-3def4204-9639-4be0-865c-7183cbdff17c has hostIP: 192.168.49.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:24:56.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9759" for this suite. 04/21/23 20:24:56.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:24:56.596
Apr 21 20:24:56.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:24:56.596
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:56.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:56.605
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 04/21/23 20:24:56.607
Apr 21 20:24:56.611: INFO: Waiting up to 5m0s for pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4" in namespace "projected-3523" to be "running and ready"
Apr 21 20:24:56.612: INFO: Pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.480775ms
Apr 21 20:24:56.612: INFO: The phase of Pod annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:24:58.615: INFO: Pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004624116s
Apr 21 20:24:58.615: INFO: The phase of Pod annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4 is Running (Ready = true)
Apr 21 20:24:58.615: INFO: Pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4" satisfied condition "running and ready"
Apr 21 20:24:59.131: INFO: Successfully updated pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 20:25:03.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3523" for this suite. 04/21/23 20:25:03.152
------------------------------
â€¢ [SLOW TEST] [6.561 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:24:56.596
    Apr 21 20:24:56.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:24:56.596
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:24:56.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:24:56.605
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 04/21/23 20:24:56.607
    Apr 21 20:24:56.611: INFO: Waiting up to 5m0s for pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4" in namespace "projected-3523" to be "running and ready"
    Apr 21 20:24:56.612: INFO: Pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.480775ms
    Apr 21 20:24:56.612: INFO: The phase of Pod annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:24:58.615: INFO: Pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004624116s
    Apr 21 20:24:58.615: INFO: The phase of Pod annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4 is Running (Ready = true)
    Apr 21 20:24:58.615: INFO: Pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4" satisfied condition "running and ready"
    Apr 21 20:24:59.131: INFO: Successfully updated pod "annotationupdate4dccd6e0-c2fa-4a88-94f1-5f0dd44331e4"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:25:03.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3523" for this suite. 04/21/23 20:25:03.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:25:03.157
Apr 21 20:25:03.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 20:25:03.158
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:25:03.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:25:03.167
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 04/21/23 20:25:03.169
Apr 21 20:25:03.173: INFO: Waiting up to 5m0s for pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6" in namespace "emptydir-8824" to be "Succeeded or Failed"
Apr 21 20:25:03.175: INFO: Pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.568263ms
Apr 21 20:25:05.177: INFO: Pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004037594s
Apr 21 20:25:07.177: INFO: Pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004319561s
STEP: Saw pod success 04/21/23 20:25:07.177
Apr 21 20:25:07.178: INFO: Pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6" satisfied condition "Succeeded or Failed"
Apr 21 20:25:07.179: INFO: Trying to get logs from node k8sconformance-m02 pod pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6 container test-container: <nil>
STEP: delete the pod 04/21/23 20:25:07.184
Apr 21 20:25:07.191: INFO: Waiting for pod pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6 to disappear
Apr 21 20:25:07.193: INFO: Pod pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 20:25:07.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8824" for this suite. 04/21/23 20:25:07.195
------------------------------
â€¢ [4.043 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:25:03.157
    Apr 21 20:25:03.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 20:25:03.158
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:25:03.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:25:03.167
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 04/21/23 20:25:03.169
    Apr 21 20:25:03.173: INFO: Waiting up to 5m0s for pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6" in namespace "emptydir-8824" to be "Succeeded or Failed"
    Apr 21 20:25:03.175: INFO: Pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.568263ms
    Apr 21 20:25:05.177: INFO: Pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004037594s
    Apr 21 20:25:07.177: INFO: Pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004319561s
    STEP: Saw pod success 04/21/23 20:25:07.177
    Apr 21 20:25:07.178: INFO: Pod "pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6" satisfied condition "Succeeded or Failed"
    Apr 21 20:25:07.179: INFO: Trying to get logs from node k8sconformance-m02 pod pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6 container test-container: <nil>
    STEP: delete the pod 04/21/23 20:25:07.184
    Apr 21 20:25:07.191: INFO: Waiting for pod pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6 to disappear
    Apr 21 20:25:07.193: INFO: Pod pod-6b16b354-2572-4d71-aca3-d191a4e8cfe6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:25:07.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8824" for this suite. 04/21/23 20:25:07.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:25:07.2
Apr 21 20:25:07.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename statefulset 04/21/23 20:25:07.201
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:25:07.208
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:25:07.21
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9480 04/21/23 20:25:07.211
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-9480 04/21/23 20:25:07.216
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9480 04/21/23 20:25:07.219
Apr 21 20:25:07.220: INFO: Found 0 stateful pods, waiting for 1
Apr 21 20:25:17.223: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 04/21/23 20:25:17.223
Apr 21 20:25:17.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 20:25:17.349: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 20:25:17.349: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 20:25:17.349: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 21 20:25:17.351: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 21 20:25:27.354: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 21 20:25:27.354: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 20:25:27.362: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Apr 21 20:25:27.362: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  }]
Apr 21 20:25:27.362: INFO: 
Apr 21 20:25:27.362: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 21 20:25:28.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998217604s
Apr 21 20:25:29.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995001809s
Apr 21 20:25:30.371: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992385614s
Apr 21 20:25:31.373: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989662862s
Apr 21 20:25:32.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986096613s
Apr 21 20:25:33.380: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982865297s
Apr 21 20:25:34.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.980371092s
Apr 21 20:25:35.386: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.976869457s
Apr 21 20:25:36.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.997885ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9480 04/21/23 20:25:37.389
Apr 21 20:25:37.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 21 20:25:37.513: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 21 20:25:37.513: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 21 20:25:37.513: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 21 20:25:37.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 21 20:25:37.644: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 21 20:25:37.644: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 21 20:25:37.644: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 21 20:25:37.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 21 20:25:37.771: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 21 20:25:37.771: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 21 20:25:37.771: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 21 20:25:37.773: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 20:25:37.773: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 20:25:37.773: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 04/21/23 20:25:37.773
Apr 21 20:25:37.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 20:25:37.899: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 20:25:37.899: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 20:25:37.899: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 21 20:25:37.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 20:25:38.017: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 20:25:38.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 20:25:38.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 21 20:25:38.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 20:25:38.157: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 20:25:38.157: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 20:25:38.157: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 21 20:25:38.157: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 20:25:38.159: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 21 20:25:48.166: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 21 20:25:48.166: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 21 20:25:48.166: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 21 20:25:48.175: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Apr 21 20:25:48.175: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  }]
Apr 21 20:25:48.175: INFO: ss-1  k8sconformance      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  }]
Apr 21 20:25:48.175: INFO: ss-2  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  }]
Apr 21 20:25:48.175: INFO: 
Apr 21 20:25:48.175: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 21 20:25:49.178: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
Apr 21 20:25:49.178: INFO: ss-0  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  }]
Apr 21 20:25:49.178: INFO: ss-2  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  }]
Apr 21 20:25:49.178: INFO: 
Apr 21 20:25:49.178: INFO: StatefulSet ss has not reached scale 0, at 2
Apr 21 20:25:50.180: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.995029747s
Apr 21 20:25:51.183: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992628041s
Apr 21 20:25:52.185: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.990548463s
Apr 21 20:25:53.187: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.988521634s
Apr 21 20:25:54.189: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.98600309s
Apr 21 20:25:55.192: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.982949403s
Apr 21 20:25:56.195: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.979940131s
Apr 21 20:25:57.198: INFO: Verifying statefulset ss doesn't scale past 0 for another 977.392059ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9480 04/21/23 20:25:58.198
Apr 21 20:25:58.200: INFO: Scaling statefulset ss to 0
Apr 21 20:25:58.206: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 21 20:25:58.207: INFO: Deleting all statefulset in ns statefulset-9480
Apr 21 20:25:58.209: INFO: Scaling statefulset ss to 0
Apr 21 20:25:58.213: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 20:25:58.215: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:25:58.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9480" for this suite. 04/21/23 20:25:58.229
------------------------------
â€¢ [SLOW TEST] [51.032 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:25:07.2
    Apr 21 20:25:07.201: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename statefulset 04/21/23 20:25:07.201
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:25:07.208
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:25:07.21
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9480 04/21/23 20:25:07.211
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-9480 04/21/23 20:25:07.216
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9480 04/21/23 20:25:07.219
    Apr 21 20:25:07.220: INFO: Found 0 stateful pods, waiting for 1
    Apr 21 20:25:17.223: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 04/21/23 20:25:17.223
    Apr 21 20:25:17.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 20:25:17.349: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 20:25:17.349: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 20:25:17.349: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 21 20:25:17.351: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Apr 21 20:25:27.354: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 21 20:25:27.354: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 20:25:27.362: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
    Apr 21 20:25:27.362: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:17 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  }]
    Apr 21 20:25:27.362: INFO: 
    Apr 21 20:25:27.362: INFO: StatefulSet ss has not reached scale 3, at 1
    Apr 21 20:25:28.365: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.998217604s
    Apr 21 20:25:29.368: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995001809s
    Apr 21 20:25:30.371: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992385614s
    Apr 21 20:25:31.373: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989662862s
    Apr 21 20:25:32.377: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986096613s
    Apr 21 20:25:33.380: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.982865297s
    Apr 21 20:25:34.382: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.980371092s
    Apr 21 20:25:35.386: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.976869457s
    Apr 21 20:25:36.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.997885ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9480 04/21/23 20:25:37.389
    Apr 21 20:25:37.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 21 20:25:37.513: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 21 20:25:37.513: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 21 20:25:37.513: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 21 20:25:37.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 21 20:25:37.644: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Apr 21 20:25:37.644: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 21 20:25:37.644: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 21 20:25:37.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 21 20:25:37.771: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Apr 21 20:25:37.771: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 21 20:25:37.771: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 21 20:25:37.773: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 20:25:37.773: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 20:25:37.773: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 04/21/23 20:25:37.773
    Apr 21 20:25:37.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 20:25:37.899: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 20:25:37.899: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 20:25:37.899: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 21 20:25:37.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 20:25:38.017: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 20:25:38.018: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 20:25:38.018: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 21 20:25:38.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9480 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 20:25:38.157: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 20:25:38.157: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 20:25:38.157: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 21 20:25:38.157: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 20:25:38.159: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Apr 21 20:25:48.166: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 21 20:25:48.166: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Apr 21 20:25:48.166: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Apr 21 20:25:48.175: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
    Apr 21 20:25:48.175: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  }]
    Apr 21 20:25:48.175: INFO: ss-1  k8sconformance      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  }]
    Apr 21 20:25:48.175: INFO: ss-2  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  }]
    Apr 21 20:25:48.175: INFO: 
    Apr 21 20:25:48.175: INFO: StatefulSet ss has not reached scale 0, at 3
    Apr 21 20:25:49.178: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
    Apr 21 20:25:49.178: INFO: ss-0  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:07 +0000 UTC  }]
    Apr 21 20:25:49.178: INFO: ss-2  k8sconformance-m02  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:38 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:25:27 +0000 UTC  }]
    Apr 21 20:25:49.178: INFO: 
    Apr 21 20:25:49.178: INFO: StatefulSet ss has not reached scale 0, at 2
    Apr 21 20:25:50.180: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.995029747s
    Apr 21 20:25:51.183: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992628041s
    Apr 21 20:25:52.185: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.990548463s
    Apr 21 20:25:53.187: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.988521634s
    Apr 21 20:25:54.189: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.98600309s
    Apr 21 20:25:55.192: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.982949403s
    Apr 21 20:25:56.195: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.979940131s
    Apr 21 20:25:57.198: INFO: Verifying statefulset ss doesn't scale past 0 for another 977.392059ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9480 04/21/23 20:25:58.198
    Apr 21 20:25:58.200: INFO: Scaling statefulset ss to 0
    Apr 21 20:25:58.206: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 21 20:25:58.207: INFO: Deleting all statefulset in ns statefulset-9480
    Apr 21 20:25:58.209: INFO: Scaling statefulset ss to 0
    Apr 21 20:25:58.213: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 20:25:58.215: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:25:58.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9480" for this suite. 04/21/23 20:25:58.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:25:58.233
Apr 21 20:25:58.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 20:25:58.234
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:25:58.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:25:58.243
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1723 04/21/23 20:25:58.245
STEP: creating replication controller nodeport-test in namespace services-1723 04/21/23 20:25:58.255
I0421 20:25:58.259917      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1723, replica count: 2
I0421 20:26:01.312494      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 21 20:26:01.312: INFO: Creating new exec pod
Apr 21 20:26:01.316: INFO: Waiting up to 5m0s for pod "execpodzfgr4" in namespace "services-1723" to be "running"
Apr 21 20:26:01.318: INFO: Pod "execpodzfgr4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.847338ms
Apr 21 20:26:03.321: INFO: Pod "execpodzfgr4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004966501s
Apr 21 20:26:03.321: INFO: Pod "execpodzfgr4" satisfied condition "running"
Apr 21 20:26:04.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-1723 exec execpodzfgr4 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Apr 21 20:26:04.449: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 21 20:26:04.449: INFO: stdout: ""
Apr 21 20:26:04.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-1723 exec execpodzfgr4 -- /bin/sh -x -c nc -v -z -w 2 10.111.5.248 80'
Apr 21 20:26:04.570: INFO: stderr: "+ nc -v -z -w 2 10.111.5.248 80\nConnection to 10.111.5.248 80 port [tcp/http] succeeded!\n"
Apr 21 20:26:04.570: INFO: stdout: ""
Apr 21 20:26:04.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-1723 exec execpodzfgr4 -- /bin/sh -x -c nc -v -z -w 2 192.168.49.2 32475'
Apr 21 20:26:04.700: INFO: stderr: "+ nc -v -z -w 2 192.168.49.2 32475\nConnection to 192.168.49.2 32475 port [tcp/*] succeeded!\n"
Apr 21 20:26:04.700: INFO: stdout: ""
Apr 21 20:26:04.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-1723 exec execpodzfgr4 -- /bin/sh -x -c nc -v -z -w 2 192.168.49.3 32475'
Apr 21 20:26:04.806: INFO: stderr: "+ nc -v -z -w 2 192.168.49.3 32475\nConnection to 192.168.49.3 32475 port [tcp/*] succeeded!\n"
Apr 21 20:26:04.806: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 20:26:04.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1723" for this suite. 04/21/23 20:26:04.809
------------------------------
â€¢ [SLOW TEST] [6.580 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:25:58.233
    Apr 21 20:25:58.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 20:25:58.234
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:25:58.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:25:58.243
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1723 04/21/23 20:25:58.245
    STEP: creating replication controller nodeport-test in namespace services-1723 04/21/23 20:25:58.255
    I0421 20:25:58.259917      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1723, replica count: 2
    I0421 20:26:01.312494      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 21 20:26:01.312: INFO: Creating new exec pod
    Apr 21 20:26:01.316: INFO: Waiting up to 5m0s for pod "execpodzfgr4" in namespace "services-1723" to be "running"
    Apr 21 20:26:01.318: INFO: Pod "execpodzfgr4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.847338ms
    Apr 21 20:26:03.321: INFO: Pod "execpodzfgr4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004966501s
    Apr 21 20:26:03.321: INFO: Pod "execpodzfgr4" satisfied condition "running"
    Apr 21 20:26:04.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-1723 exec execpodzfgr4 -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Apr 21 20:26:04.449: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Apr 21 20:26:04.449: INFO: stdout: ""
    Apr 21 20:26:04.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-1723 exec execpodzfgr4 -- /bin/sh -x -c nc -v -z -w 2 10.111.5.248 80'
    Apr 21 20:26:04.570: INFO: stderr: "+ nc -v -z -w 2 10.111.5.248 80\nConnection to 10.111.5.248 80 port [tcp/http] succeeded!\n"
    Apr 21 20:26:04.570: INFO: stdout: ""
    Apr 21 20:26:04.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-1723 exec execpodzfgr4 -- /bin/sh -x -c nc -v -z -w 2 192.168.49.2 32475'
    Apr 21 20:26:04.700: INFO: stderr: "+ nc -v -z -w 2 192.168.49.2 32475\nConnection to 192.168.49.2 32475 port [tcp/*] succeeded!\n"
    Apr 21 20:26:04.700: INFO: stdout: ""
    Apr 21 20:26:04.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-1723 exec execpodzfgr4 -- /bin/sh -x -c nc -v -z -w 2 192.168.49.3 32475'
    Apr 21 20:26:04.806: INFO: stderr: "+ nc -v -z -w 2 192.168.49.3 32475\nConnection to 192.168.49.3 32475 port [tcp/*] succeeded!\n"
    Apr 21 20:26:04.806: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:26:04.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1723" for this suite. 04/21/23 20:26:04.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:26:04.813
Apr 21 20:26:04.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename var-expansion 04/21/23 20:26:04.814
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:04.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:04.823
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 04/21/23 20:26:04.825
Apr 21 20:26:04.830: INFO: Waiting up to 5m0s for pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a" in namespace "var-expansion-4754" to be "Succeeded or Failed"
Apr 21 20:26:04.831: INFO: Pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.473358ms
Apr 21 20:26:06.833: INFO: Pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00373933s
Apr 21 20:26:08.835: INFO: Pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004760696s
STEP: Saw pod success 04/21/23 20:26:08.835
Apr 21 20:26:08.835: INFO: Pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a" satisfied condition "Succeeded or Failed"
Apr 21 20:26:08.836: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a container dapi-container: <nil>
STEP: delete the pod 04/21/23 20:26:08.841
Apr 21 20:26:08.848: INFO: Waiting for pod var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a to disappear
Apr 21 20:26:08.849: INFO: Pod var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 21 20:26:08.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4754" for this suite. 04/21/23 20:26:08.851
------------------------------
â€¢ [4.042 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:26:04.813
    Apr 21 20:26:04.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename var-expansion 04/21/23 20:26:04.814
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:04.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:04.823
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 04/21/23 20:26:04.825
    Apr 21 20:26:04.830: INFO: Waiting up to 5m0s for pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a" in namespace "var-expansion-4754" to be "Succeeded or Failed"
    Apr 21 20:26:04.831: INFO: Pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.473358ms
    Apr 21 20:26:06.833: INFO: Pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00373933s
    Apr 21 20:26:08.835: INFO: Pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004760696s
    STEP: Saw pod success 04/21/23 20:26:08.835
    Apr 21 20:26:08.835: INFO: Pod "var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a" satisfied condition "Succeeded or Failed"
    Apr 21 20:26:08.836: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a container dapi-container: <nil>
    STEP: delete the pod 04/21/23 20:26:08.841
    Apr 21 20:26:08.848: INFO: Waiting for pod var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a to disappear
    Apr 21 20:26:08.849: INFO: Pod var-expansion-28e9bcae-4b13-42fe-9d26-c7cd2119458a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:26:08.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4754" for this suite. 04/21/23 20:26:08.851
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:26:08.855
Apr 21 20:26:08.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename deployment 04/21/23 20:26:08.856
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:08.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:08.864
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Apr 21 20:26:08.866: INFO: Creating deployment "test-recreate-deployment"
Apr 21 20:26:08.869: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 21 20:26:08.872: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 21 20:26:10.877: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 21 20:26:10.878: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 21 20:26:10.885: INFO: Updating deployment test-recreate-deployment
Apr 21 20:26:10.885: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 21 20:26:10.955: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-9407  0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d 3105 2 2023-04-21 20:26:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003621b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-21 20:26:10 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-04-21 20:26:10 +0000 UTC,LastTransitionTime:2023-04-21 20:26:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 21 20:26:10.957: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9407  081b7678-5a02-4914-9acb-3c61aaed6036 3102 1 2023-04-21 20:26:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d 0xc0016564a0 0xc0016564a1}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001656548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 21 20:26:10.957: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 21 20:26:10.957: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9407  7ccb6c35-d66f-4b65-8f66-59d7b831a508 3093 2 2023-04-21 20:26:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d 0xc001656367 0xc001656368}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001656438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 21 20:26:10.959: INFO: Pod "test-recreate-deployment-cff6dc657-xqvkb" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-xqvkb test-recreate-deployment-cff6dc657- deployment-9407  f6e9d1df-1163-4fdd-8043-49111dccddee 3104 0 2023-04-21 20:26:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 081b7678-5a02-4914-9acb-3c61aaed6036 0xc001657290 0xc001657291}] [] [{kube-controller-manager Update v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"081b7678-5a02-4914-9acb-3c61aaed6036\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jngc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jngc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-04-21 20:26:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 21 20:26:10.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9407" for this suite. 04/21/23 20:26:10.961
------------------------------
â€¢ [2.111 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:26:08.855
    Apr 21 20:26:08.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename deployment 04/21/23 20:26:08.856
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:08.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:08.864
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Apr 21 20:26:08.866: INFO: Creating deployment "test-recreate-deployment"
    Apr 21 20:26:08.869: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Apr 21 20:26:08.872: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Apr 21 20:26:10.877: INFO: Waiting deployment "test-recreate-deployment" to complete
    Apr 21 20:26:10.878: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Apr 21 20:26:10.885: INFO: Updating deployment test-recreate-deployment
    Apr 21 20:26:10.885: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 21 20:26:10.955: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-9407  0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d 3105 2 2023-04-21 20:26:08 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003621b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-21 20:26:10 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-04-21 20:26:10 +0000 UTC,LastTransitionTime:2023-04-21 20:26:08 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Apr 21 20:26:10.957: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-9407  081b7678-5a02-4914-9acb-3c61aaed6036 3102 1 2023-04-21 20:26:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d 0xc0016564a0 0xc0016564a1}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001656548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 20:26:10.957: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Apr 21 20:26:10.957: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-9407  7ccb6c35-d66f-4b65-8f66-59d7b831a508 3093 2 2023-04-21 20:26:08 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d 0xc001656367 0xc001656368}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0a59c3b1-5d91-4cc8-a02d-52ec8f98ee7d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001656438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 20:26:10.959: INFO: Pod "test-recreate-deployment-cff6dc657-xqvkb" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-xqvkb test-recreate-deployment-cff6dc657- deployment-9407  f6e9d1df-1163-4fdd-8043-49111dccddee 3104 0 2023-04-21 20:26:10 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 081b7678-5a02-4914-9acb-3c61aaed6036 0xc001657290 0xc001657291}] [] [{kube-controller-manager Update v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"081b7678-5a02-4914-9acb-3c61aaed6036\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:26:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jngc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jngc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:26:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-04-21 20:26:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:26:10.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9407" for this suite. 04/21/23 20:26:10.961
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:26:10.966
Apr 21 20:26:10.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename dns 04/21/23 20:26:10.967
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:10.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:10.975
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 04/21/23 20:26:10.977
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local;sleep 1; done
 04/21/23 20:26:10.979
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local;sleep 1; done
 04/21/23 20:26:10.979
STEP: creating a pod to probe DNS 04/21/23 20:26:10.98
STEP: submitting the pod to kubernetes 04/21/23 20:26:10.98
Apr 21 20:26:10.988: INFO: Waiting up to 15m0s for pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a" in namespace "dns-3654" to be "running"
Apr 21 20:26:10.991: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.423139ms
Apr 21 20:26:13.034: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045897376s
Apr 21 20:26:14.993: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005016008s
Apr 21 20:26:16.995: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006362524s
Apr 21 20:26:18.995: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Running", Reason="", readiness=true. Elapsed: 8.006245603s
Apr 21 20:26:18.995: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a" satisfied condition "running"
STEP: retrieving the pod 04/21/23 20:26:18.995
STEP: looking for the results for each expected name from probers 04/21/23 20:26:18.997
Apr 21 20:26:18.999: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:19.001: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:19.003: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:19.004: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:19.006: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:19.008: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:19.009: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:19.011: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:19.011: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

Apr 21 20:26:24.016: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:24.017: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:24.019: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:24.021: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:24.023: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:24.024: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:24.026: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:24.027: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:24.027: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

Apr 21 20:26:29.016: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:29.018: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:29.020: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:29.022: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:29.023: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:29.025: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:29.026: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:29.028: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:29.028: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

Apr 21 20:26:34.016: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:34.018: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:34.019: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:34.021: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:34.023: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:34.024: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:34.026: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:34.027: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:34.027: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

Apr 21 20:26:39.018: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:39.020: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:39.022: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:39.023: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:39.025: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:39.027: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:39.028: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:39.030: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
Apr 21 20:26:39.030: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

Apr 21 20:26:44.029: INFO: DNS probes using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a succeeded

STEP: deleting the pod 04/21/23 20:26:44.029
STEP: deleting the test headless service 04/21/23 20:26:44.04
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 21 20:26:44.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3654" for this suite. 04/21/23 20:26:44.053
------------------------------
â€¢ [SLOW TEST] [33.092 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:26:10.966
    Apr 21 20:26:10.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename dns 04/21/23 20:26:10.967
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:10.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:10.975
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 04/21/23 20:26:10.977
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local;sleep 1; done
     04/21/23 20:26:10.979
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3654.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local;sleep 1; done
     04/21/23 20:26:10.979
    STEP: creating a pod to probe DNS 04/21/23 20:26:10.98
    STEP: submitting the pod to kubernetes 04/21/23 20:26:10.98
    Apr 21 20:26:10.988: INFO: Waiting up to 15m0s for pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a" in namespace "dns-3654" to be "running"
    Apr 21 20:26:10.991: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.423139ms
    Apr 21 20:26:13.034: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045897376s
    Apr 21 20:26:14.993: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005016008s
    Apr 21 20:26:16.995: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006362524s
    Apr 21 20:26:18.995: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a": Phase="Running", Reason="", readiness=true. Elapsed: 8.006245603s
    Apr 21 20:26:18.995: INFO: Pod "dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a" satisfied condition "running"
    STEP: retrieving the pod 04/21/23 20:26:18.995
    STEP: looking for the results for each expected name from probers 04/21/23 20:26:18.997
    Apr 21 20:26:18.999: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:19.001: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:19.003: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:19.004: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:19.006: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:19.008: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:19.009: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:19.011: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:19.011: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

    Apr 21 20:26:24.016: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:24.017: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:24.019: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:24.021: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:24.023: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:24.024: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:24.026: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:24.027: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:24.027: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

    Apr 21 20:26:29.016: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:29.018: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:29.020: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:29.022: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:29.023: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:29.025: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:29.026: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:29.028: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:29.028: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

    Apr 21 20:26:34.016: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:34.018: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:34.019: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:34.021: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:34.023: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:34.024: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:34.026: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:34.027: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:34.027: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

    Apr 21 20:26:39.018: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:39.020: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:39.022: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:39.023: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:39.025: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:39.027: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:39.028: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:39.030: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local from pod dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a: the server could not find the requested resource (get pods dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a)
    Apr 21 20:26:39.030: INFO: Lookups using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3654.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3654.svc.cluster.local jessie_udp@dns-test-service-2.dns-3654.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3654.svc.cluster.local]

    Apr 21 20:26:44.029: INFO: DNS probes using dns-3654/dns-test-2e69bb39-21bc-4fdd-84f5-4d154104be2a succeeded

    STEP: deleting the pod 04/21/23 20:26:44.029
    STEP: deleting the test headless service 04/21/23 20:26:44.04
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:26:44.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3654" for this suite. 04/21/23 20:26:44.053
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:26:44.059
Apr 21 20:26:44.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename gc 04/21/23 20:26:44.06
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:44.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:44.07
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 04/21/23 20:26:44.072
STEP: delete the rc 04/21/23 20:26:49.08
STEP: wait for all pods to be garbage collected 04/21/23 20:26:49.083
STEP: Gathering metrics 04/21/23 20:26:54.087
Apr 21 20:26:54.100: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
Apr 21 20:26:54.102: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 1.615984ms
Apr 21 20:26:54.102: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
Apr 21 20:26:54.102: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
Apr 21 20:26:54.153: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 21 20:26:54.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6584" for this suite. 04/21/23 20:26:54.156
------------------------------
â€¢ [SLOW TEST] [10.100 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:26:44.059
    Apr 21 20:26:44.059: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename gc 04/21/23 20:26:44.06
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:44.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:44.07
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 04/21/23 20:26:44.072
    STEP: delete the rc 04/21/23 20:26:49.08
    STEP: wait for all pods to be garbage collected 04/21/23 20:26:49.083
    STEP: Gathering metrics 04/21/23 20:26:54.087
    Apr 21 20:26:54.100: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
    Apr 21 20:26:54.102: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 1.615984ms
    Apr 21 20:26:54.102: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
    Apr 21 20:26:54.102: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
    Apr 21 20:26:54.153: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:26:54.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6584" for this suite. 04/21/23 20:26:54.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:26:54.16
Apr 21 20:26:54.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-watch 04/21/23 20:26:54.16
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:54.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:54.169
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Apr 21 20:26:54.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Creating first CR  04/21/23 20:26:56.704
Apr 21 20:26:56.707: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:26:56Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:26:56Z]] name:name1 resourceVersion:3284 uid:f86af85a-cf9f-485c-89ec-e66186e3c756] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 04/21/23 20:27:06.709
Apr 21 20:27:06.714: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:27:06Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:06Z]] name:name2 resourceVersion:3312 uid:adb53ee3-81cf-4e01-a670-9baacae177b2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 04/21/23 20:27:16.716
Apr 21 20:27:16.720: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:26:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:16Z]] name:name1 resourceVersion:3322 uid:f86af85a-cf9f-485c-89ec-e66186e3c756] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 04/21/23 20:27:26.721
Apr 21 20:27:26.726: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:27:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:26Z]] name:name2 resourceVersion:3332 uid:adb53ee3-81cf-4e01-a670-9baacae177b2] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 04/21/23 20:27:36.726
Apr 21 20:27:36.731: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:26:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:16Z]] name:name1 resourceVersion:3342 uid:f86af85a-cf9f-485c-89ec-e66186e3c756] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 04/21/23 20:27:46.732
Apr 21 20:27:46.737: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:27:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:26Z]] name:name2 resourceVersion:3352 uid:adb53ee3-81cf-4e01-a670-9baacae177b2] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:27:57.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-987" for this suite. 04/21/23 20:27:57.248
------------------------------
â€¢ [SLOW TEST] [63.092 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:26:54.16
    Apr 21 20:26:54.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-watch 04/21/23 20:26:54.16
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:26:54.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:26:54.169
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Apr 21 20:26:54.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Creating first CR  04/21/23 20:26:56.704
    Apr 21 20:26:56.707: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:26:56Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:26:56Z]] name:name1 resourceVersion:3284 uid:f86af85a-cf9f-485c-89ec-e66186e3c756] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 04/21/23 20:27:06.709
    Apr 21 20:27:06.714: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:27:06Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:06Z]] name:name2 resourceVersion:3312 uid:adb53ee3-81cf-4e01-a670-9baacae177b2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 04/21/23 20:27:16.716
    Apr 21 20:27:16.720: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:26:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:16Z]] name:name1 resourceVersion:3322 uid:f86af85a-cf9f-485c-89ec-e66186e3c756] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 04/21/23 20:27:26.721
    Apr 21 20:27:26.726: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:27:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:26Z]] name:name2 resourceVersion:3332 uid:adb53ee3-81cf-4e01-a670-9baacae177b2] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 04/21/23 20:27:36.726
    Apr 21 20:27:36.731: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:26:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:16Z]] name:name1 resourceVersion:3342 uid:f86af85a-cf9f-485c-89ec-e66186e3c756] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 04/21/23 20:27:46.732
    Apr 21 20:27:46.737: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-21T20:27:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-21T20:27:26Z]] name:name2 resourceVersion:3352 uid:adb53ee3-81cf-4e01-a670-9baacae177b2] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:27:57.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-987" for this suite. 04/21/23 20:27:57.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:27:57.252
Apr 21 20:27:57.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:27:57.253
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:27:57.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:27:57.262
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Apr 21 20:27:57.267: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112" in namespace "kubelet-test-6185" to be "running and ready"
Apr 21 20:27:57.269: INFO: Pod "busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112": Phase="Pending", Reason="", readiness=false. Elapsed: 1.416919ms
Apr 21 20:27:57.269: INFO: The phase of Pod busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:27:59.271: INFO: Pod "busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112": Phase="Running", Reason="", readiness=true. Elapsed: 2.003896669s
Apr 21 20:27:59.271: INFO: The phase of Pod busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112 is Running (Ready = true)
Apr 21 20:27:59.271: INFO: Pod "busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:27:59.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6185" for this suite. 04/21/23 20:27:59.284
------------------------------
â€¢ [2.037 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:27:57.252
    Apr 21 20:27:57.252: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:27:57.253
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:27:57.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:27:57.262
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Apr 21 20:27:57.267: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112" in namespace "kubelet-test-6185" to be "running and ready"
    Apr 21 20:27:57.269: INFO: Pod "busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112": Phase="Pending", Reason="", readiness=false. Elapsed: 1.416919ms
    Apr 21 20:27:57.269: INFO: The phase of Pod busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:27:59.271: INFO: Pod "busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112": Phase="Running", Reason="", readiness=true. Elapsed: 2.003896669s
    Apr 21 20:27:59.271: INFO: The phase of Pod busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112 is Running (Ready = true)
    Apr 21 20:27:59.271: INFO: Pod "busybox-readonly-fsba9911e5-8c66-494f-8a13-db5a99639112" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:27:59.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6185" for this suite. 04/21/23 20:27:59.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:27:59.289
Apr 21 20:27:59.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-preemption 04/21/23 20:27:59.29
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:27:59.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:27:59.298
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 21 20:27:59.309: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 21 20:28:59.322: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 04/21/23 20:28:59.324
Apr 21 20:28:59.338: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr 21 20:28:59.343: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr 21 20:28:59.354: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr 21 20:28:59.358: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 04/21/23 20:28:59.358
Apr 21 20:28:59.359: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6845" to be "running"
Apr 21 20:28:59.360: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.757328ms
Apr 21 20:29:01.364: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005785444s
Apr 21 20:29:01.364: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Apr 21 20:29:01.364: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6845" to be "running"
Apr 21 20:29:01.367: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.54347ms
Apr 21 20:29:01.367: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 21 20:29:01.367: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6845" to be "running"
Apr 21 20:29:01.369: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.978196ms
Apr 21 20:29:01.369: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 21 20:29:01.369: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6845" to be "running"
Apr 21 20:29:01.371: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.060387ms
Apr 21 20:29:01.371: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 04/21/23 20:29:01.371
Apr 21 20:29:01.375: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6845" to be "running"
Apr 21 20:29:01.377: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.804244ms
Apr 21 20:29:03.381: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005344035s
Apr 21 20:29:05.380: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004220068s
Apr 21 20:29:07.381: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005215205s
Apr 21 20:29:07.381: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:29:07.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6845" for this suite. 04/21/23 20:29:07.406
------------------------------
â€¢ [SLOW TEST] [68.121 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:27:59.289
    Apr 21 20:27:59.289: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-preemption 04/21/23 20:27:59.29
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:27:59.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:27:59.298
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 21 20:27:59.309: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 21 20:28:59.322: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 04/21/23 20:28:59.324
    Apr 21 20:28:59.338: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Apr 21 20:28:59.343: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Apr 21 20:28:59.354: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Apr 21 20:28:59.358: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 04/21/23 20:28:59.358
    Apr 21 20:28:59.359: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-6845" to be "running"
    Apr 21 20:28:59.360: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.757328ms
    Apr 21 20:29:01.364: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005785444s
    Apr 21 20:29:01.364: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Apr 21 20:29:01.364: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-6845" to be "running"
    Apr 21 20:29:01.367: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.54347ms
    Apr 21 20:29:01.367: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 21 20:29:01.367: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-6845" to be "running"
    Apr 21 20:29:01.369: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.978196ms
    Apr 21 20:29:01.369: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 21 20:29:01.369: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-6845" to be "running"
    Apr 21 20:29:01.371: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.060387ms
    Apr 21 20:29:01.371: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 04/21/23 20:29:01.371
    Apr 21 20:29:01.375: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-6845" to be "running"
    Apr 21 20:29:01.377: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.804244ms
    Apr 21 20:29:03.381: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005344035s
    Apr 21 20:29:05.380: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004220068s
    Apr 21 20:29:07.381: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005215205s
    Apr 21 20:29:07.381: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:29:07.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6845" for this suite. 04/21/23 20:29:07.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:29:07.413
Apr 21 20:29:07.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename disruption 04/21/23 20:29:07.414
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:07.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:07.423
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 04/21/23 20:29:07.425
STEP: Waiting for the pdb to be processed 04/21/23 20:29:07.429
STEP: updating the pdb 04/21/23 20:29:09.433
STEP: Waiting for the pdb to be processed 04/21/23 20:29:09.438
STEP: patching the pdb 04/21/23 20:29:11.443
STEP: Waiting for the pdb to be processed 04/21/23 20:29:11.449
STEP: Waiting for the pdb to be deleted 04/21/23 20:29:13.456
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 21 20:29:13.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7070" for this suite. 04/21/23 20:29:13.46
------------------------------
â€¢ [SLOW TEST] [6.050 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:29:07.413
    Apr 21 20:29:07.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename disruption 04/21/23 20:29:07.414
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:07.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:07.423
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 04/21/23 20:29:07.425
    STEP: Waiting for the pdb to be processed 04/21/23 20:29:07.429
    STEP: updating the pdb 04/21/23 20:29:09.433
    STEP: Waiting for the pdb to be processed 04/21/23 20:29:09.438
    STEP: patching the pdb 04/21/23 20:29:11.443
    STEP: Waiting for the pdb to be processed 04/21/23 20:29:11.449
    STEP: Waiting for the pdb to be deleted 04/21/23 20:29:13.456
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:29:13.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7070" for this suite. 04/21/23 20:29:13.46
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:29:13.464
Apr 21 20:29:13.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename job 04/21/23 20:29:13.465
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:13.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:13.474
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 04/21/23 20:29:13.476
STEP: Ensuring job reaches completions 04/21/23 20:29:13.479
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 21 20:29:25.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7854" for this suite. 04/21/23 20:29:25.484
------------------------------
â€¢ [SLOW TEST] [12.023 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:29:13.464
    Apr 21 20:29:13.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename job 04/21/23 20:29:13.465
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:13.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:13.474
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 04/21/23 20:29:13.476
    STEP: Ensuring job reaches completions 04/21/23 20:29:13.479
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:29:25.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7854" for this suite. 04/21/23 20:29:25.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:29:25.487
Apr 21 20:29:25.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename deployment 04/21/23 20:29:25.488
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:25.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:25.498
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 04/21/23 20:29:25.501
Apr 21 20:29:25.501: INFO: Creating simple deployment test-deployment-dnb82
Apr 21 20:29:25.509: INFO: deployment "test-deployment-dnb82" doesn't have the required revision set
STEP: Getting /status 04/21/23 20:29:27.516
Apr 21 20:29:27.518: INFO: Deployment test-deployment-dnb82 has Conditions: [{Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 04/21/23 20:29:27.518
Apr 21 20:29:27.525: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 20, 29, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 20, 29, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 20, 29, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 20, 29, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-dnb82-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 04/21/23 20:29:27.525
Apr 21 20:29:27.526: INFO: Observed &Deployment event: ADDED
Apr 21 20:29:27.526: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dnb82-54bc444df"}
Apr 21 20:29:27.526: INFO: Observed &Deployment event: MODIFIED
Apr 21 20:29:27.526: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dnb82-54bc444df"}
Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 21 20:29:27.527: INFO: Observed &Deployment event: MODIFIED
Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dnb82-54bc444df" is progressing.}
Apr 21 20:29:27.527: INFO: Observed &Deployment event: MODIFIED
Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}
Apr 21 20:29:27.527: INFO: Observed &Deployment event: MODIFIED
Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}
Apr 21 20:29:27.527: INFO: Found Deployment test-deployment-dnb82 in namespace deployment-4618 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 21 20:29:27.527: INFO: Deployment test-deployment-dnb82 has an updated status
STEP: patching the Statefulset Status 04/21/23 20:29:27.527
Apr 21 20:29:27.527: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 21 20:29:27.531: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 04/21/23 20:29:27.531
Apr 21 20:29:27.532: INFO: Observed &Deployment event: ADDED
Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dnb82-54bc444df"}
Apr 21 20:29:27.532: INFO: Observed &Deployment event: MODIFIED
Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dnb82-54bc444df"}
Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 21 20:29:27.532: INFO: Observed &Deployment event: MODIFIED
Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dnb82-54bc444df" is progressing.}
Apr 21 20:29:27.532: INFO: Observed &Deployment event: MODIFIED
Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}
Apr 21 20:29:27.533: INFO: Observed &Deployment event: MODIFIED
Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}
Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 21 20:29:27.533: INFO: Observed &Deployment event: MODIFIED
Apr 21 20:29:27.533: INFO: Found deployment test-deployment-dnb82 in namespace deployment-4618 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Apr 21 20:29:27.533: INFO: Deployment test-deployment-dnb82 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 21 20:29:27.536: INFO: Deployment "test-deployment-dnb82":
&Deployment{ObjectMeta:{test-deployment-dnb82  deployment-4618  fca15e7d-07e2-4a56-a7b4-9bcb761fccb9 3711 1 2023-04-21 20:29:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-21 20:29:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-21 20:29:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-21 20:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00443ad68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-dnb82-54bc444df",LastUpdateTime:2023-04-21 20:29:27 +0000 UTC,LastTransitionTime:2023-04-21 20:29:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 21 20:29:27.538: INFO: New ReplicaSet "test-deployment-dnb82-54bc444df" of Deployment "test-deployment-dnb82":
&ReplicaSet{ObjectMeta:{test-deployment-dnb82-54bc444df  deployment-4618  064c6c37-d65d-49cb-a4f4-4ef14f48cadd 3707 1 2023-04-21 20:29:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-dnb82 fca15e7d-07e2-4a56-a7b4-9bcb761fccb9 0xc00443b127 0xc00443b128}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:29:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fca15e7d-07e2-4a56-a7b4-9bcb761fccb9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:29:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00443b1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 21 20:29:27.539: INFO: Pod "test-deployment-dnb82-54bc444df-c6z62" is available:
&Pod{ObjectMeta:{test-deployment-dnb82-54bc444df-c6z62 test-deployment-dnb82-54bc444df- deployment-4618  c55e6df6-378e-4a3a-98a1-a34cb44f9d94 3706 0 2023-04-21 20:29:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-dnb82-54bc444df 064c6c37-d65d-49cb-a4f4-4ef14f48cadd 0xc00292f037 0xc00292f038}] [] [{kube-controller-manager Update v1 2023-04-21 20:29:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"064c6c37-d65d-49cb-a4f4-4ef14f48cadd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:29:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhr8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhr8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:29:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:29:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:29:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:29:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.59,StartTime:2023-04-21 20:29:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:29:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://45626785dff8fc69c20cc73ba3dd7748bb4981cda0138c10042be4a48f163630,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 21 20:29:27.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4618" for this suite. 04/21/23 20:29:27.541
------------------------------
â€¢ [2.058 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:29:25.487
    Apr 21 20:29:25.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename deployment 04/21/23 20:29:25.488
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:25.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:25.498
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 04/21/23 20:29:25.501
    Apr 21 20:29:25.501: INFO: Creating simple deployment test-deployment-dnb82
    Apr 21 20:29:25.509: INFO: deployment "test-deployment-dnb82" doesn't have the required revision set
    STEP: Getting /status 04/21/23 20:29:27.516
    Apr 21 20:29:27.518: INFO: Deployment test-deployment-dnb82 has Conditions: [{Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 04/21/23 20:29:27.518
    Apr 21 20:29:27.525: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 20, 29, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 20, 29, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 20, 29, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 20, 29, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-dnb82-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 04/21/23 20:29:27.525
    Apr 21 20:29:27.526: INFO: Observed &Deployment event: ADDED
    Apr 21 20:29:27.526: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dnb82-54bc444df"}
    Apr 21 20:29:27.526: INFO: Observed &Deployment event: MODIFIED
    Apr 21 20:29:27.526: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dnb82-54bc444df"}
    Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 21 20:29:27.527: INFO: Observed &Deployment event: MODIFIED
    Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dnb82-54bc444df" is progressing.}
    Apr 21 20:29:27.527: INFO: Observed &Deployment event: MODIFIED
    Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}
    Apr 21 20:29:27.527: INFO: Observed &Deployment event: MODIFIED
    Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 21 20:29:27.527: INFO: Observed Deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}
    Apr 21 20:29:27.527: INFO: Found Deployment test-deployment-dnb82 in namespace deployment-4618 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 21 20:29:27.527: INFO: Deployment test-deployment-dnb82 has an updated status
    STEP: patching the Statefulset Status 04/21/23 20:29:27.527
    Apr 21 20:29:27.527: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr 21 20:29:27.531: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 04/21/23 20:29:27.531
    Apr 21 20:29:27.532: INFO: Observed &Deployment event: ADDED
    Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dnb82-54bc444df"}
    Apr 21 20:29:27.532: INFO: Observed &Deployment event: MODIFIED
    Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dnb82-54bc444df"}
    Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 21 20:29:27.532: INFO: Observed &Deployment event: MODIFIED
    Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 21 20:29:27.532: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:25 +0000 UTC 2023-04-21 20:29:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dnb82-54bc444df" is progressing.}
    Apr 21 20:29:27.532: INFO: Observed &Deployment event: MODIFIED
    Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}
    Apr 21 20:29:27.533: INFO: Observed &Deployment event: MODIFIED
    Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-21 20:29:27 +0000 UTC 2023-04-21 20:29:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dnb82-54bc444df" has successfully progressed.}
    Apr 21 20:29:27.533: INFO: Observed deployment test-deployment-dnb82 in namespace deployment-4618 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 21 20:29:27.533: INFO: Observed &Deployment event: MODIFIED
    Apr 21 20:29:27.533: INFO: Found deployment test-deployment-dnb82 in namespace deployment-4618 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Apr 21 20:29:27.533: INFO: Deployment test-deployment-dnb82 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 21 20:29:27.536: INFO: Deployment "test-deployment-dnb82":
    &Deployment{ObjectMeta:{test-deployment-dnb82  deployment-4618  fca15e7d-07e2-4a56-a7b4-9bcb761fccb9 3711 1 2023-04-21 20:29:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-21 20:29:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-21 20:29:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-21 20:29:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00443ad68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-dnb82-54bc444df",LastUpdateTime:2023-04-21 20:29:27 +0000 UTC,LastTransitionTime:2023-04-21 20:29:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 21 20:29:27.538: INFO: New ReplicaSet "test-deployment-dnb82-54bc444df" of Deployment "test-deployment-dnb82":
    &ReplicaSet{ObjectMeta:{test-deployment-dnb82-54bc444df  deployment-4618  064c6c37-d65d-49cb-a4f4-4ef14f48cadd 3707 1 2023-04-21 20:29:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-dnb82 fca15e7d-07e2-4a56-a7b4-9bcb761fccb9 0xc00443b127 0xc00443b128}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:29:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fca15e7d-07e2-4a56-a7b4-9bcb761fccb9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:29:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00443b1d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 20:29:27.539: INFO: Pod "test-deployment-dnb82-54bc444df-c6z62" is available:
    &Pod{ObjectMeta:{test-deployment-dnb82-54bc444df-c6z62 test-deployment-dnb82-54bc444df- deployment-4618  c55e6df6-378e-4a3a-98a1-a34cb44f9d94 3706 0 2023-04-21 20:29:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-dnb82-54bc444df 064c6c37-d65d-49cb-a4f4-4ef14f48cadd 0xc00292f037 0xc00292f038}] [] [{kube-controller-manager Update v1 2023-04-21 20:29:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"064c6c37-d65d-49cb-a4f4-4ef14f48cadd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:29:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.59\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhr8j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhr8j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:29:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:29:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:29:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:29:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.59,StartTime:2023-04-21 20:29:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:29:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://45626785dff8fc69c20cc73ba3dd7748bb4981cda0138c10042be4a48f163630,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.59,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:29:27.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4618" for this suite. 04/21/23 20:29:27.541
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:29:27.546
Apr 21 20:29:27.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 20:29:27.547
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:27.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:27.555
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 04/21/23 20:29:27.557
Apr 21 20:29:27.562: INFO: Waiting up to 5m0s for pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791" in namespace "emptydir-1450" to be "Succeeded or Failed"
Apr 21 20:29:27.564: INFO: Pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455863ms
Apr 21 20:29:29.567: INFO: Pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004328155s
Apr 21 20:29:31.568: INFO: Pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005530757s
STEP: Saw pod success 04/21/23 20:29:31.568
Apr 21 20:29:31.568: INFO: Pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791" satisfied condition "Succeeded or Failed"
Apr 21 20:29:31.570: INFO: Trying to get logs from node k8sconformance-m02 pod pod-f347c7e0-45bd-48cc-92e0-9f0470863791 container test-container: <nil>
STEP: delete the pod 04/21/23 20:29:31.582
Apr 21 20:29:31.589: INFO: Waiting for pod pod-f347c7e0-45bd-48cc-92e0-9f0470863791 to disappear
Apr 21 20:29:31.591: INFO: Pod pod-f347c7e0-45bd-48cc-92e0-9f0470863791 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 20:29:31.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1450" for this suite. 04/21/23 20:29:31.593
------------------------------
â€¢ [4.050 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:29:27.546
    Apr 21 20:29:27.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 20:29:27.547
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:27.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:27.555
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 04/21/23 20:29:27.557
    Apr 21 20:29:27.562: INFO: Waiting up to 5m0s for pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791" in namespace "emptydir-1450" to be "Succeeded or Failed"
    Apr 21 20:29:27.564: INFO: Pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455863ms
    Apr 21 20:29:29.567: INFO: Pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004328155s
    Apr 21 20:29:31.568: INFO: Pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005530757s
    STEP: Saw pod success 04/21/23 20:29:31.568
    Apr 21 20:29:31.568: INFO: Pod "pod-f347c7e0-45bd-48cc-92e0-9f0470863791" satisfied condition "Succeeded or Failed"
    Apr 21 20:29:31.570: INFO: Trying to get logs from node k8sconformance-m02 pod pod-f347c7e0-45bd-48cc-92e0-9f0470863791 container test-container: <nil>
    STEP: delete the pod 04/21/23 20:29:31.582
    Apr 21 20:29:31.589: INFO: Waiting for pod pod-f347c7e0-45bd-48cc-92e0-9f0470863791 to disappear
    Apr 21 20:29:31.591: INFO: Pod pod-f347c7e0-45bd-48cc-92e0-9f0470863791 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:29:31.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1450" for this suite. 04/21/23 20:29:31.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:29:31.596
Apr 21 20:29:31.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 20:29:31.597
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:31.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:31.606
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Apr 21 20:29:31.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 create -f -'
Apr 21 20:29:32.145: INFO: stderr: ""
Apr 21 20:29:32.145: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr 21 20:29:32.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 create -f -'
Apr 21 20:29:32.299: INFO: stderr: ""
Apr 21 20:29:32.299: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/21/23 20:29:32.299
Apr 21 20:29:33.303: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 21 20:29:33.303: INFO: Found 1 / 1
Apr 21 20:29:33.303: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 21 20:29:33.304: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 21 20:29:33.305: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 21 20:29:33.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe pod agnhost-primary-z44v4'
Apr 21 20:29:33.362: INFO: stderr: ""
Apr 21 20:29:33.363: INFO: stdout: "Name:             agnhost-primary-z44v4\nNamespace:        kubectl-3048\nPriority:         0\nService Account:  default\nNode:             k8sconformance-m02/192.168.49.3\nStart Time:       Fri, 21 Apr 2023 20:29:32 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.61\nIPs:\n  IP:           10.244.1.61\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://0f0d748318d78f7684455a4eb80b7699253a32bdddc0425c4b884bc55daabb7b\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 21 Apr 2023 20:29:32 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rsnql (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-rsnql:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-3048/agnhost-primary-z44v4 to k8sconformance-m02\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Apr 21 20:29:33.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe rc agnhost-primary'
Apr 21 20:29:33.431: INFO: stderr: ""
Apr 21 20:29:33.431: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3048\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-z44v4\n"
Apr 21 20:29:33.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe service agnhost-primary'
Apr 21 20:29:33.487: INFO: stderr: ""
Apr 21 20:29:33.487: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3048\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.42.134\nIPs:               10.96.42.134\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.61:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 21 20:29:33.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe node k8sconformance'
Apr 21 20:29:33.558: INFO: stderr: ""
Apr 21 20:29:33.558: INFO: stdout: "Name:               k8sconformance\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8sconformance\n                    kubernetes.io/os=linux\n                    minikube.k8s.io/commit=ba71ee071fe416ac12db4a7c84ed9cefa6e4ef14\n                    minikube.k8s.io/name=k8sconformance\n                    minikube.k8s.io/primary=true\n                    minikube.k8s.io/updated_at=2023_04_21T20_12_52_0700\n                    minikube.k8s.io/version=v1.30.1\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 21 Apr 2023 20:12:48 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8sconformance\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 21 Apr 2023 20:29:23 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 21 Apr 2023 20:26:59 +0000   Fri, 21 Apr 2023 20:12:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 21 Apr 2023 20:26:59 +0000   Fri, 21 Apr 2023 20:12:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 21 Apr 2023 20:26:59 +0000   Fri, 21 Apr 2023 20:12:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 21 Apr 2023 20:26:59 +0000   Fri, 21 Apr 2023 20:13:01 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.49.2\n  Hostname:    k8sconformance\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      304681132Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 32871732Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    8\n  ephemeral-storage:      304681132Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 32871732Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 5e038c95692c4e038504c10142c12668\n  System UUID:                7521f7c6-f9de-485b-a085-34152ae1f15d\n  Boot ID:                    5677af5e-2872-4282-bc88-6051c1587c98\n  Kernel Version:             5.15.0-1032-gcp\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://23.0.3\n  Kubelet Version:            v1.26.3\n  Kube-Proxy Version:         v1.26.3\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-787d4945fb-bp78j                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     16m\n  kube-system                 etcd-k8sconformance                                        100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         16m\n  kube-system                 kindnet-6bcq2                                              100m (1%)     100m (1%)   50Mi (0%)        50Mi (0%)      16m\n  kube-system                 kube-apiserver-k8sconformance                              250m (3%)     0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 kube-controller-manager-k8sconformance                     200m (2%)     0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 kube-proxy-k9znz                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 kube-scheduler-k8sconformance                              100m (1%)     0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 storage-provisioner                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    850m (10%)  100m (1%)\n  memory                 220Mi (0%)  220Mi (0%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type    Reason                   Age   From             Message\n  ----    ------                   ----  ----             -------\n  Normal  Starting                 16m   kube-proxy       \n  Normal  Starting                 16m   kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  16m   kubelet          Node k8sconformance status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    16m   kubelet          Node k8sconformance status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     16m   kubelet          Node k8sconformance status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             16m   kubelet          Node k8sconformance status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  16m   kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                16m   kubelet          Node k8sconformance status is now: NodeReady\n  Normal  RegisteredNode           16m   node-controller  Node k8sconformance event: Registered Node k8sconformance in Controller\n"
Apr 21 20:29:33.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe namespace kubectl-3048'
Apr 21 20:29:33.619: INFO: stderr: ""
Apr 21 20:29:33.619: INFO: stdout: "Name:         kubectl-3048\nLabels:       e2e-framework=kubectl\n              e2e-run=3e989928-34dd-48c8-8a06-ed2a4ab89908\n              kubernetes.io/metadata.name=kubectl-3048\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 20:29:33.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3048" for this suite. 04/21/23 20:29:33.621
------------------------------
â€¢ [2.028 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:29:31.596
    Apr 21 20:29:31.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 20:29:31.597
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:31.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:31.606
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Apr 21 20:29:31.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 create -f -'
    Apr 21 20:29:32.145: INFO: stderr: ""
    Apr 21 20:29:32.145: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Apr 21 20:29:32.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 create -f -'
    Apr 21 20:29:32.299: INFO: stderr: ""
    Apr 21 20:29:32.299: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/21/23 20:29:32.299
    Apr 21 20:29:33.303: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 21 20:29:33.303: INFO: Found 1 / 1
    Apr 21 20:29:33.303: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Apr 21 20:29:33.304: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 21 20:29:33.305: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr 21 20:29:33.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe pod agnhost-primary-z44v4'
    Apr 21 20:29:33.362: INFO: stderr: ""
    Apr 21 20:29:33.363: INFO: stdout: "Name:             agnhost-primary-z44v4\nNamespace:        kubectl-3048\nPriority:         0\nService Account:  default\nNode:             k8sconformance-m02/192.168.49.3\nStart Time:       Fri, 21 Apr 2023 20:29:32 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.61\nIPs:\n  IP:           10.244.1.61\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://0f0d748318d78f7684455a4eb80b7699253a32bdddc0425c4b884bc55daabb7b\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 21 Apr 2023 20:29:32 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rsnql (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-rsnql:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-3048/agnhost-primary-z44v4 to k8sconformance-m02\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Apr 21 20:29:33.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe rc agnhost-primary'
    Apr 21 20:29:33.431: INFO: stderr: ""
    Apr 21 20:29:33.431: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3048\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-z44v4\n"
    Apr 21 20:29:33.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe service agnhost-primary'
    Apr 21 20:29:33.487: INFO: stderr: ""
    Apr 21 20:29:33.487: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3048\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.96.42.134\nIPs:               10.96.42.134\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.61:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Apr 21 20:29:33.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe node k8sconformance'
    Apr 21 20:29:33.558: INFO: stderr: ""
    Apr 21 20:29:33.558: INFO: stdout: "Name:               k8sconformance\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8sconformance\n                    kubernetes.io/os=linux\n                    minikube.k8s.io/commit=ba71ee071fe416ac12db4a7c84ed9cefa6e4ef14\n                    minikube.k8s.io/name=k8sconformance\n                    minikube.k8s.io/primary=true\n                    minikube.k8s.io/updated_at=2023_04_21T20_12_52_0700\n                    minikube.k8s.io/version=v1.30.1\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 21 Apr 2023 20:12:48 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8sconformance\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 21 Apr 2023 20:29:23 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 21 Apr 2023 20:26:59 +0000   Fri, 21 Apr 2023 20:12:46 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 21 Apr 2023 20:26:59 +0000   Fri, 21 Apr 2023 20:12:46 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 21 Apr 2023 20:26:59 +0000   Fri, 21 Apr 2023 20:12:46 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 21 Apr 2023 20:26:59 +0000   Fri, 21 Apr 2023 20:13:01 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.49.2\n  Hostname:    k8sconformance\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      304681132Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 32871732Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    8\n  ephemeral-storage:      304681132Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 32871732Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 5e038c95692c4e038504c10142c12668\n  System UUID:                7521f7c6-f9de-485b-a085-34152ae1f15d\n  Boot ID:                    5677af5e-2872-4282-bc88-6051c1587c98\n  Kernel Version:             5.15.0-1032-gcp\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://23.0.3\n  Kubelet Version:            v1.26.3\n  Kube-Proxy Version:         v1.26.3\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-787d4945fb-bp78j                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     16m\n  kube-system                 etcd-k8sconformance                                        100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         16m\n  kube-system                 kindnet-6bcq2                                              100m (1%)     100m (1%)   50Mi (0%)        50Mi (0%)      16m\n  kube-system                 kube-apiserver-k8sconformance                              250m (3%)     0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 kube-controller-manager-k8sconformance                     200m (2%)     0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 kube-proxy-k9znz                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 kube-scheduler-k8sconformance                              100m (1%)     0 (0%)      0 (0%)           0 (0%)         16m\n  kube-system                 storage-provisioner                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         16m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         15m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    850m (10%)  100m (1%)\n  memory                 220Mi (0%)  220Mi (0%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type    Reason                   Age   From             Message\n  ----    ------                   ----  ----             -------\n  Normal  Starting                 16m   kube-proxy       \n  Normal  Starting                 16m   kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  16m   kubelet          Node k8sconformance status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    16m   kubelet          Node k8sconformance status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     16m   kubelet          Node k8sconformance status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             16m   kubelet          Node k8sconformance status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  16m   kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                16m   kubelet          Node k8sconformance status is now: NodeReady\n  Normal  RegisteredNode           16m   node-controller  Node k8sconformance event: Registered Node k8sconformance in Controller\n"
    Apr 21 20:29:33.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3048 describe namespace kubectl-3048'
    Apr 21 20:29:33.619: INFO: stderr: ""
    Apr 21 20:29:33.619: INFO: stdout: "Name:         kubectl-3048\nLabels:       e2e-framework=kubectl\n              e2e-run=3e989928-34dd-48c8-8a06-ed2a4ab89908\n              kubernetes.io/metadata.name=kubectl-3048\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:29:33.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3048" for this suite. 04/21/23 20:29:33.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:29:33.625
Apr 21 20:29:33.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 20:29:33.626
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:33.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:33.636
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 04/21/23 20:29:33.638
Apr 21 20:29:33.642: INFO: created test-pod-1
Apr 21 20:29:33.647: INFO: created test-pod-2
Apr 21 20:29:33.651: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 04/21/23 20:29:33.651
Apr 21 20:29:33.651: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6290' to be running and ready
Apr 21 20:29:33.658: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr 21 20:29:33.658: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr 21 20:29:33.658: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr 21 20:29:33.658: INFO: 0 / 3 pods in namespace 'pods-6290' are running and ready (0 seconds elapsed)
Apr 21 20:29:33.658: INFO: expected 0 pod replicas in namespace 'pods-6290', 0 are Running and Ready.
Apr 21 20:29:33.658: INFO: POD         NODE                PHASE    GRACE  CONDITIONS
Apr 21 20:29:33.658: INFO: test-pod-1  k8sconformance-m02  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC  }]
Apr 21 20:29:33.659: INFO: test-pod-2  k8sconformance-m02  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC  }]
Apr 21 20:29:33.659: INFO: test-pod-3  k8sconformance-m02  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC  }]
Apr 21 20:29:33.659: INFO: 
Apr 21 20:29:35.665: INFO: 3 / 3 pods in namespace 'pods-6290' are running and ready (2 seconds elapsed)
Apr 21 20:29:35.665: INFO: expected 0 pod replicas in namespace 'pods-6290', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 04/21/23 20:29:35.677
Apr 21 20:29:35.678: INFO: Pod quantity 3 is different from expected quantity 0
Apr 21 20:29:36.681: INFO: Pod quantity 3 is different from expected quantity 0
Apr 21 20:29:37.681: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 20:29:38.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6290" for this suite. 04/21/23 20:29:38.683
------------------------------
â€¢ [SLOW TEST] [5.062 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:29:33.625
    Apr 21 20:29:33.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 20:29:33.626
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:33.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:33.636
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 04/21/23 20:29:33.638
    Apr 21 20:29:33.642: INFO: created test-pod-1
    Apr 21 20:29:33.647: INFO: created test-pod-2
    Apr 21 20:29:33.651: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 04/21/23 20:29:33.651
    Apr 21 20:29:33.651: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6290' to be running and ready
    Apr 21 20:29:33.658: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr 21 20:29:33.658: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr 21 20:29:33.658: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr 21 20:29:33.658: INFO: 0 / 3 pods in namespace 'pods-6290' are running and ready (0 seconds elapsed)
    Apr 21 20:29:33.658: INFO: expected 0 pod replicas in namespace 'pods-6290', 0 are Running and Ready.
    Apr 21 20:29:33.658: INFO: POD         NODE                PHASE    GRACE  CONDITIONS
    Apr 21 20:29:33.658: INFO: test-pod-1  k8sconformance-m02  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC  }]
    Apr 21 20:29:33.659: INFO: test-pod-2  k8sconformance-m02  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC  }]
    Apr 21 20:29:33.659: INFO: test-pod-3  k8sconformance-m02  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 20:29:33 +0000 UTC  }]
    Apr 21 20:29:33.659: INFO: 
    Apr 21 20:29:35.665: INFO: 3 / 3 pods in namespace 'pods-6290' are running and ready (2 seconds elapsed)
    Apr 21 20:29:35.665: INFO: expected 0 pod replicas in namespace 'pods-6290', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 04/21/23 20:29:35.677
    Apr 21 20:29:35.678: INFO: Pod quantity 3 is different from expected quantity 0
    Apr 21 20:29:36.681: INFO: Pod quantity 3 is different from expected quantity 0
    Apr 21 20:29:37.681: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:29:38.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6290" for this suite. 04/21/23 20:29:38.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:29:38.687
Apr 21 20:29:38.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename subpath 04/21/23 20:29:38.688
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:38.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:38.746
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/21/23 20:29:38.748
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-jcbm 04/21/23 20:29:38.753
STEP: Creating a pod to test atomic-volume-subpath 04/21/23 20:29:38.753
Apr 21 20:29:38.758: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jcbm" in namespace "subpath-9419" to be "Succeeded or Failed"
Apr 21 20:29:38.759: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.538212ms
Apr 21 20:29:40.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004895636s
Apr 21 20:29:42.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 4.004462156s
Apr 21 20:29:44.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 6.004483116s
Apr 21 20:29:46.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 8.005540657s
Apr 21 20:29:48.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 10.00477293s
Apr 21 20:29:50.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 12.005090262s
Apr 21 20:29:52.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 14.004019437s
Apr 21 20:29:54.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 16.004256781s
Apr 21 20:29:56.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 18.004448246s
Apr 21 20:29:58.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 20.005551012s
Apr 21 20:30:00.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=false. Elapsed: 22.004572228s
Apr 21 20:30:02.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005609219s
STEP: Saw pod success 04/21/23 20:30:02.763
Apr 21 20:30:02.763: INFO: Pod "pod-subpath-test-secret-jcbm" satisfied condition "Succeeded or Failed"
Apr 21 20:30:02.765: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-secret-jcbm container test-container-subpath-secret-jcbm: <nil>
STEP: delete the pod 04/21/23 20:30:02.771
Apr 21 20:30:02.777: INFO: Waiting for pod pod-subpath-test-secret-jcbm to disappear
Apr 21 20:30:02.778: INFO: Pod pod-subpath-test-secret-jcbm no longer exists
STEP: Deleting pod pod-subpath-test-secret-jcbm 04/21/23 20:30:02.778
Apr 21 20:30:02.778: INFO: Deleting pod "pod-subpath-test-secret-jcbm" in namespace "subpath-9419"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:02.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9419" for this suite. 04/21/23 20:30:02.782
------------------------------
â€¢ [SLOW TEST] [24.097 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:29:38.687
    Apr 21 20:29:38.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename subpath 04/21/23 20:29:38.688
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:29:38.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:29:38.746
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/21/23 20:29:38.748
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-jcbm 04/21/23 20:29:38.753
    STEP: Creating a pod to test atomic-volume-subpath 04/21/23 20:29:38.753
    Apr 21 20:29:38.758: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jcbm" in namespace "subpath-9419" to be "Succeeded or Failed"
    Apr 21 20:29:38.759: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.538212ms
    Apr 21 20:29:40.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004895636s
    Apr 21 20:29:42.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 4.004462156s
    Apr 21 20:29:44.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 6.004483116s
    Apr 21 20:29:46.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 8.005540657s
    Apr 21 20:29:48.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 10.00477293s
    Apr 21 20:29:50.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 12.005090262s
    Apr 21 20:29:52.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 14.004019437s
    Apr 21 20:29:54.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 16.004256781s
    Apr 21 20:29:56.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 18.004448246s
    Apr 21 20:29:58.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=true. Elapsed: 20.005551012s
    Apr 21 20:30:00.762: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Running", Reason="", readiness=false. Elapsed: 22.004572228s
    Apr 21 20:30:02.763: INFO: Pod "pod-subpath-test-secret-jcbm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005609219s
    STEP: Saw pod success 04/21/23 20:30:02.763
    Apr 21 20:30:02.763: INFO: Pod "pod-subpath-test-secret-jcbm" satisfied condition "Succeeded or Failed"
    Apr 21 20:30:02.765: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-secret-jcbm container test-container-subpath-secret-jcbm: <nil>
    STEP: delete the pod 04/21/23 20:30:02.771
    Apr 21 20:30:02.777: INFO: Waiting for pod pod-subpath-test-secret-jcbm to disappear
    Apr 21 20:30:02.778: INFO: Pod pod-subpath-test-secret-jcbm no longer exists
    STEP: Deleting pod pod-subpath-test-secret-jcbm 04/21/23 20:30:02.778
    Apr 21 20:30:02.778: INFO: Deleting pod "pod-subpath-test-secret-jcbm" in namespace "subpath-9419"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:02.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9419" for this suite. 04/21/23 20:30:02.782
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:02.785
Apr 21 20:30:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pod-network-test 04/21/23 20:30:02.786
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:02.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:02.795
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-8885 04/21/23 20:30:02.797
STEP: creating a selector 04/21/23 20:30:02.797
STEP: Creating the service pods in kubernetes 04/21/23 20:30:02.797
Apr 21 20:30:02.797: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 21 20:30:02.808: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8885" to be "running and ready"
Apr 21 20:30:02.810: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115964ms
Apr 21 20:30:02.810: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:30:04.813: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004652157s
Apr 21 20:30:04.813: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:30:06.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005840157s
Apr 21 20:30:06.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:30:08.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005609508s
Apr 21 20:30:08.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:30:10.813: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004908368s
Apr 21 20:30:10.813: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:30:12.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005328618s
Apr 21 20:30:12.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:30:14.813: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005071683s
Apr 21 20:30:14.813: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 21 20:30:14.813: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 21 20:30:14.815: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8885" to be "running and ready"
Apr 21 20:30:14.817: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.665421ms
Apr 21 20:30:14.817: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 21 20:30:14.817: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 04/21/23 20:30:14.818
Apr 21 20:30:14.822: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8885" to be "running"
Apr 21 20:30:14.823: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.552719ms
Apr 21 20:30:16.826: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003824924s
Apr 21 20:30:16.826: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 21 20:30:16.827: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 21 20:30:16.827: INFO: Breadth first check of 10.244.0.13 on host 192.168.49.2...
Apr 21 20:30:16.829: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.67:9080/dial?request=hostname&protocol=http&host=10.244.0.13&port=8083&tries=1'] Namespace:pod-network-test-8885 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:30:16.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:30:16.829: INFO: ExecWithOptions: Clientset creation
Apr 21 20:30:16.829: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8885/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.13%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 21 20:30:16.891: INFO: Waiting for responses: map[]
Apr 21 20:30:16.891: INFO: reached 10.244.0.13 after 0/1 tries
Apr 21 20:30:16.891: INFO: Breadth first check of 10.244.1.66 on host 192.168.49.3...
Apr 21 20:30:16.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.67:9080/dial?request=hostname&protocol=http&host=10.244.1.66&port=8083&tries=1'] Namespace:pod-network-test-8885 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:30:16.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:30:16.894: INFO: ExecWithOptions: Clientset creation
Apr 21 20:30:16.894: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8885/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.66%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 21 20:30:16.955: INFO: Waiting for responses: map[]
Apr 21 20:30:16.955: INFO: reached 10.244.1.66 after 0/1 tries
Apr 21 20:30:16.955: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:16.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-8885" for this suite. 04/21/23 20:30:16.958
------------------------------
â€¢ [SLOW TEST] [14.177 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:02.785
    Apr 21 20:30:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pod-network-test 04/21/23 20:30:02.786
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:02.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:02.795
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-8885 04/21/23 20:30:02.797
    STEP: creating a selector 04/21/23 20:30:02.797
    STEP: Creating the service pods in kubernetes 04/21/23 20:30:02.797
    Apr 21 20:30:02.797: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 21 20:30:02.808: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8885" to be "running and ready"
    Apr 21 20:30:02.810: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.115964ms
    Apr 21 20:30:02.810: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:30:04.813: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004652157s
    Apr 21 20:30:04.813: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:30:06.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005840157s
    Apr 21 20:30:06.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:30:08.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005609508s
    Apr 21 20:30:08.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:30:10.813: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004908368s
    Apr 21 20:30:10.813: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:30:12.814: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.005328618s
    Apr 21 20:30:12.814: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:30:14.813: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.005071683s
    Apr 21 20:30:14.813: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 21 20:30:14.813: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 21 20:30:14.815: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8885" to be "running and ready"
    Apr 21 20:30:14.817: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.665421ms
    Apr 21 20:30:14.817: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 21 20:30:14.817: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 04/21/23 20:30:14.818
    Apr 21 20:30:14.822: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8885" to be "running"
    Apr 21 20:30:14.823: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.552719ms
    Apr 21 20:30:16.826: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003824924s
    Apr 21 20:30:16.826: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 21 20:30:16.827: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Apr 21 20:30:16.827: INFO: Breadth first check of 10.244.0.13 on host 192.168.49.2...
    Apr 21 20:30:16.829: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.67:9080/dial?request=hostname&protocol=http&host=10.244.0.13&port=8083&tries=1'] Namespace:pod-network-test-8885 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:30:16.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:30:16.829: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:30:16.829: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8885/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.13%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 21 20:30:16.891: INFO: Waiting for responses: map[]
    Apr 21 20:30:16.891: INFO: reached 10.244.0.13 after 0/1 tries
    Apr 21 20:30:16.891: INFO: Breadth first check of 10.244.1.66 on host 192.168.49.3...
    Apr 21 20:30:16.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.67:9080/dial?request=hostname&protocol=http&host=10.244.1.66&port=8083&tries=1'] Namespace:pod-network-test-8885 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:30:16.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:30:16.894: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:30:16.894: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8885/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.67%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.66%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 21 20:30:16.955: INFO: Waiting for responses: map[]
    Apr 21 20:30:16.955: INFO: reached 10.244.1.66 after 0/1 tries
    Apr 21 20:30:16.955: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:16.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-8885" for this suite. 04/21/23 20:30:16.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:16.962
Apr 21 20:30:16.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 20:30:16.963
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:16.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:16.973
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 04/21/23 20:30:16.975
Apr 21 20:30:16.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:30:18.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:23.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9701" for this suite. 04/21/23 20:30:23.613
------------------------------
â€¢ [SLOW TEST] [6.655 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:16.962
    Apr 21 20:30:16.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 20:30:16.963
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:16.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:16.973
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 04/21/23 20:30:16.975
    Apr 21 20:30:16.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:30:18.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:23.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9701" for this suite. 04/21/23 20:30:23.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:23.621
Apr 21 20:30:23.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 20:30:23.622
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:23.629
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:23.631
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:30:23.633
Apr 21 20:30:23.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3" in namespace "downward-api-8582" to be "Succeeded or Failed"
Apr 21 20:30:23.639: INFO: Pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556711ms
Apr 21 20:30:25.642: INFO: Pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004191472s
Apr 21 20:30:27.642: INFO: Pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00480174s
STEP: Saw pod success 04/21/23 20:30:27.642
Apr 21 20:30:27.642: INFO: Pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3" satisfied condition "Succeeded or Failed"
Apr 21 20:30:27.646: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3 container client-container: <nil>
STEP: delete the pod 04/21/23 20:30:27.652
Apr 21 20:30:27.659: INFO: Waiting for pod downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3 to disappear
Apr 21 20:30:27.660: INFO: Pod downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:27.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8582" for this suite. 04/21/23 20:30:27.662
------------------------------
â€¢ [4.046 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:23.621
    Apr 21 20:30:23.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 20:30:23.622
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:23.629
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:23.631
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:30:23.633
    Apr 21 20:30:23.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3" in namespace "downward-api-8582" to be "Succeeded or Failed"
    Apr 21 20:30:23.639: INFO: Pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556711ms
    Apr 21 20:30:25.642: INFO: Pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004191472s
    Apr 21 20:30:27.642: INFO: Pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00480174s
    STEP: Saw pod success 04/21/23 20:30:27.642
    Apr 21 20:30:27.642: INFO: Pod "downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3" satisfied condition "Succeeded or Failed"
    Apr 21 20:30:27.646: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3 container client-container: <nil>
    STEP: delete the pod 04/21/23 20:30:27.652
    Apr 21 20:30:27.659: INFO: Waiting for pod downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3 to disappear
    Apr 21 20:30:27.660: INFO: Pod downwardapi-volume-58bffa48-9ec3-4750-8a08-cc979a5a94a3 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:27.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8582" for this suite. 04/21/23 20:30:27.662
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:27.667
Apr 21 20:30:27.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 20:30:27.668
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:27.674
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:27.676
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 04/21/23 20:30:27.678
STEP: Getting a ResourceQuota 04/21/23 20:30:27.68
STEP: Listing all ResourceQuotas with LabelSelector 04/21/23 20:30:27.682
STEP: Patching the ResourceQuota 04/21/23 20:30:27.685
STEP: Deleting a Collection of ResourceQuotas 04/21/23 20:30:27.689
STEP: Verifying the deleted ResourceQuota 04/21/23 20:30:27.693
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:27.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-905" for this suite. 04/21/23 20:30:27.696
------------------------------
â€¢ [0.032 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:27.667
    Apr 21 20:30:27.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 20:30:27.668
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:27.674
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:27.676
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 04/21/23 20:30:27.678
    STEP: Getting a ResourceQuota 04/21/23 20:30:27.68
    STEP: Listing all ResourceQuotas with LabelSelector 04/21/23 20:30:27.682
    STEP: Patching the ResourceQuota 04/21/23 20:30:27.685
    STEP: Deleting a Collection of ResourceQuotas 04/21/23 20:30:27.689
    STEP: Verifying the deleted ResourceQuota 04/21/23 20:30:27.693
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:27.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-905" for this suite. 04/21/23 20:30:27.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:27.699
Apr 21 20:30:27.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename csistoragecapacity 04/21/23 20:30:27.7
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:27.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:27.71
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 04/21/23 20:30:27.712
STEP: getting /apis/storage.k8s.io 04/21/23 20:30:27.714
STEP: getting /apis/storage.k8s.io/v1 04/21/23 20:30:27.714
STEP: creating 04/21/23 20:30:27.715
STEP: watching 04/21/23 20:30:27.723
Apr 21 20:30:27.724: INFO: starting watch
STEP: getting 04/21/23 20:30:27.727
STEP: listing in namespace 04/21/23 20:30:27.728
STEP: listing across namespaces 04/21/23 20:30:27.73
STEP: patching 04/21/23 20:30:27.731
STEP: updating 04/21/23 20:30:27.734
Apr 21 20:30:27.736: INFO: waiting for watch events with expected annotations in namespace
Apr 21 20:30:27.736: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 04/21/23 20:30:27.736
STEP: deleting a collection 04/21/23 20:30:27.742
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-3436" for this suite. 04/21/23 20:30:27.751
------------------------------
â€¢ [0.054 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:27.699
    Apr 21 20:30:27.700: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename csistoragecapacity 04/21/23 20:30:27.7
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:27.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:27.71
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 04/21/23 20:30:27.712
    STEP: getting /apis/storage.k8s.io 04/21/23 20:30:27.714
    STEP: getting /apis/storage.k8s.io/v1 04/21/23 20:30:27.714
    STEP: creating 04/21/23 20:30:27.715
    STEP: watching 04/21/23 20:30:27.723
    Apr 21 20:30:27.724: INFO: starting watch
    STEP: getting 04/21/23 20:30:27.727
    STEP: listing in namespace 04/21/23 20:30:27.728
    STEP: listing across namespaces 04/21/23 20:30:27.73
    STEP: patching 04/21/23 20:30:27.731
    STEP: updating 04/21/23 20:30:27.734
    Apr 21 20:30:27.736: INFO: waiting for watch events with expected annotations in namespace
    Apr 21 20:30:27.736: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 04/21/23 20:30:27.736
    STEP: deleting a collection 04/21/23 20:30:27.742
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:27.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-3436" for this suite. 04/21/23 20:30:27.751
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:27.754
Apr 21 20:30:27.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename disruption 04/21/23 20:30:27.755
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:27.76
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:27.762
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 04/21/23 20:30:27.767
STEP: Waiting for all pods to be running 04/21/23 20:30:29.782
Apr 21 20:30:29.787: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:31.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6035" for this suite. 04/21/23 20:30:31.794
------------------------------
â€¢ [4.046 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:27.754
    Apr 21 20:30:27.754: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename disruption 04/21/23 20:30:27.755
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:27.76
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:27.762
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 04/21/23 20:30:27.767
    STEP: Waiting for all pods to be running 04/21/23 20:30:29.782
    Apr 21 20:30:29.787: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:31.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6035" for this suite. 04/21/23 20:30:31.794
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:31.803
Apr 21 20:30:31.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-runtime 04/21/23 20:30:31.804
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:31.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:31.814
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 04/21/23 20:30:31.816
STEP: wait for the container to reach Succeeded 04/21/23 20:30:31.822
STEP: get the container status 04/21/23 20:30:35.841
STEP: the container should be terminated 04/21/23 20:30:35.843
STEP: the termination message should be set 04/21/23 20:30:35.843
Apr 21 20:30:35.843: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 04/21/23 20:30:35.843
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:35.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8739" for this suite. 04/21/23 20:30:35.855
------------------------------
â€¢ [4.054 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:31.803
    Apr 21 20:30:31.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-runtime 04/21/23 20:30:31.804
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:31.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:31.814
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 04/21/23 20:30:31.816
    STEP: wait for the container to reach Succeeded 04/21/23 20:30:31.822
    STEP: get the container status 04/21/23 20:30:35.841
    STEP: the container should be terminated 04/21/23 20:30:35.843
    STEP: the termination message should be set 04/21/23 20:30:35.843
    Apr 21 20:30:35.843: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 04/21/23 20:30:35.843
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:35.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8739" for this suite. 04/21/23 20:30:35.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:35.858
Apr 21 20:30:35.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:30:35.859
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:35.866
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:35.868
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-be7c5e69-8b64-4741-b003-9b55836f87f0 04/21/23 20:30:35.87
STEP: Creating a pod to test consume configMaps 04/21/23 20:30:35.872
Apr 21 20:30:35.878: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484" in namespace "projected-310" to be "Succeeded or Failed"
Apr 21 20:30:35.879: INFO: Pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484": Phase="Pending", Reason="", readiness=false. Elapsed: 1.537744ms
Apr 21 20:30:37.882: INFO: Pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00410151s
Apr 21 20:30:39.882: INFO: Pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0043709s
STEP: Saw pod success 04/21/23 20:30:39.882
Apr 21 20:30:39.882: INFO: Pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484" satisfied condition "Succeeded or Failed"
Apr 21 20:30:39.884: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 20:30:39.889
Apr 21 20:30:39.897: INFO: Waiting for pod pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484 to disappear
Apr 21 20:30:39.899: INFO: Pod pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:39.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-310" for this suite. 04/21/23 20:30:39.901
------------------------------
â€¢ [4.046 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:35.858
    Apr 21 20:30:35.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:30:35.859
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:35.866
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:35.868
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-be7c5e69-8b64-4741-b003-9b55836f87f0 04/21/23 20:30:35.87
    STEP: Creating a pod to test consume configMaps 04/21/23 20:30:35.872
    Apr 21 20:30:35.878: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484" in namespace "projected-310" to be "Succeeded or Failed"
    Apr 21 20:30:35.879: INFO: Pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484": Phase="Pending", Reason="", readiness=false. Elapsed: 1.537744ms
    Apr 21 20:30:37.882: INFO: Pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00410151s
    Apr 21 20:30:39.882: INFO: Pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0043709s
    STEP: Saw pod success 04/21/23 20:30:39.882
    Apr 21 20:30:39.882: INFO: Pod "pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484" satisfied condition "Succeeded or Failed"
    Apr 21 20:30:39.884: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 20:30:39.889
    Apr 21 20:30:39.897: INFO: Waiting for pod pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484 to disappear
    Apr 21 20:30:39.899: INFO: Pod pod-projected-configmaps-5f5c2953-7943-4c40-8c83-ed15483db484 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:39.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-310" for this suite. 04/21/23 20:30:39.901
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:39.904
Apr 21 20:30:39.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 20:30:39.905
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:39.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:39.915
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:39.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4540" for this suite. 04/21/23 20:30:39.92
------------------------------
â€¢ [0.019 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:39.904
    Apr 21 20:30:39.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 20:30:39.905
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:39.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:39.915
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:39.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4540" for this suite. 04/21/23 20:30:39.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:39.924
Apr 21 20:30:39.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename job 04/21/23 20:30:39.924
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:39.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:39.934
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 04/21/23 20:30:39.936
STEP: Ensuring active pods == parallelism 04/21/23 20:30:39.938
STEP: Orphaning one of the Job's Pods 04/21/23 20:30:41.942
Apr 21 20:30:42.451: INFO: Successfully updated pod "adopt-release-pmlpn"
STEP: Checking that the Job readopts the Pod 04/21/23 20:30:42.451
Apr 21 20:30:42.451: INFO: Waiting up to 15m0s for pod "adopt-release-pmlpn" in namespace "job-1723" to be "adopted"
Apr 21 20:30:42.453: INFO: Pod "adopt-release-pmlpn": Phase="Running", Reason="", readiness=true. Elapsed: 1.909152ms
Apr 21 20:30:44.455: INFO: Pod "adopt-release-pmlpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004155303s
Apr 21 20:30:44.455: INFO: Pod "adopt-release-pmlpn" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 04/21/23 20:30:44.455
Apr 21 20:30:44.964: INFO: Successfully updated pod "adopt-release-pmlpn"
STEP: Checking that the Job releases the Pod 04/21/23 20:30:44.964
Apr 21 20:30:44.964: INFO: Waiting up to 15m0s for pod "adopt-release-pmlpn" in namespace "job-1723" to be "released"
Apr 21 20:30:44.966: INFO: Pod "adopt-release-pmlpn": Phase="Running", Reason="", readiness=true. Elapsed: 1.53796ms
Apr 21 20:30:46.968: INFO: Pod "adopt-release-pmlpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004296533s
Apr 21 20:30:46.968: INFO: Pod "adopt-release-pmlpn" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:46.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1723" for this suite. 04/21/23 20:30:46.971
------------------------------
â€¢ [SLOW TEST] [7.051 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:39.924
    Apr 21 20:30:39.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename job 04/21/23 20:30:39.924
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:39.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:39.934
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 04/21/23 20:30:39.936
    STEP: Ensuring active pods == parallelism 04/21/23 20:30:39.938
    STEP: Orphaning one of the Job's Pods 04/21/23 20:30:41.942
    Apr 21 20:30:42.451: INFO: Successfully updated pod "adopt-release-pmlpn"
    STEP: Checking that the Job readopts the Pod 04/21/23 20:30:42.451
    Apr 21 20:30:42.451: INFO: Waiting up to 15m0s for pod "adopt-release-pmlpn" in namespace "job-1723" to be "adopted"
    Apr 21 20:30:42.453: INFO: Pod "adopt-release-pmlpn": Phase="Running", Reason="", readiness=true. Elapsed: 1.909152ms
    Apr 21 20:30:44.455: INFO: Pod "adopt-release-pmlpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004155303s
    Apr 21 20:30:44.455: INFO: Pod "adopt-release-pmlpn" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 04/21/23 20:30:44.455
    Apr 21 20:30:44.964: INFO: Successfully updated pod "adopt-release-pmlpn"
    STEP: Checking that the Job releases the Pod 04/21/23 20:30:44.964
    Apr 21 20:30:44.964: INFO: Waiting up to 15m0s for pod "adopt-release-pmlpn" in namespace "job-1723" to be "released"
    Apr 21 20:30:44.966: INFO: Pod "adopt-release-pmlpn": Phase="Running", Reason="", readiness=true. Elapsed: 1.53796ms
    Apr 21 20:30:46.968: INFO: Pod "adopt-release-pmlpn": Phase="Running", Reason="", readiness=true. Elapsed: 2.004296533s
    Apr 21 20:30:46.968: INFO: Pod "adopt-release-pmlpn" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:46.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1723" for this suite. 04/21/23 20:30:46.971
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:46.975
Apr 21 20:30:46.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 20:30:46.976
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:46.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:46.985
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-5dca5ad7-4f6f-4ae2-a052-1dc21acf9f37 04/21/23 20:30:46.987
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 20:30:46.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-904" for this suite. 04/21/23 20:30:46.99
------------------------------
â€¢ [0.018 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:46.975
    Apr 21 20:30:46.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 20:30:46.976
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:46.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:46.985
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-5dca5ad7-4f6f-4ae2-a052-1dc21acf9f37 04/21/23 20:30:46.987
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:30:46.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-904" for this suite. 04/21/23 20:30:46.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:30:46.995
Apr 21 20:30:46.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename job 04/21/23 20:30:46.996
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:47.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:47.004
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 04/21/23 20:30:47.006
STEP: Ensuring active pods == parallelism 04/21/23 20:30:47.01
STEP: delete a job 04/21/23 20:30:49.013
STEP: deleting Job.batch foo in namespace job-2588, will wait for the garbage collector to delete the pods 04/21/23 20:30:49.013
Apr 21 20:30:49.069: INFO: Deleting Job.batch foo took: 3.822679ms
Apr 21 20:30:49.169: INFO: Terminating Job.batch foo pods took: 100.409284ms
STEP: Ensuring job was deleted 04/21/23 20:31:20.67
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 21 20:31:20.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2588" for this suite. 04/21/23 20:31:20.676
------------------------------
â€¢ [SLOW TEST] [33.685 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:30:46.995
    Apr 21 20:30:46.995: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename job 04/21/23 20:30:46.996
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:30:47.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:30:47.004
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 04/21/23 20:30:47.006
    STEP: Ensuring active pods == parallelism 04/21/23 20:30:47.01
    STEP: delete a job 04/21/23 20:30:49.013
    STEP: deleting Job.batch foo in namespace job-2588, will wait for the garbage collector to delete the pods 04/21/23 20:30:49.013
    Apr 21 20:30:49.069: INFO: Deleting Job.batch foo took: 3.822679ms
    Apr 21 20:30:49.169: INFO: Terminating Job.batch foo pods took: 100.409284ms
    STEP: Ensuring job was deleted 04/21/23 20:31:20.67
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:31:20.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2588" for this suite. 04/21/23 20:31:20.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:31:20.681
Apr 21 20:31:20.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-webhook 04/21/23 20:31:20.681
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:20.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:20.69
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 04/21/23 20:31:20.692
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/21/23 20:31:21.325
STEP: Deploying the custom resource conversion webhook pod 04/21/23 20:31:21.33
STEP: Wait for the deployment to be ready 04/21/23 20:31:21.337
Apr 21 20:31:21.340: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 04/21/23 20:31:23.347
STEP: Verifying the service has paired with the endpoint 04/21/23 20:31:23.355
Apr 21 20:31:24.356: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Apr 21 20:31:24.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Creating a v1 custom resource 04/21/23 20:31:26.911
STEP: Create a v2 custom resource 04/21/23 20:31:26.922
STEP: List CRs in v1 04/21/23 20:31:26.96
STEP: List CRs in v2 04/21/23 20:31:26.962
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:31:27.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8545" for this suite. 04/21/23 20:31:27.501
------------------------------
â€¢ [SLOW TEST] [6.825 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:31:20.681
    Apr 21 20:31:20.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-webhook 04/21/23 20:31:20.681
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:20.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:20.69
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 04/21/23 20:31:20.692
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/21/23 20:31:21.325
    STEP: Deploying the custom resource conversion webhook pod 04/21/23 20:31:21.33
    STEP: Wait for the deployment to be ready 04/21/23 20:31:21.337
    Apr 21 20:31:21.340: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 04/21/23 20:31:23.347
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:31:23.355
    Apr 21 20:31:24.356: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Apr 21 20:31:24.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Creating a v1 custom resource 04/21/23 20:31:26.911
    STEP: Create a v2 custom resource 04/21/23 20:31:26.922
    STEP: List CRs in v1 04/21/23 20:31:26.96
    STEP: List CRs in v2 04/21/23 20:31:26.962
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:31:27.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8545" for this suite. 04/21/23 20:31:27.501
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:31:27.508
Apr 21 20:31:27.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 20:31:27.509
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:27.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:27.52
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 04/21/23 20:31:27.523
STEP: Creating a ResourceQuota 04/21/23 20:31:32.525
STEP: Ensuring resource quota status is calculated 04/21/23 20:31:32.528
STEP: Creating a ReplicationController 04/21/23 20:31:34.531
STEP: Ensuring resource quota status captures replication controller creation 04/21/23 20:31:34.54
STEP: Deleting a ReplicationController 04/21/23 20:31:36.543
STEP: Ensuring resource quota status released usage 04/21/23 20:31:36.546
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 20:31:38.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6903" for this suite. 04/21/23 20:31:38.551
------------------------------
â€¢ [SLOW TEST] [11.048 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:31:27.508
    Apr 21 20:31:27.508: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 20:31:27.509
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:27.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:27.52
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 04/21/23 20:31:27.523
    STEP: Creating a ResourceQuota 04/21/23 20:31:32.525
    STEP: Ensuring resource quota status is calculated 04/21/23 20:31:32.528
    STEP: Creating a ReplicationController 04/21/23 20:31:34.531
    STEP: Ensuring resource quota status captures replication controller creation 04/21/23 20:31:34.54
    STEP: Deleting a ReplicationController 04/21/23 20:31:36.543
    STEP: Ensuring resource quota status released usage 04/21/23 20:31:36.546
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:31:38.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6903" for this suite. 04/21/23 20:31:38.551
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:31:38.556
Apr 21 20:31:38.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 20:31:38.557
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:38.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:38.568
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 04/21/23 20:31:38.57
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 04/21/23 20:31:38.571
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 04/21/23 20:31:38.571
STEP: fetching the /apis/apiextensions.k8s.io discovery document 04/21/23 20:31:38.571
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 04/21/23 20:31:38.572
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 04/21/23 20:31:38.572
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 04/21/23 20:31:38.572
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:31:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-281" for this suite. 04/21/23 20:31:38.574
------------------------------
â€¢ [0.022 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:31:38.556
    Apr 21 20:31:38.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 20:31:38.557
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:38.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:38.568
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 04/21/23 20:31:38.57
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 04/21/23 20:31:38.571
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 04/21/23 20:31:38.571
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 04/21/23 20:31:38.571
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 04/21/23 20:31:38.572
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 04/21/23 20:31:38.572
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 04/21/23 20:31:38.572
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:31:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-281" for this suite. 04/21/23 20:31:38.574
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:31:38.578
Apr 21 20:31:38.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 20:31:38.579
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:38.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:38.586
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 04/21/23 20:31:38.588
STEP: Ensuring ResourceQuota status is calculated 04/21/23 20:31:38.592
STEP: Creating a ResourceQuota with not best effort scope 04/21/23 20:31:40.594
STEP: Ensuring ResourceQuota status is calculated 04/21/23 20:31:40.597
STEP: Creating a best-effort pod 04/21/23 20:31:42.601
STEP: Ensuring resource quota with best effort scope captures the pod usage 04/21/23 20:31:42.609
STEP: Ensuring resource quota with not best effort ignored the pod usage 04/21/23 20:31:44.612
STEP: Deleting the pod 04/21/23 20:31:46.614
STEP: Ensuring resource quota status released the pod usage 04/21/23 20:31:46.62
STEP: Creating a not best-effort pod 04/21/23 20:31:48.623
STEP: Ensuring resource quota with not best effort scope captures the pod usage 04/21/23 20:31:48.629
STEP: Ensuring resource quota with best effort scope ignored the pod usage 04/21/23 20:31:50.632
STEP: Deleting the pod 04/21/23 20:31:52.635
STEP: Ensuring resource quota status released the pod usage 04/21/23 20:31:52.643
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 20:31:54.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2902" for this suite. 04/21/23 20:31:54.648
------------------------------
â€¢ [SLOW TEST] [16.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:31:38.578
    Apr 21 20:31:38.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 20:31:38.579
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:38.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:38.586
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 04/21/23 20:31:38.588
    STEP: Ensuring ResourceQuota status is calculated 04/21/23 20:31:38.592
    STEP: Creating a ResourceQuota with not best effort scope 04/21/23 20:31:40.594
    STEP: Ensuring ResourceQuota status is calculated 04/21/23 20:31:40.597
    STEP: Creating a best-effort pod 04/21/23 20:31:42.601
    STEP: Ensuring resource quota with best effort scope captures the pod usage 04/21/23 20:31:42.609
    STEP: Ensuring resource quota with not best effort ignored the pod usage 04/21/23 20:31:44.612
    STEP: Deleting the pod 04/21/23 20:31:46.614
    STEP: Ensuring resource quota status released the pod usage 04/21/23 20:31:46.62
    STEP: Creating a not best-effort pod 04/21/23 20:31:48.623
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 04/21/23 20:31:48.629
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 04/21/23 20:31:50.632
    STEP: Deleting the pod 04/21/23 20:31:52.635
    STEP: Ensuring resource quota status released the pod usage 04/21/23 20:31:52.643
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:31:54.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2902" for this suite. 04/21/23 20:31:54.648
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:31:54.652
Apr 21 20:31:54.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-probe 04/21/23 20:31:54.658
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:54.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:54.669
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e in namespace container-probe-9576 04/21/23 20:31:54.671
Apr 21 20:31:54.677: INFO: Waiting up to 5m0s for pod "busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e" in namespace "container-probe-9576" to be "not pending"
Apr 21 20:31:54.678: INFO: Pod "busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.595269ms
Apr 21 20:31:56.682: INFO: Pod "busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004938055s
Apr 21 20:31:56.682: INFO: Pod "busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e" satisfied condition "not pending"
Apr 21 20:31:56.682: INFO: Started pod busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e in namespace container-probe-9576
STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 20:31:56.682
Apr 21 20:31:56.684: INFO: Initial restart count of pod busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e is 0
STEP: deleting the pod 04/21/23 20:35:57.054
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 21 20:35:57.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9576" for this suite. 04/21/23 20:35:57.066
------------------------------
â€¢ [SLOW TEST] [242.418 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:31:54.652
    Apr 21 20:31:54.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-probe 04/21/23 20:31:54.658
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:31:54.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:31:54.669
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e in namespace container-probe-9576 04/21/23 20:31:54.671
    Apr 21 20:31:54.677: INFO: Waiting up to 5m0s for pod "busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e" in namespace "container-probe-9576" to be "not pending"
    Apr 21 20:31:54.678: INFO: Pod "busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.595269ms
    Apr 21 20:31:56.682: INFO: Pod "busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004938055s
    Apr 21 20:31:56.682: INFO: Pod "busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e" satisfied condition "not pending"
    Apr 21 20:31:56.682: INFO: Started pod busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e in namespace container-probe-9576
    STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 20:31:56.682
    Apr 21 20:31:56.684: INFO: Initial restart count of pod busybox-72e5d7b9-dc09-49cd-bb7f-c09603c9235e is 0
    STEP: deleting the pod 04/21/23 20:35:57.054
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:35:57.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9576" for this suite. 04/21/23 20:35:57.066
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:35:57.07
Apr 21 20:35:57.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pod-network-test 04/21/23 20:35:57.071
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:35:57.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:35:57.08
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-1014 04/21/23 20:35:57.082
STEP: creating a selector 04/21/23 20:35:57.082
STEP: Creating the service pods in kubernetes 04/21/23 20:35:57.082
Apr 21 20:35:57.082: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 21 20:35:57.098: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1014" to be "running and ready"
Apr 21 20:35:57.100: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579515ms
Apr 21 20:35:57.100: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:35:59.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004579344s
Apr 21 20:35:59.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:36:01.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005074485s
Apr 21 20:36:01.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:36:03.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005337678s
Apr 21 20:36:03.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:36:05.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004086292s
Apr 21 20:36:05.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:36:07.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004853724s
Apr 21 20:36:07.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:36:09.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004951114s
Apr 21 20:36:09.103: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 21 20:36:09.103: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 21 20:36:09.105: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1014" to be "running and ready"
Apr 21 20:36:09.106: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.553923ms
Apr 21 20:36:09.106: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 21 20:36:09.106: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 04/21/23 20:36:09.108
Apr 21 20:36:09.111: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1014" to be "running"
Apr 21 20:36:09.112: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.588158ms
Apr 21 20:36:11.115: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004470699s
Apr 21 20:36:11.115: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 21 20:36:11.117: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 21 20:36:11.117: INFO: Breadth first check of 10.244.0.14 on host 192.168.49.2...
Apr 21 20:36:11.119: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.82:9080/dial?request=hostname&protocol=udp&host=10.244.0.14&port=8081&tries=1'] Namespace:pod-network-test-1014 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:36:11.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:36:11.119: INFO: ExecWithOptions: Clientset creation
Apr 21 20:36:11.119: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1014/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.82%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.14%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 21 20:36:11.187: INFO: Waiting for responses: map[]
Apr 21 20:36:11.187: INFO: reached 10.244.0.14 after 0/1 tries
Apr 21 20:36:11.187: INFO: Breadth first check of 10.244.1.81 on host 192.168.49.3...
Apr 21 20:36:11.189: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.82:9080/dial?request=hostname&protocol=udp&host=10.244.1.81&port=8081&tries=1'] Namespace:pod-network-test-1014 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:36:11.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:36:11.190: INFO: ExecWithOptions: Clientset creation
Apr 21 20:36:11.190: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1014/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.82%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.81%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 21 20:36:11.233: INFO: Waiting for responses: map[]
Apr 21 20:36:11.233: INFO: reached 10.244.1.81 after 0/1 tries
Apr 21 20:36:11.233: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 21 20:36:11.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1014" for this suite. 04/21/23 20:36:11.236
------------------------------
â€¢ [SLOW TEST] [14.171 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:35:57.07
    Apr 21 20:35:57.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pod-network-test 04/21/23 20:35:57.071
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:35:57.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:35:57.08
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-1014 04/21/23 20:35:57.082
    STEP: creating a selector 04/21/23 20:35:57.082
    STEP: Creating the service pods in kubernetes 04/21/23 20:35:57.082
    Apr 21 20:35:57.082: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 21 20:35:57.098: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1014" to be "running and ready"
    Apr 21 20:35:57.100: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.579515ms
    Apr 21 20:35:57.100: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:35:59.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004579344s
    Apr 21 20:35:59.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:36:01.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005074485s
    Apr 21 20:36:01.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:36:03.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.005337678s
    Apr 21 20:36:03.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:36:05.102: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004086292s
    Apr 21 20:36:05.102: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:36:07.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004853724s
    Apr 21 20:36:07.103: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:36:09.103: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004951114s
    Apr 21 20:36:09.103: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 21 20:36:09.103: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 21 20:36:09.105: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1014" to be "running and ready"
    Apr 21 20:36:09.106: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.553923ms
    Apr 21 20:36:09.106: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 21 20:36:09.106: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 04/21/23 20:36:09.108
    Apr 21 20:36:09.111: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1014" to be "running"
    Apr 21 20:36:09.112: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.588158ms
    Apr 21 20:36:11.115: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004470699s
    Apr 21 20:36:11.115: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 21 20:36:11.117: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Apr 21 20:36:11.117: INFO: Breadth first check of 10.244.0.14 on host 192.168.49.2...
    Apr 21 20:36:11.119: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.82:9080/dial?request=hostname&protocol=udp&host=10.244.0.14&port=8081&tries=1'] Namespace:pod-network-test-1014 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:36:11.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:36:11.119: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:36:11.119: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1014/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.82%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.14%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 21 20:36:11.187: INFO: Waiting for responses: map[]
    Apr 21 20:36:11.187: INFO: reached 10.244.0.14 after 0/1 tries
    Apr 21 20:36:11.187: INFO: Breadth first check of 10.244.1.81 on host 192.168.49.3...
    Apr 21 20:36:11.189: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.82:9080/dial?request=hostname&protocol=udp&host=10.244.1.81&port=8081&tries=1'] Namespace:pod-network-test-1014 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:36:11.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:36:11.190: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:36:11.190: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1014/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.82%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.81%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 21 20:36:11.233: INFO: Waiting for responses: map[]
    Apr 21 20:36:11.233: INFO: reached 10.244.1.81 after 0/1 tries
    Apr 21 20:36:11.233: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:36:11.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1014" for this suite. 04/21/23 20:36:11.236
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:36:11.241
Apr 21 20:36:11.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 20:36:11.242
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:11.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:11.251
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:36:11.253
Apr 21 20:36:11.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a" in namespace "downward-api-8347" to be "Succeeded or Failed"
Apr 21 20:36:11.260: INFO: Pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471053ms
Apr 21 20:36:13.263: INFO: Pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0046967s
Apr 21 20:36:15.262: INFO: Pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003994322s
STEP: Saw pod success 04/21/23 20:36:15.262
Apr 21 20:36:15.262: INFO: Pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a" satisfied condition "Succeeded or Failed"
Apr 21 20:36:15.264: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a container client-container: <nil>
STEP: delete the pod 04/21/23 20:36:15.275
Apr 21 20:36:15.283: INFO: Waiting for pod downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a to disappear
Apr 21 20:36:15.284: INFO: Pod downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 20:36:15.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8347" for this suite. 04/21/23 20:36:15.286
------------------------------
â€¢ [4.048 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:36:11.241
    Apr 21 20:36:11.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 20:36:11.242
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:11.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:11.251
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:36:11.253
    Apr 21 20:36:11.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a" in namespace "downward-api-8347" to be "Succeeded or Failed"
    Apr 21 20:36:11.260: INFO: Pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471053ms
    Apr 21 20:36:13.263: INFO: Pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0046967s
    Apr 21 20:36:15.262: INFO: Pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003994322s
    STEP: Saw pod success 04/21/23 20:36:15.262
    Apr 21 20:36:15.262: INFO: Pod "downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a" satisfied condition "Succeeded or Failed"
    Apr 21 20:36:15.264: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a container client-container: <nil>
    STEP: delete the pod 04/21/23 20:36:15.275
    Apr 21 20:36:15.283: INFO: Waiting for pod downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a to disappear
    Apr 21 20:36:15.284: INFO: Pod downwardapi-volume-8dc18d74-7040-4cad-b001-a4b5bd8f983a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:36:15.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8347" for this suite. 04/21/23 20:36:15.286
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:36:15.29
Apr 21 20:36:15.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename dns 04/21/23 20:36:15.29
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:15.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:15.298
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 04/21/23 20:36:15.3
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
 04/21/23 20:36:15.303
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
 04/21/23 20:36:15.303
STEP: creating a pod to probe DNS 04/21/23 20:36:15.303
STEP: submitting the pod to kubernetes 04/21/23 20:36:15.303
Apr 21 20:36:15.311: INFO: Waiting up to 15m0s for pod "dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2" in namespace "dns-4935" to be "running"
Apr 21 20:36:15.313: INFO: Pod "dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199981ms
Apr 21 20:36:17.316: INFO: Pod "dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004588392s
Apr 21 20:36:17.316: INFO: Pod "dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2" satisfied condition "running"
STEP: retrieving the pod 04/21/23 20:36:17.316
STEP: looking for the results for each expected name from probers 04/21/23 20:36:17.319
Apr 21 20:36:17.322: INFO: DNS probes using dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2 succeeded

STEP: deleting the pod 04/21/23 20:36:17.322
STEP: changing the externalName to bar.example.com 04/21/23 20:36:17.332
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
 04/21/23 20:36:17.338
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
 04/21/23 20:36:17.338
STEP: creating a second pod to probe DNS 04/21/23 20:36:17.338
STEP: submitting the pod to kubernetes 04/21/23 20:36:17.338
Apr 21 20:36:17.341: INFO: Waiting up to 15m0s for pod "dns-test-d3d77282-bce5-47f2-909a-620e899ac581" in namespace "dns-4935" to be "running"
Apr 21 20:36:17.343: INFO: Pod "dns-test-d3d77282-bce5-47f2-909a-620e899ac581": Phase="Pending", Reason="", readiness=false. Elapsed: 1.591046ms
Apr 21 20:36:19.346: INFO: Pod "dns-test-d3d77282-bce5-47f2-909a-620e899ac581": Phase="Running", Reason="", readiness=true. Elapsed: 2.004646089s
Apr 21 20:36:19.346: INFO: Pod "dns-test-d3d77282-bce5-47f2-909a-620e899ac581" satisfied condition "running"
STEP: retrieving the pod 04/21/23 20:36:19.346
STEP: looking for the results for each expected name from probers 04/21/23 20:36:19.348
Apr 21 20:36:19.350: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:19.352: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:19.352: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

Apr 21 20:36:24.357: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:24.359: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:24.359: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

Apr 21 20:36:29.356: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:29.358: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:29.358: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

Apr 21 20:36:34.357: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains '' instead of 'bar.example.com.'
Apr 21 20:36:34.359: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:34.359: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

Apr 21 20:36:39.356: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:39.358: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:39.358: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

Apr 21 20:36:44.356: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:44.357: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 21 20:36:44.357: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

Apr 21 20:36:49.358: INFO: DNS probes using dns-test-d3d77282-bce5-47f2-909a-620e899ac581 succeeded

STEP: deleting the pod 04/21/23 20:36:49.358
STEP: changing the service to type=ClusterIP 04/21/23 20:36:49.366
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
 04/21/23 20:36:49.377
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
 04/21/23 20:36:49.378
STEP: creating a third pod to probe DNS 04/21/23 20:36:49.378
STEP: submitting the pod to kubernetes 04/21/23 20:36:49.38
Apr 21 20:36:49.386: INFO: Waiting up to 15m0s for pod "dns-test-209c25da-7bed-44e0-9d04-384eb52a4245" in namespace "dns-4935" to be "running"
Apr 21 20:36:49.388: INFO: Pod "dns-test-209c25da-7bed-44e0-9d04-384eb52a4245": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040783ms
Apr 21 20:36:51.391: INFO: Pod "dns-test-209c25da-7bed-44e0-9d04-384eb52a4245": Phase="Running", Reason="", readiness=true. Elapsed: 2.005070603s
Apr 21 20:36:51.391: INFO: Pod "dns-test-209c25da-7bed-44e0-9d04-384eb52a4245" satisfied condition "running"
STEP: retrieving the pod 04/21/23 20:36:51.391
STEP: looking for the results for each expected name from probers 04/21/23 20:36:51.393
Apr 21 20:36:51.397: INFO: DNS probes using dns-test-209c25da-7bed-44e0-9d04-384eb52a4245 succeeded

STEP: deleting the pod 04/21/23 20:36:51.397
STEP: deleting the test externalName service 04/21/23 20:36:51.404
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 21 20:36:51.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4935" for this suite. 04/21/23 20:36:51.415
------------------------------
â€¢ [SLOW TEST] [36.129 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:36:15.29
    Apr 21 20:36:15.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename dns 04/21/23 20:36:15.29
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:15.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:15.298
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 04/21/23 20:36:15.3
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
     04/21/23 20:36:15.303
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
     04/21/23 20:36:15.303
    STEP: creating a pod to probe DNS 04/21/23 20:36:15.303
    STEP: submitting the pod to kubernetes 04/21/23 20:36:15.303
    Apr 21 20:36:15.311: INFO: Waiting up to 15m0s for pod "dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2" in namespace "dns-4935" to be "running"
    Apr 21 20:36:15.313: INFO: Pod "dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199981ms
    Apr 21 20:36:17.316: INFO: Pod "dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004588392s
    Apr 21 20:36:17.316: INFO: Pod "dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2" satisfied condition "running"
    STEP: retrieving the pod 04/21/23 20:36:17.316
    STEP: looking for the results for each expected name from probers 04/21/23 20:36:17.319
    Apr 21 20:36:17.322: INFO: DNS probes using dns-test-d16fad4f-68b5-4d12-8279-c833f9f629c2 succeeded

    STEP: deleting the pod 04/21/23 20:36:17.322
    STEP: changing the externalName to bar.example.com 04/21/23 20:36:17.332
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
     04/21/23 20:36:17.338
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
     04/21/23 20:36:17.338
    STEP: creating a second pod to probe DNS 04/21/23 20:36:17.338
    STEP: submitting the pod to kubernetes 04/21/23 20:36:17.338
    Apr 21 20:36:17.341: INFO: Waiting up to 15m0s for pod "dns-test-d3d77282-bce5-47f2-909a-620e899ac581" in namespace "dns-4935" to be "running"
    Apr 21 20:36:17.343: INFO: Pod "dns-test-d3d77282-bce5-47f2-909a-620e899ac581": Phase="Pending", Reason="", readiness=false. Elapsed: 1.591046ms
    Apr 21 20:36:19.346: INFO: Pod "dns-test-d3d77282-bce5-47f2-909a-620e899ac581": Phase="Running", Reason="", readiness=true. Elapsed: 2.004646089s
    Apr 21 20:36:19.346: INFO: Pod "dns-test-d3d77282-bce5-47f2-909a-620e899ac581" satisfied condition "running"
    STEP: retrieving the pod 04/21/23 20:36:19.346
    STEP: looking for the results for each expected name from probers 04/21/23 20:36:19.348
    Apr 21 20:36:19.350: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:19.352: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:19.352: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

    Apr 21 20:36:24.357: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:24.359: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:24.359: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

    Apr 21 20:36:29.356: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:29.358: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:29.358: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

    Apr 21 20:36:34.357: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains '' instead of 'bar.example.com.'
    Apr 21 20:36:34.359: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:34.359: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

    Apr 21 20:36:39.356: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:39.358: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:39.358: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

    Apr 21 20:36:44.356: INFO: File wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:44.357: INFO: File jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local from pod  dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 21 20:36:44.357: INFO: Lookups using dns-4935/dns-test-d3d77282-bce5-47f2-909a-620e899ac581 failed for: [wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local]

    Apr 21 20:36:49.358: INFO: DNS probes using dns-test-d3d77282-bce5-47f2-909a-620e899ac581 succeeded

    STEP: deleting the pod 04/21/23 20:36:49.358
    STEP: changing the service to type=ClusterIP 04/21/23 20:36:49.366
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
     04/21/23 20:36:49.377
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4935.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4935.svc.cluster.local; sleep 1; done
     04/21/23 20:36:49.378
    STEP: creating a third pod to probe DNS 04/21/23 20:36:49.378
    STEP: submitting the pod to kubernetes 04/21/23 20:36:49.38
    Apr 21 20:36:49.386: INFO: Waiting up to 15m0s for pod "dns-test-209c25da-7bed-44e0-9d04-384eb52a4245" in namespace "dns-4935" to be "running"
    Apr 21 20:36:49.388: INFO: Pod "dns-test-209c25da-7bed-44e0-9d04-384eb52a4245": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040783ms
    Apr 21 20:36:51.391: INFO: Pod "dns-test-209c25da-7bed-44e0-9d04-384eb52a4245": Phase="Running", Reason="", readiness=true. Elapsed: 2.005070603s
    Apr 21 20:36:51.391: INFO: Pod "dns-test-209c25da-7bed-44e0-9d04-384eb52a4245" satisfied condition "running"
    STEP: retrieving the pod 04/21/23 20:36:51.391
    STEP: looking for the results for each expected name from probers 04/21/23 20:36:51.393
    Apr 21 20:36:51.397: INFO: DNS probes using dns-test-209c25da-7bed-44e0-9d04-384eb52a4245 succeeded

    STEP: deleting the pod 04/21/23 20:36:51.397
    STEP: deleting the test externalName service 04/21/23 20:36:51.404
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:36:51.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4935" for this suite. 04/21/23 20:36:51.415
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:36:51.419
Apr 21 20:36:51.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename proxy 04/21/23 20:36:51.42
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:51.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:51.43
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Apr 21 20:36:51.432: INFO: Creating pod...
Apr 21 20:36:51.437: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8900" to be "running"
Apr 21 20:36:51.438: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.478316ms
Apr 21 20:36:53.442: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.0051975s
Apr 21 20:36:53.442: INFO: Pod "agnhost" satisfied condition "running"
Apr 21 20:36:53.442: INFO: Creating service...
Apr 21 20:36:53.451: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/DELETE
Apr 21 20:36:53.455: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 21 20:36:53.455: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/GET
Apr 21 20:36:53.457: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr 21 20:36:53.457: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/HEAD
Apr 21 20:36:53.459: INFO: http.Client request:HEAD | StatusCode:200
Apr 21 20:36:53.459: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/OPTIONS
Apr 21 20:36:53.460: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 21 20:36:53.460: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/PATCH
Apr 21 20:36:53.462: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 21 20:36:53.462: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/POST
Apr 21 20:36:53.464: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 21 20:36:53.464: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/PUT
Apr 21 20:36:53.465: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 21 20:36:53.465: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/DELETE
Apr 21 20:36:53.467: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 21 20:36:53.467: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/GET
Apr 21 20:36:53.469: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr 21 20:36:53.470: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/HEAD
Apr 21 20:36:53.471: INFO: http.Client request:HEAD | StatusCode:200
Apr 21 20:36:53.472: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/OPTIONS
Apr 21 20:36:53.474: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 21 20:36:53.474: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/PATCH
Apr 21 20:36:53.476: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 21 20:36:53.476: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/POST
Apr 21 20:36:53.478: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 21 20:36:53.478: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/PUT
Apr 21 20:36:53.480: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Apr 21 20:36:53.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-8900" for this suite. 04/21/23 20:36:53.482
------------------------------
â€¢ [2.068 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:36:51.419
    Apr 21 20:36:51.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename proxy 04/21/23 20:36:51.42
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:51.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:51.43
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Apr 21 20:36:51.432: INFO: Creating pod...
    Apr 21 20:36:51.437: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-8900" to be "running"
    Apr 21 20:36:51.438: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.478316ms
    Apr 21 20:36:53.442: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.0051975s
    Apr 21 20:36:53.442: INFO: Pod "agnhost" satisfied condition "running"
    Apr 21 20:36:53.442: INFO: Creating service...
    Apr 21 20:36:53.451: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/DELETE
    Apr 21 20:36:53.455: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 21 20:36:53.455: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/GET
    Apr 21 20:36:53.457: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Apr 21 20:36:53.457: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/HEAD
    Apr 21 20:36:53.459: INFO: http.Client request:HEAD | StatusCode:200
    Apr 21 20:36:53.459: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/OPTIONS
    Apr 21 20:36:53.460: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 21 20:36:53.460: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/PATCH
    Apr 21 20:36:53.462: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 21 20:36:53.462: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/POST
    Apr 21 20:36:53.464: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 21 20:36:53.464: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/pods/agnhost/proxy/some/path/with/PUT
    Apr 21 20:36:53.465: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr 21 20:36:53.465: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/DELETE
    Apr 21 20:36:53.467: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 21 20:36:53.467: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/GET
    Apr 21 20:36:53.469: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Apr 21 20:36:53.470: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/HEAD
    Apr 21 20:36:53.471: INFO: http.Client request:HEAD | StatusCode:200
    Apr 21 20:36:53.472: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/OPTIONS
    Apr 21 20:36:53.474: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 21 20:36:53.474: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/PATCH
    Apr 21 20:36:53.476: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 21 20:36:53.476: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/POST
    Apr 21 20:36:53.478: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 21 20:36:53.478: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-8900/services/test-service/proxy/some/path/with/PUT
    Apr 21 20:36:53.480: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:36:53.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-8900" for this suite. 04/21/23 20:36:53.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:36:53.488
Apr 21 20:36:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 20:36:53.489
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:53.498
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:53.5
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-611bd40d-9747-4968-974a-4fa47edbbaa2 04/21/23 20:36:53.503
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:36:53.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6181" for this suite. 04/21/23 20:36:53.506
------------------------------
â€¢ [0.022 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:36:53.488
    Apr 21 20:36:53.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 20:36:53.489
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:53.498
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:53.5
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-611bd40d-9747-4968-974a-4fa47edbbaa2 04/21/23 20:36:53.503
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:36:53.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6181" for this suite. 04/21/23 20:36:53.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:36:53.511
Apr 21 20:36:53.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:36:53.512
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:53.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:53.534
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-7d5ba657-1306-47d3-99ec-f6e120884e33 04/21/23 20:36:53.536
STEP: Creating a pod to test consume configMaps 04/21/23 20:36:53.538
Apr 21 20:36:53.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66" in namespace "projected-56" to be "Succeeded or Failed"
Apr 21 20:36:53.546: INFO: Pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645396ms
Apr 21 20:36:55.549: INFO: Pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004568772s
Apr 21 20:36:57.550: INFO: Pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005663151s
STEP: Saw pod success 04/21/23 20:36:57.55
Apr 21 20:36:57.550: INFO: Pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66" satisfied condition "Succeeded or Failed"
Apr 21 20:36:57.552: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 20:36:57.557
Apr 21 20:36:57.563: INFO: Waiting for pod pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66 to disappear
Apr 21 20:36:57.564: INFO: Pod pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:36:57.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-56" for this suite. 04/21/23 20:36:57.566
------------------------------
â€¢ [4.058 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:36:53.511
    Apr 21 20:36:53.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:36:53.512
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:53.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:53.534
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-7d5ba657-1306-47d3-99ec-f6e120884e33 04/21/23 20:36:53.536
    STEP: Creating a pod to test consume configMaps 04/21/23 20:36:53.538
    Apr 21 20:36:53.544: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66" in namespace "projected-56" to be "Succeeded or Failed"
    Apr 21 20:36:53.546: INFO: Pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66": Phase="Pending", Reason="", readiness=false. Elapsed: 1.645396ms
    Apr 21 20:36:55.549: INFO: Pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004568772s
    Apr 21 20:36:57.550: INFO: Pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005663151s
    STEP: Saw pod success 04/21/23 20:36:57.55
    Apr 21 20:36:57.550: INFO: Pod "pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66" satisfied condition "Succeeded or Failed"
    Apr 21 20:36:57.552: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 20:36:57.557
    Apr 21 20:36:57.563: INFO: Waiting for pod pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66 to disappear
    Apr 21 20:36:57.564: INFO: Pod pod-projected-configmaps-00624c5d-8334-4e76-8529-b3d4a4022b66 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:36:57.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-56" for this suite. 04/21/23 20:36:57.566
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:36:57.57
Apr 21 20:36:57.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename deployment 04/21/23 20:36:57.571
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:57.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:57.581
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Apr 21 20:36:57.583: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 21 20:36:57.588: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 21 20:37:02.590: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/21/23 20:37:02.59
Apr 21 20:37:02.590: INFO: Creating deployment "test-rolling-update-deployment"
Apr 21 20:37:02.593: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 21 20:37:02.596: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 21 20:37:04.600: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 21 20:37:04.602: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 21 20:37:04.606: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4682  bd7c428c-962c-42a9-b211-0986a572e153 5199 1 2023-04-21 20:37:02 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-21 20:37:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00292ee08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-21 20:37:02 +0000 UTC,LastTransitionTime:2023-04-21 20:37:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-04-21 20:37:03 +0000 UTC,LastTransitionTime:2023-04-21 20:37:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 21 20:37:04.608: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4682  7afb6f31-9212-414b-9fac-882529b59c85 5189 1 2023-04-21 20:37:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment bd7c428c-962c-42a9-b211-0986a572e153 0xc00292f2e7 0xc00292f2e8}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:37:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd7c428c-962c-42a9-b211-0986a572e153\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00292f398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 21 20:37:04.608: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 21 20:37:04.608: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4682  7558f26b-7fee-492b-9b72-07a0aa745f0a 5198 2 2023-04-21 20:36:57 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment bd7c428c-962c-42a9-b211-0986a572e153 0xc00292f1bf 0xc00292f1d0}] [] [{e2e.test Update apps/v1 2023-04-21 20:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd7c428c-962c-42a9-b211-0986a572e153\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00292f288 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 21 20:37:04.610: INFO: Pod "test-rolling-update-deployment-7549d9f46d-4crl7" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-4crl7 test-rolling-update-deployment-7549d9f46d- deployment-4682  01e9eeff-8625-4988-b47e-58b8eb0eda8d 5188 0 2023-04-21 20:37:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 7afb6f31-9212-414b-9fac-882529b59c85 0xc004124c17 0xc004124c18}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7afb6f31-9212-414b-9fac-882529b59c85\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4xd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4xd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.90,StartTime:2023-04-21 20:37:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:docker://a544f9bd7ba2caa76710f29f6d71bbb85764e2455f8b8aeb6b284d81d7c0fe42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 21 20:37:04.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4682" for this suite. 04/21/23 20:37:04.612
------------------------------
â€¢ [SLOW TEST] [7.047 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:36:57.57
    Apr 21 20:36:57.570: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename deployment 04/21/23 20:36:57.571
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:36:57.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:36:57.581
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Apr 21 20:36:57.583: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Apr 21 20:36:57.588: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 21 20:37:02.590: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/21/23 20:37:02.59
    Apr 21 20:37:02.590: INFO: Creating deployment "test-rolling-update-deployment"
    Apr 21 20:37:02.593: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Apr 21 20:37:02.596: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Apr 21 20:37:04.600: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Apr 21 20:37:04.602: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 21 20:37:04.606: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4682  bd7c428c-962c-42a9-b211-0986a572e153 5199 1 2023-04-21 20:37:02 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-21 20:37:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00292ee08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-21 20:37:02 +0000 UTC,LastTransitionTime:2023-04-21 20:37:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-04-21 20:37:03 +0000 UTC,LastTransitionTime:2023-04-21 20:37:02 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 21 20:37:04.608: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-4682  7afb6f31-9212-414b-9fac-882529b59c85 5189 1 2023-04-21 20:37:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment bd7c428c-962c-42a9-b211-0986a572e153 0xc00292f2e7 0xc00292f2e8}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:37:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd7c428c-962c-42a9-b211-0986a572e153\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00292f398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 20:37:04.608: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Apr 21 20:37:04.608: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4682  7558f26b-7fee-492b-9b72-07a0aa745f0a 5198 2 2023-04-21 20:36:57 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment bd7c428c-962c-42a9-b211-0986a572e153 0xc00292f1bf 0xc00292f1d0}] [] [{e2e.test Update apps/v1 2023-04-21 20:36:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd7c428c-962c-42a9-b211-0986a572e153\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00292f288 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 20:37:04.610: INFO: Pod "test-rolling-update-deployment-7549d9f46d-4crl7" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-4crl7 test-rolling-update-deployment-7549d9f46d- deployment-4682  01e9eeff-8625-4988-b47e-58b8eb0eda8d 5188 0 2023-04-21 20:37:02 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 7afb6f31-9212-414b-9fac-882529b59c85 0xc004124c17 0xc004124c18}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7afb6f31-9212-414b-9fac-882529b59c85\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:03 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.90\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m4xd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m4xd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.90,StartTime:2023-04-21 20:37:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:docker://a544f9bd7ba2caa76710f29f6d71bbb85764e2455f8b8aeb6b284d81d7c0fe42,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.90,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:37:04.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4682" for this suite. 04/21/23 20:37:04.612
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:37:04.617
Apr 21 20:37:04.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 20:37:04.618
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:37:04.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:37:04.626
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 04/21/23 20:37:04.628
Apr 21 20:37:04.633: INFO: Waiting up to 5m0s for pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0" in namespace "downward-api-2212" to be "Succeeded or Failed"
Apr 21 20:37:04.635: INFO: Pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.386024ms
Apr 21 20:37:06.638: INFO: Pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004561944s
Apr 21 20:37:08.638: INFO: Pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004165112s
STEP: Saw pod success 04/21/23 20:37:08.638
Apr 21 20:37:08.638: INFO: Pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0" satisfied condition "Succeeded or Failed"
Apr 21 20:37:08.640: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0 container dapi-container: <nil>
STEP: delete the pod 04/21/23 20:37:08.645
Apr 21 20:37:08.653: INFO: Waiting for pod downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0 to disappear
Apr 21 20:37:08.655: INFO: Pod downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 21 20:37:08.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2212" for this suite. 04/21/23 20:37:08.657
------------------------------
â€¢ [4.044 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:37:04.617
    Apr 21 20:37:04.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 20:37:04.618
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:37:04.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:37:04.626
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 04/21/23 20:37:04.628
    Apr 21 20:37:04.633: INFO: Waiting up to 5m0s for pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0" in namespace "downward-api-2212" to be "Succeeded or Failed"
    Apr 21 20:37:04.635: INFO: Pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.386024ms
    Apr 21 20:37:06.638: INFO: Pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004561944s
    Apr 21 20:37:08.638: INFO: Pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004165112s
    STEP: Saw pod success 04/21/23 20:37:08.638
    Apr 21 20:37:08.638: INFO: Pod "downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0" satisfied condition "Succeeded or Failed"
    Apr 21 20:37:08.640: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0 container dapi-container: <nil>
    STEP: delete the pod 04/21/23 20:37:08.645
    Apr 21 20:37:08.653: INFO: Waiting for pod downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0 to disappear
    Apr 21 20:37:08.655: INFO: Pod downward-api-60bc049f-5e0d-4e29-8d3b-d3e30b3ef6d0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:37:08.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2212" for this suite. 04/21/23 20:37:08.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:37:08.662
Apr 21 20:37:08.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename deployment 04/21/23 20:37:08.663
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:37:08.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:37:08.671
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Apr 21 20:37:08.673: INFO: Creating deployment "webserver-deployment"
Apr 21 20:37:08.677: INFO: Waiting for observed generation 1
Apr 21 20:37:10.683: INFO: Waiting for all required pods to come up
Apr 21 20:37:10.686: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 04/21/23 20:37:10.686
Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z66t4" in namespace "deployment-9226" to be "running"
Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dgdm8" in namespace "deployment-9226" to be "running"
Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4lg4f" in namespace "deployment-9226" to be "running"
Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-b77fw" in namespace "deployment-9226" to be "running"
Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bfbcn" in namespace "deployment-9226" to be "running"
Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nszpk" in namespace "deployment-9226" to be "running"
Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7mshc" in namespace "deployment-9226" to be "running"
Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rsmdg" in namespace "deployment-9226" to be "running"
Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5d494" in namespace "deployment-9226" to be "running"
Apr 21 20:37:10.689: INFO: Pod "webserver-deployment-7f5969cbc7-7mshc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.720758ms
Apr 21 20:37:10.689: INFO: Pod "webserver-deployment-7f5969cbc7-dgdm8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896972ms
Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-bfbcn": Phase="Pending", Reason="", readiness=false. Elapsed: 46.131664ms
Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-z66t4": Phase="Pending", Reason="", readiness=false. Elapsed: 46.212469ms
Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-rsmdg": Phase="Pending", Reason="", readiness=false. Elapsed: 46.140932ms
Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-nszpk": Phase="Pending", Reason="", readiness=false. Elapsed: 46.180801ms
Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-4lg4f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.387868ms
Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-5d494": Phase="Pending", Reason="", readiness=false. Elapsed: 46.28776ms
Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-b77fw": Phase="Pending", Reason="", readiness=false. Elapsed: 46.43404ms
Apr 21 20:37:12.692: INFO: Pod "webserver-deployment-7f5969cbc7-dgdm8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005414462s
Apr 21 20:37:12.692: INFO: Pod "webserver-deployment-7f5969cbc7-dgdm8" satisfied condition "running"
Apr 21 20:37:12.692: INFO: Pod "webserver-deployment-7f5969cbc7-7mshc": Phase="Running", Reason="", readiness=true. Elapsed: 2.005296812s
Apr 21 20:37:12.692: INFO: Pod "webserver-deployment-7f5969cbc7-7mshc" satisfied condition "running"
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-b77fw": Phase="Running", Reason="", readiness=true. Elapsed: 2.048565285s
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-b77fw" satisfied condition "running"
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-bfbcn": Phase="Running", Reason="", readiness=true. Elapsed: 2.048619243s
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-bfbcn" satisfied condition "running"
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-rsmdg": Phase="Running", Reason="", readiness=true. Elapsed: 2.048621866s
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-rsmdg" satisfied condition "running"
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-z66t4": Phase="Running", Reason="", readiness=true. Elapsed: 2.048845071s
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-z66t4" satisfied condition "running"
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-nszpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.048714101s
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-4lg4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.048852063s
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-4lg4f" satisfied condition "running"
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-nszpk" satisfied condition "running"
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-5d494": Phase="Running", Reason="", readiness=true. Elapsed: 2.048770288s
Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-5d494" satisfied condition "running"
Apr 21 20:37:12.735: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 21 20:37:12.738: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 21 20:37:12.744: INFO: Updating deployment webserver-deployment
Apr 21 20:37:12.744: INFO: Waiting for observed generation 2
Apr 21 20:37:14.748: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 21 20:37:14.749: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 21 20:37:14.751: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 21 20:37:14.757: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 21 20:37:14.757: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 21 20:37:14.758: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 21 20:37:14.761: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 21 20:37:14.761: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 21 20:37:14.767: INFO: Updating deployment webserver-deployment
Apr 21 20:37:14.767: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 21 20:37:14.769: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 21 20:37:14.771: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 21 20:37:16.778: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-9226  1db1da84-f925-4aba-a2e9-302a9ef6a9a5 5509 3 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f3da28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-21 20:37:14 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-04-21 20:37:14 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Apr 21 20:37:16.780: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-9226  c5059827-bc74-48a9-bbaa-9e1048c0a9ad 5501 3 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1db1da84-f925-4aba-a2e9-302a9ef6a9a5 0xc0041e9417 0xc0041e9418}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1db1da84-f925-4aba-a2e9-302a9ef6a9a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041e94b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 21 20:37:16.780: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 21 20:37:16.781: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-9226  25f4b400-243b-4274-828f-da71d0c9c2cf 5498 3 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1db1da84-f925-4aba-a2e9-302a9ef6a9a5 0xc0041e9327 0xc0041e9328}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1db1da84-f925-4aba-a2e9-302a9ef6a9a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041e93b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Apr 21 20:37:16.785: INFO: Pod "webserver-deployment-7f5969cbc7-5d494" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5d494 webserver-deployment-7f5969cbc7- deployment-9226  7fb4b763-b22c-4ecd-9640-6e3701a5855c 5342 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc002f3de50 0xc002f3de51}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r9cw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r9cw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.18,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://e382a61f8b5fe359ecc1a3bf9ce0dec6c4c01c11cbdf553617e6145fe00006a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.786: INFO: Pod "webserver-deployment-7f5969cbc7-7mshc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7mshc webserver-deployment-7f5969cbc7- deployment-9226  61d14cd8-8254-44de-9f9f-4d5ebc2f2737 5344 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360030 0xc004360031}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gqb86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gqb86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.16,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://cd9cb74e9d71022a5dcafaeb022f83fd7c6944f7954a6d04fdb054cd30ff4c1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.786: INFO: Pod "webserver-deployment-7f5969cbc7-8s489" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8s489 webserver-deployment-7f5969cbc7- deployment-9226  8b278cb9-2008-4c83-9d12-77a34be43512 5455 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360220 0xc004360221}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mwxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mwxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.786: INFO: Pod "webserver-deployment-7f5969cbc7-94hnx" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-94hnx webserver-deployment-7f5969cbc7- deployment-9226  9d7ed5b8-5634-48e8-b553-997c2f037e9c 5512 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360380 0xc004360381}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8dnxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8dnxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.787: INFO: Pod "webserver-deployment-7f5969cbc7-b77fw" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b77fw webserver-deployment-7f5969cbc7- deployment-9226  2e8d2f9f-5c2a-43b4-b5ba-406034560b02 5359 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360557 0xc004360558}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5ckk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5ckk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.15,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://4d989f63befb8b95b7823a607a6a1f88c14de6c17fc1c7bc4029481e8f9bac63,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.787: INFO: Pod "webserver-deployment-7f5969cbc7-bfbcn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bfbcn webserver-deployment-7f5969cbc7- deployment-9226  cfc5d6ca-d378-4b36-914f-b70a4f10b481 5338 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360750 0xc004360751}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwcqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwcqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.93,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://be25f9e3ef0507c33a7a7c4eb22180dca1e2e93bbecedc7cca193ddd9807d064,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.787: INFO: Pod "webserver-deployment-7f5969cbc7-dgdm8" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dgdm8 webserver-deployment-7f5969cbc7- deployment-9226  12c4a211-846f-4c67-b54a-76d0eeb734b1 5352 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360940 0xc004360941}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtrrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtrrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.19,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://880d3d687fad30b496cb8798934339e108f1376525a2dc5c6d3d4838b61c381e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.788: INFO: Pod "webserver-deployment-7f5969cbc7-dm59q" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dm59q webserver-deployment-7f5969cbc7- deployment-9226  2e79589f-1831-416f-8087-4375065d61a6 5492 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360b30 0xc004360b31}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpts4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpts4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.788: INFO: Pod "webserver-deployment-7f5969cbc7-fjwf6" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fjwf6 webserver-deployment-7f5969cbc7- deployment-9226  29c72ec5-a583-4336-a876-012c32d67d7f 5488 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360c90 0xc004360c91}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4v6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4v6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.788: INFO: Pod "webserver-deployment-7f5969cbc7-gqzsk" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gqzsk webserver-deployment-7f5969cbc7- deployment-9226  07c57f57-474d-4285-afab-f995a868675a 5449 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360df0 0xc004360df1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j97qt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j97qt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.788: INFO: Pod "webserver-deployment-7f5969cbc7-j5k4x" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j5k4x webserver-deployment-7f5969cbc7- deployment-9226  9da2d379-1dc9-4441-a09d-87910edd9f23 5470 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360f40 0xc004360f41}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kcw7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kcw7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.789: INFO: Pod "webserver-deployment-7f5969cbc7-jh6n9" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jh6n9 webserver-deployment-7f5969cbc7- deployment-9226  6e3e5d39-176a-44c5-950c-74e595b3b956 5489 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361090 0xc004361091}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6hrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6hrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.789: INFO: Pod "webserver-deployment-7f5969cbc7-jvg9v" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jvg9v webserver-deployment-7f5969cbc7- deployment-9226  0dfe32c9-3ae2-4090-96b1-e8511da25f2c 5324 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc0043611e0 0xc0043611e1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z88dr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z88dr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.92,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://c0b6b2949a9d5bd22c831bf9f98d9df22d6a66ed8dc928d85d91122db8ce8db3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.789: INFO: Pod "webserver-deployment-7f5969cbc7-nszpk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nszpk webserver-deployment-7f5969cbc7- deployment-9226  7786b9da-3a57-4e3d-aef8-f7032e22f61f 5355 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc0043613c0 0xc0043613c1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fz596,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fz596,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.95,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://26a845117a40f64f1b2719106052709f3f328e8d990ad5073cb29823caada846,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.790: INFO: Pod "webserver-deployment-7f5969cbc7-qk4bb" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qk4bb webserver-deployment-7f5969cbc7- deployment-9226  20f158a6-4653-47ea-b7dc-34a21f2bc690 5487 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361590 0xc004361591}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xf9pj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xf9pj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.790: INFO: Pod "webserver-deployment-7f5969cbc7-rg8lg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rg8lg webserver-deployment-7f5969cbc7- deployment-9226  e3601088-e85e-4294-9cb0-1c4cae2c9517 5507 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc0043616f0 0xc0043616f1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8kqz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8kqz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.790: INFO: Pod "webserver-deployment-7f5969cbc7-rsmdg" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rsmdg webserver-deployment-7f5969cbc7- deployment-9226  7484f0e6-9614-4354-8fb5-84b5c6d6c18d 5347 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc0043618b7 0xc0043618b8}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hz56r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hz56r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.17,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://23251d4dfede2ff6406c64a475d4cc6001465ffbc0729b7e7845b683711d5c47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.791: INFO: Pod "webserver-deployment-7f5969cbc7-sncpw" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sncpw webserver-deployment-7f5969cbc7- deployment-9226  3110e666-d539-4d30-809f-e09434fd4764 5494 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361aa0 0xc004361aa1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b44r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b44r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.791: INFO: Pod "webserver-deployment-7f5969cbc7-vtngf" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vtngf webserver-deployment-7f5969cbc7- deployment-9226  fb583aed-076e-4e9d-aa2a-0be9bf2519e2 5451 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361c00 0xc004361c01}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgkhr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgkhr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.791: INFO: Pod "webserver-deployment-7f5969cbc7-ww25s" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ww25s webserver-deployment-7f5969cbc7- deployment-9226  c36eeba0-6521-4f61-b33d-f07cbbd6e012 5467 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361d50 0xc004361d51}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84qbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84qbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.791: INFO: Pod "webserver-deployment-d9f79cb5-cbnlb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cbnlb webserver-deployment-d9f79cb5- deployment-9226  479bb53b-01fd-4e6a-a129-59303b64ad43 5491 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc004361e8f 0xc004361ea0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4ddrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4ddrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-f2j7z" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f2j7z webserver-deployment-d9f79cb5- deployment-9226  496fa106-26dd-41ee-aa71-94dfab6c886e 5490 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc004361fef 0xc003620000}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vhgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vhgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-f84c4" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f84c4 webserver-deployment-d9f79cb5- deployment-9226  712f9bb5-a7a3-4c7d-9d77-796a16a53190 5481 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc00362015f 0xc003620170}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqh22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqh22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-fbkxc" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fbkxc webserver-deployment-d9f79cb5- deployment-9226  82973618-5b80-467c-9835-d79a019f0c66 5500 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036202bf 0xc0036202d0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6wjd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6wjd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-gmcf9" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gmcf9 webserver-deployment-d9f79cb5- deployment-9226  d26a3bed-cf85-4283-b7ea-2f0783e35577 5462 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc00362049f 0xc0036204b0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcqcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcqcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-j447l" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j447l webserver-deployment-d9f79cb5- deployment-9226  c067d0b0-1867-475d-bf27-3514d9ee90cb 5395 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036205ff 0xc003620610}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8zfg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8zfg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-kn47f" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kn47f webserver-deployment-d9f79cb5- deployment-9226  a996a902-709c-4350-80a1-4bc2cb57ea19 5529 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036207df 0xc0036207f0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzhzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzhzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-mdgth" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mdgth webserver-deployment-d9f79cb5- deployment-9226  656f3414-2874-4c89-af78-508e5cdbc6e5 5431 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036209bf 0xc0036209d0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdb6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdb6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-ncpdb" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ncpdb webserver-deployment-d9f79cb5- deployment-9226  811027c7-be43-425d-a7f1-70ee169e842f 5434 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc003620b8f 0xc003620ba0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7pxkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7pxkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-njdpq" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-njdpq webserver-deployment-d9f79cb5- deployment-9226  23c14e8b-a8e9-46f0-8fc5-5a03faf37ff4 5483 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc003620d5f 0xc003620d70}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ccmph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ccmph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-nvzmt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nvzmt webserver-deployment-d9f79cb5- deployment-9226  89230a7b-3f79-4d21-a877-8c9c2950bb5d 5413 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc003620f3f 0xc003620f50}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwzkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwzkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.794: INFO: Pod "webserver-deployment-d9f79cb5-vsqqx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vsqqx webserver-deployment-d9f79cb5- deployment-9226  088f3e10-a8a6-440e-9f41-44d5145e41a0 5415 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc00362111f 0xc003621130}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pc6xb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pc6xb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:37:16.794: INFO: Pod "webserver-deployment-d9f79cb5-z57jr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z57jr webserver-deployment-d9f79cb5- deployment-9226  ff794384-cc51-4ff2-9cbd-e312a58d8249 5497 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036212ef 0xc003621300}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9rl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9rl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 21 20:37:16.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9226" for this suite. 04/21/23 20:37:16.835
------------------------------
â€¢ [SLOW TEST] [8.178 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:37:08.662
    Apr 21 20:37:08.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename deployment 04/21/23 20:37:08.663
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:37:08.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:37:08.671
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Apr 21 20:37:08.673: INFO: Creating deployment "webserver-deployment"
    Apr 21 20:37:08.677: INFO: Waiting for observed generation 1
    Apr 21 20:37:10.683: INFO: Waiting for all required pods to come up
    Apr 21 20:37:10.686: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 04/21/23 20:37:10.686
    Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-z66t4" in namespace "deployment-9226" to be "running"
    Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-dgdm8" in namespace "deployment-9226" to be "running"
    Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-4lg4f" in namespace "deployment-9226" to be "running"
    Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-b77fw" in namespace "deployment-9226" to be "running"
    Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bfbcn" in namespace "deployment-9226" to be "running"
    Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-nszpk" in namespace "deployment-9226" to be "running"
    Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-7mshc" in namespace "deployment-9226" to be "running"
    Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-rsmdg" in namespace "deployment-9226" to be "running"
    Apr 21 20:37:10.686: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5d494" in namespace "deployment-9226" to be "running"
    Apr 21 20:37:10.689: INFO: Pod "webserver-deployment-7f5969cbc7-7mshc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.720758ms
    Apr 21 20:37:10.689: INFO: Pod "webserver-deployment-7f5969cbc7-dgdm8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.896972ms
    Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-bfbcn": Phase="Pending", Reason="", readiness=false. Elapsed: 46.131664ms
    Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-z66t4": Phase="Pending", Reason="", readiness=false. Elapsed: 46.212469ms
    Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-rsmdg": Phase="Pending", Reason="", readiness=false. Elapsed: 46.140932ms
    Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-nszpk": Phase="Pending", Reason="", readiness=false. Elapsed: 46.180801ms
    Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-4lg4f": Phase="Pending", Reason="", readiness=false. Elapsed: 46.387868ms
    Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-5d494": Phase="Pending", Reason="", readiness=false. Elapsed: 46.28776ms
    Apr 21 20:37:10.733: INFO: Pod "webserver-deployment-7f5969cbc7-b77fw": Phase="Pending", Reason="", readiness=false. Elapsed: 46.43404ms
    Apr 21 20:37:12.692: INFO: Pod "webserver-deployment-7f5969cbc7-dgdm8": Phase="Running", Reason="", readiness=true. Elapsed: 2.005414462s
    Apr 21 20:37:12.692: INFO: Pod "webserver-deployment-7f5969cbc7-dgdm8" satisfied condition "running"
    Apr 21 20:37:12.692: INFO: Pod "webserver-deployment-7f5969cbc7-7mshc": Phase="Running", Reason="", readiness=true. Elapsed: 2.005296812s
    Apr 21 20:37:12.692: INFO: Pod "webserver-deployment-7f5969cbc7-7mshc" satisfied condition "running"
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-b77fw": Phase="Running", Reason="", readiness=true. Elapsed: 2.048565285s
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-b77fw" satisfied condition "running"
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-bfbcn": Phase="Running", Reason="", readiness=true. Elapsed: 2.048619243s
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-bfbcn" satisfied condition "running"
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-rsmdg": Phase="Running", Reason="", readiness=true. Elapsed: 2.048621866s
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-rsmdg" satisfied condition "running"
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-z66t4": Phase="Running", Reason="", readiness=true. Elapsed: 2.048845071s
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-z66t4" satisfied condition "running"
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-nszpk": Phase="Running", Reason="", readiness=true. Elapsed: 2.048714101s
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-4lg4f": Phase="Running", Reason="", readiness=true. Elapsed: 2.048852063s
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-4lg4f" satisfied condition "running"
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-nszpk" satisfied condition "running"
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-5d494": Phase="Running", Reason="", readiness=true. Elapsed: 2.048770288s
    Apr 21 20:37:12.735: INFO: Pod "webserver-deployment-7f5969cbc7-5d494" satisfied condition "running"
    Apr 21 20:37:12.735: INFO: Waiting for deployment "webserver-deployment" to complete
    Apr 21 20:37:12.738: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Apr 21 20:37:12.744: INFO: Updating deployment webserver-deployment
    Apr 21 20:37:12.744: INFO: Waiting for observed generation 2
    Apr 21 20:37:14.748: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Apr 21 20:37:14.749: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Apr 21 20:37:14.751: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Apr 21 20:37:14.757: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Apr 21 20:37:14.757: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Apr 21 20:37:14.758: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Apr 21 20:37:14.761: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Apr 21 20:37:14.761: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Apr 21 20:37:14.767: INFO: Updating deployment webserver-deployment
    Apr 21 20:37:14.767: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Apr 21 20:37:14.769: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Apr 21 20:37:14.771: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 21 20:37:16.778: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-9226  1db1da84-f925-4aba-a2e9-302a9ef6a9a5 5509 3 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002f3da28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-21 20:37:14 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-04-21 20:37:14 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Apr 21 20:37:16.780: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-9226  c5059827-bc74-48a9-bbaa-9e1048c0a9ad 5501 3 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 1db1da84-f925-4aba-a2e9-302a9ef6a9a5 0xc0041e9417 0xc0041e9418}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1db1da84-f925-4aba-a2e9-302a9ef6a9a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041e94b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 20:37:16.780: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Apr 21 20:37:16.781: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-9226  25f4b400-243b-4274-828f-da71d0c9c2cf 5498 3 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 1db1da84-f925-4aba-a2e9-302a9ef6a9a5 0xc0041e9327 0xc0041e9328}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1db1da84-f925-4aba-a2e9-302a9ef6a9a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041e93b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 20:37:16.785: INFO: Pod "webserver-deployment-7f5969cbc7-5d494" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5d494 webserver-deployment-7f5969cbc7- deployment-9226  7fb4b763-b22c-4ecd-9640-6e3701a5855c 5342 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc002f3de50 0xc002f3de51}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r9cw9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r9cw9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.18,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://e382a61f8b5fe359ecc1a3bf9ce0dec6c4c01c11cbdf553617e6145fe00006a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.18,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.786: INFO: Pod "webserver-deployment-7f5969cbc7-7mshc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-7mshc webserver-deployment-7f5969cbc7- deployment-9226  61d14cd8-8254-44de-9f9f-4d5ebc2f2737 5344 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360030 0xc004360031}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gqb86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gqb86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.16,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://cd9cb74e9d71022a5dcafaeb022f83fd7c6944f7954a6d04fdb054cd30ff4c1c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.16,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.786: INFO: Pod "webserver-deployment-7f5969cbc7-8s489" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-8s489 webserver-deployment-7f5969cbc7- deployment-9226  8b278cb9-2008-4c83-9d12-77a34be43512 5455 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360220 0xc004360221}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mwxk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mwxk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.786: INFO: Pod "webserver-deployment-7f5969cbc7-94hnx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-94hnx webserver-deployment-7f5969cbc7- deployment-9226  9d7ed5b8-5634-48e8-b553-997c2f037e9c 5512 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360380 0xc004360381}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8dnxl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8dnxl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.787: INFO: Pod "webserver-deployment-7f5969cbc7-b77fw" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-b77fw webserver-deployment-7f5969cbc7- deployment-9226  2e8d2f9f-5c2a-43b4-b5ba-406034560b02 5359 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360557 0xc004360558}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z5ckk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z5ckk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.15,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://4d989f63befb8b95b7823a607a6a1f88c14de6c17fc1c7bc4029481e8f9bac63,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.15,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.787: INFO: Pod "webserver-deployment-7f5969cbc7-bfbcn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bfbcn webserver-deployment-7f5969cbc7- deployment-9226  cfc5d6ca-d378-4b36-914f-b70a4f10b481 5338 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360750 0xc004360751}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cwcqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cwcqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.93,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://be25f9e3ef0507c33a7a7c4eb22180dca1e2e93bbecedc7cca193ddd9807d064,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.93,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.787: INFO: Pod "webserver-deployment-7f5969cbc7-dgdm8" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dgdm8 webserver-deployment-7f5969cbc7- deployment-9226  12c4a211-846f-4c67-b54a-76d0eeb734b1 5352 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360940 0xc004360941}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtrrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtrrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.19,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://880d3d687fad30b496cb8798934339e108f1376525a2dc5c6d3d4838b61c381e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.19,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.788: INFO: Pod "webserver-deployment-7f5969cbc7-dm59q" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dm59q webserver-deployment-7f5969cbc7- deployment-9226  2e79589f-1831-416f-8087-4375065d61a6 5492 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360b30 0xc004360b31}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dpts4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dpts4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.788: INFO: Pod "webserver-deployment-7f5969cbc7-fjwf6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-fjwf6 webserver-deployment-7f5969cbc7- deployment-9226  29c72ec5-a583-4336-a876-012c32d67d7f 5488 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360c90 0xc004360c91}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w4v6s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w4v6s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.788: INFO: Pod "webserver-deployment-7f5969cbc7-gqzsk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-gqzsk webserver-deployment-7f5969cbc7- deployment-9226  07c57f57-474d-4285-afab-f995a868675a 5449 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360df0 0xc004360df1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j97qt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j97qt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.788: INFO: Pod "webserver-deployment-7f5969cbc7-j5k4x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-j5k4x webserver-deployment-7f5969cbc7- deployment-9226  9da2d379-1dc9-4441-a09d-87910edd9f23 5470 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004360f40 0xc004360f41}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kcw7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kcw7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.789: INFO: Pod "webserver-deployment-7f5969cbc7-jh6n9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jh6n9 webserver-deployment-7f5969cbc7- deployment-9226  6e3e5d39-176a-44c5-950c-74e595b3b956 5489 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361090 0xc004361091}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6hrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6hrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.789: INFO: Pod "webserver-deployment-7f5969cbc7-jvg9v" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jvg9v webserver-deployment-7f5969cbc7- deployment-9226  0dfe32c9-3ae2-4090-96b1-e8511da25f2c 5324 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc0043611e0 0xc0043611e1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.92\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z88dr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z88dr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.92,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://c0b6b2949a9d5bd22c831bf9f98d9df22d6a66ed8dc928d85d91122db8ce8db3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.92,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.789: INFO: Pod "webserver-deployment-7f5969cbc7-nszpk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-nszpk webserver-deployment-7f5969cbc7- deployment-9226  7786b9da-3a57-4e3d-aef8-f7032e22f61f 5355 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc0043613c0 0xc0043613c1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.95\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fz596,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fz596,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.95,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://26a845117a40f64f1b2719106052709f3f328e8d990ad5073cb29823caada846,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.95,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.790: INFO: Pod "webserver-deployment-7f5969cbc7-qk4bb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qk4bb webserver-deployment-7f5969cbc7- deployment-9226  20f158a6-4653-47ea-b7dc-34a21f2bc690 5487 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361590 0xc004361591}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xf9pj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xf9pj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.790: INFO: Pod "webserver-deployment-7f5969cbc7-rg8lg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rg8lg webserver-deployment-7f5969cbc7- deployment-9226  e3601088-e85e-4294-9cb0-1c4cae2c9517 5507 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc0043616f0 0xc0043616f1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s8kqz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s8kqz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.790: INFO: Pod "webserver-deployment-7f5969cbc7-rsmdg" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rsmdg webserver-deployment-7f5969cbc7- deployment-9226  7484f0e6-9614-4354-8fb5-84b5c6d6c18d 5347 0 2023-04-21 20:37:08 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc0043618b7 0xc0043618b8}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hz56r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hz56r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.17,StartTime:2023-04-21 20:37:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:37:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://23251d4dfede2ff6406c64a475d4cc6001465ffbc0729b7e7845b683711d5c47,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.17,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.791: INFO: Pod "webserver-deployment-7f5969cbc7-sncpw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-sncpw webserver-deployment-7f5969cbc7- deployment-9226  3110e666-d539-4d30-809f-e09434fd4764 5494 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361aa0 0xc004361aa1}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b44r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b44r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.791: INFO: Pod "webserver-deployment-7f5969cbc7-vtngf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-vtngf webserver-deployment-7f5969cbc7- deployment-9226  fb583aed-076e-4e9d-aa2a-0be9bf2519e2 5451 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361c00 0xc004361c01}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgkhr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgkhr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.791: INFO: Pod "webserver-deployment-7f5969cbc7-ww25s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ww25s webserver-deployment-7f5969cbc7- deployment-9226  c36eeba0-6521-4f61-b33d-f07cbbd6e012 5467 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 25f4b400-243b-4274-828f-da71d0c9c2cf 0xc004361d50 0xc004361d51}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25f4b400-243b-4274-828f-da71d0c9c2cf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-84qbq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-84qbq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.791: INFO: Pod "webserver-deployment-d9f79cb5-cbnlb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cbnlb webserver-deployment-d9f79cb5- deployment-9226  479bb53b-01fd-4e6a-a129-59303b64ad43 5491 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc004361e8f 0xc004361ea0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4ddrd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4ddrd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-f2j7z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f2j7z webserver-deployment-d9f79cb5- deployment-9226  496fa106-26dd-41ee-aa71-94dfab6c886e 5490 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc004361fef 0xc003620000}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vhgt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vhgt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-f84c4" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f84c4 webserver-deployment-d9f79cb5- deployment-9226  712f9bb5-a7a3-4c7d-9d77-796a16a53190 5481 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc00362015f 0xc003620170}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qqh22,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qqh22,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-fbkxc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fbkxc webserver-deployment-d9f79cb5- deployment-9226  82973618-5b80-467c-9835-d79a019f0c66 5500 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036202bf 0xc0036202d0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6wjd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6wjd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-gmcf9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gmcf9 webserver-deployment-d9f79cb5- deployment-9226  d26a3bed-cf85-4283-b7ea-2f0783e35577 5462 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc00362049f 0xc0036204b0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcqcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcqcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.792: INFO: Pod "webserver-deployment-d9f79cb5-j447l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-j447l webserver-deployment-d9f79cb5- deployment-9226  c067d0b0-1867-475d-bf27-3514d9ee90cb 5395 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036205ff 0xc003620610}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n8zfg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n8zfg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-kn47f" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-kn47f webserver-deployment-d9f79cb5- deployment-9226  a996a902-709c-4350-80a1-4bc2cb57ea19 5529 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036207df 0xc0036207f0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xzhzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xzhzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-mdgth" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-mdgth webserver-deployment-d9f79cb5- deployment-9226  656f3414-2874-4c89-af78-508e5cdbc6e5 5431 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036209bf 0xc0036209d0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdb6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdb6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-ncpdb" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-ncpdb webserver-deployment-d9f79cb5- deployment-9226  811027c7-be43-425d-a7f1-70ee169e842f 5434 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc003620b8f 0xc003620ba0}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7pxkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7pxkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-njdpq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-njdpq webserver-deployment-d9f79cb5- deployment-9226  23c14e8b-a8e9-46f0-8fc5-5a03faf37ff4 5483 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc003620d5f 0xc003620d70}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ccmph,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ccmph,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.793: INFO: Pod "webserver-deployment-d9f79cb5-nvzmt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-nvzmt webserver-deployment-d9f79cb5- deployment-9226  89230a7b-3f79-4d21-a877-8c9c2950bb5d 5413 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc003620f3f 0xc003620f50}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwzkb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwzkb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.794: INFO: Pod "webserver-deployment-d9f79cb5-vsqqx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vsqqx webserver-deployment-d9f79cb5- deployment-9226  088f3e10-a8a6-440e-9f41-44d5145e41a0 5415 0 2023-04-21 20:37:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc00362111f 0xc003621130}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:37:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pc6xb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pc6xb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-04-21 20:37:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:37:16.794: INFO: Pod "webserver-deployment-d9f79cb5-z57jr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-z57jr webserver-deployment-d9f79cb5- deployment-9226  ff794384-cc51-4ff2-9cbd-e312a58d8249 5497 0 2023-04-21 20:37:14 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 c5059827-bc74-48a9-bbaa-9e1048c0a9ad 0xc0036212ef 0xc003621300}] [] [{kube-controller-manager Update v1 2023-04-21 20:37:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5059827-bc74-48a9-bbaa-9e1048c0a9ad\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k9rl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k9rl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:37:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:37:16.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9226" for this suite. 04/21/23 20:37:16.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:37:16.842
Apr 21 20:37:16.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename containers 04/21/23 20:37:16.844
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:37:16.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:37:16.857
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Apr 21 20:37:16.866: INFO: Waiting up to 5m0s for pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080" in namespace "containers-9392" to be "running"
Apr 21 20:37:16.868: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004599ms
Apr 21 20:37:18.871: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004776085s
Apr 21 20:37:20.884: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018242963s
Apr 21 20:37:22.941: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075668681s
Apr 21 20:37:24.871: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Running", Reason="", readiness=true. Elapsed: 8.005333784s
Apr 21 20:37:24.871: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 21 20:37:24.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9392" for this suite. 04/21/23 20:37:24.878
------------------------------
â€¢ [SLOW TEST] [8.041 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:37:16.842
    Apr 21 20:37:16.843: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename containers 04/21/23 20:37:16.844
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:37:16.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:37:16.857
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Apr 21 20:37:16.866: INFO: Waiting up to 5m0s for pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080" in namespace "containers-9392" to be "running"
    Apr 21 20:37:16.868: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004599ms
    Apr 21 20:37:18.871: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004776085s
    Apr 21 20:37:20.884: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018242963s
    Apr 21 20:37:22.941: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075668681s
    Apr 21 20:37:24.871: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080": Phase="Running", Reason="", readiness=true. Elapsed: 8.005333784s
    Apr 21 20:37:24.871: INFO: Pod "client-containers-792c7c7d-f211-471e-b0c5-e3f43151e080" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:37:24.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9392" for this suite. 04/21/23 20:37:24.878
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:37:24.883
Apr 21 20:37:24.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:37:24.884
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:37:24.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:37:24.892
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-0deb4cb3-f26e-4105-afe5-fc803faeb5bb 04/21/23 20:37:24.896
STEP: Creating configMap with name cm-test-opt-upd-9a709858-a194-455f-a011-1aaa670e17cd 04/21/23 20:37:24.898
STEP: Creating the pod 04/21/23 20:37:24.902
Apr 21 20:37:24.908: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31" in namespace "projected-4391" to be "running and ready"
Apr 21 20:37:24.909: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1.563913ms
Apr 21 20:37:24.909: INFO: The phase of Pod pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:37:26.912: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004174218s
Apr 21 20:37:26.912: INFO: The phase of Pod pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:37:28.912: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004756427s
Apr 21 20:37:28.912: INFO: The phase of Pod pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:37:30.913: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31": Phase="Running", Reason="", readiness=true. Elapsed: 6.005435989s
Apr 21 20:37:30.913: INFO: The phase of Pod pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31 is Running (Ready = true)
Apr 21 20:37:30.913: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-0deb4cb3-f26e-4105-afe5-fc803faeb5bb 04/21/23 20:37:30.929
STEP: Updating configmap cm-test-opt-upd-9a709858-a194-455f-a011-1aaa670e17cd 04/21/23 20:37:30.932
STEP: Creating configMap with name cm-test-opt-create-5d82f37d-2f3d-4d81-afa4-65452a580d1f 04/21/23 20:37:30.936
STEP: waiting to observe update in volume 04/21/23 20:37:30.939
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:38:51.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4391" for this suite. 04/21/23 20:38:51.256
------------------------------
â€¢ [SLOW TEST] [86.376 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:37:24.883
    Apr 21 20:37:24.883: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:37:24.884
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:37:24.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:37:24.892
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-0deb4cb3-f26e-4105-afe5-fc803faeb5bb 04/21/23 20:37:24.896
    STEP: Creating configMap with name cm-test-opt-upd-9a709858-a194-455f-a011-1aaa670e17cd 04/21/23 20:37:24.898
    STEP: Creating the pod 04/21/23 20:37:24.902
    Apr 21 20:37:24.908: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31" in namespace "projected-4391" to be "running and ready"
    Apr 21 20:37:24.909: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31": Phase="Pending", Reason="", readiness=false. Elapsed: 1.563913ms
    Apr 21 20:37:24.909: INFO: The phase of Pod pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:37:26.912: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004174218s
    Apr 21 20:37:26.912: INFO: The phase of Pod pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:37:28.912: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004756427s
    Apr 21 20:37:28.912: INFO: The phase of Pod pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:37:30.913: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31": Phase="Running", Reason="", readiness=true. Elapsed: 6.005435989s
    Apr 21 20:37:30.913: INFO: The phase of Pod pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31 is Running (Ready = true)
    Apr 21 20:37:30.913: INFO: Pod "pod-projected-configmaps-456ca597-0ac7-48a0-a85e-7e63c8518d31" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-0deb4cb3-f26e-4105-afe5-fc803faeb5bb 04/21/23 20:37:30.929
    STEP: Updating configmap cm-test-opt-upd-9a709858-a194-455f-a011-1aaa670e17cd 04/21/23 20:37:30.932
    STEP: Creating configMap with name cm-test-opt-create-5d82f37d-2f3d-4d81-afa4-65452a580d1f 04/21/23 20:37:30.936
    STEP: waiting to observe update in volume 04/21/23 20:37:30.939
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:38:51.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4391" for this suite. 04/21/23 20:38:51.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:38:51.261
Apr 21 20:38:51.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename dns 04/21/23 20:38:51.261
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:38:51.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:38:51.272
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 04/21/23 20:38:51.274
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 04/21/23 20:38:51.274
STEP: creating a pod to probe DNS 04/21/23 20:38:51.274
STEP: submitting the pod to kubernetes 04/21/23 20:38:51.274
Apr 21 20:38:51.280: INFO: Waiting up to 15m0s for pod "dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11" in namespace "dns-4864" to be "running"
Apr 21 20:38:51.282: INFO: Pod "dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692275ms
Apr 21 20:38:53.285: INFO: Pod "dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11": Phase="Running", Reason="", readiness=true. Elapsed: 2.004954441s
Apr 21 20:38:53.285: INFO: Pod "dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11" satisfied condition "running"
STEP: retrieving the pod 04/21/23 20:38:53.285
STEP: looking for the results for each expected name from probers 04/21/23 20:38:53.287
Apr 21 20:38:53.294: INFO: DNS probes using dns-4864/dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11 succeeded

STEP: deleting the pod 04/21/23 20:38:53.294
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 21 20:38:53.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4864" for this suite. 04/21/23 20:38:53.303
------------------------------
â€¢ [2.046 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:38:51.261
    Apr 21 20:38:51.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename dns 04/21/23 20:38:51.261
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:38:51.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:38:51.272
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     04/21/23 20:38:51.274
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     04/21/23 20:38:51.274
    STEP: creating a pod to probe DNS 04/21/23 20:38:51.274
    STEP: submitting the pod to kubernetes 04/21/23 20:38:51.274
    Apr 21 20:38:51.280: INFO: Waiting up to 15m0s for pod "dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11" in namespace "dns-4864" to be "running"
    Apr 21 20:38:51.282: INFO: Pod "dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11": Phase="Pending", Reason="", readiness=false. Elapsed: 1.692275ms
    Apr 21 20:38:53.285: INFO: Pod "dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11": Phase="Running", Reason="", readiness=true. Elapsed: 2.004954441s
    Apr 21 20:38:53.285: INFO: Pod "dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11" satisfied condition "running"
    STEP: retrieving the pod 04/21/23 20:38:53.285
    STEP: looking for the results for each expected name from probers 04/21/23 20:38:53.287
    Apr 21 20:38:53.294: INFO: DNS probes using dns-4864/dns-test-1b92c824-7410-4c69-828d-f363a7dfdb11 succeeded

    STEP: deleting the pod 04/21/23 20:38:53.294
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:38:53.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4864" for this suite. 04/21/23 20:38:53.303
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:38:53.308
Apr 21 20:38:53.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename watch 04/21/23 20:38:53.309
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:38:53.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:38:53.317
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 04/21/23 20:38:53.319
STEP: starting a background goroutine to produce watch events 04/21/23 20:38:53.321
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 04/21/23 20:38:53.321
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 21 20:38:56.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3360" for this suite. 04/21/23 20:38:56.162
------------------------------
â€¢ [2.905 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:38:53.308
    Apr 21 20:38:53.308: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename watch 04/21/23 20:38:53.309
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:38:53.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:38:53.317
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 04/21/23 20:38:53.319
    STEP: starting a background goroutine to produce watch events 04/21/23 20:38:53.321
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 04/21/23 20:38:53.321
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:38:56.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3360" for this suite. 04/21/23 20:38:56.162
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:38:56.213
Apr 21 20:38:56.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename containers 04/21/23 20:38:56.214
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:38:56.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:38:56.226
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 04/21/23 20:38:56.227
Apr 21 20:38:56.232: INFO: Waiting up to 5m0s for pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d" in namespace "containers-2867" to be "Succeeded or Failed"
Apr 21 20:38:56.233: INFO: Pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.535248ms
Apr 21 20:38:58.235: INFO: Pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003838375s
Apr 21 20:39:00.235: INFO: Pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003688327s
STEP: Saw pod success 04/21/23 20:39:00.235
Apr 21 20:39:00.235: INFO: Pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d" satisfied condition "Succeeded or Failed"
Apr 21 20:39:00.237: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d container agnhost-container: <nil>
STEP: delete the pod 04/21/23 20:39:00.242
Apr 21 20:39:00.248: INFO: Waiting for pod client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d to disappear
Apr 21 20:39:00.250: INFO: Pod client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:00.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2867" for this suite. 04/21/23 20:39:00.252
------------------------------
â€¢ [4.041 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:38:56.213
    Apr 21 20:38:56.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename containers 04/21/23 20:38:56.214
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:38:56.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:38:56.226
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 04/21/23 20:38:56.227
    Apr 21 20:38:56.232: INFO: Waiting up to 5m0s for pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d" in namespace "containers-2867" to be "Succeeded or Failed"
    Apr 21 20:38:56.233: INFO: Pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.535248ms
    Apr 21 20:38:58.235: INFO: Pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003838375s
    Apr 21 20:39:00.235: INFO: Pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003688327s
    STEP: Saw pod success 04/21/23 20:39:00.235
    Apr 21 20:39:00.235: INFO: Pod "client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d" satisfied condition "Succeeded or Failed"
    Apr 21 20:39:00.237: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 20:39:00.242
    Apr 21 20:39:00.248: INFO: Waiting for pod client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d to disappear
    Apr 21 20:39:00.250: INFO: Pod client-containers-715297fb-8ee5-46fe-ada2-ba8630f3361d no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:00.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2867" for this suite. 04/21/23 20:39:00.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:00.257
Apr 21 20:39:00.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename csiinlinevolumes 04/21/23 20:39:00.258
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:00.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:00.268
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 04/21/23 20:39:00.27
STEP: getting 04/21/23 20:39:00.28
STEP: listing in namespace 04/21/23 20:39:00.282
STEP: patching 04/21/23 20:39:00.283
STEP: deleting 04/21/23 20:39:00.287
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:00.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2295" for this suite. 04/21/23 20:39:00.298
------------------------------
â€¢ [0.043 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:00.257
    Apr 21 20:39:00.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename csiinlinevolumes 04/21/23 20:39:00.258
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:00.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:00.268
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 04/21/23 20:39:00.27
    STEP: getting 04/21/23 20:39:00.28
    STEP: listing in namespace 04/21/23 20:39:00.282
    STEP: patching 04/21/23 20:39:00.283
    STEP: deleting 04/21/23 20:39:00.287
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:00.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2295" for this suite. 04/21/23 20:39:00.298
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:00.302
Apr 21 20:39:00.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:39:00.303
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:00.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:00.312
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:39:00.314
Apr 21 20:39:00.318: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5" in namespace "projected-3378" to be "Succeeded or Failed"
Apr 21 20:39:00.320: INFO: Pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558676ms
Apr 21 20:39:02.323: INFO: Pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00428721s
Apr 21 20:39:04.323: INFO: Pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004258311s
STEP: Saw pod success 04/21/23 20:39:04.323
Apr 21 20:39:04.323: INFO: Pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5" satisfied condition "Succeeded or Failed"
Apr 21 20:39:04.325: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5 container client-container: <nil>
STEP: delete the pod 04/21/23 20:39:04.329
Apr 21 20:39:04.337: INFO: Waiting for pod downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5 to disappear
Apr 21 20:39:04.339: INFO: Pod downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:04.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3378" for this suite. 04/21/23 20:39:04.341
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:00.302
    Apr 21 20:39:00.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:39:00.303
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:00.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:00.312
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:39:00.314
    Apr 21 20:39:00.318: INFO: Waiting up to 5m0s for pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5" in namespace "projected-3378" to be "Succeeded or Failed"
    Apr 21 20:39:00.320: INFO: Pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558676ms
    Apr 21 20:39:02.323: INFO: Pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00428721s
    Apr 21 20:39:04.323: INFO: Pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004258311s
    STEP: Saw pod success 04/21/23 20:39:04.323
    Apr 21 20:39:04.323: INFO: Pod "downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5" satisfied condition "Succeeded or Failed"
    Apr 21 20:39:04.325: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5 container client-container: <nil>
    STEP: delete the pod 04/21/23 20:39:04.329
    Apr 21 20:39:04.337: INFO: Waiting for pod downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5 to disappear
    Apr 21 20:39:04.339: INFO: Pod downwardapi-volume-76caf88b-b28e-48f7-9f2f-dc74dc44b4e5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:04.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3378" for this suite. 04/21/23 20:39:04.341
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:04.344
Apr 21 20:39:04.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename init-container 04/21/23 20:39:04.345
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:04.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:04.355
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 04/21/23 20:39:04.357
Apr 21 20:39:04.357: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:10.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1081" for this suite. 04/21/23 20:39:10.126
------------------------------
â€¢ [SLOW TEST] [5.786 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:04.344
    Apr 21 20:39:04.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename init-container 04/21/23 20:39:04.345
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:04.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:04.355
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 04/21/23 20:39:04.357
    Apr 21 20:39:04.357: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:10.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1081" for this suite. 04/21/23 20:39:10.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:10.131
Apr 21 20:39:10.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename hostport 04/21/23 20:39:10.131
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:10.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:10.139
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 04/21/23 20:39:10.143
Apr 21 20:39:10.148: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6515" to be "running and ready"
Apr 21 20:39:10.150: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.722744ms
Apr 21 20:39:10.150: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:39:12.153: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004412813s
Apr 21 20:39:12.153: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr 21 20:39:12.153: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.49.2 on the node which pod1 resides and expect scheduled 04/21/23 20:39:12.153
Apr 21 20:39:12.156: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6515" to be "running and ready"
Apr 21 20:39:12.158: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.798214ms
Apr 21 20:39:12.158: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:39:14.162: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.005573262s
Apr 21 20:39:14.162: INFO: The phase of Pod pod2 is Running (Ready = false)
Apr 21 20:39:16.160: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.00409715s
Apr 21 20:39:16.160: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr 21 20:39:16.160: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.49.2 but use UDP protocol on the node which pod2 resides 04/21/23 20:39:16.16
Apr 21 20:39:16.166: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6515" to be "running and ready"
Apr 21 20:39:16.168: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.690929ms
Apr 21 20:39:16.168: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:39:18.171: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005158152s
Apr 21 20:39:18.171: INFO: The phase of Pod pod3 is Running (Ready = true)
Apr 21 20:39:18.171: INFO: Pod "pod3" satisfied condition "running and ready"
Apr 21 20:39:18.175: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6515" to be "running and ready"
Apr 21 20:39:18.177: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.749994ms
Apr 21 20:39:18.177: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:39:20.180: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00456083s
Apr 21 20:39:20.180: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Apr 21 20:39:20.180: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 04/21/23 20:39:20.181
Apr 21 20:39:20.181: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.49.2 http://127.0.0.1:54323/hostname] Namespace:hostport-6515 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:39:20.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:39:20.182: INFO: ExecWithOptions: Clientset creation
Apr 21 20:39:20.182: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6515/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.49.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.49.2, port: 54323 04/21/23 20:39:20.26
Apr 21 20:39:20.260: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.49.2:54323/hostname] Namespace:hostport-6515 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:39:20.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:39:20.260: INFO: ExecWithOptions: Clientset creation
Apr 21 20:39:20.261: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6515/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.49.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.49.2, port: 54323 UDP 04/21/23 20:39:20.306
Apr 21 20:39:20.306: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.49.2 54323] Namespace:hostport-6515 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:39:20.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:39:20.306: INFO: ExecWithOptions: Clientset creation
Apr 21 20:39:20.306: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6515/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.49.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:25.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-6515" for this suite. 04/21/23 20:39:25.383
------------------------------
â€¢ [SLOW TEST] [15.257 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:10.131
    Apr 21 20:39:10.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename hostport 04/21/23 20:39:10.131
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:10.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:10.139
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 04/21/23 20:39:10.143
    Apr 21 20:39:10.148: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-6515" to be "running and ready"
    Apr 21 20:39:10.150: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.722744ms
    Apr 21 20:39:10.150: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:39:12.153: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004412813s
    Apr 21 20:39:12.153: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr 21 20:39:12.153: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.49.2 on the node which pod1 resides and expect scheduled 04/21/23 20:39:12.153
    Apr 21 20:39:12.156: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-6515" to be "running and ready"
    Apr 21 20:39:12.158: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.798214ms
    Apr 21 20:39:12.158: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:39:14.162: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.005573262s
    Apr 21 20:39:14.162: INFO: The phase of Pod pod2 is Running (Ready = false)
    Apr 21 20:39:16.160: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.00409715s
    Apr 21 20:39:16.160: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr 21 20:39:16.160: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.49.2 but use UDP protocol on the node which pod2 resides 04/21/23 20:39:16.16
    Apr 21 20:39:16.166: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-6515" to be "running and ready"
    Apr 21 20:39:16.168: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.690929ms
    Apr 21 20:39:16.168: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:39:18.171: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005158152s
    Apr 21 20:39:18.171: INFO: The phase of Pod pod3 is Running (Ready = true)
    Apr 21 20:39:18.171: INFO: Pod "pod3" satisfied condition "running and ready"
    Apr 21 20:39:18.175: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-6515" to be "running and ready"
    Apr 21 20:39:18.177: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 1.749994ms
    Apr 21 20:39:18.177: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:39:20.180: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.00456083s
    Apr 21 20:39:20.180: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Apr 21 20:39:20.180: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 04/21/23 20:39:20.181
    Apr 21 20:39:20.181: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.49.2 http://127.0.0.1:54323/hostname] Namespace:hostport-6515 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:39:20.181: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:39:20.182: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:39:20.182: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6515/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.49.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.49.2, port: 54323 04/21/23 20:39:20.26
    Apr 21 20:39:20.260: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.49.2:54323/hostname] Namespace:hostport-6515 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:39:20.260: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:39:20.260: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:39:20.261: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6515/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.49.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.49.2, port: 54323 UDP 04/21/23 20:39:20.306
    Apr 21 20:39:20.306: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.49.2 54323] Namespace:hostport-6515 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:39:20.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:39:20.306: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:39:20.306: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-6515/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.49.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:25.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-6515" for this suite. 04/21/23 20:39:25.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:25.389
Apr 21 20:39:25.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 20:39:25.39
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:25.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:25.441
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 04/21/23 20:39:25.443
STEP: fetching the ConfigMap 04/21/23 20:39:25.446
STEP: patching the ConfigMap 04/21/23 20:39:25.447
STEP: listing all ConfigMaps in all namespaces with a label selector 04/21/23 20:39:25.45
STEP: deleting the ConfigMap by collection with a label selector 04/21/23 20:39:25.452
STEP: listing all ConfigMaps in test namespace 04/21/23 20:39:25.456
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:25.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1223" for this suite. 04/21/23 20:39:25.459
------------------------------
â€¢ [0.072 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:25.389
    Apr 21 20:39:25.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 20:39:25.39
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:25.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:25.441
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 04/21/23 20:39:25.443
    STEP: fetching the ConfigMap 04/21/23 20:39:25.446
    STEP: patching the ConfigMap 04/21/23 20:39:25.447
    STEP: listing all ConfigMaps in all namespaces with a label selector 04/21/23 20:39:25.45
    STEP: deleting the ConfigMap by collection with a label selector 04/21/23 20:39:25.452
    STEP: listing all ConfigMaps in test namespace 04/21/23 20:39:25.456
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:25.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1223" for this suite. 04/21/23 20:39:25.459
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:25.462
Apr 21 20:39:25.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 20:39:25.463
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:25.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:25.471
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 04/21/23 20:39:25.473
STEP: Creating a ResourceQuota 04/21/23 20:39:30.475
STEP: Ensuring resource quota status is calculated 04/21/23 20:39:30.478
STEP: Creating a Service 04/21/23 20:39:32.481
STEP: Creating a NodePort Service 04/21/23 20:39:32.494
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 04/21/23 20:39:32.511
STEP: Ensuring resource quota status captures service creation 04/21/23 20:39:32.528
STEP: Deleting Services 04/21/23 20:39:34.531
STEP: Ensuring resource quota status released usage 04/21/23 20:39:34.555
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:36.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6061" for this suite. 04/21/23 20:39:36.563
------------------------------
â€¢ [SLOW TEST] [11.104 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:25.462
    Apr 21 20:39:25.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 20:39:25.463
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:25.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:25.471
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 04/21/23 20:39:25.473
    STEP: Creating a ResourceQuota 04/21/23 20:39:30.475
    STEP: Ensuring resource quota status is calculated 04/21/23 20:39:30.478
    STEP: Creating a Service 04/21/23 20:39:32.481
    STEP: Creating a NodePort Service 04/21/23 20:39:32.494
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 04/21/23 20:39:32.511
    STEP: Ensuring resource quota status captures service creation 04/21/23 20:39:32.528
    STEP: Deleting Services 04/21/23 20:39:34.531
    STEP: Ensuring resource quota status released usage 04/21/23 20:39:34.555
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:36.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6061" for this suite. 04/21/23 20:39:36.563
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:36.566
Apr 21 20:39:36.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:39:36.567
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:36.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:36.577
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-54806845-e142-4f1b-b4e5-825272814b82 04/21/23 20:39:36.579
STEP: Creating a pod to test consume secrets 04/21/23 20:39:36.581
Apr 21 20:39:36.586: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d" in namespace "projected-2408" to be "Succeeded or Failed"
Apr 21 20:39:36.588: INFO: Pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.48529ms
Apr 21 20:39:38.591: INFO: Pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00440436s
Apr 21 20:39:40.591: INFO: Pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004747273s
STEP: Saw pod success 04/21/23 20:39:40.591
Apr 21 20:39:40.591: INFO: Pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d" satisfied condition "Succeeded or Failed"
Apr 21 20:39:40.593: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d container projected-secret-volume-test: <nil>
STEP: delete the pod 04/21/23 20:39:40.598
Apr 21 20:39:40.606: INFO: Waiting for pod pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d to disappear
Apr 21 20:39:40.608: INFO: Pod pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:40.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2408" for this suite. 04/21/23 20:39:40.61
------------------------------
â€¢ [4.047 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:36.566
    Apr 21 20:39:36.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:39:36.567
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:36.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:36.577
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-54806845-e142-4f1b-b4e5-825272814b82 04/21/23 20:39:36.579
    STEP: Creating a pod to test consume secrets 04/21/23 20:39:36.581
    Apr 21 20:39:36.586: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d" in namespace "projected-2408" to be "Succeeded or Failed"
    Apr 21 20:39:36.588: INFO: Pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.48529ms
    Apr 21 20:39:38.591: INFO: Pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00440436s
    Apr 21 20:39:40.591: INFO: Pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004747273s
    STEP: Saw pod success 04/21/23 20:39:40.591
    Apr 21 20:39:40.591: INFO: Pod "pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d" satisfied condition "Succeeded or Failed"
    Apr 21 20:39:40.593: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 20:39:40.598
    Apr 21 20:39:40.606: INFO: Waiting for pod pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d to disappear
    Apr 21 20:39:40.608: INFO: Pod pod-projected-secrets-a64335b0-f541-4af0-9a2c-28a22efd580d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:40.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2408" for this suite. 04/21/23 20:39:40.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:40.614
Apr 21 20:39:40.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replication-controller 04/21/23 20:39:40.615
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:40.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:40.624
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 04/21/23 20:39:40.628
STEP: waiting for RC to be added 04/21/23 20:39:40.631
STEP: waiting for available Replicas 04/21/23 20:39:40.632
STEP: patching ReplicationController 04/21/23 20:39:41.392
STEP: waiting for RC to be modified 04/21/23 20:39:41.398
STEP: patching ReplicationController status 04/21/23 20:39:41.399
STEP: waiting for RC to be modified 04/21/23 20:39:41.402
STEP: waiting for available Replicas 04/21/23 20:39:41.402
STEP: fetching ReplicationController status 04/21/23 20:39:41.406
STEP: patching ReplicationController scale 04/21/23 20:39:41.408
STEP: waiting for RC to be modified 04/21/23 20:39:41.412
STEP: waiting for ReplicationController's scale to be the max amount 04/21/23 20:39:41.412
STEP: fetching ReplicationController; ensuring that it's patched 04/21/23 20:39:42.077
STEP: updating ReplicationController status 04/21/23 20:39:42.079
STEP: waiting for RC to be modified 04/21/23 20:39:42.083
STEP: listing all ReplicationControllers 04/21/23 20:39:42.083
STEP: checking that ReplicationController has expected values 04/21/23 20:39:42.085
STEP: deleting ReplicationControllers by collection 04/21/23 20:39:42.085
STEP: waiting for ReplicationController to have a DELETED watchEvent 04/21/23 20:39:42.09
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:42.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-8560" for this suite. 04/21/23 20:39:42.129
------------------------------
â€¢ [1.518 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:40.614
    Apr 21 20:39:40.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replication-controller 04/21/23 20:39:40.615
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:40.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:40.624
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 04/21/23 20:39:40.628
    STEP: waiting for RC to be added 04/21/23 20:39:40.631
    STEP: waiting for available Replicas 04/21/23 20:39:40.632
    STEP: patching ReplicationController 04/21/23 20:39:41.392
    STEP: waiting for RC to be modified 04/21/23 20:39:41.398
    STEP: patching ReplicationController status 04/21/23 20:39:41.399
    STEP: waiting for RC to be modified 04/21/23 20:39:41.402
    STEP: waiting for available Replicas 04/21/23 20:39:41.402
    STEP: fetching ReplicationController status 04/21/23 20:39:41.406
    STEP: patching ReplicationController scale 04/21/23 20:39:41.408
    STEP: waiting for RC to be modified 04/21/23 20:39:41.412
    STEP: waiting for ReplicationController's scale to be the max amount 04/21/23 20:39:41.412
    STEP: fetching ReplicationController; ensuring that it's patched 04/21/23 20:39:42.077
    STEP: updating ReplicationController status 04/21/23 20:39:42.079
    STEP: waiting for RC to be modified 04/21/23 20:39:42.083
    STEP: listing all ReplicationControllers 04/21/23 20:39:42.083
    STEP: checking that ReplicationController has expected values 04/21/23 20:39:42.085
    STEP: deleting ReplicationControllers by collection 04/21/23 20:39:42.085
    STEP: waiting for ReplicationController to have a DELETED watchEvent 04/21/23 20:39:42.09
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:42.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-8560" for this suite. 04/21/23 20:39:42.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:42.133
Apr 21 20:39:42.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename var-expansion 04/21/23 20:39:42.133
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:42.141
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:42.143
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Apr 21 20:39:42.149: INFO: Waiting up to 2m0s for pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35" in namespace "var-expansion-726" to be "container 0 failed with reason CreateContainerConfigError"
Apr 21 20:39:42.151: INFO: Pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35": Phase="Pending", Reason="", readiness=false. Elapsed: 1.569145ms
Apr 21 20:39:44.154: INFO: Pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004679512s
Apr 21 20:39:44.154: INFO: Pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Apr 21 20:39:44.154: INFO: Deleting pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35" in namespace "var-expansion-726"
Apr 21 20:39:44.159: INFO: Wait up to 5m0s for pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 21 20:39:46.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-726" for this suite. 04/21/23 20:39:46.165
------------------------------
â€¢ [4.038 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:42.133
    Apr 21 20:39:42.133: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename var-expansion 04/21/23 20:39:42.133
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:42.141
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:42.143
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Apr 21 20:39:42.149: INFO: Waiting up to 2m0s for pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35" in namespace "var-expansion-726" to be "container 0 failed with reason CreateContainerConfigError"
    Apr 21 20:39:42.151: INFO: Pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35": Phase="Pending", Reason="", readiness=false. Elapsed: 1.569145ms
    Apr 21 20:39:44.154: INFO: Pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004679512s
    Apr 21 20:39:44.154: INFO: Pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Apr 21 20:39:44.154: INFO: Deleting pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35" in namespace "var-expansion-726"
    Apr 21 20:39:44.159: INFO: Wait up to 5m0s for pod "var-expansion-bcffbada-5eb7-4dcb-a8e6-79345be07f35" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:39:46.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-726" for this suite. 04/21/23 20:39:46.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:39:46.171
Apr 21 20:39:46.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-preemption 04/21/23 20:39:46.172
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:46.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:46.18
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 21 20:39:46.189: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 21 20:40:46.202: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:40:46.204
Apr 21 20:40:46.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-preemption-path 04/21/23 20:40:46.205
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:46.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:46.213
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Apr 21 20:40:46.223: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Apr 21 20:40:46.224: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Apr 21 20:40:46.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:40:46.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-4515" for this suite. 04/21/23 20:40:46.262
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-6211" for this suite. 04/21/23 20:40:46.265
------------------------------
â€¢ [SLOW TEST] [60.097 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:39:46.171
    Apr 21 20:39:46.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-preemption 04/21/23 20:39:46.172
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:39:46.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:39:46.18
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 21 20:39:46.189: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 21 20:40:46.202: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:40:46.204
    Apr 21 20:40:46.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-preemption-path 04/21/23 20:40:46.205
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:46.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:46.213
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Apr 21 20:40:46.223: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Apr 21 20:40:46.224: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:40:46.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:40:46.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-4515" for this suite. 04/21/23 20:40:46.262
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-6211" for this suite. 04/21/23 20:40:46.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:40:46.269
Apr 21 20:40:46.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 20:40:46.27
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:46.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:46.277
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Apr 21 20:40:46.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/21/23 20:40:47.595
Apr 21 20:40:47.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 --namespace=crd-publish-openapi-8325 create -f -'
Apr 21 20:40:48.091: INFO: stderr: ""
Apr 21 20:40:48.091: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 21 20:40:48.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 --namespace=crd-publish-openapi-8325 delete e2e-test-crd-publish-openapi-7528-crds test-cr'
Apr 21 20:40:48.146: INFO: stderr: ""
Apr 21 20:40:48.146: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 21 20:40:48.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 --namespace=crd-publish-openapi-8325 apply -f -'
Apr 21 20:40:48.288: INFO: stderr: ""
Apr 21 20:40:48.288: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 21 20:40:48.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 --namespace=crd-publish-openapi-8325 delete e2e-test-crd-publish-openapi-7528-crds test-cr'
Apr 21 20:40:48.343: INFO: stderr: ""
Apr 21 20:40:48.343: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 04/21/23 20:40:48.343
Apr 21 20:40:48.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 explain e2e-test-crd-publish-openapi-7528-crds'
Apr 21 20:40:48.479: INFO: stderr: ""
Apr 21 20:40:48.479: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7528-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:40:49.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8325" for this suite. 04/21/23 20:40:49.778
------------------------------
â€¢ [3.514 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:40:46.269
    Apr 21 20:40:46.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 20:40:46.27
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:46.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:46.277
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Apr 21 20:40:46.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/21/23 20:40:47.595
    Apr 21 20:40:47.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 --namespace=crd-publish-openapi-8325 create -f -'
    Apr 21 20:40:48.091: INFO: stderr: ""
    Apr 21 20:40:48.091: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Apr 21 20:40:48.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 --namespace=crd-publish-openapi-8325 delete e2e-test-crd-publish-openapi-7528-crds test-cr'
    Apr 21 20:40:48.146: INFO: stderr: ""
    Apr 21 20:40:48.146: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Apr 21 20:40:48.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 --namespace=crd-publish-openapi-8325 apply -f -'
    Apr 21 20:40:48.288: INFO: stderr: ""
    Apr 21 20:40:48.288: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Apr 21 20:40:48.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 --namespace=crd-publish-openapi-8325 delete e2e-test-crd-publish-openapi-7528-crds test-cr'
    Apr 21 20:40:48.343: INFO: stderr: ""
    Apr 21 20:40:48.343: INFO: stdout: "e2e-test-crd-publish-openapi-7528-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 04/21/23 20:40:48.343
    Apr 21 20:40:48.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-8325 explain e2e-test-crd-publish-openapi-7528-crds'
    Apr 21 20:40:48.479: INFO: stderr: ""
    Apr 21 20:40:48.479: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7528-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:40:49.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8325" for this suite. 04/21/23 20:40:49.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:40:49.783
Apr 21 20:40:49.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename init-container 04/21/23 20:40:49.784
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:49.791
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:49.792
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 04/21/23 20:40:49.794
Apr 21 20:40:49.794: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:40:52.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-4515" for this suite. 04/21/23 20:40:52.947
------------------------------
â€¢ [3.168 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:40:49.783
    Apr 21 20:40:49.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename init-container 04/21/23 20:40:49.784
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:49.791
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:49.792
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 04/21/23 20:40:49.794
    Apr 21 20:40:49.794: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:40:52.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-4515" for this suite. 04/21/23 20:40:52.947
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:40:52.951
Apr 21 20:40:52.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:40:52.952
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:52.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:52.961
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-cd7e4aef-ca75-4da5-ac4a-57fb477d561f 04/21/23 20:40:52.963
STEP: Creating a pod to test consume configMaps 04/21/23 20:40:52.967
Apr 21 20:40:52.971: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699" in namespace "projected-2962" to be "Succeeded or Failed"
Apr 21 20:40:52.972: INFO: Pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699": Phase="Pending", Reason="", readiness=false. Elapsed: 1.522119ms
Apr 21 20:40:54.975: INFO: Pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003936211s
Apr 21 20:40:56.975: INFO: Pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004827005s
STEP: Saw pod success 04/21/23 20:40:56.975
Apr 21 20:40:56.976: INFO: Pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699" satisfied condition "Succeeded or Failed"
Apr 21 20:40:56.977: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699 container projected-configmap-volume-test: <nil>
STEP: delete the pod 04/21/23 20:40:56.983
Apr 21 20:40:56.992: INFO: Waiting for pod pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699 to disappear
Apr 21 20:40:56.993: INFO: Pod pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:40:56.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2962" for this suite. 04/21/23 20:40:56.995
------------------------------
â€¢ [4.047 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:40:52.951
    Apr 21 20:40:52.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:40:52.952
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:52.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:52.961
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-cd7e4aef-ca75-4da5-ac4a-57fb477d561f 04/21/23 20:40:52.963
    STEP: Creating a pod to test consume configMaps 04/21/23 20:40:52.967
    Apr 21 20:40:52.971: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699" in namespace "projected-2962" to be "Succeeded or Failed"
    Apr 21 20:40:52.972: INFO: Pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699": Phase="Pending", Reason="", readiness=false. Elapsed: 1.522119ms
    Apr 21 20:40:54.975: INFO: Pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003936211s
    Apr 21 20:40:56.975: INFO: Pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004827005s
    STEP: Saw pod success 04/21/23 20:40:56.975
    Apr 21 20:40:56.976: INFO: Pod "pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699" satisfied condition "Succeeded or Failed"
    Apr 21 20:40:56.977: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 04/21/23 20:40:56.983
    Apr 21 20:40:56.992: INFO: Waiting for pod pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699 to disappear
    Apr 21 20:40:56.993: INFO: Pod pod-projected-configmaps-b98715ec-f9e2-495a-92a8-7e4e13be0699 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:40:56.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2962" for this suite. 04/21/23 20:40:56.995
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:40:56.999
Apr 21 20:40:56.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename runtimeclass 04/21/23 20:40:57
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:57.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:57.008
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 04/21/23 20:40:57.01
STEP: getting /apis/node.k8s.io 04/21/23 20:40:57.011
STEP: getting /apis/node.k8s.io/v1 04/21/23 20:40:57.012
STEP: creating 04/21/23 20:40:57.013
STEP: watching 04/21/23 20:40:57.022
Apr 21 20:40:57.022: INFO: starting watch
STEP: getting 04/21/23 20:40:57.025
STEP: listing 04/21/23 20:40:57.026
STEP: patching 04/21/23 20:40:57.027
STEP: updating 04/21/23 20:40:57.03
Apr 21 20:40:57.033: INFO: waiting for watch events with expected annotations
STEP: deleting 04/21/23 20:40:57.033
STEP: deleting a collection 04/21/23 20:40:57.038
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 21 20:40:57.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3828" for this suite. 04/21/23 20:40:57.046
------------------------------
â€¢ [0.050 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:40:56.999
    Apr 21 20:40:56.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename runtimeclass 04/21/23 20:40:57
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:57.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:57.008
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 04/21/23 20:40:57.01
    STEP: getting /apis/node.k8s.io 04/21/23 20:40:57.011
    STEP: getting /apis/node.k8s.io/v1 04/21/23 20:40:57.012
    STEP: creating 04/21/23 20:40:57.013
    STEP: watching 04/21/23 20:40:57.022
    Apr 21 20:40:57.022: INFO: starting watch
    STEP: getting 04/21/23 20:40:57.025
    STEP: listing 04/21/23 20:40:57.026
    STEP: patching 04/21/23 20:40:57.027
    STEP: updating 04/21/23 20:40:57.03
    Apr 21 20:40:57.033: INFO: waiting for watch events with expected annotations
    STEP: deleting 04/21/23 20:40:57.033
    STEP: deleting a collection 04/21/23 20:40:57.038
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:40:57.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3828" for this suite. 04/21/23 20:40:57.046
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:40:57.049
Apr 21 20:40:57.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:40:57.05
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:57.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:57.058
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:40:57.068
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:40:57.298
STEP: Deploying the webhook pod 04/21/23 20:40:57.304
STEP: Wait for the deployment to be ready 04/21/23 20:40:57.311
Apr 21 20:40:57.316: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 20:40:59.325
STEP: Verifying the service has paired with the endpoint 04/21/23 20:40:59.333
Apr 21 20:41:00.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 04/21/23 20:41:00.336
STEP: create a namespace for the webhook 04/21/23 20:41:00.348
STEP: create a configmap should be unconditionally rejected by the webhook 04/21/23 20:41:00.353
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:41:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5993" for this suite. 04/21/23 20:41:00.409
STEP: Destroying namespace "webhook-5993-markers" for this suite. 04/21/23 20:41:00.413
------------------------------
â€¢ [3.387 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:40:57.049
    Apr 21 20:40:57.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:40:57.05
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:40:57.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:40:57.058
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:40:57.068
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:40:57.298
    STEP: Deploying the webhook pod 04/21/23 20:40:57.304
    STEP: Wait for the deployment to be ready 04/21/23 20:40:57.311
    Apr 21 20:40:57.316: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 20:40:59.325
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:40:59.333
    Apr 21 20:41:00.334: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 04/21/23 20:41:00.336
    STEP: create a namespace for the webhook 04/21/23 20:41:00.348
    STEP: create a configmap should be unconditionally rejected by the webhook 04/21/23 20:41:00.353
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:41:00.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5993" for this suite. 04/21/23 20:41:00.409
    STEP: Destroying namespace "webhook-5993-markers" for this suite. 04/21/23 20:41:00.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:41:00.436
Apr 21 20:41:00.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename deployment 04/21/23 20:41:00.437
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:00.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:00.447
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Apr 21 20:41:00.449: INFO: Creating simple deployment test-new-deployment
Apr 21 20:41:00.457: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 04/21/23 20:41:02.465
STEP: updating a scale subresource 04/21/23 20:41:02.466
STEP: verifying the deployment Spec.Replicas was modified 04/21/23 20:41:02.471
STEP: Patch a scale subresource 04/21/23 20:41:02.472
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 21 20:41:02.484: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2960  4f31e1ce-0b6c-4e16-b1f6-004e285af5ab 6821 3 2023-04-21 20:41:00 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-21 20:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052a48d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-21 20:41:02 +0000 UTC,LastTransitionTime:2023-04-21 20:41:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-04-21 20:41:02 +0000 UTC,LastTransitionTime:2023-04-21 20:41:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 21 20:41:02.486: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2960  0abd9bbc-fb59-4032-a81a-b850d8daa1ac 6826 2 2023-04-21 20:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 4f31e1ce-0b6c-4e16-b1f6-004e285af5ab 0xc0052a4ff0 0xc0052a4ff1}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f31e1ce-0b6c-4e16-b1f6-004e285af5ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052a5078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 21 20:41:02.488: INFO: Pod "test-new-deployment-7f5969cbc7-r8wt9" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-r8wt9 test-new-deployment-7f5969cbc7- deployment-2960  bb15a3b1-334d-4dfd-bfb1-5928c4e4a5ab 6807 0 2023-04-21 20:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0abd9bbc-fb59-4032-a81a-b850d8daa1ac 0xc004705e80 0xc004705e81}] [] [{kube-controller-manager Update v1 2023-04-21 20:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0abd9bbc-fb59-4032-a81a-b850d8daa1ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gb4rk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gb4rk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.121,StartTime:2023-04-21 20:41:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:41:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://312bf8c4dda347191defeafe3d66f659f21a27546a75aaec1866f7e975d9ac83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 20:41:02.489: INFO: Pod "test-new-deployment-7f5969cbc7-wfpr4" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-wfpr4 test-new-deployment-7f5969cbc7- deployment-2960  291e63cd-3f0f-4a6e-98a7-caa25e80eb48 6825 0 2023-04-21 20:41:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0abd9bbc-fb59-4032-a81a-b850d8daa1ac 0xc0033f4050 0xc0033f4051}] [] [{kube-controller-manager Update v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0abd9bbc-fb59-4032-a81a-b850d8daa1ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cv8vs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cv8vs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 21 20:41:02.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2960" for this suite. 04/21/23 20:41:02.492
------------------------------
â€¢ [2.060 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:41:00.436
    Apr 21 20:41:00.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename deployment 04/21/23 20:41:00.437
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:00.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:00.447
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Apr 21 20:41:00.449: INFO: Creating simple deployment test-new-deployment
    Apr 21 20:41:00.457: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 04/21/23 20:41:02.465
    STEP: updating a scale subresource 04/21/23 20:41:02.466
    STEP: verifying the deployment Spec.Replicas was modified 04/21/23 20:41:02.471
    STEP: Patch a scale subresource 04/21/23 20:41:02.472
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 21 20:41:02.484: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-2960  4f31e1ce-0b6c-4e16-b1f6-004e285af5ab 6821 3 2023-04-21 20:41:00 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-21 20:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052a48d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-21 20:41:02 +0000 UTC,LastTransitionTime:2023-04-21 20:41:02 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-04-21 20:41:02 +0000 UTC,LastTransitionTime:2023-04-21 20:41:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 21 20:41:02.486: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-2960  0abd9bbc-fb59-4032-a81a-b850d8daa1ac 6826 2 2023-04-21 20:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 4f31e1ce-0b6c-4e16-b1f6-004e285af5ab 0xc0052a4ff0 0xc0052a4ff1}] [] [{kube-controller-manager Update apps/v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4f31e1ce-0b6c-4e16-b1f6-004e285af5ab\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052a5078 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 20:41:02.488: INFO: Pod "test-new-deployment-7f5969cbc7-r8wt9" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-r8wt9 test-new-deployment-7f5969cbc7- deployment-2960  bb15a3b1-334d-4dfd-bfb1-5928c4e4a5ab 6807 0 2023-04-21 20:41:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0abd9bbc-fb59-4032-a81a-b850d8daa1ac 0xc004705e80 0xc004705e81}] [] [{kube-controller-manager Update v1 2023-04-21 20:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0abd9bbc-fb59-4032-a81a-b850d8daa1ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gb4rk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gb4rk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.121,StartTime:2023-04-21 20:41:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 20:41:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://312bf8c4dda347191defeafe3d66f659f21a27546a75aaec1866f7e975d9ac83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 20:41:02.489: INFO: Pod "test-new-deployment-7f5969cbc7-wfpr4" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-wfpr4 test-new-deployment-7f5969cbc7- deployment-2960  291e63cd-3f0f-4a6e-98a7-caa25e80eb48 6825 0 2023-04-21 20:41:02 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 0abd9bbc-fb59-4032-a81a-b850d8daa1ac 0xc0033f4050 0xc0033f4051}] [] [{kube-controller-manager Update v1 2023-04-21 20:41:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0abd9bbc-fb59-4032-a81a-b850d8daa1ac\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cv8vs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cv8vs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 20:41:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:41:02.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2960" for this suite. 04/21/23 20:41:02.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:41:02.497
Apr 21 20:41:02.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:41:02.498
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:02.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:02.508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:41:02.517
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:41:02.974
STEP: Deploying the webhook pod 04/21/23 20:41:02.978
STEP: Wait for the deployment to be ready 04/21/23 20:41:02.987
Apr 21 20:41:02.991: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 20:41:04.996
STEP: Verifying the service has paired with the endpoint 04/21/23 20:41:05.005
Apr 21 20:41:06.005: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 04/21/23 20:41:06.007
STEP: create a pod that should be updated by the webhook 04/21/23 20:41:06.018
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:41:06.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2520" for this suite. 04/21/23 20:41:06.059
STEP: Destroying namespace "webhook-2520-markers" for this suite. 04/21/23 20:41:06.063
------------------------------
â€¢ [3.569 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:41:02.497
    Apr 21 20:41:02.497: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:41:02.498
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:02.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:02.508
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:41:02.517
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:41:02.974
    STEP: Deploying the webhook pod 04/21/23 20:41:02.978
    STEP: Wait for the deployment to be ready 04/21/23 20:41:02.987
    Apr 21 20:41:02.991: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 20:41:04.996
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:41:05.005
    Apr 21 20:41:06.005: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 04/21/23 20:41:06.007
    STEP: create a pod that should be updated by the webhook 04/21/23 20:41:06.018
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:41:06.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2520" for this suite. 04/21/23 20:41:06.059
    STEP: Destroying namespace "webhook-2520-markers" for this suite. 04/21/23 20:41:06.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:41:06.067
Apr 21 20:41:06.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replicaset 04/21/23 20:41:06.068
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:06.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:06.078
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 04/21/23 20:41:06.08
Apr 21 20:41:06.085: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 21 20:41:11.087: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/21/23 20:41:11.087
STEP: getting scale subresource 04/21/23 20:41:11.087
STEP: updating a scale subresource 04/21/23 20:41:11.089
STEP: verifying the replicaset Spec.Replicas was modified 04/21/23 20:41:11.092
STEP: Patch a scale subresource 04/21/23 20:41:11.094
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:41:11.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-6694" for this suite. 04/21/23 20:41:11.103
------------------------------
â€¢ [SLOW TEST] [5.040 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:41:06.067
    Apr 21 20:41:06.067: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replicaset 04/21/23 20:41:06.068
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:06.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:06.078
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 04/21/23 20:41:06.08
    Apr 21 20:41:06.085: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 21 20:41:11.087: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/21/23 20:41:11.087
    STEP: getting scale subresource 04/21/23 20:41:11.087
    STEP: updating a scale subresource 04/21/23 20:41:11.089
    STEP: verifying the replicaset Spec.Replicas was modified 04/21/23 20:41:11.092
    STEP: Patch a scale subresource 04/21/23 20:41:11.094
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:41:11.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-6694" for this suite. 04/21/23 20:41:11.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:41:11.107
Apr 21 20:41:11.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename gc 04/21/23 20:41:11.108
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:11.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:11.135
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 04/21/23 20:41:11.139
STEP: delete the rc 04/21/23 20:41:16.15
STEP: wait for the rc to be deleted 04/21/23 20:41:16.164
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 04/21/23 20:41:21.169
STEP: Gathering metrics 04/21/23 20:41:51.177
Apr 21 20:41:51.506: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
Apr 21 20:41:51.508: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 2.069323ms
Apr 21 20:41:51.508: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
Apr 21 20:41:51.508: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
Apr 21 20:41:51.578: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 21 20:41:51.578: INFO: Deleting pod "simpletest.rc-2cjmr" in namespace "gc-2783"
Apr 21 20:41:51.587: INFO: Deleting pod "simpletest.rc-2g7pk" in namespace "gc-2783"
Apr 21 20:41:51.595: INFO: Deleting pod "simpletest.rc-2gx7r" in namespace "gc-2783"
Apr 21 20:41:51.601: INFO: Deleting pod "simpletest.rc-2kssm" in namespace "gc-2783"
Apr 21 20:41:51.636: INFO: Deleting pod "simpletest.rc-2m9mc" in namespace "gc-2783"
Apr 21 20:41:51.647: INFO: Deleting pod "simpletest.rc-2pjwf" in namespace "gc-2783"
Apr 21 20:41:51.658: INFO: Deleting pod "simpletest.rc-4nfct" in namespace "gc-2783"
Apr 21 20:41:51.669: INFO: Deleting pod "simpletest.rc-5frr2" in namespace "gc-2783"
Apr 21 20:41:51.676: INFO: Deleting pod "simpletest.rc-5jld7" in namespace "gc-2783"
Apr 21 20:41:51.685: INFO: Deleting pod "simpletest.rc-8654d" in namespace "gc-2783"
Apr 21 20:41:51.696: INFO: Deleting pod "simpletest.rc-885pp" in namespace "gc-2783"
Apr 21 20:41:51.733: INFO: Deleting pod "simpletest.rc-8bc8x" in namespace "gc-2783"
Apr 21 20:41:51.750: INFO: Deleting pod "simpletest.rc-8g2kp" in namespace "gc-2783"
Apr 21 20:41:51.764: INFO: Deleting pod "simpletest.rc-8h794" in namespace "gc-2783"
Apr 21 20:41:51.776: INFO: Deleting pod "simpletest.rc-8hnsw" in namespace "gc-2783"
Apr 21 20:41:51.833: INFO: Deleting pod "simpletest.rc-8kchn" in namespace "gc-2783"
Apr 21 20:41:51.846: INFO: Deleting pod "simpletest.rc-95h7w" in namespace "gc-2783"
Apr 21 20:41:51.859: INFO: Deleting pod "simpletest.rc-9k7xr" in namespace "gc-2783"
Apr 21 20:41:51.872: INFO: Deleting pod "simpletest.rc-9nvjt" in namespace "gc-2783"
Apr 21 20:41:51.886: INFO: Deleting pod "simpletest.rc-9rbr9" in namespace "gc-2783"
Apr 21 20:41:51.939: INFO: Deleting pod "simpletest.rc-9vhq4" in namespace "gc-2783"
Apr 21 20:41:51.961: INFO: Deleting pod "simpletest.rc-b2vzr" in namespace "gc-2783"
Apr 21 20:41:52.042: INFO: Deleting pod "simpletest.rc-bsbb8" in namespace "gc-2783"
Apr 21 20:41:52.060: INFO: Deleting pod "simpletest.rc-c5knt" in namespace "gc-2783"
Apr 21 20:41:52.071: INFO: Deleting pod "simpletest.rc-cdhxb" in namespace "gc-2783"
Apr 21 20:41:52.078: INFO: Deleting pod "simpletest.rc-chnzb" in namespace "gc-2783"
Apr 21 20:41:52.140: INFO: Deleting pod "simpletest.rc-crjsh" in namespace "gc-2783"
Apr 21 20:41:52.149: INFO: Deleting pod "simpletest.rc-crx48" in namespace "gc-2783"
Apr 21 20:41:52.168: INFO: Deleting pod "simpletest.rc-cvt55" in namespace "gc-2783"
Apr 21 20:41:52.183: INFO: Deleting pod "simpletest.rc-d7klz" in namespace "gc-2783"
Apr 21 20:41:52.252: INFO: Deleting pod "simpletest.rc-dd8gl" in namespace "gc-2783"
Apr 21 20:41:52.261: INFO: Deleting pod "simpletest.rc-dl89f" in namespace "gc-2783"
Apr 21 20:41:52.273: INFO: Deleting pod "simpletest.rc-dwdjw" in namespace "gc-2783"
Apr 21 20:41:52.335: INFO: Deleting pod "simpletest.rc-fbmwz" in namespace "gc-2783"
Apr 21 20:41:52.356: INFO: Deleting pod "simpletest.rc-fzl8l" in namespace "gc-2783"
Apr 21 20:41:52.366: INFO: Deleting pod "simpletest.rc-g4mtn" in namespace "gc-2783"
Apr 21 20:41:52.438: INFO: Deleting pod "simpletest.rc-g7plr" in namespace "gc-2783"
Apr 21 20:41:52.451: INFO: Deleting pod "simpletest.rc-j9tfx" in namespace "gc-2783"
Apr 21 20:41:52.468: INFO: Deleting pod "simpletest.rc-jcsvz" in namespace "gc-2783"
Apr 21 20:41:52.478: INFO: Deleting pod "simpletest.rc-jhlw6" in namespace "gc-2783"
Apr 21 20:41:52.535: INFO: Deleting pod "simpletest.rc-jtkhw" in namespace "gc-2783"
Apr 21 20:41:52.549: INFO: Deleting pod "simpletest.rc-kjt5s" in namespace "gc-2783"
Apr 21 20:41:52.563: INFO: Deleting pod "simpletest.rc-kshxd" in namespace "gc-2783"
Apr 21 20:41:52.576: INFO: Deleting pod "simpletest.rc-l5hl8" in namespace "gc-2783"
Apr 21 20:41:52.645: INFO: Deleting pod "simpletest.rc-l78s6" in namespace "gc-2783"
Apr 21 20:41:52.662: INFO: Deleting pod "simpletest.rc-l7cr8" in namespace "gc-2783"
Apr 21 20:41:52.677: INFO: Deleting pod "simpletest.rc-lkrqz" in namespace "gc-2783"
Apr 21 20:41:52.734: INFO: Deleting pod "simpletest.rc-lqrwf" in namespace "gc-2783"
Apr 21 20:41:52.757: INFO: Deleting pod "simpletest.rc-m2769" in namespace "gc-2783"
Apr 21 20:41:52.834: INFO: Deleting pod "simpletest.rc-m7stt" in namespace "gc-2783"
Apr 21 20:41:52.861: INFO: Deleting pod "simpletest.rc-mdh6p" in namespace "gc-2783"
Apr 21 20:41:52.932: INFO: Deleting pod "simpletest.rc-mdr8h" in namespace "gc-2783"
Apr 21 20:41:52.951: INFO: Deleting pod "simpletest.rc-mh677" in namespace "gc-2783"
Apr 21 20:41:52.966: INFO: Deleting pod "simpletest.rc-mhnhr" in namespace "gc-2783"
Apr 21 20:41:52.978: INFO: Deleting pod "simpletest.rc-n6hmn" in namespace "gc-2783"
Apr 21 20:41:53.053: INFO: Deleting pod "simpletest.rc-n8jnb" in namespace "gc-2783"
Apr 21 20:41:53.063: INFO: Deleting pod "simpletest.rc-ncjtl" in namespace "gc-2783"
Apr 21 20:41:53.134: INFO: Deleting pod "simpletest.rc-nhvc4" in namespace "gc-2783"
Apr 21 20:41:53.158: INFO: Deleting pod "simpletest.rc-nsmnr" in namespace "gc-2783"
Apr 21 20:41:53.173: INFO: Deleting pod "simpletest.rc-p2vvw" in namespace "gc-2783"
Apr 21 20:41:53.239: INFO: Deleting pod "simpletest.rc-p62x9" in namespace "gc-2783"
Apr 21 20:41:53.254: INFO: Deleting pod "simpletest.rc-p9s4h" in namespace "gc-2783"
Apr 21 20:41:53.334: INFO: Deleting pod "simpletest.rc-pffnp" in namespace "gc-2783"
Apr 21 20:41:53.353: INFO: Deleting pod "simpletest.rc-pkmq8" in namespace "gc-2783"
Apr 21 20:41:53.368: INFO: Deleting pod "simpletest.rc-prbfj" in namespace "gc-2783"
Apr 21 20:41:53.449: INFO: Deleting pod "simpletest.rc-pxwz7" in namespace "gc-2783"
Apr 21 20:41:53.464: INFO: Deleting pod "simpletest.rc-qh8rl" in namespace "gc-2783"
Apr 21 20:41:53.538: INFO: Deleting pod "simpletest.rc-qkl52" in namespace "gc-2783"
Apr 21 20:41:53.565: INFO: Deleting pod "simpletest.rc-qrqkw" in namespace "gc-2783"
Apr 21 20:41:53.638: INFO: Deleting pod "simpletest.rc-qx7m7" in namespace "gc-2783"
Apr 21 20:41:53.665: INFO: Deleting pod "simpletest.rc-r9ls4" in namespace "gc-2783"
Apr 21 20:41:53.741: INFO: Deleting pod "simpletest.rc-rj92n" in namespace "gc-2783"
Apr 21 20:41:53.764: INFO: Deleting pod "simpletest.rc-s2lvg" in namespace "gc-2783"
Apr 21 20:41:53.861: INFO: Deleting pod "simpletest.rc-sc445" in namespace "gc-2783"
Apr 21 20:41:53.938: INFO: Deleting pod "simpletest.rc-shqmm" in namespace "gc-2783"
Apr 21 20:41:53.970: INFO: Deleting pod "simpletest.rc-sjvps" in namespace "gc-2783"
Apr 21 20:41:54.041: INFO: Deleting pod "simpletest.rc-sn74s" in namespace "gc-2783"
Apr 21 20:41:54.064: INFO: Deleting pod "simpletest.rc-srwxv" in namespace "gc-2783"
Apr 21 20:41:54.140: INFO: Deleting pod "simpletest.rc-ss66n" in namespace "gc-2783"
Apr 21 20:41:54.165: INFO: Deleting pod "simpletest.rc-stz9t" in namespace "gc-2783"
Apr 21 20:41:54.247: INFO: Deleting pod "simpletest.rc-tksbr" in namespace "gc-2783"
Apr 21 20:41:54.280: INFO: Deleting pod "simpletest.rc-tpgvb" in namespace "gc-2783"
Apr 21 20:41:54.344: INFO: Deleting pod "simpletest.rc-v5pkw" in namespace "gc-2783"
Apr 21 20:41:54.434: INFO: Deleting pod "simpletest.rc-vclgl" in namespace "gc-2783"
Apr 21 20:41:54.445: INFO: Deleting pod "simpletest.rc-vn5r5" in namespace "gc-2783"
Apr 21 20:41:54.454: INFO: Deleting pod "simpletest.rc-vnv4b" in namespace "gc-2783"
Apr 21 20:41:54.462: INFO: Deleting pod "simpletest.rc-vzkm8" in namespace "gc-2783"
Apr 21 20:41:54.545: INFO: Deleting pod "simpletest.rc-w6xvd" in namespace "gc-2783"
Apr 21 20:41:54.554: INFO: Deleting pod "simpletest.rc-w7h4w" in namespace "gc-2783"
Apr 21 20:41:54.564: INFO: Deleting pod "simpletest.rc-w9g9z" in namespace "gc-2783"
Apr 21 20:41:54.646: INFO: Deleting pod "simpletest.rc-wfb5s" in namespace "gc-2783"
Apr 21 20:41:54.653: INFO: Deleting pod "simpletest.rc-wrcgp" in namespace "gc-2783"
Apr 21 20:41:54.663: INFO: Deleting pod "simpletest.rc-x6q9g" in namespace "gc-2783"
Apr 21 20:41:54.737: INFO: Deleting pod "simpletest.rc-xfxsb" in namespace "gc-2783"
Apr 21 20:41:54.753: INFO: Deleting pod "simpletest.rc-xw8r7" in namespace "gc-2783"
Apr 21 20:41:54.836: INFO: Deleting pod "simpletest.rc-z24hf" in namespace "gc-2783"
Apr 21 20:41:54.847: INFO: Deleting pod "simpletest.rc-zjwfq" in namespace "gc-2783"
Apr 21 20:41:54.938: INFO: Deleting pod "simpletest.rc-zkwj4" in namespace "gc-2783"
Apr 21 20:41:54.947: INFO: Deleting pod "simpletest.rc-zmlh4" in namespace "gc-2783"
Apr 21 20:41:54.962: INFO: Deleting pod "simpletest.rc-zsx4j" in namespace "gc-2783"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 21 20:41:55.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-2783" for this suite. 04/21/23 20:41:55.047
------------------------------
â€¢ [SLOW TEST] [43.945 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:41:11.107
    Apr 21 20:41:11.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename gc 04/21/23 20:41:11.108
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:11.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:11.135
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 04/21/23 20:41:11.139
    STEP: delete the rc 04/21/23 20:41:16.15
    STEP: wait for the rc to be deleted 04/21/23 20:41:16.164
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 04/21/23 20:41:21.169
    STEP: Gathering metrics 04/21/23 20:41:51.177
    Apr 21 20:41:51.506: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
    Apr 21 20:41:51.508: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 2.069323ms
    Apr 21 20:41:51.508: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
    Apr 21 20:41:51.508: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
    Apr 21 20:41:51.578: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Apr 21 20:41:51.578: INFO: Deleting pod "simpletest.rc-2cjmr" in namespace "gc-2783"
    Apr 21 20:41:51.587: INFO: Deleting pod "simpletest.rc-2g7pk" in namespace "gc-2783"
    Apr 21 20:41:51.595: INFO: Deleting pod "simpletest.rc-2gx7r" in namespace "gc-2783"
    Apr 21 20:41:51.601: INFO: Deleting pod "simpletest.rc-2kssm" in namespace "gc-2783"
    Apr 21 20:41:51.636: INFO: Deleting pod "simpletest.rc-2m9mc" in namespace "gc-2783"
    Apr 21 20:41:51.647: INFO: Deleting pod "simpletest.rc-2pjwf" in namespace "gc-2783"
    Apr 21 20:41:51.658: INFO: Deleting pod "simpletest.rc-4nfct" in namespace "gc-2783"
    Apr 21 20:41:51.669: INFO: Deleting pod "simpletest.rc-5frr2" in namespace "gc-2783"
    Apr 21 20:41:51.676: INFO: Deleting pod "simpletest.rc-5jld7" in namespace "gc-2783"
    Apr 21 20:41:51.685: INFO: Deleting pod "simpletest.rc-8654d" in namespace "gc-2783"
    Apr 21 20:41:51.696: INFO: Deleting pod "simpletest.rc-885pp" in namespace "gc-2783"
    Apr 21 20:41:51.733: INFO: Deleting pod "simpletest.rc-8bc8x" in namespace "gc-2783"
    Apr 21 20:41:51.750: INFO: Deleting pod "simpletest.rc-8g2kp" in namespace "gc-2783"
    Apr 21 20:41:51.764: INFO: Deleting pod "simpletest.rc-8h794" in namespace "gc-2783"
    Apr 21 20:41:51.776: INFO: Deleting pod "simpletest.rc-8hnsw" in namespace "gc-2783"
    Apr 21 20:41:51.833: INFO: Deleting pod "simpletest.rc-8kchn" in namespace "gc-2783"
    Apr 21 20:41:51.846: INFO: Deleting pod "simpletest.rc-95h7w" in namespace "gc-2783"
    Apr 21 20:41:51.859: INFO: Deleting pod "simpletest.rc-9k7xr" in namespace "gc-2783"
    Apr 21 20:41:51.872: INFO: Deleting pod "simpletest.rc-9nvjt" in namespace "gc-2783"
    Apr 21 20:41:51.886: INFO: Deleting pod "simpletest.rc-9rbr9" in namespace "gc-2783"
    Apr 21 20:41:51.939: INFO: Deleting pod "simpletest.rc-9vhq4" in namespace "gc-2783"
    Apr 21 20:41:51.961: INFO: Deleting pod "simpletest.rc-b2vzr" in namespace "gc-2783"
    Apr 21 20:41:52.042: INFO: Deleting pod "simpletest.rc-bsbb8" in namespace "gc-2783"
    Apr 21 20:41:52.060: INFO: Deleting pod "simpletest.rc-c5knt" in namespace "gc-2783"
    Apr 21 20:41:52.071: INFO: Deleting pod "simpletest.rc-cdhxb" in namespace "gc-2783"
    Apr 21 20:41:52.078: INFO: Deleting pod "simpletest.rc-chnzb" in namespace "gc-2783"
    Apr 21 20:41:52.140: INFO: Deleting pod "simpletest.rc-crjsh" in namespace "gc-2783"
    Apr 21 20:41:52.149: INFO: Deleting pod "simpletest.rc-crx48" in namespace "gc-2783"
    Apr 21 20:41:52.168: INFO: Deleting pod "simpletest.rc-cvt55" in namespace "gc-2783"
    Apr 21 20:41:52.183: INFO: Deleting pod "simpletest.rc-d7klz" in namespace "gc-2783"
    Apr 21 20:41:52.252: INFO: Deleting pod "simpletest.rc-dd8gl" in namespace "gc-2783"
    Apr 21 20:41:52.261: INFO: Deleting pod "simpletest.rc-dl89f" in namespace "gc-2783"
    Apr 21 20:41:52.273: INFO: Deleting pod "simpletest.rc-dwdjw" in namespace "gc-2783"
    Apr 21 20:41:52.335: INFO: Deleting pod "simpletest.rc-fbmwz" in namespace "gc-2783"
    Apr 21 20:41:52.356: INFO: Deleting pod "simpletest.rc-fzl8l" in namespace "gc-2783"
    Apr 21 20:41:52.366: INFO: Deleting pod "simpletest.rc-g4mtn" in namespace "gc-2783"
    Apr 21 20:41:52.438: INFO: Deleting pod "simpletest.rc-g7plr" in namespace "gc-2783"
    Apr 21 20:41:52.451: INFO: Deleting pod "simpletest.rc-j9tfx" in namespace "gc-2783"
    Apr 21 20:41:52.468: INFO: Deleting pod "simpletest.rc-jcsvz" in namespace "gc-2783"
    Apr 21 20:41:52.478: INFO: Deleting pod "simpletest.rc-jhlw6" in namespace "gc-2783"
    Apr 21 20:41:52.535: INFO: Deleting pod "simpletest.rc-jtkhw" in namespace "gc-2783"
    Apr 21 20:41:52.549: INFO: Deleting pod "simpletest.rc-kjt5s" in namespace "gc-2783"
    Apr 21 20:41:52.563: INFO: Deleting pod "simpletest.rc-kshxd" in namespace "gc-2783"
    Apr 21 20:41:52.576: INFO: Deleting pod "simpletest.rc-l5hl8" in namespace "gc-2783"
    Apr 21 20:41:52.645: INFO: Deleting pod "simpletest.rc-l78s6" in namespace "gc-2783"
    Apr 21 20:41:52.662: INFO: Deleting pod "simpletest.rc-l7cr8" in namespace "gc-2783"
    Apr 21 20:41:52.677: INFO: Deleting pod "simpletest.rc-lkrqz" in namespace "gc-2783"
    Apr 21 20:41:52.734: INFO: Deleting pod "simpletest.rc-lqrwf" in namespace "gc-2783"
    Apr 21 20:41:52.757: INFO: Deleting pod "simpletest.rc-m2769" in namespace "gc-2783"
    Apr 21 20:41:52.834: INFO: Deleting pod "simpletest.rc-m7stt" in namespace "gc-2783"
    Apr 21 20:41:52.861: INFO: Deleting pod "simpletest.rc-mdh6p" in namespace "gc-2783"
    Apr 21 20:41:52.932: INFO: Deleting pod "simpletest.rc-mdr8h" in namespace "gc-2783"
    Apr 21 20:41:52.951: INFO: Deleting pod "simpletest.rc-mh677" in namespace "gc-2783"
    Apr 21 20:41:52.966: INFO: Deleting pod "simpletest.rc-mhnhr" in namespace "gc-2783"
    Apr 21 20:41:52.978: INFO: Deleting pod "simpletest.rc-n6hmn" in namespace "gc-2783"
    Apr 21 20:41:53.053: INFO: Deleting pod "simpletest.rc-n8jnb" in namespace "gc-2783"
    Apr 21 20:41:53.063: INFO: Deleting pod "simpletest.rc-ncjtl" in namespace "gc-2783"
    Apr 21 20:41:53.134: INFO: Deleting pod "simpletest.rc-nhvc4" in namespace "gc-2783"
    Apr 21 20:41:53.158: INFO: Deleting pod "simpletest.rc-nsmnr" in namespace "gc-2783"
    Apr 21 20:41:53.173: INFO: Deleting pod "simpletest.rc-p2vvw" in namespace "gc-2783"
    Apr 21 20:41:53.239: INFO: Deleting pod "simpletest.rc-p62x9" in namespace "gc-2783"
    Apr 21 20:41:53.254: INFO: Deleting pod "simpletest.rc-p9s4h" in namespace "gc-2783"
    Apr 21 20:41:53.334: INFO: Deleting pod "simpletest.rc-pffnp" in namespace "gc-2783"
    Apr 21 20:41:53.353: INFO: Deleting pod "simpletest.rc-pkmq8" in namespace "gc-2783"
    Apr 21 20:41:53.368: INFO: Deleting pod "simpletest.rc-prbfj" in namespace "gc-2783"
    Apr 21 20:41:53.449: INFO: Deleting pod "simpletest.rc-pxwz7" in namespace "gc-2783"
    Apr 21 20:41:53.464: INFO: Deleting pod "simpletest.rc-qh8rl" in namespace "gc-2783"
    Apr 21 20:41:53.538: INFO: Deleting pod "simpletest.rc-qkl52" in namespace "gc-2783"
    Apr 21 20:41:53.565: INFO: Deleting pod "simpletest.rc-qrqkw" in namespace "gc-2783"
    Apr 21 20:41:53.638: INFO: Deleting pod "simpletest.rc-qx7m7" in namespace "gc-2783"
    Apr 21 20:41:53.665: INFO: Deleting pod "simpletest.rc-r9ls4" in namespace "gc-2783"
    Apr 21 20:41:53.741: INFO: Deleting pod "simpletest.rc-rj92n" in namespace "gc-2783"
    Apr 21 20:41:53.764: INFO: Deleting pod "simpletest.rc-s2lvg" in namespace "gc-2783"
    Apr 21 20:41:53.861: INFO: Deleting pod "simpletest.rc-sc445" in namespace "gc-2783"
    Apr 21 20:41:53.938: INFO: Deleting pod "simpletest.rc-shqmm" in namespace "gc-2783"
    Apr 21 20:41:53.970: INFO: Deleting pod "simpletest.rc-sjvps" in namespace "gc-2783"
    Apr 21 20:41:54.041: INFO: Deleting pod "simpletest.rc-sn74s" in namespace "gc-2783"
    Apr 21 20:41:54.064: INFO: Deleting pod "simpletest.rc-srwxv" in namespace "gc-2783"
    Apr 21 20:41:54.140: INFO: Deleting pod "simpletest.rc-ss66n" in namespace "gc-2783"
    Apr 21 20:41:54.165: INFO: Deleting pod "simpletest.rc-stz9t" in namespace "gc-2783"
    Apr 21 20:41:54.247: INFO: Deleting pod "simpletest.rc-tksbr" in namespace "gc-2783"
    Apr 21 20:41:54.280: INFO: Deleting pod "simpletest.rc-tpgvb" in namespace "gc-2783"
    Apr 21 20:41:54.344: INFO: Deleting pod "simpletest.rc-v5pkw" in namespace "gc-2783"
    Apr 21 20:41:54.434: INFO: Deleting pod "simpletest.rc-vclgl" in namespace "gc-2783"
    Apr 21 20:41:54.445: INFO: Deleting pod "simpletest.rc-vn5r5" in namespace "gc-2783"
    Apr 21 20:41:54.454: INFO: Deleting pod "simpletest.rc-vnv4b" in namespace "gc-2783"
    Apr 21 20:41:54.462: INFO: Deleting pod "simpletest.rc-vzkm8" in namespace "gc-2783"
    Apr 21 20:41:54.545: INFO: Deleting pod "simpletest.rc-w6xvd" in namespace "gc-2783"
    Apr 21 20:41:54.554: INFO: Deleting pod "simpletest.rc-w7h4w" in namespace "gc-2783"
    Apr 21 20:41:54.564: INFO: Deleting pod "simpletest.rc-w9g9z" in namespace "gc-2783"
    Apr 21 20:41:54.646: INFO: Deleting pod "simpletest.rc-wfb5s" in namespace "gc-2783"
    Apr 21 20:41:54.653: INFO: Deleting pod "simpletest.rc-wrcgp" in namespace "gc-2783"
    Apr 21 20:41:54.663: INFO: Deleting pod "simpletest.rc-x6q9g" in namespace "gc-2783"
    Apr 21 20:41:54.737: INFO: Deleting pod "simpletest.rc-xfxsb" in namespace "gc-2783"
    Apr 21 20:41:54.753: INFO: Deleting pod "simpletest.rc-xw8r7" in namespace "gc-2783"
    Apr 21 20:41:54.836: INFO: Deleting pod "simpletest.rc-z24hf" in namespace "gc-2783"
    Apr 21 20:41:54.847: INFO: Deleting pod "simpletest.rc-zjwfq" in namespace "gc-2783"
    Apr 21 20:41:54.938: INFO: Deleting pod "simpletest.rc-zkwj4" in namespace "gc-2783"
    Apr 21 20:41:54.947: INFO: Deleting pod "simpletest.rc-zmlh4" in namespace "gc-2783"
    Apr 21 20:41:54.962: INFO: Deleting pod "simpletest.rc-zsx4j" in namespace "gc-2783"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:41:55.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-2783" for this suite. 04/21/23 20:41:55.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:41:55.055
Apr 21 20:41:55.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 20:41:55.056
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:55.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:55.147
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 04/21/23 20:41:55.154
STEP: watching for the Service to be added 04/21/23 20:41:55.166
Apr 21 20:41:55.168: INFO: Found Service test-service-lhthw in namespace services-1617 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Apr 21 20:41:55.168: INFO: Service test-service-lhthw created
STEP: Getting /status 04/21/23 20:41:55.168
Apr 21 20:41:55.234: INFO: Service test-service-lhthw has LoadBalancer: {[]}
STEP: patching the ServiceStatus 04/21/23 20:41:55.235
STEP: watching for the Service to be patched 04/21/23 20:41:55.242
Apr 21 20:41:55.244: INFO: observed Service test-service-lhthw in namespace services-1617 with annotations: map[] & LoadBalancer: {[]}
Apr 21 20:41:55.244: INFO: Found Service test-service-lhthw in namespace services-1617 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Apr 21 20:41:55.244: INFO: Service test-service-lhthw has service status patched
STEP: updating the ServiceStatus 04/21/23 20:41:55.244
Apr 21 20:41:55.251: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 04/21/23 20:41:55.251
Apr 21 20:41:55.253: INFO: Observed Service test-service-lhthw in namespace services-1617 with annotations: map[] & Conditions: {[]}
Apr 21 20:41:55.253: INFO: Observed event: &Service{ObjectMeta:{test-service-lhthw  services-1617  91288b62-ef34-4b76-933e-5b7d071fd37c 8209 0 2023-04-21 20:41:55 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-21 20:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-21 20:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.99.44.71,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.99.44.71],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Apr 21 20:41:55.253: INFO: Found Service test-service-lhthw in namespace services-1617 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 21 20:41:55.253: INFO: Service test-service-lhthw has service status updated
STEP: patching the service 04/21/23 20:41:55.253
STEP: watching for the Service to be patched 04/21/23 20:41:55.334
Apr 21 20:41:55.336: INFO: observed Service test-service-lhthw in namespace services-1617 with labels: map[test-service-static:true]
Apr 21 20:41:55.336: INFO: observed Service test-service-lhthw in namespace services-1617 with labels: map[test-service-static:true]
Apr 21 20:41:55.336: INFO: observed Service test-service-lhthw in namespace services-1617 with labels: map[test-service-static:true]
Apr 21 20:41:55.336: INFO: Found Service test-service-lhthw in namespace services-1617 with labels: map[test-service:patched test-service-static:true]
Apr 21 20:41:55.336: INFO: Service test-service-lhthw patched
STEP: deleting the service 04/21/23 20:41:55.336
STEP: watching for the Service to be deleted 04/21/23 20:41:55.355
Apr 21 20:41:55.356: INFO: Observed event: ADDED
Apr 21 20:41:55.356: INFO: Observed event: MODIFIED
Apr 21 20:41:55.356: INFO: Observed event: MODIFIED
Apr 21 20:41:55.356: INFO: Observed event: MODIFIED
Apr 21 20:41:55.356: INFO: Found Service test-service-lhthw in namespace services-1617 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Apr 21 20:41:55.356: INFO: Service test-service-lhthw deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 20:41:55.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1617" for this suite. 04/21/23 20:41:55.359
------------------------------
â€¢ [0.314 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:41:55.055
    Apr 21 20:41:55.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 20:41:55.056
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:55.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:55.147
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 04/21/23 20:41:55.154
    STEP: watching for the Service to be added 04/21/23 20:41:55.166
    Apr 21 20:41:55.168: INFO: Found Service test-service-lhthw in namespace services-1617 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Apr 21 20:41:55.168: INFO: Service test-service-lhthw created
    STEP: Getting /status 04/21/23 20:41:55.168
    Apr 21 20:41:55.234: INFO: Service test-service-lhthw has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 04/21/23 20:41:55.235
    STEP: watching for the Service to be patched 04/21/23 20:41:55.242
    Apr 21 20:41:55.244: INFO: observed Service test-service-lhthw in namespace services-1617 with annotations: map[] & LoadBalancer: {[]}
    Apr 21 20:41:55.244: INFO: Found Service test-service-lhthw in namespace services-1617 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Apr 21 20:41:55.244: INFO: Service test-service-lhthw has service status patched
    STEP: updating the ServiceStatus 04/21/23 20:41:55.244
    Apr 21 20:41:55.251: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 04/21/23 20:41:55.251
    Apr 21 20:41:55.253: INFO: Observed Service test-service-lhthw in namespace services-1617 with annotations: map[] & Conditions: {[]}
    Apr 21 20:41:55.253: INFO: Observed event: &Service{ObjectMeta:{test-service-lhthw  services-1617  91288b62-ef34-4b76-933e-5b7d071fd37c 8209 0 2023-04-21 20:41:55 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-21 20:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-21 20:41:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.99.44.71,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.99.44.71],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Apr 21 20:41:55.253: INFO: Found Service test-service-lhthw in namespace services-1617 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 21 20:41:55.253: INFO: Service test-service-lhthw has service status updated
    STEP: patching the service 04/21/23 20:41:55.253
    STEP: watching for the Service to be patched 04/21/23 20:41:55.334
    Apr 21 20:41:55.336: INFO: observed Service test-service-lhthw in namespace services-1617 with labels: map[test-service-static:true]
    Apr 21 20:41:55.336: INFO: observed Service test-service-lhthw in namespace services-1617 with labels: map[test-service-static:true]
    Apr 21 20:41:55.336: INFO: observed Service test-service-lhthw in namespace services-1617 with labels: map[test-service-static:true]
    Apr 21 20:41:55.336: INFO: Found Service test-service-lhthw in namespace services-1617 with labels: map[test-service:patched test-service-static:true]
    Apr 21 20:41:55.336: INFO: Service test-service-lhthw patched
    STEP: deleting the service 04/21/23 20:41:55.336
    STEP: watching for the Service to be deleted 04/21/23 20:41:55.355
    Apr 21 20:41:55.356: INFO: Observed event: ADDED
    Apr 21 20:41:55.356: INFO: Observed event: MODIFIED
    Apr 21 20:41:55.356: INFO: Observed event: MODIFIED
    Apr 21 20:41:55.356: INFO: Observed event: MODIFIED
    Apr 21 20:41:55.356: INFO: Found Service test-service-lhthw in namespace services-1617 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Apr 21 20:41:55.356: INFO: Service test-service-lhthw deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:41:55.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1617" for this suite. 04/21/23 20:41:55.359
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:41:55.369
Apr 21 20:41:55.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 20:41:55.37
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:55.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:55.447
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Apr 21 20:41:55.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:41:56.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7064" for this suite. 04/21/23 20:41:56.538
------------------------------
â€¢ [1.172 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:41:55.369
    Apr 21 20:41:55.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 20:41:55.37
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:55.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:55.447
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Apr 21 20:41:55.450: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:41:56.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7064" for this suite. 04/21/23 20:41:56.538
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:41:56.542
Apr 21 20:41:56.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 20:41:56.543
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:56.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:56.557
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 04/21/23 20:41:56.559
Apr 21 20:41:56.564: INFO: Waiting up to 5m0s for pod "pod-a4a667f2-c3de-4208-85d8-b78339209734" in namespace "emptydir-6270" to be "Succeeded or Failed"
Apr 21 20:41:56.566: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077505ms
Apr 21 20:41:58.575: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011036812s
Apr 21 20:42:00.570: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006019907s
Apr 21 20:42:02.568: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004379135s
Apr 21 20:42:04.568: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00458666s
STEP: Saw pod success 04/21/23 20:42:04.568
Apr 21 20:42:04.568: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734" satisfied condition "Succeeded or Failed"
Apr 21 20:42:04.570: INFO: Trying to get logs from node k8sconformance-m02 pod pod-a4a667f2-c3de-4208-85d8-b78339209734 container test-container: <nil>
STEP: delete the pod 04/21/23 20:42:04.578
Apr 21 20:42:04.585: INFO: Waiting for pod pod-a4a667f2-c3de-4208-85d8-b78339209734 to disappear
Apr 21 20:42:04.587: INFO: Pod pod-a4a667f2-c3de-4208-85d8-b78339209734 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:04.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6270" for this suite. 04/21/23 20:42:04.589
------------------------------
â€¢ [SLOW TEST] [8.051 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:41:56.542
    Apr 21 20:41:56.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 20:41:56.543
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:41:56.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:41:56.557
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 04/21/23 20:41:56.559
    Apr 21 20:41:56.564: INFO: Waiting up to 5m0s for pod "pod-a4a667f2-c3de-4208-85d8-b78339209734" in namespace "emptydir-6270" to be "Succeeded or Failed"
    Apr 21 20:41:56.566: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Pending", Reason="", readiness=false. Elapsed: 2.077505ms
    Apr 21 20:41:58.575: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011036812s
    Apr 21 20:42:00.570: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006019907s
    Apr 21 20:42:02.568: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004379135s
    Apr 21 20:42:04.568: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.00458666s
    STEP: Saw pod success 04/21/23 20:42:04.568
    Apr 21 20:42:04.568: INFO: Pod "pod-a4a667f2-c3de-4208-85d8-b78339209734" satisfied condition "Succeeded or Failed"
    Apr 21 20:42:04.570: INFO: Trying to get logs from node k8sconformance-m02 pod pod-a4a667f2-c3de-4208-85d8-b78339209734 container test-container: <nil>
    STEP: delete the pod 04/21/23 20:42:04.578
    Apr 21 20:42:04.585: INFO: Waiting for pod pod-a4a667f2-c3de-4208-85d8-b78339209734 to disappear
    Apr 21 20:42:04.587: INFO: Pod pod-a4a667f2-c3de-4208-85d8-b78339209734 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:04.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6270" for this suite. 04/21/23 20:42:04.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:04.594
Apr 21 20:42:04.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 20:42:04.595
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:04.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:04.604
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-gt6ln" 04/21/23 20:42:04.607
Apr 21 20:42:04.611: INFO: Resource quota "e2e-rq-status-gt6ln" reports spec: hard cpu limit of 500m
Apr 21 20:42:04.611: INFO: Resource quota "e2e-rq-status-gt6ln" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-gt6ln" /status 04/21/23 20:42:04.611
STEP: Confirm /status for "e2e-rq-status-gt6ln" resourceQuota via watch 04/21/23 20:42:04.615
Apr 21 20:42:04.616: INFO: observed resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList(nil)
Apr 21 20:42:04.616: INFO: Found resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Apr 21 20:42:04.616: INFO: ResourceQuota "e2e-rq-status-gt6ln" /status was updated
STEP: Patching hard spec values for cpu & memory 04/21/23 20:42:04.617
Apr 21 20:42:04.621: INFO: Resource quota "e2e-rq-status-gt6ln" reports spec: hard cpu limit of 1
Apr 21 20:42:04.621: INFO: Resource quota "e2e-rq-status-gt6ln" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-gt6ln" /status 04/21/23 20:42:04.621
STEP: Confirm /status for "e2e-rq-status-gt6ln" resourceQuota via watch 04/21/23 20:42:04.624
Apr 21 20:42:04.625: INFO: observed resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Apr 21 20:42:04.625: INFO: Found resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Apr 21 20:42:04.625: INFO: ResourceQuota "e2e-rq-status-gt6ln" /status was patched
STEP: Get "e2e-rq-status-gt6ln" /status 04/21/23 20:42:04.625
Apr 21 20:42:04.627: INFO: Resourcequota "e2e-rq-status-gt6ln" reports status: hard cpu of 1
Apr 21 20:42:04.627: INFO: Resourcequota "e2e-rq-status-gt6ln" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-gt6ln" /status before checking Spec is unchanged 04/21/23 20:42:04.628
Apr 21 20:42:04.631: INFO: Resourcequota "e2e-rq-status-gt6ln" reports status: hard cpu of 2
Apr 21 20:42:04.631: INFO: Resourcequota "e2e-rq-status-gt6ln" reports status: hard memory of 2Gi
Apr 21 20:42:04.632: INFO: Found resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Apr 21 20:42:09.636: INFO: ResourceQuota "e2e-rq-status-gt6ln" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:09.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4126" for this suite. 04/21/23 20:42:09.639
------------------------------
â€¢ [SLOW TEST] [5.049 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:04.594
    Apr 21 20:42:04.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 20:42:04.595
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:04.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:04.604
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-gt6ln" 04/21/23 20:42:04.607
    Apr 21 20:42:04.611: INFO: Resource quota "e2e-rq-status-gt6ln" reports spec: hard cpu limit of 500m
    Apr 21 20:42:04.611: INFO: Resource quota "e2e-rq-status-gt6ln" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-gt6ln" /status 04/21/23 20:42:04.611
    STEP: Confirm /status for "e2e-rq-status-gt6ln" resourceQuota via watch 04/21/23 20:42:04.615
    Apr 21 20:42:04.616: INFO: observed resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList(nil)
    Apr 21 20:42:04.616: INFO: Found resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Apr 21 20:42:04.616: INFO: ResourceQuota "e2e-rq-status-gt6ln" /status was updated
    STEP: Patching hard spec values for cpu & memory 04/21/23 20:42:04.617
    Apr 21 20:42:04.621: INFO: Resource quota "e2e-rq-status-gt6ln" reports spec: hard cpu limit of 1
    Apr 21 20:42:04.621: INFO: Resource quota "e2e-rq-status-gt6ln" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-gt6ln" /status 04/21/23 20:42:04.621
    STEP: Confirm /status for "e2e-rq-status-gt6ln" resourceQuota via watch 04/21/23 20:42:04.624
    Apr 21 20:42:04.625: INFO: observed resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Apr 21 20:42:04.625: INFO: Found resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Apr 21 20:42:04.625: INFO: ResourceQuota "e2e-rq-status-gt6ln" /status was patched
    STEP: Get "e2e-rq-status-gt6ln" /status 04/21/23 20:42:04.625
    Apr 21 20:42:04.627: INFO: Resourcequota "e2e-rq-status-gt6ln" reports status: hard cpu of 1
    Apr 21 20:42:04.627: INFO: Resourcequota "e2e-rq-status-gt6ln" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-gt6ln" /status before checking Spec is unchanged 04/21/23 20:42:04.628
    Apr 21 20:42:04.631: INFO: Resourcequota "e2e-rq-status-gt6ln" reports status: hard cpu of 2
    Apr 21 20:42:04.631: INFO: Resourcequota "e2e-rq-status-gt6ln" reports status: hard memory of 2Gi
    Apr 21 20:42:04.632: INFO: Found resourceQuota "e2e-rq-status-gt6ln" in namespace "resourcequota-4126" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Apr 21 20:42:09.636: INFO: ResourceQuota "e2e-rq-status-gt6ln" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:09.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4126" for this suite. 04/21/23 20:42:09.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:09.645
Apr 21 20:42:09.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 20:42:09.645
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:09.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:09.655
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 04/21/23 20:42:09.657
Apr 21 20:42:09.663: INFO: Waiting up to 5m0s for pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2" in namespace "emptydir-896" to be "Succeeded or Failed"
Apr 21 20:42:09.664: INFO: Pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.529397ms
Apr 21 20:42:11.668: INFO: Pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005148799s
Apr 21 20:42:13.668: INFO: Pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005080103s
STEP: Saw pod success 04/21/23 20:42:13.668
Apr 21 20:42:13.668: INFO: Pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2" satisfied condition "Succeeded or Failed"
Apr 21 20:42:13.669: INFO: Trying to get logs from node k8sconformance-m02 pod pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2 container test-container: <nil>
STEP: delete the pod 04/21/23 20:42:13.674
Apr 21 20:42:13.682: INFO: Waiting for pod pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2 to disappear
Apr 21 20:42:13.684: INFO: Pod pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:13.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-896" for this suite. 04/21/23 20:42:13.686
------------------------------
â€¢ [4.046 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:09.645
    Apr 21 20:42:09.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 20:42:09.645
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:09.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:09.655
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 04/21/23 20:42:09.657
    Apr 21 20:42:09.663: INFO: Waiting up to 5m0s for pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2" in namespace "emptydir-896" to be "Succeeded or Failed"
    Apr 21 20:42:09.664: INFO: Pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.529397ms
    Apr 21 20:42:11.668: INFO: Pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005148799s
    Apr 21 20:42:13.668: INFO: Pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005080103s
    STEP: Saw pod success 04/21/23 20:42:13.668
    Apr 21 20:42:13.668: INFO: Pod "pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2" satisfied condition "Succeeded or Failed"
    Apr 21 20:42:13.669: INFO: Trying to get logs from node k8sconformance-m02 pod pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2 container test-container: <nil>
    STEP: delete the pod 04/21/23 20:42:13.674
    Apr 21 20:42:13.682: INFO: Waiting for pod pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2 to disappear
    Apr 21 20:42:13.684: INFO: Pod pod-4cf6c794-7e46-419c-9b5b-06f7f00407a2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:13.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-896" for this suite. 04/21/23 20:42:13.686
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:13.691
Apr 21 20:42:13.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:42:13.692
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:13.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:13.702
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:42:13.704
Apr 21 20:42:13.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc" in namespace "projected-6667" to be "Succeeded or Failed"
Apr 21 20:42:13.711: INFO: Pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471346ms
Apr 21 20:42:15.714: INFO: Pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004630591s
Apr 21 20:42:17.714: INFO: Pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004361693s
STEP: Saw pod success 04/21/23 20:42:17.714
Apr 21 20:42:17.714: INFO: Pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc" satisfied condition "Succeeded or Failed"
Apr 21 20:42:17.716: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc container client-container: <nil>
STEP: delete the pod 04/21/23 20:42:17.721
Apr 21 20:42:17.728: INFO: Waiting for pod downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc to disappear
Apr 21 20:42:17.730: INFO: Pod downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:17.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6667" for this suite. 04/21/23 20:42:17.732
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:13.691
    Apr 21 20:42:13.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:42:13.692
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:13.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:13.702
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:42:13.704
    Apr 21 20:42:13.709: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc" in namespace "projected-6667" to be "Succeeded or Failed"
    Apr 21 20:42:13.711: INFO: Pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.471346ms
    Apr 21 20:42:15.714: INFO: Pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004630591s
    Apr 21 20:42:17.714: INFO: Pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004361693s
    STEP: Saw pod success 04/21/23 20:42:17.714
    Apr 21 20:42:17.714: INFO: Pod "downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc" satisfied condition "Succeeded or Failed"
    Apr 21 20:42:17.716: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc container client-container: <nil>
    STEP: delete the pod 04/21/23 20:42:17.721
    Apr 21 20:42:17.728: INFO: Waiting for pod downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc to disappear
    Apr 21 20:42:17.730: INFO: Pod downwardapi-volume-3543625b-26ab-4ba3-b5e2-1d4b3c18cdfc no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:17.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6667" for this suite. 04/21/23 20:42:17.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:17.736
Apr 21 20:42:17.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 20:42:17.737
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:17.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:17.745
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 04/21/23 20:42:17.747
STEP: Ensuring ResourceQuota status is calculated 04/21/23 20:42:17.75
STEP: Creating a ResourceQuota with not terminating scope 04/21/23 20:42:19.753
STEP: Ensuring ResourceQuota status is calculated 04/21/23 20:42:19.757
STEP: Creating a long running pod 04/21/23 20:42:21.76
STEP: Ensuring resource quota with not terminating scope captures the pod usage 04/21/23 20:42:21.769
STEP: Ensuring resource quota with terminating scope ignored the pod usage 04/21/23 20:42:23.772
STEP: Deleting the pod 04/21/23 20:42:25.776
STEP: Ensuring resource quota status released the pod usage 04/21/23 20:42:25.786
STEP: Creating a terminating pod 04/21/23 20:42:27.788
STEP: Ensuring resource quota with terminating scope captures the pod usage 04/21/23 20:42:27.796
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 04/21/23 20:42:29.799
STEP: Deleting the pod 04/21/23 20:42:31.803
STEP: Ensuring resource quota status released the pod usage 04/21/23 20:42:31.808
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:33.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4095" for this suite. 04/21/23 20:42:33.813
------------------------------
â€¢ [SLOW TEST] [16.082 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:17.736
    Apr 21 20:42:17.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 20:42:17.737
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:17.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:17.745
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 04/21/23 20:42:17.747
    STEP: Ensuring ResourceQuota status is calculated 04/21/23 20:42:17.75
    STEP: Creating a ResourceQuota with not terminating scope 04/21/23 20:42:19.753
    STEP: Ensuring ResourceQuota status is calculated 04/21/23 20:42:19.757
    STEP: Creating a long running pod 04/21/23 20:42:21.76
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 04/21/23 20:42:21.769
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 04/21/23 20:42:23.772
    STEP: Deleting the pod 04/21/23 20:42:25.776
    STEP: Ensuring resource quota status released the pod usage 04/21/23 20:42:25.786
    STEP: Creating a terminating pod 04/21/23 20:42:27.788
    STEP: Ensuring resource quota with terminating scope captures the pod usage 04/21/23 20:42:27.796
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 04/21/23 20:42:29.799
    STEP: Deleting the pod 04/21/23 20:42:31.803
    STEP: Ensuring resource quota status released the pod usage 04/21/23 20:42:31.808
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:33.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4095" for this suite. 04/21/23 20:42:33.813
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:33.818
Apr 21 20:42:33.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:42:33.819
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:33.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:33.828
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:42:33.83
Apr 21 20:42:33.836: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0" in namespace "projected-1963" to be "Succeeded or Failed"
Apr 21 20:42:33.838: INFO: Pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.710532ms
Apr 21 20:42:35.841: INFO: Pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004842484s
Apr 21 20:42:37.841: INFO: Pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004615802s
STEP: Saw pod success 04/21/23 20:42:37.841
Apr 21 20:42:37.841: INFO: Pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0" satisfied condition "Succeeded or Failed"
Apr 21 20:42:37.843: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-3f783444-3163-445c-a723-58639461abc0 container client-container: <nil>
STEP: delete the pod 04/21/23 20:42:37.847
Apr 21 20:42:37.855: INFO: Waiting for pod downwardapi-volume-3f783444-3163-445c-a723-58639461abc0 to disappear
Apr 21 20:42:37.857: INFO: Pod downwardapi-volume-3f783444-3163-445c-a723-58639461abc0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:37.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1963" for this suite. 04/21/23 20:42:37.859
------------------------------
â€¢ [4.045 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:33.818
    Apr 21 20:42:33.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:42:33.819
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:33.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:33.828
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:42:33.83
    Apr 21 20:42:33.836: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0" in namespace "projected-1963" to be "Succeeded or Failed"
    Apr 21 20:42:33.838: INFO: Pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.710532ms
    Apr 21 20:42:35.841: INFO: Pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004842484s
    Apr 21 20:42:37.841: INFO: Pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004615802s
    STEP: Saw pod success 04/21/23 20:42:37.841
    Apr 21 20:42:37.841: INFO: Pod "downwardapi-volume-3f783444-3163-445c-a723-58639461abc0" satisfied condition "Succeeded or Failed"
    Apr 21 20:42:37.843: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-3f783444-3163-445c-a723-58639461abc0 container client-container: <nil>
    STEP: delete the pod 04/21/23 20:42:37.847
    Apr 21 20:42:37.855: INFO: Waiting for pod downwardapi-volume-3f783444-3163-445c-a723-58639461abc0 to disappear
    Apr 21 20:42:37.857: INFO: Pod downwardapi-volume-3f783444-3163-445c-a723-58639461abc0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:37.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1963" for this suite. 04/21/23 20:42:37.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:37.864
Apr 21 20:42:37.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-lifecycle-hook 04/21/23 20:42:37.865
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:37.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:37.873
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/21/23 20:42:37.877
Apr 21 20:42:37.882: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9498" to be "running and ready"
Apr 21 20:42:37.884: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.501546ms
Apr 21 20:42:37.884: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:42:39.887: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004112011s
Apr 21 20:42:39.887: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 21 20:42:39.887: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 04/21/23 20:42:39.888
Apr 21 20:42:39.893: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9498" to be "running and ready"
Apr 21 20:42:39.895: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542809ms
Apr 21 20:42:39.895: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:42:41.898: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004489697s
Apr 21 20:42:41.898: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Apr 21 20:42:41.898: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 04/21/23 20:42:41.9
STEP: delete the pod with lifecycle hook 04/21/23 20:42:41.905
Apr 21 20:42:41.910: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 21 20:42:41.911: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 21 20:42:43.912: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 21 20:42:43.914: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:43.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9498" for this suite. 04/21/23 20:42:43.916
------------------------------
â€¢ [SLOW TEST] [6.056 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:37.864
    Apr 21 20:42:37.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/21/23 20:42:37.865
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:37.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:37.873
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/21/23 20:42:37.877
    Apr 21 20:42:37.882: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9498" to be "running and ready"
    Apr 21 20:42:37.884: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.501546ms
    Apr 21 20:42:37.884: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:42:39.887: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004112011s
    Apr 21 20:42:39.887: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 21 20:42:39.887: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 04/21/23 20:42:39.888
    Apr 21 20:42:39.893: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9498" to be "running and ready"
    Apr 21 20:42:39.895: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542809ms
    Apr 21 20:42:39.895: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:42:41.898: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004489697s
    Apr 21 20:42:41.898: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Apr 21 20:42:41.898: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 04/21/23 20:42:41.9
    STEP: delete the pod with lifecycle hook 04/21/23 20:42:41.905
    Apr 21 20:42:41.910: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Apr 21 20:42:41.911: INFO: Pod pod-with-poststart-exec-hook still exists
    Apr 21 20:42:43.912: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Apr 21 20:42:43.914: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:43.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9498" for this suite. 04/21/23 20:42:43.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:43.921
Apr 21 20:42:43.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:42:43.922
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:43.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:43.931
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:42:43.939
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:42:44.537
STEP: Deploying the webhook pod 04/21/23 20:42:44.541
STEP: Wait for the deployment to be ready 04/21/23 20:42:44.55
Apr 21 20:42:44.554: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 20:42:46.575
STEP: Verifying the service has paired with the endpoint 04/21/23 20:42:46.584
Apr 21 20:42:47.584: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 04/21/23 20:42:47.586
STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:42:47.597
STEP: Updating a validating webhook configuration's rules to not include the create operation 04/21/23 20:42:47.603
STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:42:47.609
STEP: Patching a validating webhook configuration's rules to include the create operation 04/21/23 20:42:47.614
STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:42:47.619
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:47.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8514" for this suite. 04/21/23 20:42:47.647
STEP: Destroying namespace "webhook-8514-markers" for this suite. 04/21/23 20:42:47.65
------------------------------
â€¢ [3.735 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:43.921
    Apr 21 20:42:43.921: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:42:43.922
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:43.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:43.931
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:42:43.939
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:42:44.537
    STEP: Deploying the webhook pod 04/21/23 20:42:44.541
    STEP: Wait for the deployment to be ready 04/21/23 20:42:44.55
    Apr 21 20:42:44.554: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 20:42:46.575
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:42:46.584
    Apr 21 20:42:47.584: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 04/21/23 20:42:47.586
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:42:47.597
    STEP: Updating a validating webhook configuration's rules to not include the create operation 04/21/23 20:42:47.603
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:42:47.609
    STEP: Patching a validating webhook configuration's rules to include the create operation 04/21/23 20:42:47.614
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:42:47.619
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:47.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8514" for this suite. 04/21/23 20:42:47.647
    STEP: Destroying namespace "webhook-8514-markers" for this suite. 04/21/23 20:42:47.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:47.657
Apr 21 20:42:47.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-pred 04/21/23 20:42:47.657
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:47.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:47.668
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 21 20:42:47.670: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 21 20:42:47.674: INFO: Waiting for terminating namespaces to be deleted...
Apr 21 20:42:47.676: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Apr 21 20:42:47.679: INFO: coredns-787d4945fb-bp78j from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.679: INFO: 	Container coredns ready: true, restart count 1
Apr 21 20:42:47.679: INFO: etcd-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.679: INFO: 	Container etcd ready: true, restart count 0
Apr 21 20:42:47.679: INFO: kindnet-6bcq2 from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.679: INFO: 	Container kindnet-cni ready: true, restart count 0
Apr 21 20:42:47.679: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.679: INFO: 	Container kube-apiserver ready: true, restart count 0
Apr 21 20:42:47.679: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.679: INFO: 	Container kube-controller-manager ready: true, restart count 0
Apr 21 20:42:47.679: INFO: kube-proxy-k9znz from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.679: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 21 20:42:47.679: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.679: INFO: 	Container kube-scheduler ready: true, restart count 0
Apr 21 20:42:47.679: INFO: storage-provisioner from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.679: INFO: 	Container storage-provisioner ready: true, restart count 0
Apr 21 20:42:47.679: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 20:42:47.679: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 20:42:47.679: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 21 20:42:47.679: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Apr 21 20:42:47.682: INFO: pod-handle-http-request from container-lifecycle-hook-9498 started at 2023-04-21 20:42:37 +0000 UTC (2 container statuses recorded)
Apr 21 20:42:47.682: INFO: 	Container container-handle-http-request ready: true, restart count 0
Apr 21 20:42:47.682: INFO: 	Container container-handle-https-request ready: true, restart count 0
Apr 21 20:42:47.682: INFO: kindnet-dtkcc from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.682: INFO: 	Container kindnet-cni ready: true, restart count 0
Apr 21 20:42:47.682: INFO: kube-proxy-fvpz4 from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.682: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 21 20:42:47.682: INFO: sonobuoy from sonobuoy started at 2023-04-21 20:13:43 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.682: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 21 20:42:47.682: INFO: sonobuoy-e2e-job-ddbedf54a58f4885 from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 20:42:47.682: INFO: 	Container e2e ready: true, restart count 0
Apr 21 20:42:47.682: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 20:42:47.682: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 20:42:47.682: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 20:42:47.682: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 21 20:42:47.682: INFO: webhook-to-be-mutated from webhook-2520 started at 2023-04-21 20:41:06 +0000 UTC (1 container statuses recorded)
Apr 21 20:42:47.682: INFO: 	Container example ready: false, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 04/21/23 20:42:47.682
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.17580debe8a0f6e7], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 04/21/23 20:42:47.696
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:48.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-6016" for this suite. 04/21/23 20:42:48.699
------------------------------
â€¢ [1.046 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:47.657
    Apr 21 20:42:47.657: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-pred 04/21/23 20:42:47.657
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:47.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:47.668
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 21 20:42:47.670: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 21 20:42:47.674: INFO: Waiting for terminating namespaces to be deleted...
    Apr 21 20:42:47.676: INFO: 
    Logging pods the apiserver thinks is on node k8sconformance before test
    Apr 21 20:42:47.679: INFO: coredns-787d4945fb-bp78j from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.679: INFO: 	Container coredns ready: true, restart count 1
    Apr 21 20:42:47.679: INFO: etcd-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.679: INFO: 	Container etcd ready: true, restart count 0
    Apr 21 20:42:47.679: INFO: kindnet-6bcq2 from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.679: INFO: 	Container kindnet-cni ready: true, restart count 0
    Apr 21 20:42:47.679: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.679: INFO: 	Container kube-apiserver ready: true, restart count 0
    Apr 21 20:42:47.679: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.679: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Apr 21 20:42:47.679: INFO: kube-proxy-k9znz from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.679: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 21 20:42:47.679: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.679: INFO: 	Container kube-scheduler ready: true, restart count 0
    Apr 21 20:42:47.679: INFO: storage-provisioner from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.679: INFO: 	Container storage-provisioner ready: true, restart count 0
    Apr 21 20:42:47.679: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 20:42:47.679: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 20:42:47.679: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 21 20:42:47.679: INFO: 
    Logging pods the apiserver thinks is on node k8sconformance-m02 before test
    Apr 21 20:42:47.682: INFO: pod-handle-http-request from container-lifecycle-hook-9498 started at 2023-04-21 20:42:37 +0000 UTC (2 container statuses recorded)
    Apr 21 20:42:47.682: INFO: 	Container container-handle-http-request ready: true, restart count 0
    Apr 21 20:42:47.682: INFO: 	Container container-handle-https-request ready: true, restart count 0
    Apr 21 20:42:47.682: INFO: kindnet-dtkcc from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.682: INFO: 	Container kindnet-cni ready: true, restart count 0
    Apr 21 20:42:47.682: INFO: kube-proxy-fvpz4 from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.682: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 21 20:42:47.682: INFO: sonobuoy from sonobuoy started at 2023-04-21 20:13:43 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.682: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 21 20:42:47.682: INFO: sonobuoy-e2e-job-ddbedf54a58f4885 from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 20:42:47.682: INFO: 	Container e2e ready: true, restart count 0
    Apr 21 20:42:47.682: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 20:42:47.682: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 20:42:47.682: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 20:42:47.682: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 21 20:42:47.682: INFO: webhook-to-be-mutated from webhook-2520 started at 2023-04-21 20:41:06 +0000 UTC (1 container statuses recorded)
    Apr 21 20:42:47.682: INFO: 	Container example ready: false, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 04/21/23 20:42:47.682
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.17580debe8a0f6e7], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] 04/21/23 20:42:47.696
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:48.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-6016" for this suite. 04/21/23 20:42:48.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:48.704
Apr 21 20:42:48.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:42:48.705
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:48.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:48.714
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:42:48.723
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:42:49.166
STEP: Deploying the webhook pod 04/21/23 20:42:49.169
STEP: Wait for the deployment to be ready 04/21/23 20:42:49.179
Apr 21 20:42:49.182: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 20:42:51.189
STEP: Verifying the service has paired with the endpoint 04/21/23 20:42:51.197
Apr 21 20:42:52.197: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 04/21/23 20:42:52.247
STEP: Creating a configMap that should be mutated 04/21/23 20:42:52.256
STEP: Deleting the collection of validation webhooks 04/21/23 20:42:52.273
STEP: Creating a configMap that should not be mutated 04/21/23 20:42:52.297
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:52.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3670" for this suite. 04/21/23 20:42:52.347
STEP: Destroying namespace "webhook-3670-markers" for this suite. 04/21/23 20:42:52.353
------------------------------
â€¢ [3.654 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:48.704
    Apr 21 20:42:48.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:42:48.705
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:48.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:48.714
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:42:48.723
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:42:49.166
    STEP: Deploying the webhook pod 04/21/23 20:42:49.169
    STEP: Wait for the deployment to be ready 04/21/23 20:42:49.179
    Apr 21 20:42:49.182: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 20:42:51.189
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:42:51.197
    Apr 21 20:42:52.197: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 04/21/23 20:42:52.247
    STEP: Creating a configMap that should be mutated 04/21/23 20:42:52.256
    STEP: Deleting the collection of validation webhooks 04/21/23 20:42:52.273
    STEP: Creating a configMap that should not be mutated 04/21/23 20:42:52.297
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:52.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3670" for this suite. 04/21/23 20:42:52.347
    STEP: Destroying namespace "webhook-3670-markers" for this suite. 04/21/23 20:42:52.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:52.358
Apr 21 20:42:52.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 20:42:52.359
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:52.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:52.37
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 04/21/23 20:42:52.372
Apr 21 20:42:52.373: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6316 proxy --unix-socket=/tmp/kubectl-proxy-unix576419582/test'
STEP: retrieving proxy /api/ output 04/21/23 20:42:52.417
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:52.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6316" for this suite. 04/21/23 20:42:52.42
------------------------------
â€¢ [0.074 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:52.358
    Apr 21 20:42:52.358: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 20:42:52.359
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:52.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:52.37
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 04/21/23 20:42:52.372
    Apr 21 20:42:52.373: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6316 proxy --unix-socket=/tmp/kubectl-proxy-unix576419582/test'
    STEP: retrieving proxy /api/ output 04/21/23 20:42:52.417
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:52.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6316" for this suite. 04/21/23 20:42:52.42
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:52.432
Apr 21 20:42:52.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:42:52.433
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:52.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:52.442
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:42:52.444
Apr 21 20:42:52.450: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7" in namespace "projected-1285" to be "Succeeded or Failed"
Apr 21 20:42:52.452: INFO: Pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.589358ms
Apr 21 20:42:54.454: INFO: Pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00384814s
Apr 21 20:42:56.455: INFO: Pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004973862s
STEP: Saw pod success 04/21/23 20:42:56.455
Apr 21 20:42:56.455: INFO: Pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7" satisfied condition "Succeeded or Failed"
Apr 21 20:42:56.457: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7 container client-container: <nil>
STEP: delete the pod 04/21/23 20:42:56.462
Apr 21 20:42:56.470: INFO: Waiting for pod downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7 to disappear
Apr 21 20:42:56.471: INFO: Pod downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 20:42:56.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1285" for this suite. 04/21/23 20:42:56.473
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:52.432
    Apr 21 20:42:52.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:42:52.433
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:52.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:52.442
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:42:52.444
    Apr 21 20:42:52.450: INFO: Waiting up to 5m0s for pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7" in namespace "projected-1285" to be "Succeeded or Failed"
    Apr 21 20:42:52.452: INFO: Pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.589358ms
    Apr 21 20:42:54.454: INFO: Pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00384814s
    Apr 21 20:42:56.455: INFO: Pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004973862s
    STEP: Saw pod success 04/21/23 20:42:56.455
    Apr 21 20:42:56.455: INFO: Pod "downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7" satisfied condition "Succeeded or Failed"
    Apr 21 20:42:56.457: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7 container client-container: <nil>
    STEP: delete the pod 04/21/23 20:42:56.462
    Apr 21 20:42:56.470: INFO: Waiting for pod downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7 to disappear
    Apr 21 20:42:56.471: INFO: Pod downwardapi-volume-74df5aa7-d580-4b47-bd94-fa59ff6a1af7 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:42:56.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1285" for this suite. 04/21/23 20:42:56.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:42:56.477
Apr 21 20:42:56.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename svcaccounts 04/21/23 20:42:56.477
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:56.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:56.486
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Apr 21 20:42:56.493: INFO: created pod
Apr 21 20:42:56.493: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7843" to be "Succeeded or Failed"
Apr 21 20:42:56.495: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.389096ms
Apr 21 20:42:58.498: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004412236s
Apr 21 20:43:00.497: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003859408s
STEP: Saw pod success 04/21/23 20:43:00.497
Apr 21 20:43:00.497: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Apr 21 20:43:30.498: INFO: polling logs
Apr 21 20:43:30.504: INFO: Pod logs: 
I0421 20:42:57.143764       1 log.go:198] OK: Got token
I0421 20:42:57.143798       1 log.go:198] validating with in-cluster discovery
I0421 20:42:57.144069       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0421 20:42:57.144092       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7843:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682110376, NotBefore:1682109776, IssuedAt:1682109776, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7843", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"fef3d5a6-5689-47a2-a439-a66119d3aee2"}}}
I0421 20:42:57.166649       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0421 20:42:57.170573       1 log.go:198] OK: Validated signature on JWT
I0421 20:42:57.170642       1 log.go:198] OK: Got valid claims from token!
I0421 20:42:57.170670       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7843:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682110376, NotBefore:1682109776, IssuedAt:1682109776, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7843", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"fef3d5a6-5689-47a2-a439-a66119d3aee2"}}}

Apr 21 20:43:30.504: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 21 20:43:30.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7843" for this suite. 04/21/23 20:43:30.509
------------------------------
â€¢ [SLOW TEST] [34.036 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:42:56.477
    Apr 21 20:42:56.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename svcaccounts 04/21/23 20:42:56.477
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:42:56.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:42:56.486
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Apr 21 20:42:56.493: INFO: created pod
    Apr 21 20:42:56.493: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7843" to be "Succeeded or Failed"
    Apr 21 20:42:56.495: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.389096ms
    Apr 21 20:42:58.498: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004412236s
    Apr 21 20:43:00.497: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003859408s
    STEP: Saw pod success 04/21/23 20:43:00.497
    Apr 21 20:43:00.497: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Apr 21 20:43:30.498: INFO: polling logs
    Apr 21 20:43:30.504: INFO: Pod logs: 
    I0421 20:42:57.143764       1 log.go:198] OK: Got token
    I0421 20:42:57.143798       1 log.go:198] validating with in-cluster discovery
    I0421 20:42:57.144069       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0421 20:42:57.144092       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7843:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682110376, NotBefore:1682109776, IssuedAt:1682109776, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7843", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"fef3d5a6-5689-47a2-a439-a66119d3aee2"}}}
    I0421 20:42:57.166649       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0421 20:42:57.170573       1 log.go:198] OK: Validated signature on JWT
    I0421 20:42:57.170642       1 log.go:198] OK: Got valid claims from token!
    I0421 20:42:57.170670       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7843:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1682110376, NotBefore:1682109776, IssuedAt:1682109776, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7843", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"fef3d5a6-5689-47a2-a439-a66119d3aee2"}}}

    Apr 21 20:43:30.504: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:43:30.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7843" for this suite. 04/21/23 20:43:30.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:43:30.513
Apr 21 20:43:30.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename security-context-test 04/21/23 20:43:30.514
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:30.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:30.523
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Apr 21 20:43:30.529: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d" in namespace "security-context-test-3541" to be "Succeeded or Failed"
Apr 21 20:43:30.530: INFO: Pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.563806ms
Apr 21 20:43:32.533: INFO: Pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004647997s
Apr 21 20:43:34.532: INFO: Pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003701597s
Apr 21 20:43:34.532: INFO: Pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 21 20:43:34.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-3541" for this suite. 04/21/23 20:43:34.535
------------------------------
â€¢ [4.026 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:43:30.513
    Apr 21 20:43:30.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename security-context-test 04/21/23 20:43:30.514
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:30.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:30.523
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Apr 21 20:43:30.529: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d" in namespace "security-context-test-3541" to be "Succeeded or Failed"
    Apr 21 20:43:30.530: INFO: Pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.563806ms
    Apr 21 20:43:32.533: INFO: Pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004647997s
    Apr 21 20:43:34.532: INFO: Pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003701597s
    Apr 21 20:43:34.532: INFO: Pod "busybox-readonly-false-12da1be3-c625-43c8-98ce-4ea17007da6d" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:43:34.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-3541" for this suite. 04/21/23 20:43:34.535
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:43:34.539
Apr 21 20:43:34.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replicaset 04/21/23 20:43:34.539
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:34.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:34.549
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 04/21/23 20:43:34.552
STEP: Verify that the required pods have come up. 04/21/23 20:43:34.555
Apr 21 20:43:34.556: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 21 20:43:39.559: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/21/23 20:43:39.559
STEP: Getting /status 04/21/23 20:43:39.559
Apr 21 20:43:39.562: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 04/21/23 20:43:39.562
Apr 21 20:43:39.568: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 04/21/23 20:43:39.568
Apr 21 20:43:39.569: INFO: Observed &ReplicaSet event: ADDED
Apr 21 20:43:39.569: INFO: Observed &ReplicaSet event: MODIFIED
Apr 21 20:43:39.569: INFO: Observed &ReplicaSet event: MODIFIED
Apr 21 20:43:39.569: INFO: Observed &ReplicaSet event: MODIFIED
Apr 21 20:43:39.569: INFO: Found replicaset test-rs in namespace replicaset-5554 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 21 20:43:39.570: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 04/21/23 20:43:39.57
Apr 21 20:43:39.570: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 21 20:43:39.575: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 04/21/23 20:43:39.575
Apr 21 20:43:39.576: INFO: Observed &ReplicaSet event: ADDED
Apr 21 20:43:39.577: INFO: Observed &ReplicaSet event: MODIFIED
Apr 21 20:43:39.577: INFO: Observed &ReplicaSet event: MODIFIED
Apr 21 20:43:39.577: INFO: Observed &ReplicaSet event: MODIFIED
Apr 21 20:43:39.577: INFO: Observed replicaset test-rs in namespace replicaset-5554 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 21 20:43:39.577: INFO: Observed &ReplicaSet event: MODIFIED
Apr 21 20:43:39.577: INFO: Found replicaset test-rs in namespace replicaset-5554 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Apr 21 20:43:39.577: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:43:39.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5554" for this suite. 04/21/23 20:43:39.579
------------------------------
â€¢ [SLOW TEST] [5.043 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:43:34.539
    Apr 21 20:43:34.539: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replicaset 04/21/23 20:43:34.539
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:34.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:34.549
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 04/21/23 20:43:34.552
    STEP: Verify that the required pods have come up. 04/21/23 20:43:34.555
    Apr 21 20:43:34.556: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 21 20:43:39.559: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/21/23 20:43:39.559
    STEP: Getting /status 04/21/23 20:43:39.559
    Apr 21 20:43:39.562: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 04/21/23 20:43:39.562
    Apr 21 20:43:39.568: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 04/21/23 20:43:39.568
    Apr 21 20:43:39.569: INFO: Observed &ReplicaSet event: ADDED
    Apr 21 20:43:39.569: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 21 20:43:39.569: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 21 20:43:39.569: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 21 20:43:39.569: INFO: Found replicaset test-rs in namespace replicaset-5554 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 21 20:43:39.570: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 04/21/23 20:43:39.57
    Apr 21 20:43:39.570: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr 21 20:43:39.575: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 04/21/23 20:43:39.575
    Apr 21 20:43:39.576: INFO: Observed &ReplicaSet event: ADDED
    Apr 21 20:43:39.577: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 21 20:43:39.577: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 21 20:43:39.577: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 21 20:43:39.577: INFO: Observed replicaset test-rs in namespace replicaset-5554 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 21 20:43:39.577: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 21 20:43:39.577: INFO: Found replicaset test-rs in namespace replicaset-5554 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Apr 21 20:43:39.577: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:43:39.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5554" for this suite. 04/21/23 20:43:39.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:43:39.583
Apr 21 20:43:39.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename gc 04/21/23 20:43:39.584
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:39.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:39.592
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 04/21/23 20:43:39.594
STEP: Wait for the Deployment to create new ReplicaSet 04/21/23 20:43:39.597
STEP: delete the deployment 04/21/23 20:43:40.103
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 04/21/23 20:43:40.108
STEP: Gathering metrics 04/21/23 20:43:40.62
Apr 21 20:43:40.633: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
Apr 21 20:43:40.635: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 1.831145ms
Apr 21 20:43:40.635: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
Apr 21 20:43:40.635: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
Apr 21 20:43:40.684: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 21 20:43:40.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5442" for this suite. 04/21/23 20:43:40.687
------------------------------
â€¢ [1.107 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:43:39.583
    Apr 21 20:43:39.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename gc 04/21/23 20:43:39.584
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:39.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:39.592
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 04/21/23 20:43:39.594
    STEP: Wait for the Deployment to create new ReplicaSet 04/21/23 20:43:39.597
    STEP: delete the deployment 04/21/23 20:43:40.103
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 04/21/23 20:43:40.108
    STEP: Gathering metrics 04/21/23 20:43:40.62
    Apr 21 20:43:40.633: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
    Apr 21 20:43:40.635: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 1.831145ms
    Apr 21 20:43:40.635: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
    Apr 21 20:43:40.635: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
    Apr 21 20:43:40.684: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:43:40.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5442" for this suite. 04/21/23 20:43:40.687
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:43:40.692
Apr 21 20:43:40.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:43:40.693
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:40.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:40.701
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 04/21/23 20:43:40.707
Apr 21 20:43:40.707: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019" in namespace "kubelet-test-3084" to be "completed"
Apr 21 20:43:40.709: INFO: Pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019": Phase="Pending", Reason="", readiness=false. Elapsed: 1.410551ms
Apr 21 20:43:42.711: INFO: Pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003666527s
Apr 21 20:43:44.713: INFO: Pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005739653s
Apr 21 20:43:44.713: INFO: Pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:43:44.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3084" for this suite. 04/21/23 20:43:44.721
------------------------------
â€¢ [4.032 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:43:40.692
    Apr 21 20:43:40.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:43:40.693
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:40.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:40.701
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 04/21/23 20:43:40.707
    Apr 21 20:43:40.707: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019" in namespace "kubelet-test-3084" to be "completed"
    Apr 21 20:43:40.709: INFO: Pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019": Phase="Pending", Reason="", readiness=false. Elapsed: 1.410551ms
    Apr 21 20:43:42.711: INFO: Pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003666527s
    Apr 21 20:43:44.713: INFO: Pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005739653s
    Apr 21 20:43:44.713: INFO: Pod "agnhost-host-aliases8608663a-461a-4a7f-b84c-fb9cef19b019" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:43:44.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3084" for this suite. 04/21/23 20:43:44.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:43:44.727
Apr 21 20:43:44.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 20:43:44.728
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:44.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:44.739
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-239/configmap-test-1ee83deb-4746-4fec-ad47-8cdfc0d5cec2 04/21/23 20:43:44.74
STEP: Creating a pod to test consume configMaps 04/21/23 20:43:44.743
Apr 21 20:43:44.749: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1" in namespace "configmap-239" to be "Succeeded or Failed"
Apr 21 20:43:44.751: INFO: Pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.350007ms
Apr 21 20:43:46.754: INFO: Pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004362528s
Apr 21 20:43:48.754: INFO: Pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004338797s
STEP: Saw pod success 04/21/23 20:43:48.754
Apr 21 20:43:48.754: INFO: Pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1" satisfied condition "Succeeded or Failed"
Apr 21 20:43:48.755: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1 container env-test: <nil>
STEP: delete the pod 04/21/23 20:43:48.761
Apr 21 20:43:48.766: INFO: Waiting for pod pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1 to disappear
Apr 21 20:43:48.767: INFO: Pod pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:43:48.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-239" for this suite. 04/21/23 20:43:48.769
------------------------------
â€¢ [4.047 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:43:44.727
    Apr 21 20:43:44.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 20:43:44.728
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:44.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:44.739
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-239/configmap-test-1ee83deb-4746-4fec-ad47-8cdfc0d5cec2 04/21/23 20:43:44.74
    STEP: Creating a pod to test consume configMaps 04/21/23 20:43:44.743
    Apr 21 20:43:44.749: INFO: Waiting up to 5m0s for pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1" in namespace "configmap-239" to be "Succeeded or Failed"
    Apr 21 20:43:44.751: INFO: Pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.350007ms
    Apr 21 20:43:46.754: INFO: Pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004362528s
    Apr 21 20:43:48.754: INFO: Pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004338797s
    STEP: Saw pod success 04/21/23 20:43:48.754
    Apr 21 20:43:48.754: INFO: Pod "pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1" satisfied condition "Succeeded or Failed"
    Apr 21 20:43:48.755: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1 container env-test: <nil>
    STEP: delete the pod 04/21/23 20:43:48.761
    Apr 21 20:43:48.766: INFO: Waiting for pod pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1 to disappear
    Apr 21 20:43:48.767: INFO: Pod pod-configmaps-6e81fd30-6d99-42c7-8002-01796e690ad1 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:43:48.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-239" for this suite. 04/21/23 20:43:48.769
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:43:48.774
Apr 21 20:43:48.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename dns 04/21/23 20:43:48.775
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:48.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:48.783
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 04/21/23 20:43:48.791
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3626.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3626.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 222.1.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.1.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.1.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.1.222_tcp@PTR;sleep 1; done
 04/21/23 20:43:48.802
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3626.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3626.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 222.1.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.1.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.1.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.1.222_tcp@PTR;sleep 1; done
 04/21/23 20:43:48.802
STEP: creating a pod to probe DNS 04/21/23 20:43:48.802
STEP: submitting the pod to kubernetes 04/21/23 20:43:48.802
Apr 21 20:43:48.809: INFO: Waiting up to 15m0s for pod "dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b" in namespace "dns-3626" to be "running"
Apr 21 20:43:48.811: INFO: Pod "dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309535ms
Apr 21 20:43:50.814: INFO: Pod "dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005667859s
Apr 21 20:43:50.814: INFO: Pod "dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b" satisfied condition "running"
STEP: retrieving the pod 04/21/23 20:43:50.814
STEP: looking for the results for each expected name from probers 04/21/23 20:43:50.816
Apr 21 20:43:50.818: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:50.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:50.822: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:50.823: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:50.831: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:50.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:50.834: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:50.835: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:50.841: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

Apr 21 20:43:55.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:55.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:55.849: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:55.851: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:55.859: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:55.860: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:55.862: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:55.863: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:43:55.869: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

Apr 21 20:44:00.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:00.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:00.849: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:00.851: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:00.859: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:00.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:00.862: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:00.864: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:00.870: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

Apr 21 20:44:05.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:05.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:05.849: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:05.851: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:05.859: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:05.860: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:05.862: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:05.863: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:05.869: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

Apr 21 20:44:10.844: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:10.846: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:10.848: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:10.850: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:10.857: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:10.859: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:10.860: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:10.862: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:10.867: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

Apr 21 20:44:15.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:15.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:15.848: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:15.850: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:15.858: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:15.859: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:15.861: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:15.862: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
Apr 21 20:44:15.868: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

Apr 21 20:44:20.868: INFO: DNS probes using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b succeeded

STEP: deleting the pod 04/21/23 20:44:20.868
STEP: deleting the test service 04/21/23 20:44:20.877
STEP: deleting the test headless service 04/21/23 20:44:20.899
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 21 20:44:20.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3626" for this suite. 04/21/23 20:44:20.912
------------------------------
â€¢ [SLOW TEST] [32.143 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:43:48.774
    Apr 21 20:43:48.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename dns 04/21/23 20:43:48.775
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:43:48.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:43:48.783
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 04/21/23 20:43:48.791
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3626.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3626.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 222.1.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.1.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.1.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.1.222_tcp@PTR;sleep 1; done
     04/21/23 20:43:48.802
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3626.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3626.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3626.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3626.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3626.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 222.1.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.1.222_udp@PTR;check="$$(dig +tcp +noall +answer +search 222.1.102.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.102.1.222_tcp@PTR;sleep 1; done
     04/21/23 20:43:48.802
    STEP: creating a pod to probe DNS 04/21/23 20:43:48.802
    STEP: submitting the pod to kubernetes 04/21/23 20:43:48.802
    Apr 21 20:43:48.809: INFO: Waiting up to 15m0s for pod "dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b" in namespace "dns-3626" to be "running"
    Apr 21 20:43:48.811: INFO: Pod "dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309535ms
    Apr 21 20:43:50.814: INFO: Pod "dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b": Phase="Running", Reason="", readiness=true. Elapsed: 2.005667859s
    Apr 21 20:43:50.814: INFO: Pod "dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b" satisfied condition "running"
    STEP: retrieving the pod 04/21/23 20:43:50.814
    STEP: looking for the results for each expected name from probers 04/21/23 20:43:50.816
    Apr 21 20:43:50.818: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:50.820: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:50.822: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:50.823: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:50.831: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:50.833: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:50.834: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:50.835: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:50.841: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

    Apr 21 20:43:55.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:55.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:55.849: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:55.851: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:55.859: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:55.860: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:55.862: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:55.863: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:43:55.869: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

    Apr 21 20:44:00.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:00.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:00.849: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:00.851: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:00.859: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:00.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:00.862: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:00.864: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:00.870: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

    Apr 21 20:44:05.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:05.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:05.849: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:05.851: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:05.859: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:05.860: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:05.862: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:05.863: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:05.869: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

    Apr 21 20:44:10.844: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:10.846: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:10.848: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:10.850: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:10.857: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:10.859: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:10.860: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:10.862: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:10.867: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

    Apr 21 20:44:15.845: INFO: Unable to read wheezy_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:15.847: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:15.848: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:15.850: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:15.858: INFO: Unable to read jessie_udp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:15.859: INFO: Unable to read jessie_tcp@dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:15.861: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:15.862: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local from pod dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b: the server could not find the requested resource (get pods dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b)
    Apr 21 20:44:15.868: INFO: Lookups using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b failed for: [wheezy_udp@dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@dns-test-service.dns-3626.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_udp@dns-test-service.dns-3626.svc.cluster.local jessie_tcp@dns-test-service.dns-3626.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3626.svc.cluster.local]

    Apr 21 20:44:20.868: INFO: DNS probes using dns-3626/dns-test-bde4ebe5-0815-4023-af1e-ecf9a0c7d00b succeeded

    STEP: deleting the pod 04/21/23 20:44:20.868
    STEP: deleting the test service 04/21/23 20:44:20.877
    STEP: deleting the test headless service 04/21/23 20:44:20.899
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:44:20.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3626" for this suite. 04/21/23 20:44:20.912
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:44:20.919
Apr 21 20:44:20.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:44:20.92
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:44:20.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:44:20.936
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-4d79ae47-95ac-41d0-b3a8-4c25260e8e13 04/21/23 20:44:20.939
STEP: Creating a pod to test consume secrets 04/21/23 20:44:20.942
Apr 21 20:44:20.948: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934" in namespace "projected-8773" to be "Succeeded or Failed"
Apr 21 20:44:20.950: INFO: Pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041953ms
Apr 21 20:44:22.953: INFO: Pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005042162s
Apr 21 20:44:24.952: INFO: Pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004504857s
STEP: Saw pod success 04/21/23 20:44:24.952
Apr 21 20:44:24.952: INFO: Pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934" satisfied condition "Succeeded or Failed"
Apr 21 20:44:24.954: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/21/23 20:44:24.959
Apr 21 20:44:24.967: INFO: Waiting for pod pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934 to disappear
Apr 21 20:44:24.969: INFO: Pod pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 21 20:44:24.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8773" for this suite. 04/21/23 20:44:24.97
------------------------------
â€¢ [4.055 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:44:20.919
    Apr 21 20:44:20.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:44:20.92
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:44:20.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:44:20.936
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-4d79ae47-95ac-41d0-b3a8-4c25260e8e13 04/21/23 20:44:20.939
    STEP: Creating a pod to test consume secrets 04/21/23 20:44:20.942
    Apr 21 20:44:20.948: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934" in namespace "projected-8773" to be "Succeeded or Failed"
    Apr 21 20:44:20.950: INFO: Pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041953ms
    Apr 21 20:44:22.953: INFO: Pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005042162s
    Apr 21 20:44:24.952: INFO: Pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004504857s
    STEP: Saw pod success 04/21/23 20:44:24.952
    Apr 21 20:44:24.952: INFO: Pod "pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934" satisfied condition "Succeeded or Failed"
    Apr 21 20:44:24.954: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 20:44:24.959
    Apr 21 20:44:24.967: INFO: Waiting for pod pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934 to disappear
    Apr 21 20:44:24.969: INFO: Pod pod-projected-secrets-9b4effa5-1649-4746-b3d8-05952f720934 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:44:24.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8773" for this suite. 04/21/23 20:44:24.97
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:44:24.974
Apr 21 20:44:24.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename limitrange 04/21/23 20:44:24.975
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:44:24.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:44:24.983
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-shnvk" in namespace "limitrange-2442" 04/21/23 20:44:24.984
STEP: Creating another limitRange in another namespace 04/21/23 20:44:24.987
Apr 21 20:44:24.993: INFO: Namespace "e2e-limitrange-shnvk-423" created
Apr 21 20:44:24.993: INFO: Creating LimitRange "e2e-limitrange-shnvk" in namespace "e2e-limitrange-shnvk-423"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-shnvk" 04/21/23 20:44:24.996
Apr 21 20:44:24.997: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-shnvk" in "limitrange-2442" namespace 04/21/23 20:44:24.997
Apr 21 20:44:25.002: INFO: LimitRange "e2e-limitrange-shnvk" has been patched
STEP: Delete LimitRange "e2e-limitrange-shnvk" by Collection with labelSelector: "e2e-limitrange-shnvk=patched" 04/21/23 20:44:25.002
STEP: Confirm that the limitRange "e2e-limitrange-shnvk" has been deleted 04/21/23 20:44:25.006
Apr 21 20:44:25.006: INFO: Requesting list of LimitRange to confirm quantity
Apr 21 20:44:25.007: INFO: Found 0 LimitRange with label "e2e-limitrange-shnvk=patched"
Apr 21 20:44:25.007: INFO: LimitRange "e2e-limitrange-shnvk" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-shnvk" 04/21/23 20:44:25.007
Apr 21 20:44:25.008: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Apr 21 20:44:25.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-2442" for this suite. 04/21/23 20:44:25.01
STEP: Destroying namespace "e2e-limitrange-shnvk-423" for this suite. 04/21/23 20:44:25.014
------------------------------
â€¢ [0.042 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:44:24.974
    Apr 21 20:44:24.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename limitrange 04/21/23 20:44:24.975
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:44:24.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:44:24.983
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-shnvk" in namespace "limitrange-2442" 04/21/23 20:44:24.984
    STEP: Creating another limitRange in another namespace 04/21/23 20:44:24.987
    Apr 21 20:44:24.993: INFO: Namespace "e2e-limitrange-shnvk-423" created
    Apr 21 20:44:24.993: INFO: Creating LimitRange "e2e-limitrange-shnvk" in namespace "e2e-limitrange-shnvk-423"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-shnvk" 04/21/23 20:44:24.996
    Apr 21 20:44:24.997: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-shnvk" in "limitrange-2442" namespace 04/21/23 20:44:24.997
    Apr 21 20:44:25.002: INFO: LimitRange "e2e-limitrange-shnvk" has been patched
    STEP: Delete LimitRange "e2e-limitrange-shnvk" by Collection with labelSelector: "e2e-limitrange-shnvk=patched" 04/21/23 20:44:25.002
    STEP: Confirm that the limitRange "e2e-limitrange-shnvk" has been deleted 04/21/23 20:44:25.006
    Apr 21 20:44:25.006: INFO: Requesting list of LimitRange to confirm quantity
    Apr 21 20:44:25.007: INFO: Found 0 LimitRange with label "e2e-limitrange-shnvk=patched"
    Apr 21 20:44:25.007: INFO: LimitRange "e2e-limitrange-shnvk" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-shnvk" 04/21/23 20:44:25.007
    Apr 21 20:44:25.008: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:44:25.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-2442" for this suite. 04/21/23 20:44:25.01
    STEP: Destroying namespace "e2e-limitrange-shnvk-423" for this suite. 04/21/23 20:44:25.014
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:44:25.017
Apr 21 20:44:25.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename proxy 04/21/23 20:44:25.017
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:44:25.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:44:25.025
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Apr 21 20:44:25.026: INFO: Creating pod...
Apr 21 20:44:25.031: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3716" to be "running"
Apr 21 20:44:25.033: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461121ms
Apr 21 20:44:27.036: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004410345s
Apr 21 20:44:27.036: INFO: Pod "agnhost" satisfied condition "running"
Apr 21 20:44:27.036: INFO: Creating service...
Apr 21 20:44:27.045: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=DELETE
Apr 21 20:44:27.047: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 21 20:44:27.047: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=OPTIONS
Apr 21 20:44:27.049: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 21 20:44:27.049: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=PATCH
Apr 21 20:44:27.051: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 21 20:44:27.051: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=POST
Apr 21 20:44:27.053: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 21 20:44:27.053: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=PUT
Apr 21 20:44:27.055: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 21 20:44:27.055: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=DELETE
Apr 21 20:44:27.057: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 21 20:44:27.057: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=OPTIONS
Apr 21 20:44:27.059: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 21 20:44:27.059: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=PATCH
Apr 21 20:44:27.061: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 21 20:44:27.061: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=POST
Apr 21 20:44:27.064: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 21 20:44:27.064: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=PUT
Apr 21 20:44:27.066: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 21 20:44:27.066: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=GET
Apr 21 20:44:27.071: INFO: http.Client request:GET StatusCode:301
Apr 21 20:44:27.071: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=GET
Apr 21 20:44:27.074: INFO: http.Client request:GET StatusCode:301
Apr 21 20:44:27.074: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=HEAD
Apr 21 20:44:27.075: INFO: http.Client request:HEAD StatusCode:301
Apr 21 20:44:27.075: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=HEAD
Apr 21 20:44:27.077: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Apr 21 20:44:27.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-3716" for this suite. 04/21/23 20:44:27.079
------------------------------
â€¢ [2.067 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:44:25.017
    Apr 21 20:44:25.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename proxy 04/21/23 20:44:25.017
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:44:25.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:44:25.025
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Apr 21 20:44:25.026: INFO: Creating pod...
    Apr 21 20:44:25.031: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-3716" to be "running"
    Apr 21 20:44:25.033: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461121ms
    Apr 21 20:44:27.036: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.004410345s
    Apr 21 20:44:27.036: INFO: Pod "agnhost" satisfied condition "running"
    Apr 21 20:44:27.036: INFO: Creating service...
    Apr 21 20:44:27.045: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=DELETE
    Apr 21 20:44:27.047: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 21 20:44:27.047: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=OPTIONS
    Apr 21 20:44:27.049: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 21 20:44:27.049: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=PATCH
    Apr 21 20:44:27.051: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 21 20:44:27.051: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=POST
    Apr 21 20:44:27.053: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 21 20:44:27.053: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=PUT
    Apr 21 20:44:27.055: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr 21 20:44:27.055: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=DELETE
    Apr 21 20:44:27.057: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 21 20:44:27.057: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Apr 21 20:44:27.059: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 21 20:44:27.059: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=PATCH
    Apr 21 20:44:27.061: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 21 20:44:27.061: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=POST
    Apr 21 20:44:27.064: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 21 20:44:27.064: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=PUT
    Apr 21 20:44:27.066: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr 21 20:44:27.066: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=GET
    Apr 21 20:44:27.071: INFO: http.Client request:GET StatusCode:301
    Apr 21 20:44:27.071: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=GET
    Apr 21 20:44:27.074: INFO: http.Client request:GET StatusCode:301
    Apr 21 20:44:27.074: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/pods/agnhost/proxy?method=HEAD
    Apr 21 20:44:27.075: INFO: http.Client request:HEAD StatusCode:301
    Apr 21 20:44:27.075: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-3716/services/e2e-proxy-test-service/proxy?method=HEAD
    Apr 21 20:44:27.077: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:44:27.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-3716" for this suite. 04/21/23 20:44:27.079
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:44:27.084
Apr 21 20:44:27.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-probe 04/21/23 20:44:27.085
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:44:27.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:44:27.098
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-9084a640-7683-4292-9a33-1485b4605e31 in namespace container-probe-8463 04/21/23 20:44:27.1
Apr 21 20:44:27.106: INFO: Waiting up to 5m0s for pod "busybox-9084a640-7683-4292-9a33-1485b4605e31" in namespace "container-probe-8463" to be "not pending"
Apr 21 20:44:27.108: INFO: Pod "busybox-9084a640-7683-4292-9a33-1485b4605e31": Phase="Pending", Reason="", readiness=false. Elapsed: 1.725046ms
Apr 21 20:44:29.111: INFO: Pod "busybox-9084a640-7683-4292-9a33-1485b4605e31": Phase="Running", Reason="", readiness=true. Elapsed: 2.004869757s
Apr 21 20:44:29.111: INFO: Pod "busybox-9084a640-7683-4292-9a33-1485b4605e31" satisfied condition "not pending"
Apr 21 20:44:29.111: INFO: Started pod busybox-9084a640-7683-4292-9a33-1485b4605e31 in namespace container-probe-8463
STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 20:44:29.111
Apr 21 20:44:29.113: INFO: Initial restart count of pod busybox-9084a640-7683-4292-9a33-1485b4605e31 is 0
Apr 21 20:45:19.189: INFO: Restart count of pod container-probe-8463/busybox-9084a640-7683-4292-9a33-1485b4605e31 is now 1 (50.076513828s elapsed)
STEP: deleting the pod 04/21/23 20:45:19.189
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 21 20:45:19.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8463" for this suite. 04/21/23 20:45:19.197
------------------------------
â€¢ [SLOW TEST] [52.117 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:44:27.084
    Apr 21 20:44:27.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-probe 04/21/23 20:44:27.085
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:44:27.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:44:27.098
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-9084a640-7683-4292-9a33-1485b4605e31 in namespace container-probe-8463 04/21/23 20:44:27.1
    Apr 21 20:44:27.106: INFO: Waiting up to 5m0s for pod "busybox-9084a640-7683-4292-9a33-1485b4605e31" in namespace "container-probe-8463" to be "not pending"
    Apr 21 20:44:27.108: INFO: Pod "busybox-9084a640-7683-4292-9a33-1485b4605e31": Phase="Pending", Reason="", readiness=false. Elapsed: 1.725046ms
    Apr 21 20:44:29.111: INFO: Pod "busybox-9084a640-7683-4292-9a33-1485b4605e31": Phase="Running", Reason="", readiness=true. Elapsed: 2.004869757s
    Apr 21 20:44:29.111: INFO: Pod "busybox-9084a640-7683-4292-9a33-1485b4605e31" satisfied condition "not pending"
    Apr 21 20:44:29.111: INFO: Started pod busybox-9084a640-7683-4292-9a33-1485b4605e31 in namespace container-probe-8463
    STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 20:44:29.111
    Apr 21 20:44:29.113: INFO: Initial restart count of pod busybox-9084a640-7683-4292-9a33-1485b4605e31 is 0
    Apr 21 20:45:19.189: INFO: Restart count of pod container-probe-8463/busybox-9084a640-7683-4292-9a33-1485b4605e31 is now 1 (50.076513828s elapsed)
    STEP: deleting the pod 04/21/23 20:45:19.189
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:45:19.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8463" for this suite. 04/21/23 20:45:19.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:45:19.202
Apr 21 20:45:19.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename runtimeclass 04/21/23 20:45:19.203
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:45:19.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:45:19.211
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Apr 21 20:45:19.220: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3564 to be scheduled
Apr 21 20:45:19.222: INFO: 1 pods are not scheduled: [runtimeclass-3564/test-runtimeclass-runtimeclass-3564-preconfigured-handler-m2g8w(26d607d6-403c-474f-bc70-68cef6e7ac1f)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 21 20:45:21.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-3564" for this suite. 04/21/23 20:45:21.231
------------------------------
â€¢ [2.032 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:45:19.202
    Apr 21 20:45:19.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename runtimeclass 04/21/23 20:45:19.203
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:45:19.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:45:19.211
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Apr 21 20:45:19.220: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-3564 to be scheduled
    Apr 21 20:45:19.222: INFO: 1 pods are not scheduled: [runtimeclass-3564/test-runtimeclass-runtimeclass-3564-preconfigured-handler-m2g8w(26d607d6-403c-474f-bc70-68cef6e7ac1f)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:45:21.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-3564" for this suite. 04/21/23 20:45:21.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:45:21.234
Apr 21 20:45:21.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename taint-multiple-pods 04/21/23 20:45:21.235
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:45:21.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:45:21.244
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Apr 21 20:45:21.245: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 21 20:46:21.256: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Apr 21 20:46:21.258: INFO: Starting informer...
STEP: Starting pods... 04/21/23 20:46:21.258
Apr 21 20:46:21.468: INFO: Pod1 is running on k8sconformance-m02. Tainting Node
Apr 21 20:46:21.676: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7070" to be "running"
Apr 21 20:46:21.677: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.793916ms
Apr 21 20:46:23.681: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004985133s
Apr 21 20:46:23.681: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Apr 21 20:46:23.681: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7070" to be "running"
Apr 21 20:46:23.682: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.63093ms
Apr 21 20:46:23.682: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Apr 21 20:46:23.682: INFO: Pod2 is running on k8sconformance-m02. Tainting Node
STEP: Trying to apply a taint on the Node 04/21/23 20:46:23.682
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/21/23 20:46:23.689
STEP: Waiting for Pod1 and Pod2 to be deleted 04/21/23 20:46:23.691
Apr 21 20:46:29.210: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 21 20:46:49.062: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/21/23 20:46:49.07
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:46:49.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-7070" for this suite. 04/21/23 20:46:49.074
------------------------------
â€¢ [SLOW TEST] [87.844 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:45:21.234
    Apr 21 20:45:21.235: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename taint-multiple-pods 04/21/23 20:45:21.235
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:45:21.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:45:21.244
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Apr 21 20:45:21.245: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 21 20:46:21.256: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Apr 21 20:46:21.258: INFO: Starting informer...
    STEP: Starting pods... 04/21/23 20:46:21.258
    Apr 21 20:46:21.468: INFO: Pod1 is running on k8sconformance-m02. Tainting Node
    Apr 21 20:46:21.676: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7070" to be "running"
    Apr 21 20:46:21.677: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.793916ms
    Apr 21 20:46:23.681: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004985133s
    Apr 21 20:46:23.681: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Apr 21 20:46:23.681: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7070" to be "running"
    Apr 21 20:46:23.682: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 1.63093ms
    Apr 21 20:46:23.682: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Apr 21 20:46:23.682: INFO: Pod2 is running on k8sconformance-m02. Tainting Node
    STEP: Trying to apply a taint on the Node 04/21/23 20:46:23.682
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/21/23 20:46:23.689
    STEP: Waiting for Pod1 and Pod2 to be deleted 04/21/23 20:46:23.691
    Apr 21 20:46:29.210: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Apr 21 20:46:49.062: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/21/23 20:46:49.07
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:46:49.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-7070" for this suite. 04/21/23 20:46:49.074
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:46:49.079
Apr 21 20:46:49.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename namespaces 04/21/23 20:46:49.08
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:46:49.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:46:49.094
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-w9gjs" 04/21/23 20:46:49.096
Apr 21 20:46:49.102: INFO: Namespace "e2e-ns-w9gjs-9283" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-w9gjs-9283" 04/21/23 20:46:49.102
Apr 21 20:46:49.106: INFO: Namespace "e2e-ns-w9gjs-9283" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-w9gjs-9283" 04/21/23 20:46:49.106
Apr 21 20:46:49.110: INFO: Namespace "e2e-ns-w9gjs-9283" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:46:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4831" for this suite. 04/21/23 20:46:49.112
STEP: Destroying namespace "e2e-ns-w9gjs-9283" for this suite. 04/21/23 20:46:49.115
------------------------------
â€¢ [0.039 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:46:49.079
    Apr 21 20:46:49.079: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename namespaces 04/21/23 20:46:49.08
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:46:49.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:46:49.094
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-w9gjs" 04/21/23 20:46:49.096
    Apr 21 20:46:49.102: INFO: Namespace "e2e-ns-w9gjs-9283" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-w9gjs-9283" 04/21/23 20:46:49.102
    Apr 21 20:46:49.106: INFO: Namespace "e2e-ns-w9gjs-9283" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-w9gjs-9283" 04/21/23 20:46:49.106
    Apr 21 20:46:49.110: INFO: Namespace "e2e-ns-w9gjs-9283" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:46:49.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4831" for this suite. 04/21/23 20:46:49.112
    STEP: Destroying namespace "e2e-ns-w9gjs-9283" for this suite. 04/21/23 20:46:49.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:46:49.119
Apr 21 20:46:49.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename containers 04/21/23 20:46:49.12
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:46:49.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:46:49.129
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 04/21/23 20:46:49.131
Apr 21 20:46:49.135: INFO: Waiting up to 5m0s for pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23" in namespace "containers-1493" to be "Succeeded or Failed"
Apr 21 20:46:49.137: INFO: Pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033988ms
Apr 21 20:46:51.140: INFO: Pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004779677s
Apr 21 20:46:53.141: INFO: Pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005969723s
STEP: Saw pod success 04/21/23 20:46:53.141
Apr 21 20:46:53.141: INFO: Pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23" satisfied condition "Succeeded or Failed"
Apr 21 20:46:53.143: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 20:46:53.152
Apr 21 20:46:53.159: INFO: Waiting for pod client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23 to disappear
Apr 21 20:46:53.160: INFO: Pod client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 21 20:46:53.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1493" for this suite. 04/21/23 20:46:53.162
------------------------------
â€¢ [4.047 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:46:49.119
    Apr 21 20:46:49.119: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename containers 04/21/23 20:46:49.12
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:46:49.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:46:49.129
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 04/21/23 20:46:49.131
    Apr 21 20:46:49.135: INFO: Waiting up to 5m0s for pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23" in namespace "containers-1493" to be "Succeeded or Failed"
    Apr 21 20:46:49.137: INFO: Pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033988ms
    Apr 21 20:46:51.140: INFO: Pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004779677s
    Apr 21 20:46:53.141: INFO: Pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005969723s
    STEP: Saw pod success 04/21/23 20:46:53.141
    Apr 21 20:46:53.141: INFO: Pod "client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23" satisfied condition "Succeeded or Failed"
    Apr 21 20:46:53.143: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 20:46:53.152
    Apr 21 20:46:53.159: INFO: Waiting for pod client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23 to disappear
    Apr 21 20:46:53.160: INFO: Pod client-containers-8fba3f2f-66b7-4776-a6af-f8a32952ff23 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:46:53.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1493" for this suite. 04/21/23 20:46:53.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:46:53.167
Apr 21 20:46:53.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename taint-single-pod 04/21/23 20:46:53.168
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:46:53.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:46:53.176
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Apr 21 20:46:53.178: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 21 20:47:53.190: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Apr 21 20:47:53.191: INFO: Starting informer...
STEP: Starting pod... 04/21/23 20:47:53.191
Apr 21 20:47:53.400: INFO: Pod is running on k8sconformance-m02. Tainting Node
STEP: Trying to apply a taint on the Node 04/21/23 20:47:53.4
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/21/23 20:47:53.408
STEP: Waiting short time to make sure Pod is queued for deletion 04/21/23 20:47:53.41
Apr 21 20:47:53.410: INFO: Pod wasn't evicted. Proceeding
Apr 21 20:47:53.410: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/21/23 20:47:53.422
STEP: Waiting some time to make sure that toleration time passed. 04/21/23 20:47:53.426
Apr 21 20:49:08.429: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:08.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-9746" for this suite. 04/21/23 20:49:08.432
------------------------------
â€¢ [SLOW TEST] [135.270 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:46:53.167
    Apr 21 20:46:53.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename taint-single-pod 04/21/23 20:46:53.168
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:46:53.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:46:53.176
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Apr 21 20:46:53.178: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 21 20:47:53.190: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Apr 21 20:47:53.191: INFO: Starting informer...
    STEP: Starting pod... 04/21/23 20:47:53.191
    Apr 21 20:47:53.400: INFO: Pod is running on k8sconformance-m02. Tainting Node
    STEP: Trying to apply a taint on the Node 04/21/23 20:47:53.4
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/21/23 20:47:53.408
    STEP: Waiting short time to make sure Pod is queued for deletion 04/21/23 20:47:53.41
    Apr 21 20:47:53.410: INFO: Pod wasn't evicted. Proceeding
    Apr 21 20:47:53.410: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/21/23 20:47:53.422
    STEP: Waiting some time to make sure that toleration time passed. 04/21/23 20:47:53.426
    Apr 21 20:49:08.429: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:08.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-9746" for this suite. 04/21/23 20:49:08.432
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:08.437
Apr 21 20:49:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename gc 04/21/23 20:49:08.438
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:08.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:08.447
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 04/21/23 20:49:08.45
STEP: delete the rc 04/21/23 20:49:13.463
STEP: wait for the rc to be deleted 04/21/23 20:49:13.479
Apr 21 20:49:14.565: INFO: 80 pods remaining
Apr 21 20:49:14.565: INFO: 80 pods has nil DeletionTimestamp
Apr 21 20:49:14.565: INFO: 
Apr 21 20:49:15.549: INFO: 73 pods remaining
Apr 21 20:49:15.549: INFO: 71 pods has nil DeletionTimestamp
Apr 21 20:49:15.549: INFO: 
Apr 21 20:49:16.534: INFO: 60 pods remaining
Apr 21 20:49:16.534: INFO: 60 pods has nil DeletionTimestamp
Apr 21 20:49:16.534: INFO: 
Apr 21 20:49:17.546: INFO: 40 pods remaining
Apr 21 20:49:17.546: INFO: 40 pods has nil DeletionTimestamp
Apr 21 20:49:17.546: INFO: 
Apr 21 20:49:18.556: INFO: 34 pods remaining
Apr 21 20:49:18.556: INFO: 31 pods has nil DeletionTimestamp
Apr 21 20:49:18.556: INFO: 
Apr 21 20:49:19.536: INFO: 20 pods remaining
Apr 21 20:49:19.536: INFO: 20 pods has nil DeletionTimestamp
Apr 21 20:49:19.536: INFO: 
STEP: Gathering metrics 04/21/23 20:49:20.539
Apr 21 20:49:22.281: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
Apr 21 20:49:22.333: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 52.564787ms
Apr 21 20:49:22.333: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
Apr 21 20:49:22.333: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
Apr 21 20:49:24.365: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:24.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5200" for this suite. 04/21/23 20:49:24.369
------------------------------
â€¢ [SLOW TEST] [15.998 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:08.437
    Apr 21 20:49:08.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename gc 04/21/23 20:49:08.438
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:08.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:08.447
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 04/21/23 20:49:08.45
    STEP: delete the rc 04/21/23 20:49:13.463
    STEP: wait for the rc to be deleted 04/21/23 20:49:13.479
    Apr 21 20:49:14.565: INFO: 80 pods remaining
    Apr 21 20:49:14.565: INFO: 80 pods has nil DeletionTimestamp
    Apr 21 20:49:14.565: INFO: 
    Apr 21 20:49:15.549: INFO: 73 pods remaining
    Apr 21 20:49:15.549: INFO: 71 pods has nil DeletionTimestamp
    Apr 21 20:49:15.549: INFO: 
    Apr 21 20:49:16.534: INFO: 60 pods remaining
    Apr 21 20:49:16.534: INFO: 60 pods has nil DeletionTimestamp
    Apr 21 20:49:16.534: INFO: 
    Apr 21 20:49:17.546: INFO: 40 pods remaining
    Apr 21 20:49:17.546: INFO: 40 pods has nil DeletionTimestamp
    Apr 21 20:49:17.546: INFO: 
    Apr 21 20:49:18.556: INFO: 34 pods remaining
    Apr 21 20:49:18.556: INFO: 31 pods has nil DeletionTimestamp
    Apr 21 20:49:18.556: INFO: 
    Apr 21 20:49:19.536: INFO: 20 pods remaining
    Apr 21 20:49:19.536: INFO: 20 pods has nil DeletionTimestamp
    Apr 21 20:49:19.536: INFO: 
    STEP: Gathering metrics 04/21/23 20:49:20.539
    Apr 21 20:49:22.281: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
    Apr 21 20:49:22.333: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 52.564787ms
    Apr 21 20:49:22.333: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
    Apr 21 20:49:22.333: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
    Apr 21 20:49:24.365: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:24.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5200" for this suite. 04/21/23 20:49:24.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:24.437
Apr 21 20:49:24.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:49:24.438
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:24.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:24.453
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Apr 21 20:49:24.462: INFO: Waiting up to 5m0s for pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c" in namespace "kubelet-test-4173" to be "running and ready"
Apr 21 20:49:24.465: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.50461ms
Apr 21 20:49:24.465: INFO: The phase of Pod busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:49:26.468: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005411618s
Apr 21 20:49:26.468: INFO: The phase of Pod busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:49:28.467: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004874759s
Apr 21 20:49:28.467: INFO: The phase of Pod busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:49:30.468: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c": Phase="Running", Reason="", readiness=true. Elapsed: 6.005213863s
Apr 21 20:49:30.468: INFO: The phase of Pod busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c is Running (Ready = true)
Apr 21 20:49:30.468: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:30.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4173" for this suite. 04/21/23 20:49:30.87
------------------------------
â€¢ [SLOW TEST] [6.440 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:24.437
    Apr 21 20:49:24.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:49:24.438
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:24.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:24.453
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Apr 21 20:49:24.462: INFO: Waiting up to 5m0s for pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c" in namespace "kubelet-test-4173" to be "running and ready"
    Apr 21 20:49:24.465: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.50461ms
    Apr 21 20:49:24.465: INFO: The phase of Pod busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:49:26.468: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005411618s
    Apr 21 20:49:26.468: INFO: The phase of Pod busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:49:28.467: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004874759s
    Apr 21 20:49:28.467: INFO: The phase of Pod busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:49:30.468: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c": Phase="Running", Reason="", readiness=true. Elapsed: 6.005213863s
    Apr 21 20:49:30.468: INFO: The phase of Pod busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c is Running (Ready = true)
    Apr 21 20:49:30.468: INFO: Pod "busybox-scheduling-114f1cee-8649-486a-bad6-78603e18e00c" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:30.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4173" for this suite. 04/21/23 20:49:30.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:30.877
Apr 21 20:49:30.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename cronjob 04/21/23 20:49:30.878
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:30.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:30.887
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 04/21/23 20:49:30.888
STEP: creating 04/21/23 20:49:30.889
STEP: getting 04/21/23 20:49:30.891
STEP: listing 04/21/23 20:49:30.893
STEP: watching 04/21/23 20:49:30.894
Apr 21 20:49:30.894: INFO: starting watch
STEP: cluster-wide listing 04/21/23 20:49:30.895
STEP: cluster-wide watching 04/21/23 20:49:30.896
Apr 21 20:49:30.896: INFO: starting watch
STEP: patching 04/21/23 20:49:30.897
STEP: updating 04/21/23 20:49:30.902
Apr 21 20:49:30.906: INFO: waiting for watch events with expected annotations
Apr 21 20:49:30.906: INFO: saw patched and updated annotations
STEP: patching /status 04/21/23 20:49:30.906
STEP: updating /status 04/21/23 20:49:30.91
STEP: get /status 04/21/23 20:49:30.913
STEP: deleting 04/21/23 20:49:30.915
STEP: deleting a collection 04/21/23 20:49:30.922
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:30.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4066" for this suite. 04/21/23 20:49:30.928
------------------------------
â€¢ [0.054 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:30.877
    Apr 21 20:49:30.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename cronjob 04/21/23 20:49:30.878
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:30.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:30.887
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 04/21/23 20:49:30.888
    STEP: creating 04/21/23 20:49:30.889
    STEP: getting 04/21/23 20:49:30.891
    STEP: listing 04/21/23 20:49:30.893
    STEP: watching 04/21/23 20:49:30.894
    Apr 21 20:49:30.894: INFO: starting watch
    STEP: cluster-wide listing 04/21/23 20:49:30.895
    STEP: cluster-wide watching 04/21/23 20:49:30.896
    Apr 21 20:49:30.896: INFO: starting watch
    STEP: patching 04/21/23 20:49:30.897
    STEP: updating 04/21/23 20:49:30.902
    Apr 21 20:49:30.906: INFO: waiting for watch events with expected annotations
    Apr 21 20:49:30.906: INFO: saw patched and updated annotations
    STEP: patching /status 04/21/23 20:49:30.906
    STEP: updating /status 04/21/23 20:49:30.91
    STEP: get /status 04/21/23 20:49:30.913
    STEP: deleting 04/21/23 20:49:30.915
    STEP: deleting a collection 04/21/23 20:49:30.922
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:30.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4066" for this suite. 04/21/23 20:49:30.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:30.932
Apr 21 20:49:30.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename ingress 04/21/23 20:49:30.933
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:30.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:30.94
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 04/21/23 20:49:30.942
STEP: getting /apis/networking.k8s.io 04/21/23 20:49:30.944
STEP: getting /apis/networking.k8s.iov1 04/21/23 20:49:30.944
STEP: creating 04/21/23 20:49:30.945
STEP: getting 04/21/23 20:49:30.953
STEP: listing 04/21/23 20:49:30.954
STEP: watching 04/21/23 20:49:30.955
Apr 21 20:49:30.955: INFO: starting watch
STEP: cluster-wide listing 04/21/23 20:49:30.956
STEP: cluster-wide watching 04/21/23 20:49:30.957
Apr 21 20:49:30.957: INFO: starting watch
STEP: patching 04/21/23 20:49:30.958
STEP: updating 04/21/23 20:49:30.962
Apr 21 20:49:30.965: INFO: waiting for watch events with expected annotations
Apr 21 20:49:30.966: INFO: saw patched and updated annotations
STEP: patching /status 04/21/23 20:49:30.966
STEP: updating /status 04/21/23 20:49:30.969
STEP: get /status 04/21/23 20:49:30.972
STEP: deleting 04/21/23 20:49:30.974
STEP: deleting a collection 04/21/23 20:49:30.979
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:30.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-7534" for this suite. 04/21/23 20:49:30.989
------------------------------
â€¢ [0.059 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:30.932
    Apr 21 20:49:30.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename ingress 04/21/23 20:49:30.933
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:30.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:30.94
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 04/21/23 20:49:30.942
    STEP: getting /apis/networking.k8s.io 04/21/23 20:49:30.944
    STEP: getting /apis/networking.k8s.iov1 04/21/23 20:49:30.944
    STEP: creating 04/21/23 20:49:30.945
    STEP: getting 04/21/23 20:49:30.953
    STEP: listing 04/21/23 20:49:30.954
    STEP: watching 04/21/23 20:49:30.955
    Apr 21 20:49:30.955: INFO: starting watch
    STEP: cluster-wide listing 04/21/23 20:49:30.956
    STEP: cluster-wide watching 04/21/23 20:49:30.957
    Apr 21 20:49:30.957: INFO: starting watch
    STEP: patching 04/21/23 20:49:30.958
    STEP: updating 04/21/23 20:49:30.962
    Apr 21 20:49:30.965: INFO: waiting for watch events with expected annotations
    Apr 21 20:49:30.966: INFO: saw patched and updated annotations
    STEP: patching /status 04/21/23 20:49:30.966
    STEP: updating /status 04/21/23 20:49:30.969
    STEP: get /status 04/21/23 20:49:30.972
    STEP: deleting 04/21/23 20:49:30.974
    STEP: deleting a collection 04/21/23 20:49:30.979
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:30.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-7534" for this suite. 04/21/23 20:49:30.989
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:30.992
Apr 21 20:49:30.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:49:30.993
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:30.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:31.001
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:31.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-3737" for this suite. 04/21/23 20:49:31.016
------------------------------
â€¢ [0.027 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:30.992
    Apr 21 20:49:30.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubelet-test 04/21/23 20:49:30.993
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:30.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:31.001
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:31.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-3737" for this suite. 04/21/23 20:49:31.016
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:31.019
Apr 21 20:49:31.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 20:49:31.02
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:31.026
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:31.029
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 04/21/23 20:49:31.032
STEP: setting up watch 04/21/23 20:49:31.032
STEP: submitting the pod to kubernetes 04/21/23 20:49:31.135
STEP: verifying the pod is in kubernetes 04/21/23 20:49:31.143
STEP: verifying pod creation was observed 04/21/23 20:49:31.145
Apr 21 20:49:31.145: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a" in namespace "pods-3660" to be "running"
Apr 21 20:49:31.147: INFO: Pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926592ms
Apr 21 20:49:33.149: INFO: Pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004218633s
Apr 21 20:49:35.149: INFO: Pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a": Phase="Running", Reason="", readiness=true. Elapsed: 4.004296628s
Apr 21 20:49:35.149: INFO: Pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a" satisfied condition "running"
STEP: deleting the pod gracefully 04/21/23 20:49:35.151
STEP: verifying pod deletion was observed 04/21/23 20:49:35.156
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:36.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3660" for this suite. 04/21/23 20:49:36.4
------------------------------
â€¢ [SLOW TEST] [5.384 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:31.019
    Apr 21 20:49:31.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 20:49:31.02
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:31.026
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:31.029
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 04/21/23 20:49:31.032
    STEP: setting up watch 04/21/23 20:49:31.032
    STEP: submitting the pod to kubernetes 04/21/23 20:49:31.135
    STEP: verifying the pod is in kubernetes 04/21/23 20:49:31.143
    STEP: verifying pod creation was observed 04/21/23 20:49:31.145
    Apr 21 20:49:31.145: INFO: Waiting up to 5m0s for pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a" in namespace "pods-3660" to be "running"
    Apr 21 20:49:31.147: INFO: Pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.926592ms
    Apr 21 20:49:33.149: INFO: Pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004218633s
    Apr 21 20:49:35.149: INFO: Pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a": Phase="Running", Reason="", readiness=true. Elapsed: 4.004296628s
    Apr 21 20:49:35.149: INFO: Pod "pod-submit-remove-b2577fd6-ac60-4cda-897a-e94270d63f2a" satisfied condition "running"
    STEP: deleting the pod gracefully 04/21/23 20:49:35.151
    STEP: verifying pod deletion was observed 04/21/23 20:49:35.156
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:36.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3660" for this suite. 04/21/23 20:49:36.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:36.404
Apr 21 20:49:36.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 20:49:36.405
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:36.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:36.415
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-707 04/21/23 20:49:36.417
STEP: changing the ExternalName service to type=NodePort 04/21/23 20:49:36.42
STEP: creating replication controller externalname-service in namespace services-707 04/21/23 20:49:36.435
I0421 20:49:36.439373      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-707, replica count: 2
I0421 20:49:39.490086      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 21 20:49:39.490: INFO: Creating new exec pod
Apr 21 20:49:39.493: INFO: Waiting up to 5m0s for pod "execpodtghmk" in namespace "services-707" to be "running"
Apr 21 20:49:39.495: INFO: Pod "execpodtghmk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.658439ms
Apr 21 20:49:41.497: INFO: Pod "execpodtghmk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004294032s
Apr 21 20:49:41.497: INFO: Pod "execpodtghmk" satisfied condition "running"
Apr 21 20:49:42.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-707 exec execpodtghmk -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Apr 21 20:49:42.631: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 21 20:49:42.631: INFO: stdout: ""
Apr 21 20:49:42.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-707 exec execpodtghmk -- /bin/sh -x -c nc -v -z -w 2 10.105.195.24 80'
Apr 21 20:49:42.747: INFO: stderr: "+ nc -v -z -w 2 10.105.195.24 80\nConnection to 10.105.195.24 80 port [tcp/http] succeeded!\n"
Apr 21 20:49:42.747: INFO: stdout: ""
Apr 21 20:49:42.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-707 exec execpodtghmk -- /bin/sh -x -c nc -v -z -w 2 192.168.49.2 32535'
Apr 21 20:49:42.868: INFO: stderr: "+ nc -v -z -w 2 192.168.49.2 32535\nConnection to 192.168.49.2 32535 port [tcp/*] succeeded!\n"
Apr 21 20:49:42.868: INFO: stdout: ""
Apr 21 20:49:42.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-707 exec execpodtghmk -- /bin/sh -x -c nc -v -z -w 2 192.168.49.3 32535'
Apr 21 20:49:42.974: INFO: stderr: "+ nc -v -z -w 2 192.168.49.3 32535\nConnection to 192.168.49.3 32535 port [tcp/*] succeeded!\n"
Apr 21 20:49:42.974: INFO: stdout: ""
Apr 21 20:49:42.974: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:42.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-707" for this suite. 04/21/23 20:49:42.991
------------------------------
â€¢ [SLOW TEST] [6.593 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:36.404
    Apr 21 20:49:36.404: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 20:49:36.405
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:36.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:36.415
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-707 04/21/23 20:49:36.417
    STEP: changing the ExternalName service to type=NodePort 04/21/23 20:49:36.42
    STEP: creating replication controller externalname-service in namespace services-707 04/21/23 20:49:36.435
    I0421 20:49:36.439373      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-707, replica count: 2
    I0421 20:49:39.490086      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 21 20:49:39.490: INFO: Creating new exec pod
    Apr 21 20:49:39.493: INFO: Waiting up to 5m0s for pod "execpodtghmk" in namespace "services-707" to be "running"
    Apr 21 20:49:39.495: INFO: Pod "execpodtghmk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.658439ms
    Apr 21 20:49:41.497: INFO: Pod "execpodtghmk": Phase="Running", Reason="", readiness=true. Elapsed: 2.004294032s
    Apr 21 20:49:41.497: INFO: Pod "execpodtghmk" satisfied condition "running"
    Apr 21 20:49:42.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-707 exec execpodtghmk -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Apr 21 20:49:42.631: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Apr 21 20:49:42.631: INFO: stdout: ""
    Apr 21 20:49:42.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-707 exec execpodtghmk -- /bin/sh -x -c nc -v -z -w 2 10.105.195.24 80'
    Apr 21 20:49:42.747: INFO: stderr: "+ nc -v -z -w 2 10.105.195.24 80\nConnection to 10.105.195.24 80 port [tcp/http] succeeded!\n"
    Apr 21 20:49:42.747: INFO: stdout: ""
    Apr 21 20:49:42.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-707 exec execpodtghmk -- /bin/sh -x -c nc -v -z -w 2 192.168.49.2 32535'
    Apr 21 20:49:42.868: INFO: stderr: "+ nc -v -z -w 2 192.168.49.2 32535\nConnection to 192.168.49.2 32535 port [tcp/*] succeeded!\n"
    Apr 21 20:49:42.868: INFO: stdout: ""
    Apr 21 20:49:42.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-707 exec execpodtghmk -- /bin/sh -x -c nc -v -z -w 2 192.168.49.3 32535'
    Apr 21 20:49:42.974: INFO: stderr: "+ nc -v -z -w 2 192.168.49.3 32535\nConnection to 192.168.49.3 32535 port [tcp/*] succeeded!\n"
    Apr 21 20:49:42.974: INFO: stdout: ""
    Apr 21 20:49:42.974: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:42.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-707" for this suite. 04/21/23 20:49:42.991
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:42.997
Apr 21 20:49:42.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replicaset 04/21/23 20:49:42.998
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:43.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:43.008
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 04/21/23 20:49:43.01
STEP: Verify that the required pods have come up 04/21/23 20:49:43.013
Apr 21 20:49:43.015: INFO: Pod name sample-pod: Found 0 pods out of 3
Apr 21 20:49:48.017: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 04/21/23 20:49:48.017
Apr 21 20:49:48.019: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 04/21/23 20:49:48.019
STEP: DeleteCollection of the ReplicaSets 04/21/23 20:49:48.021
STEP: After DeleteCollection verify that ReplicaSets have been deleted 04/21/23 20:49:48.027
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:48.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-548" for this suite. 04/21/23 20:49:48.033
------------------------------
â€¢ [SLOW TEST] [5.045 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:42.997
    Apr 21 20:49:42.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replicaset 04/21/23 20:49:42.998
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:43.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:43.008
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 04/21/23 20:49:43.01
    STEP: Verify that the required pods have come up 04/21/23 20:49:43.013
    Apr 21 20:49:43.015: INFO: Pod name sample-pod: Found 0 pods out of 3
    Apr 21 20:49:48.017: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 04/21/23 20:49:48.017
    Apr 21 20:49:48.019: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 04/21/23 20:49:48.019
    STEP: DeleteCollection of the ReplicaSets 04/21/23 20:49:48.021
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 04/21/23 20:49:48.027
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:48.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-548" for this suite. 04/21/23 20:49:48.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:48.044
Apr 21 20:49:48.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 20:49:48.045
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:48.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:48.059
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 04/21/23 20:49:48.061
STEP: submitting the pod to kubernetes 04/21/23 20:49:48.062
Apr 21 20:49:48.066: INFO: Waiting up to 5m0s for pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e" in namespace "pods-627" to be "running and ready"
Apr 21 20:49:48.068: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.390935ms
Apr 21 20:49:48.068: INFO: The phase of Pod pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:49:50.071: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004640239s
Apr 21 20:49:50.071: INFO: The phase of Pod pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e is Running (Ready = true)
Apr 21 20:49:50.071: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 04/21/23 20:49:50.073
STEP: updating the pod 04/21/23 20:49:50.075
Apr 21 20:49:50.583: INFO: Successfully updated pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e"
Apr 21 20:49:50.583: INFO: Waiting up to 5m0s for pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e" in namespace "pods-627" to be "running"
Apr 21 20:49:50.585: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e": Phase="Running", Reason="", readiness=true. Elapsed: 1.604745ms
Apr 21 20:49:50.585: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 04/21/23 20:49:50.585
Apr 21 20:49:50.586: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:50.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-627" for this suite. 04/21/23 20:49:50.588
------------------------------
â€¢ [2.548 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:48.044
    Apr 21 20:49:48.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 20:49:48.045
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:48.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:48.059
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 04/21/23 20:49:48.061
    STEP: submitting the pod to kubernetes 04/21/23 20:49:48.062
    Apr 21 20:49:48.066: INFO: Waiting up to 5m0s for pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e" in namespace "pods-627" to be "running and ready"
    Apr 21 20:49:48.068: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.390935ms
    Apr 21 20:49:48.068: INFO: The phase of Pod pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:49:50.071: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004640239s
    Apr 21 20:49:50.071: INFO: The phase of Pod pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e is Running (Ready = true)
    Apr 21 20:49:50.071: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 04/21/23 20:49:50.073
    STEP: updating the pod 04/21/23 20:49:50.075
    Apr 21 20:49:50.583: INFO: Successfully updated pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e"
    Apr 21 20:49:50.583: INFO: Waiting up to 5m0s for pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e" in namespace "pods-627" to be "running"
    Apr 21 20:49:50.585: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e": Phase="Running", Reason="", readiness=true. Elapsed: 1.604745ms
    Apr 21 20:49:50.585: INFO: Pod "pod-update-0924964b-d451-4fb8-b628-fea1d10bf52e" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 04/21/23 20:49:50.585
    Apr 21 20:49:50.586: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:50.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-627" for this suite. 04/21/23 20:49:50.588
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:50.592
Apr 21 20:49:50.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename server-version 04/21/23 20:49:50.592
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:50.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:50.602
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 04/21/23 20:49:50.603
STEP: Confirm major version 04/21/23 20:49:50.604
Apr 21 20:49:50.604: INFO: Major version: 1
STEP: Confirm minor version 04/21/23 20:49:50.604
Apr 21 20:49:50.604: INFO: cleanMinorVersion: 26
Apr 21 20:49:50.604: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:50.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-3550" for this suite. 04/21/23 20:49:50.606
------------------------------
â€¢ [0.017 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:50.592
    Apr 21 20:49:50.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename server-version 04/21/23 20:49:50.592
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:50.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:50.602
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 04/21/23 20:49:50.603
    STEP: Confirm major version 04/21/23 20:49:50.604
    Apr 21 20:49:50.604: INFO: Major version: 1
    STEP: Confirm minor version 04/21/23 20:49:50.604
    Apr 21 20:49:50.604: INFO: cleanMinorVersion: 26
    Apr 21 20:49:50.604: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:50.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-3550" for this suite. 04/21/23 20:49:50.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:50.61
Apr 21 20:49:50.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:49:50.61
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:50.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:50.619
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:49:50.627
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:49:50.893
STEP: Deploying the webhook pod 04/21/23 20:49:50.898
STEP: Wait for the deployment to be ready 04/21/23 20:49:50.906
Apr 21 20:49:50.911: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 20:49:52.918
STEP: Verifying the service has paired with the endpoint 04/21/23 20:49:52.929
Apr 21 20:49:53.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 04/21/23 20:49:53.932
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 04/21/23 20:49:53.933
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 04/21/23 20:49:53.933
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 04/21/23 20:49:53.933
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 04/21/23 20:49:53.934
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 04/21/23 20:49:53.934
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 04/21/23 20:49:53.935
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:53.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1402" for this suite. 04/21/23 20:49:53.961
STEP: Destroying namespace "webhook-1402-markers" for this suite. 04/21/23 20:49:53.965
------------------------------
â€¢ [3.359 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:50.61
    Apr 21 20:49:50.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:49:50.61
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:50.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:50.619
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:49:50.627
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:49:50.893
    STEP: Deploying the webhook pod 04/21/23 20:49:50.898
    STEP: Wait for the deployment to be ready 04/21/23 20:49:50.906
    Apr 21 20:49:50.911: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 20:49:52.918
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:49:52.929
    Apr 21 20:49:53.930: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 04/21/23 20:49:53.932
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 04/21/23 20:49:53.933
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 04/21/23 20:49:53.933
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 04/21/23 20:49:53.933
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 04/21/23 20:49:53.934
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 04/21/23 20:49:53.934
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 04/21/23 20:49:53.935
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:53.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1402" for this suite. 04/21/23 20:49:53.961
    STEP: Destroying namespace "webhook-1402-markers" for this suite. 04/21/23 20:49:53.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:53.97
Apr 21 20:49:53.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename security-context-test 04/21/23 20:49:53.971
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:53.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:53.981
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Apr 21 20:49:53.987: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f" in namespace "security-context-test-219" to be "Succeeded or Failed"
Apr 21 20:49:53.989: INFO: Pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.528454ms
Apr 21 20:49:55.991: INFO: Pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003569113s
Apr 21 20:49:57.991: INFO: Pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004033624s
Apr 21 20:49:57.991: INFO: Pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 21 20:49:57.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-219" for this suite. 04/21/23 20:49:57.994
------------------------------
â€¢ [4.029 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:53.97
    Apr 21 20:49:53.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename security-context-test 04/21/23 20:49:53.971
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:53.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:53.981
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Apr 21 20:49:53.987: INFO: Waiting up to 5m0s for pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f" in namespace "security-context-test-219" to be "Succeeded or Failed"
    Apr 21 20:49:53.989: INFO: Pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.528454ms
    Apr 21 20:49:55.991: INFO: Pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003569113s
    Apr 21 20:49:57.991: INFO: Pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004033624s
    Apr 21 20:49:57.991: INFO: Pod "busybox-user-65534-ef4e49c6-f099-4b96-a1dd-43141ebacc3f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:49:57.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-219" for this suite. 04/21/23 20:49:57.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:49:58
Apr 21 20:49:58.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 20:49:58.001
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:58.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:58.009
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Apr 21 20:49:58.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:04.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7937" for this suite. 04/21/23 20:50:04.175
------------------------------
â€¢ [SLOW TEST] [6.178 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:49:58
    Apr 21 20:49:58.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 20:49:58.001
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:49:58.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:49:58.009
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Apr 21 20:49:58.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:04.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7937" for this suite. 04/21/23 20:50:04.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:04.179
Apr 21 20:50:04.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 20:50:04.179
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:04.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:04.189
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 04/21/23 20:50:04.191
Apr 21 20:50:04.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-7323 cluster-info'
Apr 21 20:50:04.245: INFO: stderr: ""
Apr 21 20:50:04.245: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:04.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7323" for this suite. 04/21/23 20:50:04.249
------------------------------
â€¢ [0.074 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:04.179
    Apr 21 20:50:04.179: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 20:50:04.179
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:04.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:04.189
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 04/21/23 20:50:04.191
    Apr 21 20:50:04.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-7323 cluster-info'
    Apr 21 20:50:04.245: INFO: stderr: ""
    Apr 21 20:50:04.245: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:04.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7323" for this suite. 04/21/23 20:50:04.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:04.255
Apr 21 20:50:04.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:50:04.256
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:04.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:04.266
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-3c1fc45d-3d68-45fa-9a3b-637389fe3045 04/21/23 20:50:04.268
STEP: Creating a pod to test consume secrets 04/21/23 20:50:04.27
Apr 21 20:50:04.276: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f" in namespace "projected-9283" to be "Succeeded or Failed"
Apr 21 20:50:04.277: INFO: Pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666861ms
Apr 21 20:50:06.281: INFO: Pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004915316s
Apr 21 20:50:08.280: INFO: Pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004096645s
STEP: Saw pod success 04/21/23 20:50:08.28
Apr 21 20:50:08.280: INFO: Pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f" satisfied condition "Succeeded or Failed"
Apr 21 20:50:08.282: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f container secret-volume-test: <nil>
STEP: delete the pod 04/21/23 20:50:08.287
Apr 21 20:50:08.293: INFO: Waiting for pod pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f to disappear
Apr 21 20:50:08.295: INFO: Pod pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:08.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9283" for this suite. 04/21/23 20:50:08.297
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:04.255
    Apr 21 20:50:04.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:50:04.256
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:04.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:04.266
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-3c1fc45d-3d68-45fa-9a3b-637389fe3045 04/21/23 20:50:04.268
    STEP: Creating a pod to test consume secrets 04/21/23 20:50:04.27
    Apr 21 20:50:04.276: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f" in namespace "projected-9283" to be "Succeeded or Failed"
    Apr 21 20:50:04.277: INFO: Pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666861ms
    Apr 21 20:50:06.281: INFO: Pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004915316s
    Apr 21 20:50:08.280: INFO: Pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004096645s
    STEP: Saw pod success 04/21/23 20:50:08.28
    Apr 21 20:50:08.280: INFO: Pod "pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f" satisfied condition "Succeeded or Failed"
    Apr 21 20:50:08.282: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f container secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 20:50:08.287
    Apr 21 20:50:08.293: INFO: Waiting for pod pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f to disappear
    Apr 21 20:50:08.295: INFO: Pod pod-projected-secrets-76e387f9-da73-4216-8381-dee3095ca33f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:08.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9283" for this suite. 04/21/23 20:50:08.297
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:08.3
Apr 21 20:50:08.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pod-network-test 04/21/23 20:50:08.301
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:08.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:08.31
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4521 04/21/23 20:50:08.312
STEP: creating a selector 04/21/23 20:50:08.312
STEP: Creating the service pods in kubernetes 04/21/23 20:50:08.312
Apr 21 20:50:08.312: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 21 20:50:08.326: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4521" to be "running and ready"
Apr 21 20:50:08.328: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.972031ms
Apr 21 20:50:08.328: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:50:10.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00491665s
Apr 21 20:50:10.331: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:50:12.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005292331s
Apr 21 20:50:12.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:50:14.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004331398s
Apr 21 20:50:14.331: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:50:16.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005159997s
Apr 21 20:50:16.331: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:50:18.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004336399s
Apr 21 20:50:18.331: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 20:50:20.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004250424s
Apr 21 20:50:20.330: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 21 20:50:20.330: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 21 20:50:20.332: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4521" to be "running and ready"
Apr 21 20:50:20.334: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.517237ms
Apr 21 20:50:20.334: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 21 20:50:20.334: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 04/21/23 20:50:20.335
Apr 21 20:50:20.343: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4521" to be "running"
Apr 21 20:50:20.344: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574923ms
Apr 21 20:50:22.347: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004581707s
Apr 21 20:50:22.347: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 21 20:50:22.349: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4521" to be "running"
Apr 21 20:50:22.350: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.457397ms
Apr 21 20:50:22.350: INFO: Pod "host-test-container-pod" satisfied condition "running"
Apr 21 20:50:22.352: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 21 20:50:22.352: INFO: Going to poll 10.244.0.125 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Apr 21 20:50:22.353: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.125 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4521 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:50:22.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:50:22.353: INFO: ExecWithOptions: Clientset creation
Apr 21 20:50:22.353: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4521/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.125+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 21 20:50:23.409: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 21 20:50:23.409: INFO: Going to poll 10.244.1.248 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Apr 21 20:50:23.411: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.248 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4521 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:50:23.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:50:23.411: INFO: ExecWithOptions: Clientset creation
Apr 21 20:50:23.411: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4521/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.248+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 21 20:50:24.477: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:24.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-4521" for this suite. 04/21/23 20:50:24.48
------------------------------
â€¢ [SLOW TEST] [16.184 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:08.3
    Apr 21 20:50:08.300: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pod-network-test 04/21/23 20:50:08.301
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:08.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:08.31
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4521 04/21/23 20:50:08.312
    STEP: creating a selector 04/21/23 20:50:08.312
    STEP: Creating the service pods in kubernetes 04/21/23 20:50:08.312
    Apr 21 20:50:08.312: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 21 20:50:08.326: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4521" to be "running and ready"
    Apr 21 20:50:08.328: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.972031ms
    Apr 21 20:50:08.328: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:50:10.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00491665s
    Apr 21 20:50:10.331: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:50:12.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005292331s
    Apr 21 20:50:12.332: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:50:14.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004331398s
    Apr 21 20:50:14.331: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:50:16.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.005159997s
    Apr 21 20:50:16.331: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:50:18.331: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004336399s
    Apr 21 20:50:18.331: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 20:50:20.330: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.004250424s
    Apr 21 20:50:20.330: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 21 20:50:20.330: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 21 20:50:20.332: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4521" to be "running and ready"
    Apr 21 20:50:20.334: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.517237ms
    Apr 21 20:50:20.334: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 21 20:50:20.334: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 04/21/23 20:50:20.335
    Apr 21 20:50:20.343: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4521" to be "running"
    Apr 21 20:50:20.344: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.574923ms
    Apr 21 20:50:22.347: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004581707s
    Apr 21 20:50:22.347: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 21 20:50:22.349: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4521" to be "running"
    Apr 21 20:50:22.350: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.457397ms
    Apr 21 20:50:22.350: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Apr 21 20:50:22.352: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Apr 21 20:50:22.352: INFO: Going to poll 10.244.0.125 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Apr 21 20:50:22.353: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.125 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4521 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:50:22.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:50:22.353: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:50:22.353: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4521/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.125+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 21 20:50:23.409: INFO: Found all 1 expected endpoints: [netserver-0]
    Apr 21 20:50:23.409: INFO: Going to poll 10.244.1.248 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Apr 21 20:50:23.411: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.248 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4521 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:50:23.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:50:23.411: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:50:23.411: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4521/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.248+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 21 20:50:24.477: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:24.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-4521" for this suite. 04/21/23 20:50:24.48
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:24.484
Apr 21 20:50:24.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 20:50:24.485
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:24.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:24.495
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 04/21/23 20:50:24.497
STEP: Creating a ResourceQuota 04/21/23 20:50:29.499
STEP: Ensuring resource quota status is calculated 04/21/23 20:50:29.502
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:31.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7217" for this suite. 04/21/23 20:50:31.508
------------------------------
â€¢ [SLOW TEST] [7.027 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:24.484
    Apr 21 20:50:24.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 20:50:24.485
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:24.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:24.495
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 04/21/23 20:50:24.497
    STEP: Creating a ResourceQuota 04/21/23 20:50:29.499
    STEP: Ensuring resource quota status is calculated 04/21/23 20:50:29.502
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:31.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7217" for this suite. 04/21/23 20:50:31.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:31.512
Apr 21 20:50:31.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename var-expansion 04/21/23 20:50:31.512
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:31.521
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:31.523
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 04/21/23 20:50:31.525
Apr 21 20:50:31.529: INFO: Waiting up to 5m0s for pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94" in namespace "var-expansion-8064" to be "Succeeded or Failed"
Apr 21 20:50:31.530: INFO: Pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578704ms
Apr 21 20:50:33.534: INFO: Pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005200306s
Apr 21 20:50:35.533: INFO: Pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004189864s
STEP: Saw pod success 04/21/23 20:50:35.533
Apr 21 20:50:35.533: INFO: Pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94" satisfied condition "Succeeded or Failed"
Apr 21 20:50:35.535: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94 container dapi-container: <nil>
STEP: delete the pod 04/21/23 20:50:35.539
Apr 21 20:50:35.546: INFO: Waiting for pod var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94 to disappear
Apr 21 20:50:35.547: INFO: Pod var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:35.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8064" for this suite. 04/21/23 20:50:35.549
------------------------------
â€¢ [4.041 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:31.512
    Apr 21 20:50:31.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename var-expansion 04/21/23 20:50:31.512
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:31.521
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:31.523
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 04/21/23 20:50:31.525
    Apr 21 20:50:31.529: INFO: Waiting up to 5m0s for pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94" in namespace "var-expansion-8064" to be "Succeeded or Failed"
    Apr 21 20:50:31.530: INFO: Pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94": Phase="Pending", Reason="", readiness=false. Elapsed: 1.578704ms
    Apr 21 20:50:33.534: INFO: Pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005200306s
    Apr 21 20:50:35.533: INFO: Pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004189864s
    STEP: Saw pod success 04/21/23 20:50:35.533
    Apr 21 20:50:35.533: INFO: Pod "var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94" satisfied condition "Succeeded or Failed"
    Apr 21 20:50:35.535: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94 container dapi-container: <nil>
    STEP: delete the pod 04/21/23 20:50:35.539
    Apr 21 20:50:35.546: INFO: Waiting for pod var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94 to disappear
    Apr 21 20:50:35.547: INFO: Pod var-expansion-03f3d5d2-9922-49c7-a8e4-2da768aecf94 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:35.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8064" for this suite. 04/21/23 20:50:35.549
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:35.553
Apr 21 20:50:35.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 20:50:35.554
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:35.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:35.562
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Apr 21 20:50:35.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:36.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5401" for this suite. 04/21/23 20:50:36.092
------------------------------
â€¢ [0.542 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:35.553
    Apr 21 20:50:35.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 20:50:35.554
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:35.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:35.562
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Apr 21 20:50:35.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:36.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5401" for this suite. 04/21/23 20:50:36.092
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:36.097
Apr 21 20:50:36.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 20:50:36.097
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:36.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:36.106
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Apr 21 20:50:36.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/21/23 20:50:37.429
Apr 21 20:50:37.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 --namespace=crd-publish-openapi-641 create -f -'
Apr 21 20:50:39.925: INFO: stderr: ""
Apr 21 20:50:39.925: INFO: stdout: "e2e-test-crd-publish-openapi-9201-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 21 20:50:39.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 --namespace=crd-publish-openapi-641 delete e2e-test-crd-publish-openapi-9201-crds test-cr'
Apr 21 20:50:39.982: INFO: stderr: ""
Apr 21 20:50:39.982: INFO: stdout: "e2e-test-crd-publish-openapi-9201-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 21 20:50:39.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 --namespace=crd-publish-openapi-641 apply -f -'
Apr 21 20:50:40.130: INFO: stderr: ""
Apr 21 20:50:40.130: INFO: stdout: "e2e-test-crd-publish-openapi-9201-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 21 20:50:40.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 --namespace=crd-publish-openapi-641 delete e2e-test-crd-publish-openapi-9201-crds test-cr'
Apr 21 20:50:40.187: INFO: stderr: ""
Apr 21 20:50:40.187: INFO: stdout: "e2e-test-crd-publish-openapi-9201-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 04/21/23 20:50:40.187
Apr 21 20:50:40.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 explain e2e-test-crd-publish-openapi-9201-crds'
Apr 21 20:50:40.325: INFO: stderr: ""
Apr 21 20:50:40.325: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9201-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:41.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-641" for this suite. 04/21/23 20:50:41.643
------------------------------
â€¢ [SLOW TEST] [5.549 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:36.097
    Apr 21 20:50:36.097: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 20:50:36.097
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:36.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:36.106
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Apr 21 20:50:36.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/21/23 20:50:37.429
    Apr 21 20:50:37.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 --namespace=crd-publish-openapi-641 create -f -'
    Apr 21 20:50:39.925: INFO: stderr: ""
    Apr 21 20:50:39.925: INFO: stdout: "e2e-test-crd-publish-openapi-9201-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Apr 21 20:50:39.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 --namespace=crd-publish-openapi-641 delete e2e-test-crd-publish-openapi-9201-crds test-cr'
    Apr 21 20:50:39.982: INFO: stderr: ""
    Apr 21 20:50:39.982: INFO: stdout: "e2e-test-crd-publish-openapi-9201-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Apr 21 20:50:39.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 --namespace=crd-publish-openapi-641 apply -f -'
    Apr 21 20:50:40.130: INFO: stderr: ""
    Apr 21 20:50:40.130: INFO: stdout: "e2e-test-crd-publish-openapi-9201-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Apr 21 20:50:40.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 --namespace=crd-publish-openapi-641 delete e2e-test-crd-publish-openapi-9201-crds test-cr'
    Apr 21 20:50:40.187: INFO: stderr: ""
    Apr 21 20:50:40.187: INFO: stdout: "e2e-test-crd-publish-openapi-9201-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 04/21/23 20:50:40.187
    Apr 21 20:50:40.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-641 explain e2e-test-crd-publish-openapi-9201-crds'
    Apr 21 20:50:40.325: INFO: stderr: ""
    Apr 21 20:50:40.325: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9201-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:41.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-641" for this suite. 04/21/23 20:50:41.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:41.647
Apr 21 20:50:41.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 20:50:41.648
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:41.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:41.656
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-27867492-d907-4b2a-a8aa-0cd2493ed041 04/21/23 20:50:41.658
STEP: Creating a pod to test consume secrets 04/21/23 20:50:41.661
Apr 21 20:50:41.665: INFO: Waiting up to 5m0s for pod "pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b" in namespace "secrets-5744" to be "Succeeded or Failed"
Apr 21 20:50:41.666: INFO: Pod "pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.619613ms
Apr 21 20:50:43.670: INFO: Pod "pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005045389s
STEP: Saw pod success 04/21/23 20:50:43.67
Apr 21 20:50:43.670: INFO: Pod "pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b" satisfied condition "Succeeded or Failed"
Apr 21 20:50:43.672: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b container secret-volume-test: <nil>
STEP: delete the pod 04/21/23 20:50:43.677
Apr 21 20:50:43.683: INFO: Waiting for pod pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b to disappear
Apr 21 20:50:43.684: INFO: Pod pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:43.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5744" for this suite. 04/21/23 20:50:43.686
------------------------------
â€¢ [2.042 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:41.647
    Apr 21 20:50:41.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 20:50:41.648
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:41.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:41.656
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-27867492-d907-4b2a-a8aa-0cd2493ed041 04/21/23 20:50:41.658
    STEP: Creating a pod to test consume secrets 04/21/23 20:50:41.661
    Apr 21 20:50:41.665: INFO: Waiting up to 5m0s for pod "pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b" in namespace "secrets-5744" to be "Succeeded or Failed"
    Apr 21 20:50:41.666: INFO: Pod "pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.619613ms
    Apr 21 20:50:43.670: INFO: Pod "pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005045389s
    STEP: Saw pod success 04/21/23 20:50:43.67
    Apr 21 20:50:43.670: INFO: Pod "pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b" satisfied condition "Succeeded or Failed"
    Apr 21 20:50:43.672: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b container secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 20:50:43.677
    Apr 21 20:50:43.683: INFO: Waiting for pod pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b to disappear
    Apr 21 20:50:43.684: INFO: Pod pod-secrets-fd41f21d-cd46-4384-b701-fa2b8598e83b no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:43.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5744" for this suite. 04/21/23 20:50:43.686
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:43.689
Apr 21 20:50:43.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 20:50:43.69
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:43.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:43.699
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 04/21/23 20:50:43.701
STEP: listing secrets in all namespaces to ensure that there are more than zero 04/21/23 20:50:43.703
STEP: patching the secret 04/21/23 20:50:43.705
STEP: deleting the secret using a LabelSelector 04/21/23 20:50:43.711
STEP: listing secrets in all namespaces, searching for label name and value in patch 04/21/23 20:50:43.714
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:43.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4075" for this suite. 04/21/23 20:50:43.717
------------------------------
â€¢ [0.031 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:43.689
    Apr 21 20:50:43.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 20:50:43.69
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:43.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:43.699
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 04/21/23 20:50:43.701
    STEP: listing secrets in all namespaces to ensure that there are more than zero 04/21/23 20:50:43.703
    STEP: patching the secret 04/21/23 20:50:43.705
    STEP: deleting the secret using a LabelSelector 04/21/23 20:50:43.711
    STEP: listing secrets in all namespaces, searching for label name and value in patch 04/21/23 20:50:43.714
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:43.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4075" for this suite. 04/21/23 20:50:43.717
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:43.721
Apr 21 20:50:43.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 20:50:43.721
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:43.727
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:43.729
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 04/21/23 20:50:43.731
Apr 21 20:50:43.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2" in namespace "downward-api-9806" to be "Succeeded or Failed"
Apr 21 20:50:43.736: INFO: Pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461279ms
Apr 21 20:50:45.739: INFO: Pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004049526s
Apr 21 20:50:47.739: INFO: Pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004638418s
STEP: Saw pod success 04/21/23 20:50:47.739
Apr 21 20:50:47.739: INFO: Pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2" satisfied condition "Succeeded or Failed"
Apr 21 20:50:47.741: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2 container client-container: <nil>
STEP: delete the pod 04/21/23 20:50:47.746
Apr 21 20:50:47.752: INFO: Waiting for pod downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2 to disappear
Apr 21 20:50:47.754: INFO: Pod downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 20:50:47.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9806" for this suite. 04/21/23 20:50:47.756
------------------------------
â€¢ [4.038 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:43.721
    Apr 21 20:50:43.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 20:50:43.721
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:43.727
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:43.729
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 04/21/23 20:50:43.731
    Apr 21 20:50:43.735: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2" in namespace "downward-api-9806" to be "Succeeded or Failed"
    Apr 21 20:50:43.736: INFO: Pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.461279ms
    Apr 21 20:50:45.739: INFO: Pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004049526s
    Apr 21 20:50:47.739: INFO: Pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004638418s
    STEP: Saw pod success 04/21/23 20:50:47.739
    Apr 21 20:50:47.739: INFO: Pod "downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2" satisfied condition "Succeeded or Failed"
    Apr 21 20:50:47.741: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2 container client-container: <nil>
    STEP: delete the pod 04/21/23 20:50:47.746
    Apr 21 20:50:47.752: INFO: Waiting for pod downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2 to disappear
    Apr 21 20:50:47.754: INFO: Pod downwardapi-volume-0fcb7c07-0466-40cd-ac56-ad3c0c83e7c2 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:50:47.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9806" for this suite. 04/21/23 20:50:47.756
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:50:47.759
Apr 21 20:50:47.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-probe 04/21/23 20:50:47.76
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:47.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:47.77
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 21 20:51:47.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7995" for this suite. 04/21/23 20:51:47.781
------------------------------
â€¢ [SLOW TEST] [60.025 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:50:47.759
    Apr 21 20:50:47.759: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-probe 04/21/23 20:50:47.76
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:50:47.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:50:47.77
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:51:47.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7995" for this suite. 04/21/23 20:51:47.781
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:51:47.785
Apr 21 20:51:47.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename podtemplate 04/21/23 20:51:47.786
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:51:47.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:51:47.796
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 04/21/23 20:51:47.798
STEP: Replace a pod template 04/21/23 20:51:47.8
Apr 21 20:51:47.805: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Apr 21 20:51:47.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5493" for this suite. 04/21/23 20:51:47.807
------------------------------
â€¢ [0.025 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:51:47.785
    Apr 21 20:51:47.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename podtemplate 04/21/23 20:51:47.786
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:51:47.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:51:47.796
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 04/21/23 20:51:47.798
    STEP: Replace a pod template 04/21/23 20:51:47.8
    Apr 21 20:51:47.805: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:51:47.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5493" for this suite. 04/21/23 20:51:47.807
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:51:47.811
Apr 21 20:51:47.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename var-expansion 04/21/23 20:51:47.811
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:51:47.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:51:47.819
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Apr 21 20:51:47.826: INFO: Waiting up to 2m0s for pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7" in namespace "var-expansion-1680" to be "container 0 failed with reason CreateContainerConfigError"
Apr 21 20:51:47.831: INFO: Pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.526868ms
Apr 21 20:51:49.833: INFO: Pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007187527s
Apr 21 20:51:49.833: INFO: Pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Apr 21 20:51:49.833: INFO: Deleting pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7" in namespace "var-expansion-1680"
Apr 21 20:51:49.837: INFO: Wait up to 5m0s for pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 21 20:51:51.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1680" for this suite. 04/21/23 20:51:51.845
------------------------------
â€¢ [4.038 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:51:47.811
    Apr 21 20:51:47.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename var-expansion 04/21/23 20:51:47.811
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:51:47.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:51:47.819
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Apr 21 20:51:47.826: INFO: Waiting up to 2m0s for pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7" in namespace "var-expansion-1680" to be "container 0 failed with reason CreateContainerConfigError"
    Apr 21 20:51:47.831: INFO: Pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.526868ms
    Apr 21 20:51:49.833: INFO: Pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007187527s
    Apr 21 20:51:49.833: INFO: Pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Apr 21 20:51:49.833: INFO: Deleting pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7" in namespace "var-expansion-1680"
    Apr 21 20:51:49.837: INFO: Wait up to 5m0s for pod "var-expansion-684dca21-5101-4ccf-a468-184ffd6c86c7" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:51:51.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1680" for this suite. 04/21/23 20:51:51.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:51:51.849
Apr 21 20:51:51.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 20:51:51.849
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:51:51.858
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:51:51.859
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-d575a422-129d-4912-9079-c4caadb8e891 04/21/23 20:51:51.861
STEP: Creating a pod to test consume configMaps 04/21/23 20:51:51.863
Apr 21 20:51:51.869: INFO: Waiting up to 5m0s for pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef" in namespace "configmap-4214" to be "Succeeded or Failed"
Apr 21 20:51:51.870: INFO: Pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.397879ms
Apr 21 20:51:53.873: INFO: Pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004440927s
Apr 21 20:51:55.873: INFO: Pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004288627s
STEP: Saw pod success 04/21/23 20:51:55.873
Apr 21 20:51:55.873: INFO: Pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef" satisfied condition "Succeeded or Failed"
Apr 21 20:51:55.875: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef container agnhost-container: <nil>
STEP: delete the pod 04/21/23 20:51:55.88
Apr 21 20:51:55.886: INFO: Waiting for pod pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef to disappear
Apr 21 20:51:55.888: INFO: Pod pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:51:55.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4214" for this suite. 04/21/23 20:51:55.89
------------------------------
â€¢ [4.045 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:51:51.849
    Apr 21 20:51:51.849: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 20:51:51.849
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:51:51.858
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:51:51.859
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-d575a422-129d-4912-9079-c4caadb8e891 04/21/23 20:51:51.861
    STEP: Creating a pod to test consume configMaps 04/21/23 20:51:51.863
    Apr 21 20:51:51.869: INFO: Waiting up to 5m0s for pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef" in namespace "configmap-4214" to be "Succeeded or Failed"
    Apr 21 20:51:51.870: INFO: Pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef": Phase="Pending", Reason="", readiness=false. Elapsed: 1.397879ms
    Apr 21 20:51:53.873: INFO: Pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004440927s
    Apr 21 20:51:55.873: INFO: Pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004288627s
    STEP: Saw pod success 04/21/23 20:51:55.873
    Apr 21 20:51:55.873: INFO: Pod "pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef" satisfied condition "Succeeded or Failed"
    Apr 21 20:51:55.875: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 20:51:55.88
    Apr 21 20:51:55.886: INFO: Waiting for pod pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef to disappear
    Apr 21 20:51:55.888: INFO: Pod pod-configmaps-86048974-a1fa-4763-a42b-1440cd3d4aef no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:51:55.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4214" for this suite. 04/21/23 20:51:55.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:51:55.893
Apr 21 20:51:55.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 20:51:55.894
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:51:55.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:51:55.905
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/21/23 20:51:55.907
Apr 21 20:51:55.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6607 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr 21 20:51:55.965: INFO: stderr: ""
Apr 21 20:51:55.965: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 04/21/23 20:51:55.965
STEP: verifying the pod e2e-test-httpd-pod was created 04/21/23 20:52:01.017
Apr 21 20:52:01.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6607 get pod e2e-test-httpd-pod -o json'
Apr 21 20:52:01.070: INFO: stderr: ""
Apr 21 20:52:01.070: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-04-21T20:51:55Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6607\",\n        \"resourceVersion\": \"11946\",\n        \"uid\": \"63c5da17-d0b4-4a99-9b22-57bcc5d03061\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qw2jd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8sconformance-m02\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qw2jd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-21T20:51:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-21T20:51:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-21T20:51:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-21T20:51:55Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://46d972511df055c510d60ff0a24666a5603e70c62b4c33dd2bc8e231d1b8174a\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-21T20:51:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.49.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.5\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.5\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-21T20:51:55Z\"\n    }\n}\n"
STEP: replace the image in the pod 04/21/23 20:52:01.07
Apr 21 20:52:01.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6607 replace -f -'
Apr 21 20:52:01.530: INFO: stderr: ""
Apr 21 20:52:01.530: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 04/21/23 20:52:01.53
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Apr 21 20:52:01.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6607 delete pods e2e-test-httpd-pod'
Apr 21 20:52:03.216: INFO: stderr: ""
Apr 21 20:52:03.216: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 20:52:03.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6607" for this suite. 04/21/23 20:52:03.219
------------------------------
â€¢ [SLOW TEST] [7.329 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:51:55.893
    Apr 21 20:51:55.894: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 20:51:55.894
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:51:55.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:51:55.905
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/21/23 20:51:55.907
    Apr 21 20:51:55.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6607 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Apr 21 20:51:55.965: INFO: stderr: ""
    Apr 21 20:51:55.965: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 04/21/23 20:51:55.965
    STEP: verifying the pod e2e-test-httpd-pod was created 04/21/23 20:52:01.017
    Apr 21 20:52:01.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6607 get pod e2e-test-httpd-pod -o json'
    Apr 21 20:52:01.070: INFO: stderr: ""
    Apr 21 20:52:01.070: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-04-21T20:51:55Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6607\",\n        \"resourceVersion\": \"11946\",\n        \"uid\": \"63c5da17-d0b4-4a99-9b22-57bcc5d03061\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-qw2jd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8sconformance-m02\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-qw2jd\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-21T20:51:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-21T20:51:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-21T20:51:57Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-21T20:51:55Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://46d972511df055c510d60ff0a24666a5603e70c62b4c33dd2bc8e231d1b8174a\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-21T20:51:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.49.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.5\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.5\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-21T20:51:55Z\"\n    }\n}\n"
    STEP: replace the image in the pod 04/21/23 20:52:01.07
    Apr 21 20:52:01.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6607 replace -f -'
    Apr 21 20:52:01.530: INFO: stderr: ""
    Apr 21 20:52:01.530: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 04/21/23 20:52:01.53
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Apr 21 20:52:01.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-6607 delete pods e2e-test-httpd-pod'
    Apr 21 20:52:03.216: INFO: stderr: ""
    Apr 21 20:52:03.216: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:52:03.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6607" for this suite. 04/21/23 20:52:03.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:52:03.224
Apr 21 20:52:03.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:52:03.225
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:03.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:03.236
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:52:03.245
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:52:03.463
STEP: Deploying the webhook pod 04/21/23 20:52:03.467
STEP: Wait for the deployment to be ready 04/21/23 20:52:03.477
Apr 21 20:52:03.480: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 04/21/23 20:52:05.485
STEP: Verifying the service has paired with the endpoint 04/21/23 20:52:05.493
Apr 21 20:52:06.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 04/21/23 20:52:06.496
STEP: create a configmap that should be updated by the webhook 04/21/23 20:52:06.508
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:52:06.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9479" for this suite. 04/21/23 20:52:06.546
STEP: Destroying namespace "webhook-9479-markers" for this suite. 04/21/23 20:52:06.55
------------------------------
â€¢ [3.332 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:52:03.224
    Apr 21 20:52:03.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:52:03.225
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:03.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:03.236
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:52:03.245
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:52:03.463
    STEP: Deploying the webhook pod 04/21/23 20:52:03.467
    STEP: Wait for the deployment to be ready 04/21/23 20:52:03.477
    Apr 21 20:52:03.480: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 04/21/23 20:52:05.485
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:52:05.493
    Apr 21 20:52:06.494: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 04/21/23 20:52:06.496
    STEP: create a configmap that should be updated by the webhook 04/21/23 20:52:06.508
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:52:06.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9479" for this suite. 04/21/23 20:52:06.546
    STEP: Destroying namespace "webhook-9479-markers" for this suite. 04/21/23 20:52:06.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:52:06.557
Apr 21 20:52:06.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 20:52:06.558
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:06.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:06.567
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 04/21/23 20:52:06.569
Apr 21 20:52:06.575: INFO: Waiting up to 5m0s for pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5" in namespace "downward-api-8144" to be "Succeeded or Failed"
Apr 21 20:52:06.577: INFO: Pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573517ms
Apr 21 20:52:08.579: INFO: Pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004034804s
Apr 21 20:52:10.579: INFO: Pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004105622s
STEP: Saw pod success 04/21/23 20:52:10.579
Apr 21 20:52:10.580: INFO: Pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5" satisfied condition "Succeeded or Failed"
Apr 21 20:52:10.581: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5 container dapi-container: <nil>
STEP: delete the pod 04/21/23 20:52:10.587
Apr 21 20:52:10.595: INFO: Waiting for pod downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5 to disappear
Apr 21 20:52:10.597: INFO: Pod downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 21 20:52:10.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8144" for this suite. 04/21/23 20:52:10.599
------------------------------
â€¢ [4.045 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:52:06.557
    Apr 21 20:52:06.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 20:52:06.558
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:06.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:06.567
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 04/21/23 20:52:06.569
    Apr 21 20:52:06.575: INFO: Waiting up to 5m0s for pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5" in namespace "downward-api-8144" to be "Succeeded or Failed"
    Apr 21 20:52:06.577: INFO: Pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573517ms
    Apr 21 20:52:08.579: INFO: Pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004034804s
    Apr 21 20:52:10.579: INFO: Pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004105622s
    STEP: Saw pod success 04/21/23 20:52:10.579
    Apr 21 20:52:10.580: INFO: Pod "downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5" satisfied condition "Succeeded or Failed"
    Apr 21 20:52:10.581: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5 container dapi-container: <nil>
    STEP: delete the pod 04/21/23 20:52:10.587
    Apr 21 20:52:10.595: INFO: Waiting for pod downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5 to disappear
    Apr 21 20:52:10.597: INFO: Pod downward-api-23670716-7c2b-4e0b-b3ba-31fac77d14e5 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:52:10.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8144" for this suite. 04/21/23 20:52:10.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:52:10.603
Apr 21 20:52:10.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:52:10.604
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:10.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:10.615
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:52:10.623
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:52:11.24
STEP: Deploying the webhook pod 04/21/23 20:52:11.245
STEP: Wait for the deployment to be ready 04/21/23 20:52:11.253
Apr 21 20:52:11.258: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 20:52:13.266
STEP: Verifying the service has paired with the endpoint 04/21/23 20:52:13.274
Apr 21 20:52:14.275: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 04/21/23 20:52:14.318
STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:52:14.344
STEP: Deleting the collection of validation webhooks 04/21/23 20:52:14.369
STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:52:14.393
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:52:14.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6394" for this suite. 04/21/23 20:52:14.428
STEP: Destroying namespace "webhook-6394-markers" for this suite. 04/21/23 20:52:14.431
------------------------------
â€¢ [3.831 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:52:10.603
    Apr 21 20:52:10.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:52:10.604
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:10.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:10.615
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:52:10.623
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:52:11.24
    STEP: Deploying the webhook pod 04/21/23 20:52:11.245
    STEP: Wait for the deployment to be ready 04/21/23 20:52:11.253
    Apr 21 20:52:11.258: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 20:52:13.266
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:52:13.274
    Apr 21 20:52:14.275: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 04/21/23 20:52:14.318
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:52:14.344
    STEP: Deleting the collection of validation webhooks 04/21/23 20:52:14.369
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/21/23 20:52:14.393
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:52:14.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6394" for this suite. 04/21/23 20:52:14.428
    STEP: Destroying namespace "webhook-6394-markers" for this suite. 04/21/23 20:52:14.431
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:52:14.435
Apr 21 20:52:14.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename namespaces 04/21/23 20:52:14.436
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:14.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:14.448
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-9786" 04/21/23 20:52:14.45
Apr 21 20:52:14.454: INFO: Namespace "namespaces-9786" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"3e989928-34dd-48c8-8a06-ed2a4ab89908", "kubernetes.io/metadata.name":"namespaces-9786", "namespaces-9786":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:52:14.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9786" for this suite. 04/21/23 20:52:14.457
------------------------------
â€¢ [0.025 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:52:14.435
    Apr 21 20:52:14.435: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename namespaces 04/21/23 20:52:14.436
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:14.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:14.448
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-9786" 04/21/23 20:52:14.45
    Apr 21 20:52:14.454: INFO: Namespace "namespaces-9786" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"3e989928-34dd-48c8-8a06-ed2a4ab89908", "kubernetes.io/metadata.name":"namespaces-9786", "namespaces-9786":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:52:14.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9786" for this suite. 04/21/23 20:52:14.457
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:52:14.463
Apr 21 20:52:14.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 20:52:14.464
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:14.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:14.476
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-4f7aa8f5-a16e-4a34-b3f0-8551079ec0b8 04/21/23 20:52:14.48
STEP: Creating the pod 04/21/23 20:52:14.482
Apr 21 20:52:14.488: INFO: Waiting up to 5m0s for pod "pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3" in namespace "configmap-5952" to be "running and ready"
Apr 21 20:52:14.490: INFO: Pod "pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.851743ms
Apr 21 20:52:14.490: INFO: The phase of Pod pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:52:16.493: INFO: Pod "pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004592417s
Apr 21 20:52:16.493: INFO: The phase of Pod pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3 is Running (Ready = true)
Apr 21 20:52:16.493: INFO: Pod "pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-4f7aa8f5-a16e-4a34-b3f0-8551079ec0b8 04/21/23 20:52:16.5
STEP: waiting to observe update in volume 04/21/23 20:52:16.503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:53:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5952" for this suite. 04/21/23 20:53:28.767
------------------------------
â€¢ [SLOW TEST] [74.307 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:52:14.463
    Apr 21 20:52:14.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 20:52:14.464
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:52:14.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:52:14.476
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-4f7aa8f5-a16e-4a34-b3f0-8551079ec0b8 04/21/23 20:52:14.48
    STEP: Creating the pod 04/21/23 20:52:14.482
    Apr 21 20:52:14.488: INFO: Waiting up to 5m0s for pod "pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3" in namespace "configmap-5952" to be "running and ready"
    Apr 21 20:52:14.490: INFO: Pod "pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.851743ms
    Apr 21 20:52:14.490: INFO: The phase of Pod pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:52:16.493: INFO: Pod "pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004592417s
    Apr 21 20:52:16.493: INFO: The phase of Pod pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3 is Running (Ready = true)
    Apr 21 20:52:16.493: INFO: Pod "pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-4f7aa8f5-a16e-4a34-b3f0-8551079ec0b8 04/21/23 20:52:16.5
    STEP: waiting to observe update in volume 04/21/23 20:52:16.503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:53:28.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5952" for this suite. 04/21/23 20:53:28.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:53:28.771
Apr 21 20:53:28.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:53:28.772
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:28.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:28.782
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:53:28.79
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:53:29.712
STEP: Deploying the webhook pod 04/21/23 20:53:29.716
STEP: Wait for the deployment to be ready 04/21/23 20:53:29.723
Apr 21 20:53:29.728: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 20:53:31.735
STEP: Verifying the service has paired with the endpoint 04/21/23 20:53:31.743
Apr 21 20:53:32.743: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 04/21/23 20:53:32.745
STEP: Updating a mutating webhook configuration's rules to not include the create operation 04/21/23 20:53:32.757
STEP: Creating a configMap that should not be mutated 04/21/23 20:53:32.762
STEP: Patching a mutating webhook configuration's rules to include the create operation 04/21/23 20:53:32.768
STEP: Creating a configMap that should be mutated 04/21/23 20:53:32.773
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:53:32.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5753" for this suite. 04/21/23 20:53:32.811
STEP: Destroying namespace "webhook-5753-markers" for this suite. 04/21/23 20:53:32.817
------------------------------
â€¢ [4.050 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:53:28.771
    Apr 21 20:53:28.771: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:53:28.772
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:28.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:28.782
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:53:28.79
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:53:29.712
    STEP: Deploying the webhook pod 04/21/23 20:53:29.716
    STEP: Wait for the deployment to be ready 04/21/23 20:53:29.723
    Apr 21 20:53:29.728: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 20:53:31.735
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:53:31.743
    Apr 21 20:53:32.743: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 04/21/23 20:53:32.745
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 04/21/23 20:53:32.757
    STEP: Creating a configMap that should not be mutated 04/21/23 20:53:32.762
    STEP: Patching a mutating webhook configuration's rules to include the create operation 04/21/23 20:53:32.768
    STEP: Creating a configMap that should be mutated 04/21/23 20:53:32.773
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:53:32.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5753" for this suite. 04/21/23 20:53:32.811
    STEP: Destroying namespace "webhook-5753-markers" for this suite. 04/21/23 20:53:32.817
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:53:32.822
Apr 21 20:53:32.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-pred 04/21/23 20:53:32.822
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:32.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:32.832
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 21 20:53:32.834: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 21 20:53:32.838: INFO: Waiting for terminating namespaces to be deleted...
Apr 21 20:53:32.839: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Apr 21 20:53:32.842: INFO: coredns-787d4945fb-bp78j from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.842: INFO: 	Container coredns ready: true, restart count 1
Apr 21 20:53:32.842: INFO: etcd-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.842: INFO: 	Container etcd ready: true, restart count 0
Apr 21 20:53:32.842: INFO: kindnet-6bcq2 from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.842: INFO: 	Container kindnet-cni ready: true, restart count 0
Apr 21 20:53:32.842: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.842: INFO: 	Container kube-apiserver ready: true, restart count 0
Apr 21 20:53:32.842: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.842: INFO: 	Container kube-controller-manager ready: true, restart count 0
Apr 21 20:53:32.842: INFO: kube-proxy-k9znz from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.842: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 21 20:53:32.842: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.842: INFO: 	Container kube-scheduler ready: true, restart count 0
Apr 21 20:53:32.842: INFO: storage-provisioner from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.842: INFO: 	Container storage-provisioner ready: true, restart count 0
Apr 21 20:53:32.842: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 20:53:32.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 20:53:32.842: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 21 20:53:32.842: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Apr 21 20:53:32.845: INFO: pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3 from configmap-5952 started at 2023-04-21 20:52:14 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.845: INFO: 	Container agnhost-container ready: true, restart count 0
Apr 21 20:53:32.845: INFO: kindnet-znkt6 from kube-system started at 2023-04-21 20:47:53 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.845: INFO: 	Container kindnet-cni ready: true, restart count 0
Apr 21 20:53:32.845: INFO: kube-proxy-fvpz4 from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.845: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 21 20:53:32.845: INFO: sonobuoy from sonobuoy started at 2023-04-21 20:13:43 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:32.845: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 21 20:53:32.845: INFO: sonobuoy-e2e-job-ddbedf54a58f4885 from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 20:53:32.845: INFO: 	Container e2e ready: true, restart count 0
Apr 21 20:53:32.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 20:53:32.845: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 20:53:32.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 20:53:32.845: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node k8sconformance 04/21/23 20:53:32.855
STEP: verifying the node has the label node k8sconformance-m02 04/21/23 20:53:32.863
Apr 21 20:53:32.869: INFO: Pod pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3 requesting resource cpu=0m on Node k8sconformance-m02
Apr 21 20:53:32.869: INFO: Pod coredns-787d4945fb-bp78j requesting resource cpu=100m on Node k8sconformance
Apr 21 20:53:32.869: INFO: Pod etcd-k8sconformance requesting resource cpu=100m on Node k8sconformance
Apr 21 20:53:32.869: INFO: Pod kindnet-6bcq2 requesting resource cpu=100m on Node k8sconformance
Apr 21 20:53:32.869: INFO: Pod kindnet-znkt6 requesting resource cpu=100m on Node k8sconformance-m02
Apr 21 20:53:32.869: INFO: Pod kube-apiserver-k8sconformance requesting resource cpu=250m on Node k8sconformance
Apr 21 20:53:32.869: INFO: Pod kube-controller-manager-k8sconformance requesting resource cpu=200m on Node k8sconformance
Apr 21 20:53:32.869: INFO: Pod kube-proxy-fvpz4 requesting resource cpu=0m on Node k8sconformance-m02
Apr 21 20:53:32.869: INFO: Pod kube-proxy-k9znz requesting resource cpu=0m on Node k8sconformance
Apr 21 20:53:32.869: INFO: Pod kube-scheduler-k8sconformance requesting resource cpu=100m on Node k8sconformance
Apr 21 20:53:32.869: INFO: Pod storage-provisioner requesting resource cpu=0m on Node k8sconformance
Apr 21 20:53:32.869: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8sconformance-m02
Apr 21 20:53:32.869: INFO: Pod sonobuoy-e2e-job-ddbedf54a58f4885 requesting resource cpu=0m on Node k8sconformance-m02
Apr 21 20:53:32.869: INFO: Pod sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf requesting resource cpu=0m on Node k8sconformance-m02
Apr 21 20:53:32.869: INFO: Pod sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt requesting resource cpu=0m on Node k8sconformance
STEP: Starting Pods to consume most of the cluster CPU. 04/21/23 20:53:32.869
Apr 21 20:53:32.869: INFO: Creating a pod which consumes cpu=5005m on Node k8sconformance
Apr 21 20:53:32.875: INFO: Creating a pod which consumes cpu=5530m on Node k8sconformance-m02
Apr 21 20:53:32.878: INFO: Waiting up to 5m0s for pod "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657" in namespace "sched-pred-9711" to be "running"
Apr 21 20:53:32.881: INFO: Pod "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.697499ms
Apr 21 20:53:34.884: INFO: Pod "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657": Phase="Running", Reason="", readiness=false. Elapsed: 2.005479119s
Apr 21 20:53:34.884: INFO: Pod "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657" satisfied condition "running"
Apr 21 20:53:34.884: INFO: Waiting up to 5m0s for pod "filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617" in namespace "sched-pred-9711" to be "running"
Apr 21 20:53:34.886: INFO: Pod "filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617": Phase="Running", Reason="", readiness=false. Elapsed: 1.739838ms
Apr 21 20:53:34.886: INFO: Pod "filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 04/21/23 20:53:34.886
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617.17580e8220d73cbc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9711/filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617 to k8sconformance-m02] 04/21/23 20:53:34.888
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617.17580e82439e7503], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/21/23 20:53:34.888
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617.17580e8244d9e7bd], Reason = [Created], Message = [Created container filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617] 04/21/23 20:53:34.888
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617.17580e824a344526], Reason = [Failed], Message = [Error: failed to start container "filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617": Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "553000": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod389b851e-0d6f-43f3-b979-c428e9ff270b/filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617/cpu.cfs_quota_us: invalid argument: unknown] 04/21/23 20:53:34.888
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0c90404-9203-40e6-a927-7a5112b73657.17580e8220d091bb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9711/filler-pod-b0c90404-9203-40e6-a927-7a5112b73657 to k8sconformance] 04/21/23 20:53:34.888
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0c90404-9203-40e6-a927-7a5112b73657.17580e8243a11328], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/21/23 20:53:34.888
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0c90404-9203-40e6-a927-7a5112b73657.17580e824531adc4], Reason = [Created], Message = [Created container filler-pod-b0c90404-9203-40e6-a927-7a5112b73657] 04/21/23 20:53:34.888
STEP: Considering event: 
Type = [Warning], Name = [filler-pod-b0c90404-9203-40e6-a927-7a5112b73657.17580e824a331b49], Reason = [Failed], Message = [Error: failed to start container "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657": Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "500500": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/podae9a4500-e56e-4d16-8827-7384dcfbb7f8/filler-pod-b0c90404-9203-40e6-a927-7a5112b73657/cpu.cfs_quota_us: invalid argument: unknown] 04/21/23 20:53:34.888
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.17580e8298bd38f7], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 04/21/23 20:53:34.896
STEP: removing the label node off the node k8sconformance 04/21/23 20:53:35.896
STEP: verifying the node doesn't have the label node 04/21/23 20:53:35.905
STEP: removing the label node off the node k8sconformance-m02 04/21/23 20:53:35.907
STEP: verifying the node doesn't have the label node 04/21/23 20:53:35.915
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:53:35.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9711" for this suite. 04/21/23 20:53:35.919
------------------------------
â€¢ [3.104 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:53:32.822
    Apr 21 20:53:32.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-pred 04/21/23 20:53:32.822
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:32.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:32.832
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 21 20:53:32.834: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 21 20:53:32.838: INFO: Waiting for terminating namespaces to be deleted...
    Apr 21 20:53:32.839: INFO: 
    Logging pods the apiserver thinks is on node k8sconformance before test
    Apr 21 20:53:32.842: INFO: coredns-787d4945fb-bp78j from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.842: INFO: 	Container coredns ready: true, restart count 1
    Apr 21 20:53:32.842: INFO: etcd-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.842: INFO: 	Container etcd ready: true, restart count 0
    Apr 21 20:53:32.842: INFO: kindnet-6bcq2 from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.842: INFO: 	Container kindnet-cni ready: true, restart count 0
    Apr 21 20:53:32.842: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.842: INFO: 	Container kube-apiserver ready: true, restart count 0
    Apr 21 20:53:32.842: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.842: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Apr 21 20:53:32.842: INFO: kube-proxy-k9znz from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.842: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 21 20:53:32.842: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.842: INFO: 	Container kube-scheduler ready: true, restart count 0
    Apr 21 20:53:32.842: INFO: storage-provisioner from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.842: INFO: 	Container storage-provisioner ready: true, restart count 0
    Apr 21 20:53:32.842: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 20:53:32.842: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 20:53:32.842: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 21 20:53:32.842: INFO: 
    Logging pods the apiserver thinks is on node k8sconformance-m02 before test
    Apr 21 20:53:32.845: INFO: pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3 from configmap-5952 started at 2023-04-21 20:52:14 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.845: INFO: 	Container agnhost-container ready: true, restart count 0
    Apr 21 20:53:32.845: INFO: kindnet-znkt6 from kube-system started at 2023-04-21 20:47:53 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.845: INFO: 	Container kindnet-cni ready: true, restart count 0
    Apr 21 20:53:32.845: INFO: kube-proxy-fvpz4 from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.845: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 21 20:53:32.845: INFO: sonobuoy from sonobuoy started at 2023-04-21 20:13:43 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:32.845: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 21 20:53:32.845: INFO: sonobuoy-e2e-job-ddbedf54a58f4885 from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 20:53:32.845: INFO: 	Container e2e ready: true, restart count 0
    Apr 21 20:53:32.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 20:53:32.845: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 20:53:32.845: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 20:53:32.845: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node k8sconformance 04/21/23 20:53:32.855
    STEP: verifying the node has the label node k8sconformance-m02 04/21/23 20:53:32.863
    Apr 21 20:53:32.869: INFO: Pod pod-configmaps-0646da16-96b2-4682-8741-dc41999f9dd3 requesting resource cpu=0m on Node k8sconformance-m02
    Apr 21 20:53:32.869: INFO: Pod coredns-787d4945fb-bp78j requesting resource cpu=100m on Node k8sconformance
    Apr 21 20:53:32.869: INFO: Pod etcd-k8sconformance requesting resource cpu=100m on Node k8sconformance
    Apr 21 20:53:32.869: INFO: Pod kindnet-6bcq2 requesting resource cpu=100m on Node k8sconformance
    Apr 21 20:53:32.869: INFO: Pod kindnet-znkt6 requesting resource cpu=100m on Node k8sconformance-m02
    Apr 21 20:53:32.869: INFO: Pod kube-apiserver-k8sconformance requesting resource cpu=250m on Node k8sconformance
    Apr 21 20:53:32.869: INFO: Pod kube-controller-manager-k8sconformance requesting resource cpu=200m on Node k8sconformance
    Apr 21 20:53:32.869: INFO: Pod kube-proxy-fvpz4 requesting resource cpu=0m on Node k8sconformance-m02
    Apr 21 20:53:32.869: INFO: Pod kube-proxy-k9znz requesting resource cpu=0m on Node k8sconformance
    Apr 21 20:53:32.869: INFO: Pod kube-scheduler-k8sconformance requesting resource cpu=100m on Node k8sconformance
    Apr 21 20:53:32.869: INFO: Pod storage-provisioner requesting resource cpu=0m on Node k8sconformance
    Apr 21 20:53:32.869: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8sconformance-m02
    Apr 21 20:53:32.869: INFO: Pod sonobuoy-e2e-job-ddbedf54a58f4885 requesting resource cpu=0m on Node k8sconformance-m02
    Apr 21 20:53:32.869: INFO: Pod sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf requesting resource cpu=0m on Node k8sconformance-m02
    Apr 21 20:53:32.869: INFO: Pod sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt requesting resource cpu=0m on Node k8sconformance
    STEP: Starting Pods to consume most of the cluster CPU. 04/21/23 20:53:32.869
    Apr 21 20:53:32.869: INFO: Creating a pod which consumes cpu=5005m on Node k8sconformance
    Apr 21 20:53:32.875: INFO: Creating a pod which consumes cpu=5530m on Node k8sconformance-m02
    Apr 21 20:53:32.878: INFO: Waiting up to 5m0s for pod "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657" in namespace "sched-pred-9711" to be "running"
    Apr 21 20:53:32.881: INFO: Pod "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.697499ms
    Apr 21 20:53:34.884: INFO: Pod "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657": Phase="Running", Reason="", readiness=false. Elapsed: 2.005479119s
    Apr 21 20:53:34.884: INFO: Pod "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657" satisfied condition "running"
    Apr 21 20:53:34.884: INFO: Waiting up to 5m0s for pod "filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617" in namespace "sched-pred-9711" to be "running"
    Apr 21 20:53:34.886: INFO: Pod "filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617": Phase="Running", Reason="", readiness=false. Elapsed: 1.739838ms
    Apr 21 20:53:34.886: INFO: Pod "filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 04/21/23 20:53:34.886
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617.17580e8220d73cbc], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9711/filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617 to k8sconformance-m02] 04/21/23 20:53:34.888
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617.17580e82439e7503], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/21/23 20:53:34.888
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617.17580e8244d9e7bd], Reason = [Created], Message = [Created container filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617] 04/21/23 20:53:34.888
    STEP: Considering event: 
    Type = [Warning], Name = [filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617.17580e824a344526], Reason = [Failed], Message = [Error: failed to start container "filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617": Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "553000": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod389b851e-0d6f-43f3-b979-c428e9ff270b/filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617/cpu.cfs_quota_us: invalid argument: unknown] 04/21/23 20:53:34.888
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b0c90404-9203-40e6-a927-7a5112b73657.17580e8220d091bb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9711/filler-pod-b0c90404-9203-40e6-a927-7a5112b73657 to k8sconformance] 04/21/23 20:53:34.888
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b0c90404-9203-40e6-a927-7a5112b73657.17580e8243a11328], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/21/23 20:53:34.888
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b0c90404-9203-40e6-a927-7a5112b73657.17580e824531adc4], Reason = [Created], Message = [Created container filler-pod-b0c90404-9203-40e6-a927-7a5112b73657] 04/21/23 20:53:34.888
    STEP: Considering event: 
    Type = [Warning], Name = [filler-pod-b0c90404-9203-40e6-a927-7a5112b73657.17580e824a331b49], Reason = [Failed], Message = [Error: failed to start container "filler-pod-b0c90404-9203-40e6-a927-7a5112b73657": Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "500500": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/podae9a4500-e56e-4d16-8827-7384dcfbb7f8/filler-pod-b0c90404-9203-40e6-a927-7a5112b73657/cpu.cfs_quota_us: invalid argument: unknown] 04/21/23 20:53:34.888
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.17580e8298bd38f7], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] 04/21/23 20:53:34.896
    STEP: removing the label node off the node k8sconformance 04/21/23 20:53:35.896
    STEP: verifying the node doesn't have the label node 04/21/23 20:53:35.905
    STEP: removing the label node off the node k8sconformance-m02 04/21/23 20:53:35.907
    STEP: verifying the node doesn't have the label node 04/21/23 20:53:35.915
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:53:35.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9711" for this suite. 04/21/23 20:53:35.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:53:35.927
Apr 21 20:53:35.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-pred 04/21/23 20:53:35.928
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:35.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:35.941
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 21 20:53:35.943: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 21 20:53:35.947: INFO: Waiting for terminating namespaces to be deleted...
Apr 21 20:53:35.948: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Apr 21 20:53:35.952: INFO: coredns-787d4945fb-bp78j from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container coredns ready: true, restart count 1
Apr 21 20:53:35.952: INFO: etcd-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container etcd ready: true, restart count 0
Apr 21 20:53:35.952: INFO: kindnet-6bcq2 from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container kindnet-cni ready: true, restart count 0
Apr 21 20:53:35.952: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container kube-apiserver ready: true, restart count 0
Apr 21 20:53:35.952: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container kube-controller-manager ready: true, restart count 0
Apr 21 20:53:35.952: INFO: kube-proxy-k9znz from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 21 20:53:35.952: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container kube-scheduler ready: true, restart count 0
Apr 21 20:53:35.952: INFO: storage-provisioner from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container storage-provisioner ready: true, restart count 0
Apr 21 20:53:35.952: INFO: filler-pod-b0c90404-9203-40e6-a927-7a5112b73657 from sched-pred-9711 started at 2023-04-21 20:53:32 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container filler-pod-b0c90404-9203-40e6-a927-7a5112b73657 ready: false, restart count 0
Apr 21 20:53:35.952: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 20:53:35.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 20:53:35.952: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 21 20:53:35.952: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Apr 21 20:53:35.956: INFO: kindnet-znkt6 from kube-system started at 2023-04-21 20:47:53 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.956: INFO: 	Container kindnet-cni ready: true, restart count 0
Apr 21 20:53:35.956: INFO: kube-proxy-fvpz4 from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.956: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 21 20:53:35.956: INFO: filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617 from sched-pred-9711 started at 2023-04-21 20:53:32 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.956: INFO: 	Container filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617 ready: false, restart count 0
Apr 21 20:53:35.956: INFO: sonobuoy from sonobuoy started at 2023-04-21 20:13:43 +0000 UTC (1 container statuses recorded)
Apr 21 20:53:35.956: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 21 20:53:35.956: INFO: sonobuoy-e2e-job-ddbedf54a58f4885 from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 20:53:35.956: INFO: 	Container e2e ready: true, restart count 0
Apr 21 20:53:35.956: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 20:53:35.956: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 20:53:35.956: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 20:53:35.956: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/21/23 20:53:35.956
Apr 21 20:53:35.961: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3349" to be "running"
Apr 21 20:53:35.963: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.895519ms
Apr 21 20:53:37.965: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003924775s
Apr 21 20:53:37.965: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/21/23 20:53:37.967
STEP: Trying to apply a random label on the found node. 04/21/23 20:53:37.974
STEP: verifying the node has the label kubernetes.io/e2e-5eb5b380-01ba-404e-b7e2-68b7e7de30ef 42 04/21/23 20:53:37.981
STEP: Trying to relaunch the pod, now with labels. 04/21/23 20:53:37.983
Apr 21 20:53:37.987: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3349" to be "not pending"
Apr 21 20:53:37.990: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.138882ms
Apr 21 20:53:39.993: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005546356s
Apr 21 20:53:39.993: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-5eb5b380-01ba-404e-b7e2-68b7e7de30ef off the node k8sconformance-m02 04/21/23 20:53:39.995
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5eb5b380-01ba-404e-b7e2-68b7e7de30ef 04/21/23 20:53:40.003
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:53:40.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3349" for this suite. 04/21/23 20:53:40.008
------------------------------
â€¢ [4.084 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:53:35.927
    Apr 21 20:53:35.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-pred 04/21/23 20:53:35.928
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:35.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:35.941
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 21 20:53:35.943: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 21 20:53:35.947: INFO: Waiting for terminating namespaces to be deleted...
    Apr 21 20:53:35.948: INFO: 
    Logging pods the apiserver thinks is on node k8sconformance before test
    Apr 21 20:53:35.952: INFO: coredns-787d4945fb-bp78j from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container coredns ready: true, restart count 1
    Apr 21 20:53:35.952: INFO: etcd-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container etcd ready: true, restart count 0
    Apr 21 20:53:35.952: INFO: kindnet-6bcq2 from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container kindnet-cni ready: true, restart count 0
    Apr 21 20:53:35.952: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container kube-apiserver ready: true, restart count 0
    Apr 21 20:53:35.952: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Apr 21 20:53:35.952: INFO: kube-proxy-k9znz from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 21 20:53:35.952: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container kube-scheduler ready: true, restart count 0
    Apr 21 20:53:35.952: INFO: storage-provisioner from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container storage-provisioner ready: true, restart count 0
    Apr 21 20:53:35.952: INFO: filler-pod-b0c90404-9203-40e6-a927-7a5112b73657 from sched-pred-9711 started at 2023-04-21 20:53:32 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container filler-pod-b0c90404-9203-40e6-a927-7a5112b73657 ready: false, restart count 0
    Apr 21 20:53:35.952: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 20:53:35.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 20:53:35.952: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 21 20:53:35.952: INFO: 
    Logging pods the apiserver thinks is on node k8sconformance-m02 before test
    Apr 21 20:53:35.956: INFO: kindnet-znkt6 from kube-system started at 2023-04-21 20:47:53 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.956: INFO: 	Container kindnet-cni ready: true, restart count 0
    Apr 21 20:53:35.956: INFO: kube-proxy-fvpz4 from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.956: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 21 20:53:35.956: INFO: filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617 from sched-pred-9711 started at 2023-04-21 20:53:32 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.956: INFO: 	Container filler-pod-3c6aa64b-17e0-43d9-9f25-66906f098617 ready: false, restart count 0
    Apr 21 20:53:35.956: INFO: sonobuoy from sonobuoy started at 2023-04-21 20:13:43 +0000 UTC (1 container statuses recorded)
    Apr 21 20:53:35.956: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 21 20:53:35.956: INFO: sonobuoy-e2e-job-ddbedf54a58f4885 from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 20:53:35.956: INFO: 	Container e2e ready: true, restart count 0
    Apr 21 20:53:35.956: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 20:53:35.956: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 20:53:35.956: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 20:53:35.956: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/21/23 20:53:35.956
    Apr 21 20:53:35.961: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3349" to be "running"
    Apr 21 20:53:35.963: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.895519ms
    Apr 21 20:53:37.965: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.003924775s
    Apr 21 20:53:37.965: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/21/23 20:53:37.967
    STEP: Trying to apply a random label on the found node. 04/21/23 20:53:37.974
    STEP: verifying the node has the label kubernetes.io/e2e-5eb5b380-01ba-404e-b7e2-68b7e7de30ef 42 04/21/23 20:53:37.981
    STEP: Trying to relaunch the pod, now with labels. 04/21/23 20:53:37.983
    Apr 21 20:53:37.987: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3349" to be "not pending"
    Apr 21 20:53:37.990: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.138882ms
    Apr 21 20:53:39.993: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.005546356s
    Apr 21 20:53:39.993: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-5eb5b380-01ba-404e-b7e2-68b7e7de30ef off the node k8sconformance-m02 04/21/23 20:53:39.995
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-5eb5b380-01ba-404e-b7e2-68b7e7de30ef 04/21/23 20:53:40.003
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:53:40.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3349" for this suite. 04/21/23 20:53:40.008
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:53:40.011
Apr 21 20:53:40.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 20:53:40.012
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:40.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:40.023
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-3146 04/21/23 20:53:40.025
STEP: creating service affinity-clusterip-transition in namespace services-3146 04/21/23 20:53:40.025
STEP: creating replication controller affinity-clusterip-transition in namespace services-3146 04/21/23 20:53:40.033
I0421 20:53:40.038717      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3146, replica count: 3
I0421 20:53:43.090252      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 21 20:53:43.093: INFO: Creating new exec pod
Apr 21 20:53:43.097: INFO: Waiting up to 5m0s for pod "execpod-affinityq8cwg" in namespace "services-3146" to be "running"
Apr 21 20:53:43.098: INFO: Pod "execpod-affinityq8cwg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643663ms
Apr 21 20:53:45.101: INFO: Pod "execpod-affinityq8cwg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004710771s
Apr 21 20:53:45.101: INFO: Pod "execpod-affinityq8cwg" satisfied condition "running"
Apr 21 20:53:46.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3146 exec execpod-affinityq8cwg -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Apr 21 20:53:46.219: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr 21 20:53:46.219: INFO: stdout: ""
Apr 21 20:53:46.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3146 exec execpod-affinityq8cwg -- /bin/sh -x -c nc -v -z -w 2 10.100.104.60 80'
Apr 21 20:53:46.336: INFO: stderr: "+ nc -v -z -w 2 10.100.104.60 80\nConnection to 10.100.104.60 80 port [tcp/http] succeeded!\n"
Apr 21 20:53:46.336: INFO: stdout: ""
Apr 21 20:53:46.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3146 exec execpod-affinityq8cwg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.104.60:80/ ; done'
Apr 21 20:53:46.510: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n"
Apr 21 20:53:46.510: INFO: stdout: "\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-jwfft"
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-jwfft
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-jwfft
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-jwfft
Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-nl6fl
Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-jwfft
Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-jwfft
Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-jwfft
Apr 21 20:53:46.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3146 exec execpod-affinityq8cwg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.104.60:80/ ; done'
Apr 21 20:53:46.692: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n"
Apr 21 20:53:46.692: INFO: stdout: "\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c"
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
Apr 21 20:53:46.692: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3146, will wait for the garbage collector to delete the pods 04/21/23 20:53:46.701
Apr 21 20:53:46.757: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.871303ms
Apr 21 20:53:46.857: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.227962ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 20:53:48.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3146" for this suite. 04/21/23 20:53:48.87
------------------------------
â€¢ [SLOW TEST] [8.862 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:53:40.011
    Apr 21 20:53:40.011: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 20:53:40.012
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:40.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:40.023
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-3146 04/21/23 20:53:40.025
    STEP: creating service affinity-clusterip-transition in namespace services-3146 04/21/23 20:53:40.025
    STEP: creating replication controller affinity-clusterip-transition in namespace services-3146 04/21/23 20:53:40.033
    I0421 20:53:40.038717      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3146, replica count: 3
    I0421 20:53:43.090252      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 21 20:53:43.093: INFO: Creating new exec pod
    Apr 21 20:53:43.097: INFO: Waiting up to 5m0s for pod "execpod-affinityq8cwg" in namespace "services-3146" to be "running"
    Apr 21 20:53:43.098: INFO: Pod "execpod-affinityq8cwg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.643663ms
    Apr 21 20:53:45.101: INFO: Pod "execpod-affinityq8cwg": Phase="Running", Reason="", readiness=true. Elapsed: 2.004710771s
    Apr 21 20:53:45.101: INFO: Pod "execpod-affinityq8cwg" satisfied condition "running"
    Apr 21 20:53:46.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3146 exec execpod-affinityq8cwg -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Apr 21 20:53:46.219: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Apr 21 20:53:46.219: INFO: stdout: ""
    Apr 21 20:53:46.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3146 exec execpod-affinityq8cwg -- /bin/sh -x -c nc -v -z -w 2 10.100.104.60 80'
    Apr 21 20:53:46.336: INFO: stderr: "+ nc -v -z -w 2 10.100.104.60 80\nConnection to 10.100.104.60 80 port [tcp/http] succeeded!\n"
    Apr 21 20:53:46.336: INFO: stdout: ""
    Apr 21 20:53:46.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3146 exec execpod-affinityq8cwg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.104.60:80/ ; done'
    Apr 21 20:53:46.510: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n"
    Apr 21 20:53:46.510: INFO: stdout: "\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-nl6fl\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-jwfft\naffinity-clusterip-transition-jwfft"
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-jwfft
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-jwfft
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-jwfft
    Apr 21 20:53:46.510: INFO: Received response from host: affinity-clusterip-transition-nl6fl
    Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-nl6fl
    Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-jwfft
    Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-jwfft
    Apr 21 20:53:46.511: INFO: Received response from host: affinity-clusterip-transition-jwfft
    Apr 21 20:53:46.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3146 exec execpod-affinityq8cwg -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.104.60:80/ ; done'
    Apr 21 20:53:46.692: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.104.60:80/\n"
    Apr 21 20:53:46.692: INFO: stdout: "\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c\naffinity-clusterip-transition-fgl9c"
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Received response from host: affinity-clusterip-transition-fgl9c
    Apr 21 20:53:46.692: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3146, will wait for the garbage collector to delete the pods 04/21/23 20:53:46.701
    Apr 21 20:53:46.757: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.871303ms
    Apr 21 20:53:46.857: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.227962ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:53:48.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3146" for this suite. 04/21/23 20:53:48.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:53:48.874
Apr 21 20:53:48.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-probe 04/21/23 20:53:48.875
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:48.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:48.885
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e in namespace container-probe-7486 04/21/23 20:53:48.887
Apr 21 20:53:48.892: INFO: Waiting up to 5m0s for pod "liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e" in namespace "container-probe-7486" to be "not pending"
Apr 21 20:53:48.893: INFO: Pod "liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.519823ms
Apr 21 20:53:50.896: INFO: Pod "liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004792737s
Apr 21 20:53:50.896: INFO: Pod "liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e" satisfied condition "not pending"
Apr 21 20:53:50.896: INFO: Started pod liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e in namespace container-probe-7486
STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 20:53:50.896
Apr 21 20:53:50.898: INFO: Initial restart count of pod liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e is 0
STEP: deleting the pod 04/21/23 20:57:51.271
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 21 20:57:51.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7486" for this suite. 04/21/23 20:57:51.282
------------------------------
â€¢ [SLOW TEST] [242.412 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:53:48.874
    Apr 21 20:53:48.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-probe 04/21/23 20:53:48.875
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:53:48.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:53:48.885
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e in namespace container-probe-7486 04/21/23 20:53:48.887
    Apr 21 20:53:48.892: INFO: Waiting up to 5m0s for pod "liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e" in namespace "container-probe-7486" to be "not pending"
    Apr 21 20:53:48.893: INFO: Pod "liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.519823ms
    Apr 21 20:53:50.896: INFO: Pod "liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e": Phase="Running", Reason="", readiness=true. Elapsed: 2.004792737s
    Apr 21 20:53:50.896: INFO: Pod "liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e" satisfied condition "not pending"
    Apr 21 20:53:50.896: INFO: Started pod liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e in namespace container-probe-7486
    STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 20:53:50.896
    Apr 21 20:53:50.898: INFO: Initial restart count of pod liveness-0e569ede-8c29-4c62-ac35-05f6963c2b0e is 0
    STEP: deleting the pod 04/21/23 20:57:51.271
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:57:51.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7486" for this suite. 04/21/23 20:57:51.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:57:51.286
Apr 21 20:57:51.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 20:57:51.287
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:57:51.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:57:51.296
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-b6d27fac-b9da-44e6-b975-33e7cbece67b 04/21/23 20:57:51.3
STEP: Creating configMap with name cm-test-opt-upd-db8106fe-2795-43f4-81c4-0f1f80972e0c 04/21/23 20:57:51.302
STEP: Creating the pod 04/21/23 20:57:51.305
Apr 21 20:57:51.311: INFO: Waiting up to 5m0s for pod "pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2" in namespace "configmap-3751" to be "running and ready"
Apr 21 20:57:51.312: INFO: Pod "pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683539ms
Apr 21 20:57:51.312: INFO: The phase of Pod pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:57:53.315: INFO: Pod "pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004429534s
Apr 21 20:57:53.315: INFO: The phase of Pod pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2 is Running (Ready = true)
Apr 21 20:57:53.315: INFO: Pod "pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-b6d27fac-b9da-44e6-b975-33e7cbece67b 04/21/23 20:57:53.342
STEP: Updating configmap cm-test-opt-upd-db8106fe-2795-43f4-81c4-0f1f80972e0c 04/21/23 20:57:53.345
STEP: Creating configMap with name cm-test-opt-create-263cecd2-fcdf-4bc3-90dc-ccf5a2530989 04/21/23 20:57:53.348
STEP: waiting to observe update in volume 04/21/23 20:57:53.35
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:57:55.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3751" for this suite. 04/21/23 20:57:55.374
------------------------------
â€¢ [4.092 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:57:51.286
    Apr 21 20:57:51.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 20:57:51.287
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:57:51.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:57:51.296
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-b6d27fac-b9da-44e6-b975-33e7cbece67b 04/21/23 20:57:51.3
    STEP: Creating configMap with name cm-test-opt-upd-db8106fe-2795-43f4-81c4-0f1f80972e0c 04/21/23 20:57:51.302
    STEP: Creating the pod 04/21/23 20:57:51.305
    Apr 21 20:57:51.311: INFO: Waiting up to 5m0s for pod "pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2" in namespace "configmap-3751" to be "running and ready"
    Apr 21 20:57:51.312: INFO: Pod "pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.683539ms
    Apr 21 20:57:51.312: INFO: The phase of Pod pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:57:53.315: INFO: Pod "pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004429534s
    Apr 21 20:57:53.315: INFO: The phase of Pod pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2 is Running (Ready = true)
    Apr 21 20:57:53.315: INFO: Pod "pod-configmaps-0de8b99e-c634-4051-8958-794ceddf39b2" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-b6d27fac-b9da-44e6-b975-33e7cbece67b 04/21/23 20:57:53.342
    STEP: Updating configmap cm-test-opt-upd-db8106fe-2795-43f4-81c4-0f1f80972e0c 04/21/23 20:57:53.345
    STEP: Creating configMap with name cm-test-opt-create-263cecd2-fcdf-4bc3-90dc-ccf5a2530989 04/21/23 20:57:53.348
    STEP: waiting to observe update in volume 04/21/23 20:57:53.35
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:57:55.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3751" for this suite. 04/21/23 20:57:55.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:57:55.379
Apr 21 20:57:55.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename ephemeral-containers-test 04/21/23 20:57:55.38
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:57:55.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:57:55.39
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 04/21/23 20:57:55.392
Apr 21 20:57:55.398: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6690" to be "running and ready"
Apr 21 20:57:55.400: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618887ms
Apr 21 20:57:55.400: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:57:57.403: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00460202s
Apr 21 20:57:57.403: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Apr 21 20:57:57.403: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 04/21/23 20:57:57.404
Apr 21 20:57:57.414: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6690" to be "container debugger running"
Apr 21 20:57:57.415: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.577487ms
Apr 21 20:57:59.418: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003867588s
Apr 21 20:58:01.418: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004585669s
Apr 21 20:58:01.418: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 04/21/23 20:58:01.418
Apr 21 20:58:01.419: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6690 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 20:58:01.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 20:58:01.419: INFO: ExecWithOptions: Clientset creation
Apr 21 20:58:01.419: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-6690/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Apr 21 20:58:01.485: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:58:01.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-6690" for this suite. 04/21/23 20:58:01.492
------------------------------
â€¢ [SLOW TEST] [6.117 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:57:55.379
    Apr 21 20:57:55.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename ephemeral-containers-test 04/21/23 20:57:55.38
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:57:55.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:57:55.39
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 04/21/23 20:57:55.392
    Apr 21 20:57:55.398: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6690" to be "running and ready"
    Apr 21 20:57:55.400: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618887ms
    Apr 21 20:57:55.400: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:57:57.403: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00460202s
    Apr 21 20:57:57.403: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Apr 21 20:57:57.403: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 04/21/23 20:57:57.404
    Apr 21 20:57:57.414: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6690" to be "container debugger running"
    Apr 21 20:57:57.415: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.577487ms
    Apr 21 20:57:59.418: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.003867588s
    Apr 21 20:58:01.418: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.004585669s
    Apr 21 20:58:01.418: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 04/21/23 20:58:01.418
    Apr 21 20:58:01.419: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6690 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 20:58:01.419: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 20:58:01.419: INFO: ExecWithOptions: Clientset creation
    Apr 21 20:58:01.419: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-6690/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Apr 21 20:58:01.485: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:58:01.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-6690" for this suite. 04/21/23 20:58:01.492
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:58:01.496
Apr 21 20:58:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename subpath 04/21/23 20:58:01.497
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:01.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:01.506
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/21/23 20:58:01.507
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-67bm 04/21/23 20:58:01.512
STEP: Creating a pod to test atomic-volume-subpath 04/21/23 20:58:01.512
Apr 21 20:58:01.516: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-67bm" in namespace "subpath-3465" to be "Succeeded or Failed"
Apr 21 20:58:01.518: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.569788ms
Apr 21 20:58:03.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 2.004632496s
Apr 21 20:58:05.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 4.00430357s
Apr 21 20:58:07.522: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 6.005602219s
Apr 21 20:58:09.520: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 8.004193318s
Apr 21 20:58:11.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 10.004403931s
Apr 21 20:58:13.522: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 12.005621559s
Apr 21 20:58:15.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 14.004355637s
Apr 21 20:58:17.522: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 16.005378839s
Apr 21 20:58:19.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 18.004925902s
Apr 21 20:58:21.520: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 20.004276104s
Apr 21 20:58:23.520: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=false. Elapsed: 22.003644946s
Apr 21 20:58:25.520: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004166536s
STEP: Saw pod success 04/21/23 20:58:25.52
Apr 21 20:58:25.520: INFO: Pod "pod-subpath-test-configmap-67bm" satisfied condition "Succeeded or Failed"
Apr 21 20:58:25.522: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-67bm container test-container-subpath-configmap-67bm: <nil>
STEP: delete the pod 04/21/23 20:58:25.528
Apr 21 20:58:25.536: INFO: Waiting for pod pod-subpath-test-configmap-67bm to disappear
Apr 21 20:58:25.539: INFO: Pod pod-subpath-test-configmap-67bm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-67bm 04/21/23 20:58:25.539
Apr 21 20:58:25.539: INFO: Deleting pod "pod-subpath-test-configmap-67bm" in namespace "subpath-3465"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 21 20:58:25.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-3465" for this suite. 04/21/23 20:58:25.542
------------------------------
â€¢ [SLOW TEST] [24.049 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:58:01.496
    Apr 21 20:58:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename subpath 04/21/23 20:58:01.497
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:01.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:01.506
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/21/23 20:58:01.507
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-67bm 04/21/23 20:58:01.512
    STEP: Creating a pod to test atomic-volume-subpath 04/21/23 20:58:01.512
    Apr 21 20:58:01.516: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-67bm" in namespace "subpath-3465" to be "Succeeded or Failed"
    Apr 21 20:58:01.518: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.569788ms
    Apr 21 20:58:03.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 2.004632496s
    Apr 21 20:58:05.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 4.00430357s
    Apr 21 20:58:07.522: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 6.005602219s
    Apr 21 20:58:09.520: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 8.004193318s
    Apr 21 20:58:11.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 10.004403931s
    Apr 21 20:58:13.522: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 12.005621559s
    Apr 21 20:58:15.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 14.004355637s
    Apr 21 20:58:17.522: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 16.005378839s
    Apr 21 20:58:19.521: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 18.004925902s
    Apr 21 20:58:21.520: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=true. Elapsed: 20.004276104s
    Apr 21 20:58:23.520: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Running", Reason="", readiness=false. Elapsed: 22.003644946s
    Apr 21 20:58:25.520: INFO: Pod "pod-subpath-test-configmap-67bm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004166536s
    STEP: Saw pod success 04/21/23 20:58:25.52
    Apr 21 20:58:25.520: INFO: Pod "pod-subpath-test-configmap-67bm" satisfied condition "Succeeded or Failed"
    Apr 21 20:58:25.522: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-67bm container test-container-subpath-configmap-67bm: <nil>
    STEP: delete the pod 04/21/23 20:58:25.528
    Apr 21 20:58:25.536: INFO: Waiting for pod pod-subpath-test-configmap-67bm to disappear
    Apr 21 20:58:25.539: INFO: Pod pod-subpath-test-configmap-67bm no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-67bm 04/21/23 20:58:25.539
    Apr 21 20:58:25.539: INFO: Deleting pod "pod-subpath-test-configmap-67bm" in namespace "subpath-3465"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:58:25.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-3465" for this suite. 04/21/23 20:58:25.542
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:58:25.545
Apr 21 20:58:25.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 20:58:25.546
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:25.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:25.556
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 04/21/23 20:58:25.558
Apr 21 20:58:25.562: INFO: Waiting up to 5m0s for pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f" in namespace "emptydir-2193" to be "Succeeded or Failed"
Apr 21 20:58:25.564: INFO: Pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455901ms
Apr 21 20:58:27.566: INFO: Pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003915974s
Apr 21 20:58:29.567: INFO: Pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004946912s
STEP: Saw pod success 04/21/23 20:58:29.567
Apr 21 20:58:29.568: INFO: Pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f" satisfied condition "Succeeded or Failed"
Apr 21 20:58:29.569: INFO: Trying to get logs from node k8sconformance-m02 pod pod-523cbc10-94e9-4822-a02b-af732f07c35f container test-container: <nil>
STEP: delete the pod 04/21/23 20:58:29.574
Apr 21 20:58:29.581: INFO: Waiting for pod pod-523cbc10-94e9-4822-a02b-af732f07c35f to disappear
Apr 21 20:58:29.582: INFO: Pod pod-523cbc10-94e9-4822-a02b-af732f07c35f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 20:58:29.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2193" for this suite. 04/21/23 20:58:29.584
------------------------------
â€¢ [4.042 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:58:25.545
    Apr 21 20:58:25.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 20:58:25.546
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:25.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:25.556
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 04/21/23 20:58:25.558
    Apr 21 20:58:25.562: INFO: Waiting up to 5m0s for pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f" in namespace "emptydir-2193" to be "Succeeded or Failed"
    Apr 21 20:58:25.564: INFO: Pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.455901ms
    Apr 21 20:58:27.566: INFO: Pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003915974s
    Apr 21 20:58:29.567: INFO: Pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004946912s
    STEP: Saw pod success 04/21/23 20:58:29.567
    Apr 21 20:58:29.568: INFO: Pod "pod-523cbc10-94e9-4822-a02b-af732f07c35f" satisfied condition "Succeeded or Failed"
    Apr 21 20:58:29.569: INFO: Trying to get logs from node k8sconformance-m02 pod pod-523cbc10-94e9-4822-a02b-af732f07c35f container test-container: <nil>
    STEP: delete the pod 04/21/23 20:58:29.574
    Apr 21 20:58:29.581: INFO: Waiting for pod pod-523cbc10-94e9-4822-a02b-af732f07c35f to disappear
    Apr 21 20:58:29.582: INFO: Pod pod-523cbc10-94e9-4822-a02b-af732f07c35f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:58:29.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2193" for this suite. 04/21/23 20:58:29.584
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:58:29.587
Apr 21 20:58:29.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 20:58:29.588
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:29.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:29.597
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 20:58:29.605
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:58:29.984
STEP: Deploying the webhook pod 04/21/23 20:58:29.989
STEP: Wait for the deployment to be ready 04/21/23 20:58:29.997
Apr 21 20:58:30.001: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 20:58:32.007
STEP: Verifying the service has paired with the endpoint 04/21/23 20:58:32.015
Apr 21 20:58:33.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/21/23 20:58:33.017
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/21/23 20:58:33.029
STEP: Creating a dummy validating-webhook-configuration object 04/21/23 20:58:33.039
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 04/21/23 20:58:33.044
STEP: Creating a dummy mutating-webhook-configuration object 04/21/23 20:58:33.047
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 04/21/23 20:58:33.053
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 20:58:33.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2147" for this suite. 04/21/23 20:58:33.095
STEP: Destroying namespace "webhook-2147-markers" for this suite. 04/21/23 20:58:33.099
------------------------------
â€¢ [3.515 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:58:29.587
    Apr 21 20:58:29.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 20:58:29.588
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:29.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:29.597
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 20:58:29.605
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 20:58:29.984
    STEP: Deploying the webhook pod 04/21/23 20:58:29.989
    STEP: Wait for the deployment to be ready 04/21/23 20:58:29.997
    Apr 21 20:58:30.001: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 20:58:32.007
    STEP: Verifying the service has paired with the endpoint 04/21/23 20:58:32.015
    Apr 21 20:58:33.015: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/21/23 20:58:33.017
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/21/23 20:58:33.029
    STEP: Creating a dummy validating-webhook-configuration object 04/21/23 20:58:33.039
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 04/21/23 20:58:33.044
    STEP: Creating a dummy mutating-webhook-configuration object 04/21/23 20:58:33.047
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 04/21/23 20:58:33.053
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:58:33.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2147" for this suite. 04/21/23 20:58:33.095
    STEP: Destroying namespace "webhook-2147-markers" for this suite. 04/21/23 20:58:33.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:58:33.103
Apr 21 20:58:33.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 20:58:33.103
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:33.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:33.135
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-a14ceaf1-8749-4316-b09a-6148704b9c80 04/21/23 20:58:33.137
STEP: Creating a pod to test consume configMaps 04/21/23 20:58:33.14
Apr 21 20:58:33.144: INFO: Waiting up to 5m0s for pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a" in namespace "configmap-6652" to be "Succeeded or Failed"
Apr 21 20:58:33.146: INFO: Pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691211ms
Apr 21 20:58:35.149: INFO: Pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004304241s
Apr 21 20:58:37.150: INFO: Pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005203929s
STEP: Saw pod success 04/21/23 20:58:37.15
Apr 21 20:58:37.150: INFO: Pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a" satisfied condition "Succeeded or Failed"
Apr 21 20:58:37.151: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a container configmap-volume-test: <nil>
STEP: delete the pod 04/21/23 20:58:37.156
Apr 21 20:58:37.163: INFO: Waiting for pod pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a to disappear
Apr 21 20:58:37.165: INFO: Pod pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:58:37.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6652" for this suite. 04/21/23 20:58:37.167
------------------------------
â€¢ [4.068 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:58:33.103
    Apr 21 20:58:33.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 20:58:33.103
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:33.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:33.135
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-a14ceaf1-8749-4316-b09a-6148704b9c80 04/21/23 20:58:33.137
    STEP: Creating a pod to test consume configMaps 04/21/23 20:58:33.14
    Apr 21 20:58:33.144: INFO: Waiting up to 5m0s for pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a" in namespace "configmap-6652" to be "Succeeded or Failed"
    Apr 21 20:58:33.146: INFO: Pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691211ms
    Apr 21 20:58:35.149: INFO: Pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004304241s
    Apr 21 20:58:37.150: INFO: Pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005203929s
    STEP: Saw pod success 04/21/23 20:58:37.15
    Apr 21 20:58:37.150: INFO: Pod "pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a" satisfied condition "Succeeded or Failed"
    Apr 21 20:58:37.151: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a container configmap-volume-test: <nil>
    STEP: delete the pod 04/21/23 20:58:37.156
    Apr 21 20:58:37.163: INFO: Waiting for pod pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a to disappear
    Apr 21 20:58:37.165: INFO: Pod pod-configmaps-366fd176-40fb-4c40-928f-289f53d5549a no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:58:37.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6652" for this suite. 04/21/23 20:58:37.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:58:37.171
Apr 21 20:58:37.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:58:37.171
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:37.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:37.18
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-49654965-d359-42a0-ada4-c36d144fbe4c 04/21/23 20:58:37.182
STEP: Creating a pod to test consume configMaps 04/21/23 20:58:37.185
Apr 21 20:58:37.189: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0" in namespace "projected-9451" to be "Succeeded or Failed"
Apr 21 20:58:37.191: INFO: Pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.476941ms
Apr 21 20:58:39.194: INFO: Pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004288477s
Apr 21 20:58:41.194: INFO: Pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004278254s
STEP: Saw pod success 04/21/23 20:58:41.194
Apr 21 20:58:41.194: INFO: Pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0" satisfied condition "Succeeded or Failed"
Apr 21 20:58:41.196: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 20:58:41.201
Apr 21 20:58:41.208: INFO: Waiting for pod pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0 to disappear
Apr 21 20:58:41.209: INFO: Pod pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 21 20:58:41.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9451" for this suite. 04/21/23 20:58:41.211
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:58:37.171
    Apr 21 20:58:37.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:58:37.171
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:37.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:37.18
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-49654965-d359-42a0-ada4-c36d144fbe4c 04/21/23 20:58:37.182
    STEP: Creating a pod to test consume configMaps 04/21/23 20:58:37.185
    Apr 21 20:58:37.189: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0" in namespace "projected-9451" to be "Succeeded or Failed"
    Apr 21 20:58:37.191: INFO: Pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.476941ms
    Apr 21 20:58:39.194: INFO: Pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004288477s
    Apr 21 20:58:41.194: INFO: Pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004278254s
    STEP: Saw pod success 04/21/23 20:58:41.194
    Apr 21 20:58:41.194: INFO: Pod "pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0" satisfied condition "Succeeded or Failed"
    Apr 21 20:58:41.196: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 20:58:41.201
    Apr 21 20:58:41.208: INFO: Waiting for pod pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0 to disappear
    Apr 21 20:58:41.209: INFO: Pod pod-projected-configmaps-d6c5522e-7da1-4a5d-92e6-234390c33db0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:58:41.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9451" for this suite. 04/21/23 20:58:41.211
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:58:41.214
Apr 21 20:58:41.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 20:58:41.215
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:41.222
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:41.223
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-1b47444f-5384-49d9-a1b5-834a79266d8e 04/21/23 20:58:41.225
STEP: Creating a pod to test consume secrets 04/21/23 20:58:41.228
Apr 21 20:58:41.233: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54" in namespace "projected-1003" to be "Succeeded or Failed"
Apr 21 20:58:41.234: INFO: Pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54": Phase="Pending", Reason="", readiness=false. Elapsed: 1.554283ms
Apr 21 20:58:43.237: INFO: Pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004244137s
Apr 21 20:58:45.237: INFO: Pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004219042s
STEP: Saw pod success 04/21/23 20:58:45.237
Apr 21 20:58:45.237: INFO: Pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54" satisfied condition "Succeeded or Failed"
Apr 21 20:58:45.239: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/21/23 20:58:45.244
Apr 21 20:58:45.252: INFO: Waiting for pod pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54 to disappear
Apr 21 20:58:45.253: INFO: Pod pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 21 20:58:45.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1003" for this suite. 04/21/23 20:58:45.255
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:58:41.214
    Apr 21 20:58:41.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 20:58:41.215
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:41.222
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:41.223
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-1b47444f-5384-49d9-a1b5-834a79266d8e 04/21/23 20:58:41.225
    STEP: Creating a pod to test consume secrets 04/21/23 20:58:41.228
    Apr 21 20:58:41.233: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54" in namespace "projected-1003" to be "Succeeded or Failed"
    Apr 21 20:58:41.234: INFO: Pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54": Phase="Pending", Reason="", readiness=false. Elapsed: 1.554283ms
    Apr 21 20:58:43.237: INFO: Pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004244137s
    Apr 21 20:58:45.237: INFO: Pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004219042s
    STEP: Saw pod success 04/21/23 20:58:45.237
    Apr 21 20:58:45.237: INFO: Pod "pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54" satisfied condition "Succeeded or Failed"
    Apr 21 20:58:45.239: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 20:58:45.244
    Apr 21 20:58:45.252: INFO: Waiting for pod pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54 to disappear
    Apr 21 20:58:45.253: INFO: Pod pod-projected-secrets-7c3e0a30-e651-4dcb-9ae3-5fb1efe4bc54 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:58:45.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1003" for this suite. 04/21/23 20:58:45.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:58:45.259
Apr 21 20:58:45.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-probe 04/21/23 20:58:45.259
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:45.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:45.267
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Apr 21 20:58:45.273: INFO: Waiting up to 5m0s for pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0" in namespace "container-probe-7480" to be "running and ready"
Apr 21 20:58:45.274: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.390747ms
Apr 21 20:58:45.274: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 20:58:47.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004055263s
Apr 21 20:58:47.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:58:49.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005103411s
Apr 21 20:58:49.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:58:51.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 6.003904685s
Apr 21 20:58:51.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:58:53.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004117508s
Apr 21 20:58:53.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:58:55.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004309582s
Apr 21 20:58:55.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:58:57.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005201043s
Apr 21 20:58:57.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:58:59.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 14.003778792s
Apr 21 20:58:59.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:59:01.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005418827s
Apr 21 20:59:01.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:59:03.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005435347s
Apr 21 20:59:03.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:59:05.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 20.003870628s
Apr 21 20:59:05.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
Apr 21 20:59:07.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005152883s
Apr 21 20:59:07.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = true)
Apr 21 20:59:07.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0" satisfied condition "running and ready"
Apr 21 20:59:07.280: INFO: Container started at 2023-04-21 20:58:45 +0000 UTC, pod became ready at 2023-04-21 20:59:05 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 21 20:59:07.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7480" for this suite. 04/21/23 20:59:07.282
------------------------------
â€¢ [SLOW TEST] [22.027 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:58:45.259
    Apr 21 20:58:45.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-probe 04/21/23 20:58:45.259
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:58:45.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:58:45.267
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Apr 21 20:58:45.273: INFO: Waiting up to 5m0s for pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0" in namespace "container-probe-7480" to be "running and ready"
    Apr 21 20:58:45.274: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.390747ms
    Apr 21 20:58:45.274: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 20:58:47.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 2.004055263s
    Apr 21 20:58:47.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:58:49.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005103411s
    Apr 21 20:58:49.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:58:51.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 6.003904685s
    Apr 21 20:58:51.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:58:53.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004117508s
    Apr 21 20:58:53.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:58:55.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004309582s
    Apr 21 20:58:55.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:58:57.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 12.005201043s
    Apr 21 20:58:57.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:58:59.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 14.003778792s
    Apr 21 20:58:59.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:59:01.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 16.005418827s
    Apr 21 20:59:01.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:59:03.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 18.005435347s
    Apr 21 20:59:03.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:59:05.277: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=false. Elapsed: 20.003870628s
    Apr 21 20:59:05.277: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = false)
    Apr 21 20:59:07.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0": Phase="Running", Reason="", readiness=true. Elapsed: 22.005152883s
    Apr 21 20:59:07.278: INFO: The phase of Pod test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0 is Running (Ready = true)
    Apr 21 20:59:07.278: INFO: Pod "test-webserver-8089ca96-72f3-42e3-8f31-6c71f9fe73f0" satisfied condition "running and ready"
    Apr 21 20:59:07.280: INFO: Container started at 2023-04-21 20:58:45 +0000 UTC, pod became ready at 2023-04-21 20:59:05 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:59:07.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7480" for this suite. 04/21/23 20:59:07.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:59:07.286
Apr 21 20:59:07.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename gc 04/21/23 20:59:07.287
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:59:07.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:59:07.297
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 04/21/23 20:59:07.298
STEP: Wait for the Deployment to create new ReplicaSet 04/21/23 20:59:07.301
STEP: delete the deployment 04/21/23 20:59:07.806
STEP: wait for all rs to be garbage collected 04/21/23 20:59:07.811
STEP: expected 0 rs, got 1 rs 04/21/23 20:59:07.815
STEP: expected 0 pods, got 2 pods 04/21/23 20:59:07.817
STEP: Gathering metrics 04/21/23 20:59:08.323
Apr 21 20:59:08.337: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
Apr 21 20:59:08.339: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 2.010169ms
Apr 21 20:59:08.339: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
Apr 21 20:59:08.339: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
Apr 21 20:59:08.395: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 21 20:59:08.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8180" for this suite. 04/21/23 20:59:08.398
------------------------------
â€¢ [1.115 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:59:07.286
    Apr 21 20:59:07.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename gc 04/21/23 20:59:07.287
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:59:07.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:59:07.297
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 04/21/23 20:59:07.298
    STEP: Wait for the Deployment to create new ReplicaSet 04/21/23 20:59:07.301
    STEP: delete the deployment 04/21/23 20:59:07.806
    STEP: wait for all rs to be garbage collected 04/21/23 20:59:07.811
    STEP: expected 0 rs, got 1 rs 04/21/23 20:59:07.815
    STEP: expected 0 pods, got 2 pods 04/21/23 20:59:07.817
    STEP: Gathering metrics 04/21/23 20:59:08.323
    Apr 21 20:59:08.337: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
    Apr 21 20:59:08.339: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 2.010169ms
    Apr 21 20:59:08.339: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
    Apr 21 20:59:08.339: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
    Apr 21 20:59:08.395: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:59:08.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8180" for this suite. 04/21/23 20:59:08.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:59:08.401
Apr 21 20:59:08.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename endpointslice 04/21/23 20:59:08.402
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:59:08.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:59:08.412
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Apr 21 20:59:08.419: INFO: Endpoints addresses: [192.168.49.2] , ports: [8443]
Apr 21 20:59:08.419: INFO: EndpointSlices addresses: [192.168.49.2] , ports: [8443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 21 20:59:08.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3408" for this suite. 04/21/23 20:59:08.421
------------------------------
â€¢ [0.022 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:59:08.401
    Apr 21 20:59:08.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename endpointslice 04/21/23 20:59:08.402
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:59:08.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:59:08.412
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Apr 21 20:59:08.419: INFO: Endpoints addresses: [192.168.49.2] , ports: [8443]
    Apr 21 20:59:08.419: INFO: EndpointSlices addresses: [192.168.49.2] , ports: [8443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:59:08.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3408" for this suite. 04/21/23 20:59:08.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:59:08.424
Apr 21 20:59:08.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 20:59:08.425
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:59:08.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:59:08.434
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 04/21/23 20:59:08.436
STEP: Getting a ResourceQuota 04/21/23 20:59:08.438
STEP: Updating a ResourceQuota 04/21/23 20:59:08.44
STEP: Verifying a ResourceQuota was modified 04/21/23 20:59:08.446
STEP: Deleting a ResourceQuota 04/21/23 20:59:08.447
STEP: Verifying the deleted ResourceQuota 04/21/23 20:59:08.452
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 20:59:08.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3649" for this suite. 04/21/23 20:59:08.456
------------------------------
â€¢ [0.034 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:59:08.424
    Apr 21 20:59:08.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 20:59:08.425
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:59:08.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:59:08.434
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 04/21/23 20:59:08.436
    STEP: Getting a ResourceQuota 04/21/23 20:59:08.438
    STEP: Updating a ResourceQuota 04/21/23 20:59:08.44
    STEP: Verifying a ResourceQuota was modified 04/21/23 20:59:08.446
    STEP: Deleting a ResourceQuota 04/21/23 20:59:08.447
    STEP: Verifying the deleted ResourceQuota 04/21/23 20:59:08.452
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 20:59:08.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3649" for this suite. 04/21/23 20:59:08.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 20:59:08.459
Apr 21 20:59:08.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename cronjob 04/21/23 20:59:08.46
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:59:08.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:59:08.468
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 04/21/23 20:59:08.47
STEP: Ensuring no jobs are scheduled 04/21/23 20:59:08.472
STEP: Ensuring no job exists by listing jobs explicitly 04/21/23 21:04:08.477
STEP: Removing cronjob 04/21/23 21:04:08.479
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:08.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-5473" for this suite. 04/21/23 21:04:08.484
------------------------------
â€¢ [SLOW TEST] [300.028 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 20:59:08.459
    Apr 21 20:59:08.459: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename cronjob 04/21/23 20:59:08.46
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 20:59:08.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 20:59:08.468
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 04/21/23 20:59:08.47
    STEP: Ensuring no jobs are scheduled 04/21/23 20:59:08.472
    STEP: Ensuring no job exists by listing jobs explicitly 04/21/23 21:04:08.477
    STEP: Removing cronjob 04/21/23 21:04:08.479
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:08.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-5473" for this suite. 04/21/23 21:04:08.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:08.488
Apr 21 21:04:08.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:04:08.489
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:08.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:08.503
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-ce9fff35-0795-4cae-8f85-cc0a8a10d9a6 04/21/23 21:04:08.505
STEP: Creating a pod to test consume secrets 04/21/23 21:04:08.508
Apr 21 21:04:08.512: INFO: Waiting up to 5m0s for pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a" in namespace "secrets-43" to be "Succeeded or Failed"
Apr 21 21:04:08.513: INFO: Pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.628332ms
Apr 21 21:04:10.517: INFO: Pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004779822s
Apr 21 21:04:12.517: INFO: Pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005625446s
STEP: Saw pod success 04/21/23 21:04:12.517
Apr 21 21:04:12.518: INFO: Pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a" satisfied condition "Succeeded or Failed"
Apr 21 21:04:12.519: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a container secret-env-test: <nil>
STEP: delete the pod 04/21/23 21:04:12.53
Apr 21 21:04:12.539: INFO: Waiting for pod pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a to disappear
Apr 21 21:04:12.541: INFO: Pod pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:12.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-43" for this suite. 04/21/23 21:04:12.543
------------------------------
â€¢ [4.057 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:08.488
    Apr 21 21:04:08.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:04:08.489
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:08.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:08.503
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-ce9fff35-0795-4cae-8f85-cc0a8a10d9a6 04/21/23 21:04:08.505
    STEP: Creating a pod to test consume secrets 04/21/23 21:04:08.508
    Apr 21 21:04:08.512: INFO: Waiting up to 5m0s for pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a" in namespace "secrets-43" to be "Succeeded or Failed"
    Apr 21 21:04:08.513: INFO: Pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.628332ms
    Apr 21 21:04:10.517: INFO: Pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004779822s
    Apr 21 21:04:12.517: INFO: Pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005625446s
    STEP: Saw pod success 04/21/23 21:04:12.517
    Apr 21 21:04:12.518: INFO: Pod "pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a" satisfied condition "Succeeded or Failed"
    Apr 21 21:04:12.519: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a container secret-env-test: <nil>
    STEP: delete the pod 04/21/23 21:04:12.53
    Apr 21 21:04:12.539: INFO: Waiting for pod pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a to disappear
    Apr 21 21:04:12.541: INFO: Pod pod-secrets-514022f2-8681-40e2-8af6-bb2b935f644a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:12.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-43" for this suite. 04/21/23 21:04:12.543
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:12.546
Apr 21 21:04:12.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:04:12.547
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:12.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:12.556
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 04/21/23 21:04:12.558
Apr 21 21:04:12.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 04/21/23 21:04:17.827
Apr 21 21:04:17.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:04:19.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:24.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2400" for this suite. 04/21/23 21:04:24.434
------------------------------
â€¢ [SLOW TEST] [11.892 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:12.546
    Apr 21 21:04:12.547: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:04:12.547
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:12.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:12.556
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 04/21/23 21:04:12.558
    Apr 21 21:04:12.559: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 04/21/23 21:04:17.827
    Apr 21 21:04:17.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:04:19.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:24.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2400" for this suite. 04/21/23 21:04:24.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:24.44
Apr 21 21:04:24.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 21:04:24.44
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:24.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:24.45
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 04/21/23 21:04:24.451
Apr 21 21:04:24.457: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242" in namespace "downward-api-3141" to be "Succeeded or Failed"
Apr 21 21:04:24.459: INFO: Pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242": Phase="Pending", Reason="", readiness=false. Elapsed: 1.778076ms
Apr 21 21:04:26.462: INFO: Pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00503073s
Apr 21 21:04:28.462: INFO: Pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004236919s
STEP: Saw pod success 04/21/23 21:04:28.462
Apr 21 21:04:28.462: INFO: Pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242" satisfied condition "Succeeded or Failed"
Apr 21 21:04:28.464: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242 container client-container: <nil>
STEP: delete the pod 04/21/23 21:04:28.469
Apr 21 21:04:28.477: INFO: Waiting for pod downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242 to disappear
Apr 21 21:04:28.478: INFO: Pod downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:28.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3141" for this suite. 04/21/23 21:04:28.482
------------------------------
â€¢ [4.046 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:24.44
    Apr 21 21:04:24.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 21:04:24.44
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:24.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:24.45
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 04/21/23 21:04:24.451
    Apr 21 21:04:24.457: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242" in namespace "downward-api-3141" to be "Succeeded or Failed"
    Apr 21 21:04:24.459: INFO: Pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242": Phase="Pending", Reason="", readiness=false. Elapsed: 1.778076ms
    Apr 21 21:04:26.462: INFO: Pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00503073s
    Apr 21 21:04:28.462: INFO: Pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004236919s
    STEP: Saw pod success 04/21/23 21:04:28.462
    Apr 21 21:04:28.462: INFO: Pod "downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242" satisfied condition "Succeeded or Failed"
    Apr 21 21:04:28.464: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242 container client-container: <nil>
    STEP: delete the pod 04/21/23 21:04:28.469
    Apr 21 21:04:28.477: INFO: Waiting for pod downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242 to disappear
    Apr 21 21:04:28.478: INFO: Pod downwardapi-volume-07cfd601-a20f-4aeb-9e30-903340b62242 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:28.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3141" for this suite. 04/21/23 21:04:28.482
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:28.486
Apr 21 21:04:28.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename disruption 04/21/23 21:04:28.486
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:28.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:28.497
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 04/21/23 21:04:28.499
STEP: Waiting for the pdb to be processed 04/21/23 21:04:28.502
STEP: First trying to evict a pod which shouldn't be evictable 04/21/23 21:04:30.51
STEP: Waiting for all pods to be running 04/21/23 21:04:30.51
Apr 21 21:04:30.512: INFO: pods: 0 < 3
STEP: locating a running pod 04/21/23 21:04:32.515
STEP: Updating the pdb to allow a pod to be evicted 04/21/23 21:04:32.52
STEP: Waiting for the pdb to be processed 04/21/23 21:04:32.527
STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/21/23 21:04:34.531
STEP: Waiting for all pods to be running 04/21/23 21:04:34.532
STEP: Waiting for the pdb to observed all healthy pods 04/21/23 21:04:34.534
STEP: Patching the pdb to disallow a pod to be evicted 04/21/23 21:04:34.55
STEP: Waiting for the pdb to be processed 04/21/23 21:04:34.56
STEP: Waiting for all pods to be running 04/21/23 21:04:36.566
STEP: locating a running pod 04/21/23 21:04:36.568
STEP: Deleting the pdb to allow a pod to be evicted 04/21/23 21:04:36.573
STEP: Waiting for the pdb to be deleted 04/21/23 21:04:36.576
STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/21/23 21:04:36.578
STEP: Waiting for all pods to be running 04/21/23 21:04:36.578
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:36.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2365" for this suite. 04/21/23 21:04:36.59
------------------------------
â€¢ [SLOW TEST] [8.109 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:28.486
    Apr 21 21:04:28.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename disruption 04/21/23 21:04:28.486
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:28.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:28.497
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 04/21/23 21:04:28.499
    STEP: Waiting for the pdb to be processed 04/21/23 21:04:28.502
    STEP: First trying to evict a pod which shouldn't be evictable 04/21/23 21:04:30.51
    STEP: Waiting for all pods to be running 04/21/23 21:04:30.51
    Apr 21 21:04:30.512: INFO: pods: 0 < 3
    STEP: locating a running pod 04/21/23 21:04:32.515
    STEP: Updating the pdb to allow a pod to be evicted 04/21/23 21:04:32.52
    STEP: Waiting for the pdb to be processed 04/21/23 21:04:32.527
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/21/23 21:04:34.531
    STEP: Waiting for all pods to be running 04/21/23 21:04:34.532
    STEP: Waiting for the pdb to observed all healthy pods 04/21/23 21:04:34.534
    STEP: Patching the pdb to disallow a pod to be evicted 04/21/23 21:04:34.55
    STEP: Waiting for the pdb to be processed 04/21/23 21:04:34.56
    STEP: Waiting for all pods to be running 04/21/23 21:04:36.566
    STEP: locating a running pod 04/21/23 21:04:36.568
    STEP: Deleting the pdb to allow a pod to be evicted 04/21/23 21:04:36.573
    STEP: Waiting for the pdb to be deleted 04/21/23 21:04:36.576
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/21/23 21:04:36.578
    STEP: Waiting for all pods to be running 04/21/23 21:04:36.578
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:36.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2365" for this suite. 04/21/23 21:04:36.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:36.596
Apr 21 21:04:36.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:04:36.597
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:36.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:36.606
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 04/21/23 21:04:36.608
Apr 21 21:04:36.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3214 create -f -'
Apr 21 21:04:36.990: INFO: stderr: ""
Apr 21 21:04:36.990: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/21/23 21:04:36.99
Apr 21 21:04:37.993: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 21 21:04:37.993: INFO: Found 0 / 1
Apr 21 21:04:38.993: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 21 21:04:38.993: INFO: Found 1 / 1
Apr 21 21:04:38.993: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 04/21/23 21:04:38.993
Apr 21 21:04:38.995: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 21 21:04:38.995: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 21 21:04:38.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3214 patch pod agnhost-primary-qqvrs -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 21 21:04:39.054: INFO: stderr: ""
Apr 21 21:04:39.054: INFO: stdout: "pod/agnhost-primary-qqvrs patched\n"
STEP: checking annotations 04/21/23 21:04:39.054
Apr 21 21:04:39.056: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 21 21:04:39.056: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:39.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3214" for this suite. 04/21/23 21:04:39.058
------------------------------
â€¢ [2.467 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:36.596
    Apr 21 21:04:36.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:04:36.597
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:36.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:36.606
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 04/21/23 21:04:36.608
    Apr 21 21:04:36.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3214 create -f -'
    Apr 21 21:04:36.990: INFO: stderr: ""
    Apr 21 21:04:36.990: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/21/23 21:04:36.99
    Apr 21 21:04:37.993: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 21 21:04:37.993: INFO: Found 0 / 1
    Apr 21 21:04:38.993: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 21 21:04:38.993: INFO: Found 1 / 1
    Apr 21 21:04:38.993: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 04/21/23 21:04:38.993
    Apr 21 21:04:38.995: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 21 21:04:38.995: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr 21 21:04:38.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3214 patch pod agnhost-primary-qqvrs -p {"metadata":{"annotations":{"x":"y"}}}'
    Apr 21 21:04:39.054: INFO: stderr: ""
    Apr 21 21:04:39.054: INFO: stdout: "pod/agnhost-primary-qqvrs patched\n"
    STEP: checking annotations 04/21/23 21:04:39.054
    Apr 21 21:04:39.056: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 21 21:04:39.056: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:39.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3214" for this suite. 04/21/23 21:04:39.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:39.063
Apr 21 21:04:39.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename events 04/21/23 21:04:39.064
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:39.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:39.075
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 04/21/23 21:04:39.077
Apr 21 21:04:39.080: INFO: created test-event-1
Apr 21 21:04:39.082: INFO: created test-event-2
Apr 21 21:04:39.086: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 04/21/23 21:04:39.086
STEP: delete collection of events 04/21/23 21:04:39.088
Apr 21 21:04:39.088: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 04/21/23 21:04:39.098
Apr 21 21:04:39.098: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:39.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6227" for this suite. 04/21/23 21:04:39.102
------------------------------
â€¢ [0.042 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:39.063
    Apr 21 21:04:39.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename events 04/21/23 21:04:39.064
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:39.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:39.075
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 04/21/23 21:04:39.077
    Apr 21 21:04:39.080: INFO: created test-event-1
    Apr 21 21:04:39.082: INFO: created test-event-2
    Apr 21 21:04:39.086: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 04/21/23 21:04:39.086
    STEP: delete collection of events 04/21/23 21:04:39.088
    Apr 21 21:04:39.088: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 04/21/23 21:04:39.098
    Apr 21 21:04:39.098: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:39.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6227" for this suite. 04/21/23 21:04:39.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:39.106
Apr 21 21:04:39.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 21:04:39.107
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:39.115
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:39.117
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 04/21/23 21:04:39.119
Apr 21 21:04:39.124: INFO: Waiting up to 5m0s for pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48" in namespace "downward-api-6344" to be "Succeeded or Failed"
Apr 21 21:04:39.126: INFO: Pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48": Phase="Pending", Reason="", readiness=false. Elapsed: 1.844277ms
Apr 21 21:04:41.128: INFO: Pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004646608s
Apr 21 21:04:43.130: INFO: Pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005791818s
STEP: Saw pod success 04/21/23 21:04:43.13
Apr 21 21:04:43.130: INFO: Pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48" satisfied condition "Succeeded or Failed"
Apr 21 21:04:43.132: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48 container dapi-container: <nil>
STEP: delete the pod 04/21/23 21:04:43.137
Apr 21 21:04:43.147: INFO: Waiting for pod downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48 to disappear
Apr 21 21:04:43.149: INFO: Pod downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:43.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6344" for this suite. 04/21/23 21:04:43.151
------------------------------
â€¢ [4.048 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:39.106
    Apr 21 21:04:39.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 21:04:39.107
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:39.115
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:39.117
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 04/21/23 21:04:39.119
    Apr 21 21:04:39.124: INFO: Waiting up to 5m0s for pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48" in namespace "downward-api-6344" to be "Succeeded or Failed"
    Apr 21 21:04:39.126: INFO: Pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48": Phase="Pending", Reason="", readiness=false. Elapsed: 1.844277ms
    Apr 21 21:04:41.128: INFO: Pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004646608s
    Apr 21 21:04:43.130: INFO: Pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005791818s
    STEP: Saw pod success 04/21/23 21:04:43.13
    Apr 21 21:04:43.130: INFO: Pod "downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48" satisfied condition "Succeeded or Failed"
    Apr 21 21:04:43.132: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48 container dapi-container: <nil>
    STEP: delete the pod 04/21/23 21:04:43.137
    Apr 21 21:04:43.147: INFO: Waiting for pod downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48 to disappear
    Apr 21 21:04:43.149: INFO: Pod downward-api-5de90726-35ec-4f0d-8c92-5f3a961d6b48 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:43.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6344" for this suite. 04/21/23 21:04:43.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:43.156
Apr 21 21:04:43.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:04:43.157
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:43.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:43.165
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 04/21/23 21:04:43.167
Apr 21 21:04:43.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 create -f -'
Apr 21 21:04:43.315: INFO: stderr: ""
Apr 21 21:04:43.315: INFO: stdout: "pod/pause created\n"
Apr 21 21:04:43.315: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 21 21:04:43.315: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5162" to be "running and ready"
Apr 21 21:04:43.317: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.793529ms
Apr 21 21:04:43.317: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8sconformance-m02' to be 'Running' but was 'Pending'
Apr 21 21:04:45.322: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007051053s
Apr 21 21:04:45.322: INFO: Pod "pause" satisfied condition "running and ready"
Apr 21 21:04:45.322: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 04/21/23 21:04:45.322
Apr 21 21:04:45.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 label pods pause testing-label=testing-label-value'
Apr 21 21:04:45.386: INFO: stderr: ""
Apr 21 21:04:45.386: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 04/21/23 21:04:45.386
Apr 21 21:04:45.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 get pod pause -L testing-label'
Apr 21 21:04:45.453: INFO: stderr: ""
Apr 21 21:04:45.453: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 04/21/23 21:04:45.453
Apr 21 21:04:45.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 label pods pause testing-label-'
Apr 21 21:04:45.515: INFO: stderr: ""
Apr 21 21:04:45.515: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 04/21/23 21:04:45.515
Apr 21 21:04:45.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 get pod pause -L testing-label'
Apr 21 21:04:45.570: INFO: stderr: ""
Apr 21 21:04:45.570: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 04/21/23 21:04:45.57
Apr 21 21:04:45.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 delete --grace-period=0 --force -f -'
Apr 21 21:04:45.631: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 21 21:04:45.631: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 21 21:04:45.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 get rc,svc -l name=pause --no-headers'
Apr 21 21:04:45.688: INFO: stderr: "No resources found in kubectl-5162 namespace.\n"
Apr 21 21:04:45.689: INFO: stdout: ""
Apr 21 21:04:45.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 21 21:04:45.743: INFO: stderr: ""
Apr 21 21:04:45.743: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:45.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5162" for this suite. 04/21/23 21:04:45.746
------------------------------
â€¢ [2.594 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:43.156
    Apr 21 21:04:43.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:04:43.157
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:43.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:43.165
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 04/21/23 21:04:43.167
    Apr 21 21:04:43.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 create -f -'
    Apr 21 21:04:43.315: INFO: stderr: ""
    Apr 21 21:04:43.315: INFO: stdout: "pod/pause created\n"
    Apr 21 21:04:43.315: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Apr 21 21:04:43.315: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-5162" to be "running and ready"
    Apr 21 21:04:43.317: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.793529ms
    Apr 21 21:04:43.317: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'k8sconformance-m02' to be 'Running' but was 'Pending'
    Apr 21 21:04:45.322: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007051053s
    Apr 21 21:04:45.322: INFO: Pod "pause" satisfied condition "running and ready"
    Apr 21 21:04:45.322: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 04/21/23 21:04:45.322
    Apr 21 21:04:45.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 label pods pause testing-label=testing-label-value'
    Apr 21 21:04:45.386: INFO: stderr: ""
    Apr 21 21:04:45.386: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 04/21/23 21:04:45.386
    Apr 21 21:04:45.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 get pod pause -L testing-label'
    Apr 21 21:04:45.453: INFO: stderr: ""
    Apr 21 21:04:45.453: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 04/21/23 21:04:45.453
    Apr 21 21:04:45.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 label pods pause testing-label-'
    Apr 21 21:04:45.515: INFO: stderr: ""
    Apr 21 21:04:45.515: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 04/21/23 21:04:45.515
    Apr 21 21:04:45.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 get pod pause -L testing-label'
    Apr 21 21:04:45.570: INFO: stderr: ""
    Apr 21 21:04:45.570: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 04/21/23 21:04:45.57
    Apr 21 21:04:45.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 delete --grace-period=0 --force -f -'
    Apr 21 21:04:45.631: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 21 21:04:45.631: INFO: stdout: "pod \"pause\" force deleted\n"
    Apr 21 21:04:45.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 get rc,svc -l name=pause --no-headers'
    Apr 21 21:04:45.688: INFO: stderr: "No resources found in kubectl-5162 namespace.\n"
    Apr 21 21:04:45.689: INFO: stdout: ""
    Apr 21 21:04:45.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5162 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr 21 21:04:45.743: INFO: stderr: ""
    Apr 21 21:04:45.743: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:45.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5162" for this suite. 04/21/23 21:04:45.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:45.75
Apr 21 21:04:45.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 21:04:45.751
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:45.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:45.761
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 21:04:45.77
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:04:46.329
STEP: Deploying the webhook pod 04/21/23 21:04:46.334
STEP: Wait for the deployment to be ready 04/21/23 21:04:46.343
Apr 21 21:04:46.347: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 04/21/23 21:04:48.354
STEP: Verifying the service has paired with the endpoint 04/21/23 21:04:48.362
Apr 21 21:04:49.362: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Apr 21 21:04:49.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1846-crds.webhook.example.com via the AdmissionRegistration API 04/21/23 21:04:49.875
STEP: Creating a custom resource that should be mutated by the webhook 04/21/23 21:04:49.887
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:52.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7345" for this suite. 04/21/23 21:04:52.466
STEP: Destroying namespace "webhook-7345-markers" for this suite. 04/21/23 21:04:52.472
------------------------------
â€¢ [SLOW TEST] [6.728 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:45.75
    Apr 21 21:04:45.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 21:04:45.751
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:45.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:45.761
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 21:04:45.77
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:04:46.329
    STEP: Deploying the webhook pod 04/21/23 21:04:46.334
    STEP: Wait for the deployment to be ready 04/21/23 21:04:46.343
    Apr 21 21:04:46.347: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 04/21/23 21:04:48.354
    STEP: Verifying the service has paired with the endpoint 04/21/23 21:04:48.362
    Apr 21 21:04:49.362: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Apr 21 21:04:49.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1846-crds.webhook.example.com via the AdmissionRegistration API 04/21/23 21:04:49.875
    STEP: Creating a custom resource that should be mutated by the webhook 04/21/23 21:04:49.887
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:52.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7345" for this suite. 04/21/23 21:04:52.466
    STEP: Destroying namespace "webhook-7345-markers" for this suite. 04/21/23 21:04:52.472
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:52.479
Apr 21 21:04:52.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 21:04:52.48
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:52.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:52.493
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 21:04:52.537
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:04:53.079
STEP: Deploying the webhook pod 04/21/23 21:04:53.085
STEP: Wait for the deployment to be ready 04/21/23 21:04:53.094
Apr 21 21:04:53.100: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 21:04:55.107
STEP: Verifying the service has paired with the endpoint 04/21/23 21:04:55.116
Apr 21 21:04:56.116: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 04/21/23 21:04:56.119
STEP: Creating a custom resource definition that should be denied by the webhook 04/21/23 21:04:56.131
Apr 21 21:04:56.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:04:56.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8687" for this suite. 04/21/23 21:04:56.168
STEP: Destroying namespace "webhook-8687-markers" for this suite. 04/21/23 21:04:56.172
------------------------------
â€¢ [3.698 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:52.479
    Apr 21 21:04:52.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 21:04:52.48
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:52.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:52.493
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 21:04:52.537
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:04:53.079
    STEP: Deploying the webhook pod 04/21/23 21:04:53.085
    STEP: Wait for the deployment to be ready 04/21/23 21:04:53.094
    Apr 21 21:04:53.100: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 21:04:55.107
    STEP: Verifying the service has paired with the endpoint 04/21/23 21:04:55.116
    Apr 21 21:04:56.116: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 04/21/23 21:04:56.119
    STEP: Creating a custom resource definition that should be denied by the webhook 04/21/23 21:04:56.131
    Apr 21 21:04:56.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:04:56.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8687" for this suite. 04/21/23 21:04:56.168
    STEP: Destroying namespace "webhook-8687-markers" for this suite. 04/21/23 21:04:56.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:04:56.177
Apr 21 21:04:56.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 21:04:56.178
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:56.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:56.189
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 04/21/23 21:05:13.194
STEP: Creating a ResourceQuota 04/21/23 21:05:18.197
STEP: Ensuring resource quota status is calculated 04/21/23 21:05:18.202
STEP: Creating a ConfigMap 04/21/23 21:05:20.206
STEP: Ensuring resource quota status captures configMap creation 04/21/23 21:05:20.214
STEP: Deleting a ConfigMap 04/21/23 21:05:22.217
STEP: Ensuring resource quota status released usage 04/21/23 21:05:22.222
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 21:05:24.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6070" for this suite. 04/21/23 21:05:24.227
------------------------------
â€¢ [SLOW TEST] [28.054 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:04:56.177
    Apr 21 21:04:56.177: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 21:04:56.178
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:04:56.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:04:56.189
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 04/21/23 21:05:13.194
    STEP: Creating a ResourceQuota 04/21/23 21:05:18.197
    STEP: Ensuring resource quota status is calculated 04/21/23 21:05:18.202
    STEP: Creating a ConfigMap 04/21/23 21:05:20.206
    STEP: Ensuring resource quota status captures configMap creation 04/21/23 21:05:20.214
    STEP: Deleting a ConfigMap 04/21/23 21:05:22.217
    STEP: Ensuring resource quota status released usage 04/21/23 21:05:22.222
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:05:24.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6070" for this suite. 04/21/23 21:05:24.227
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:05:24.231
Apr 21 21:05:24.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 21:05:24.232
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:24.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:24.242
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 04/21/23 21:05:24.244
Apr 21 21:05:24.244: INFO: Creating e2e-svc-a-7mccg
Apr 21 21:05:24.252: INFO: Creating e2e-svc-b-74f2b
Apr 21 21:05:24.260: INFO: Creating e2e-svc-c-h8fv4
STEP: deleting service collection 04/21/23 21:05:24.271
Apr 21 21:05:24.292: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 21:05:24.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5693" for this suite. 04/21/23 21:05:24.294
------------------------------
â€¢ [0.066 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:05:24.231
    Apr 21 21:05:24.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 21:05:24.232
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:24.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:24.242
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 04/21/23 21:05:24.244
    Apr 21 21:05:24.244: INFO: Creating e2e-svc-a-7mccg
    Apr 21 21:05:24.252: INFO: Creating e2e-svc-b-74f2b
    Apr 21 21:05:24.260: INFO: Creating e2e-svc-c-h8fv4
    STEP: deleting service collection 04/21/23 21:05:24.271
    Apr 21 21:05:24.292: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:05:24.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5693" for this suite. 04/21/23 21:05:24.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:05:24.298
Apr 21 21:05:24.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename job 04/21/23 21:05:24.299
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:24.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:24.31
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 04/21/23 21:05:24.333
STEP: Ensure pods equal to parallelism count is attached to the job 04/21/23 21:05:24.338
STEP: patching /status 04/21/23 21:05:26.343
STEP: updating /status 04/21/23 21:05:26.348
STEP: get /status 04/21/23 21:05:26.371
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 21 21:05:26.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6316" for this suite. 04/21/23 21:05:26.375
------------------------------
â€¢ [2.080 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:05:24.298
    Apr 21 21:05:24.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename job 04/21/23 21:05:24.299
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:24.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:24.31
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 04/21/23 21:05:24.333
    STEP: Ensure pods equal to parallelism count is attached to the job 04/21/23 21:05:24.338
    STEP: patching /status 04/21/23 21:05:26.343
    STEP: updating /status 04/21/23 21:05:26.348
    STEP: get /status 04/21/23 21:05:26.371
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:05:26.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6316" for this suite. 04/21/23 21:05:26.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:05:26.379
Apr 21 21:05:26.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:05:26.38
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:26.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:26.39
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 04/21/23 21:05:26.392
Apr 21 21:05:26.392: INFO: namespace kubectl-4343
Apr 21 21:05:26.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-4343 create -f -'
Apr 21 21:05:26.928: INFO: stderr: ""
Apr 21 21:05:26.928: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/21/23 21:05:26.928
Apr 21 21:05:27.931: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 21 21:05:27.931: INFO: Found 0 / 1
Apr 21 21:05:28.931: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 21 21:05:28.931: INFO: Found 1 / 1
Apr 21 21:05:28.931: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 21 21:05:28.933: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 21 21:05:28.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 21 21:05:28.933: INFO: wait on agnhost-primary startup in kubectl-4343 
Apr 21 21:05:28.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-4343 logs agnhost-primary-w6swc agnhost-primary'
Apr 21 21:05:28.994: INFO: stderr: ""
Apr 21 21:05:28.994: INFO: stdout: "Paused\n"
STEP: exposing RC 04/21/23 21:05:28.994
Apr 21 21:05:28.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-4343 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr 21 21:05:29.064: INFO: stderr: ""
Apr 21 21:05:29.064: INFO: stdout: "service/rm2 exposed\n"
Apr 21 21:05:29.066: INFO: Service rm2 in namespace kubectl-4343 found.
STEP: exposing service 04/21/23 21:05:31.071
Apr 21 21:05:31.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-4343 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr 21 21:05:31.136: INFO: stderr: ""
Apr 21 21:05:31.136: INFO: stdout: "service/rm3 exposed\n"
Apr 21 21:05:31.138: INFO: Service rm3 in namespace kubectl-4343 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:05:33.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4343" for this suite. 04/21/23 21:05:33.145
------------------------------
â€¢ [SLOW TEST] [6.770 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:05:26.379
    Apr 21 21:05:26.379: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:05:26.38
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:26.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:26.39
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 04/21/23 21:05:26.392
    Apr 21 21:05:26.392: INFO: namespace kubectl-4343
    Apr 21 21:05:26.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-4343 create -f -'
    Apr 21 21:05:26.928: INFO: stderr: ""
    Apr 21 21:05:26.928: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/21/23 21:05:26.928
    Apr 21 21:05:27.931: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 21 21:05:27.931: INFO: Found 0 / 1
    Apr 21 21:05:28.931: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 21 21:05:28.931: INFO: Found 1 / 1
    Apr 21 21:05:28.931: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Apr 21 21:05:28.933: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 21 21:05:28.933: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr 21 21:05:28.933: INFO: wait on agnhost-primary startup in kubectl-4343 
    Apr 21 21:05:28.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-4343 logs agnhost-primary-w6swc agnhost-primary'
    Apr 21 21:05:28.994: INFO: stderr: ""
    Apr 21 21:05:28.994: INFO: stdout: "Paused\n"
    STEP: exposing RC 04/21/23 21:05:28.994
    Apr 21 21:05:28.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-4343 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Apr 21 21:05:29.064: INFO: stderr: ""
    Apr 21 21:05:29.064: INFO: stdout: "service/rm2 exposed\n"
    Apr 21 21:05:29.066: INFO: Service rm2 in namespace kubectl-4343 found.
    STEP: exposing service 04/21/23 21:05:31.071
    Apr 21 21:05:31.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-4343 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Apr 21 21:05:31.136: INFO: stderr: ""
    Apr 21 21:05:31.136: INFO: stdout: "service/rm3 exposed\n"
    Apr 21 21:05:31.138: INFO: Service rm3 in namespace kubectl-4343 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:05:33.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4343" for this suite. 04/21/23 21:05:33.145
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:05:33.15
Apr 21 21:05:33.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 21:05:33.15
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:33.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:33.162
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 04/21/23 21:05:33.164
Apr 21 21:05:33.169: INFO: Waiting up to 5m0s for pod "pod-51896801-1448-4256-9102-74392fcf76be" in namespace "emptydir-3406" to be "Succeeded or Failed"
Apr 21 21:05:33.171: INFO: Pod "pod-51896801-1448-4256-9102-74392fcf76be": Phase="Pending", Reason="", readiness=false. Elapsed: 1.687295ms
Apr 21 21:05:35.174: INFO: Pod "pod-51896801-1448-4256-9102-74392fcf76be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004458011s
Apr 21 21:05:37.174: INFO: Pod "pod-51896801-1448-4256-9102-74392fcf76be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004740256s
STEP: Saw pod success 04/21/23 21:05:37.174
Apr 21 21:05:37.174: INFO: Pod "pod-51896801-1448-4256-9102-74392fcf76be" satisfied condition "Succeeded or Failed"
Apr 21 21:05:37.176: INFO: Trying to get logs from node k8sconformance-m02 pod pod-51896801-1448-4256-9102-74392fcf76be container test-container: <nil>
STEP: delete the pod 04/21/23 21:05:37.182
Apr 21 21:05:37.189: INFO: Waiting for pod pod-51896801-1448-4256-9102-74392fcf76be to disappear
Apr 21 21:05:37.191: INFO: Pod pod-51896801-1448-4256-9102-74392fcf76be no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:05:37.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3406" for this suite. 04/21/23 21:05:37.193
------------------------------
â€¢ [4.047 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:05:33.15
    Apr 21 21:05:33.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 21:05:33.15
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:33.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:33.162
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 04/21/23 21:05:33.164
    Apr 21 21:05:33.169: INFO: Waiting up to 5m0s for pod "pod-51896801-1448-4256-9102-74392fcf76be" in namespace "emptydir-3406" to be "Succeeded or Failed"
    Apr 21 21:05:33.171: INFO: Pod "pod-51896801-1448-4256-9102-74392fcf76be": Phase="Pending", Reason="", readiness=false. Elapsed: 1.687295ms
    Apr 21 21:05:35.174: INFO: Pod "pod-51896801-1448-4256-9102-74392fcf76be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004458011s
    Apr 21 21:05:37.174: INFO: Pod "pod-51896801-1448-4256-9102-74392fcf76be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004740256s
    STEP: Saw pod success 04/21/23 21:05:37.174
    Apr 21 21:05:37.174: INFO: Pod "pod-51896801-1448-4256-9102-74392fcf76be" satisfied condition "Succeeded or Failed"
    Apr 21 21:05:37.176: INFO: Trying to get logs from node k8sconformance-m02 pod pod-51896801-1448-4256-9102-74392fcf76be container test-container: <nil>
    STEP: delete the pod 04/21/23 21:05:37.182
    Apr 21 21:05:37.189: INFO: Waiting for pod pod-51896801-1448-4256-9102-74392fcf76be to disappear
    Apr 21 21:05:37.191: INFO: Pod pod-51896801-1448-4256-9102-74392fcf76be no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:05:37.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3406" for this suite. 04/21/23 21:05:37.193
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:05:37.197
Apr 21 21:05:37.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:05:37.198
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:37.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:37.208
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 04/21/23 21:05:37.209
Apr 21 21:05:37.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:05:38.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:05:43.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6470" for this suite. 04/21/23 21:05:43.986
------------------------------
â€¢ [SLOW TEST] [6.793 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:05:37.197
    Apr 21 21:05:37.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:05:37.198
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:37.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:37.208
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 04/21/23 21:05:37.209
    Apr 21 21:05:37.210: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:05:38.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:05:43.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6470" for this suite. 04/21/23 21:05:43.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:05:43.99
Apr 21 21:05:43.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename gc 04/21/23 21:05:43.991
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:44.002
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 04/21/23 21:05:44.006
STEP: create the rc2 04/21/23 21:05:44.009
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 04/21/23 21:05:49.041
STEP: delete the rc simpletest-rc-to-be-deleted 04/21/23 21:05:50.558
STEP: wait for the rc to be deleted 04/21/23 21:05:50.567
Apr 21 21:05:55.663: INFO: 64 pods remaining
Apr 21 21:05:55.663: INFO: 64 pods has nil DeletionTimestamp
Apr 21 21:05:55.663: INFO: 
STEP: Gathering metrics 04/21/23 21:06:00.58
Apr 21 21:06:01.047: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
Apr 21 21:06:01.050: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 2.233141ms
Apr 21 21:06:01.050: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
Apr 21 21:06:01.050: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
Apr 21 21:06:01.857: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 21 21:06:01.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-29vmn" in namespace "gc-874"
Apr 21 21:06:01.944: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fjwv" in namespace "gc-874"
Apr 21 21:06:01.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-2k9lz" in namespace "gc-874"
Apr 21 21:06:02.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zqbb" in namespace "gc-874"
Apr 21 21:06:02.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fl4t" in namespace "gc-874"
Apr 21 21:06:02.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hlbr" in namespace "gc-874"
Apr 21 21:06:02.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-4klv5" in namespace "gc-874"
Apr 21 21:06:02.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kwzh" in namespace "gc-874"
Apr 21 21:06:02.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ncrl" in namespace "gc-874"
Apr 21 21:06:02.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-4psdt" in namespace "gc-874"
Apr 21 21:06:02.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xhst" in namespace "gc-874"
Apr 21 21:06:02.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-5r8sq" in namespace "gc-874"
Apr 21 21:06:02.281: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p4v9" in namespace "gc-874"
Apr 21 21:06:02.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-6whzc" in namespace "gc-874"
Apr 21 21:06:02.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q6t2" in namespace "gc-874"
Apr 21 21:06:02.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mxrt" in namespace "gc-874"
Apr 21 21:06:02.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-94mpf" in namespace "gc-874"
Apr 21 21:06:02.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-982rm" in namespace "gc-874"
Apr 21 21:06:02.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p6fb" in namespace "gc-874"
Apr 21 21:06:02.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pfvn" in namespace "gc-874"
Apr 21 21:06:02.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pstn" in namespace "gc-874"
Apr 21 21:06:02.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-b68ms" in namespace "gc-874"
Apr 21 21:06:02.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6w5h" in namespace "gc-874"
Apr 21 21:06:02.566: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqzzm" in namespace "gc-874"
Apr 21 21:06:02.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvqfl" in namespace "gc-874"
Apr 21 21:06:02.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzhs6" in namespace "gc-874"
Apr 21 21:06:02.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2nwn" in namespace "gc-874"
Apr 21 21:06:02.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-f779c" in namespace "gc-874"
Apr 21 21:06:02.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7hkq" in namespace "gc-874"
Apr 21 21:06:02.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8gj2" in namespace "gc-874"
Apr 21 21:06:02.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-f98qr" in namespace "gc-874"
Apr 21 21:06:02.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9ddd" in namespace "gc-874"
Apr 21 21:06:02.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-ff5cb" in namespace "gc-874"
Apr 21 21:06:02.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgh6x" in namespace "gc-874"
Apr 21 21:06:02.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-fml5k" in namespace "gc-874"
Apr 21 21:06:02.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-fr42n" in namespace "gc-874"
Apr 21 21:06:02.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv87g" in namespace "gc-874"
Apr 21 21:06:02.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwgrt" in namespace "gc-874"
Apr 21 21:06:03.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggcll" in namespace "gc-874"
Apr 21 21:06:03.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqbgd" in namespace "gc-874"
Apr 21 21:06:03.062: INFO: Deleting pod "simpletest-rc-to-be-deleted-grtvl" in namespace "gc-874"
Apr 21 21:06:03.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwbc2" in namespace "gc-874"
Apr 21 21:06:03.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4jwg" in namespace "gc-874"
Apr 21 21:06:03.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsdlk" in namespace "gc-874"
Apr 21 21:06:03.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-j75mk" in namespace "gc-874"
Apr 21 21:06:03.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-j78fj" in namespace "gc-874"
Apr 21 21:06:03.252: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnlns" in namespace "gc-874"
Apr 21 21:06:03.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt8gg" in namespace "gc-874"
Apr 21 21:06:03.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtdlr" in namespace "gc-874"
Apr 21 21:06:03.342: INFO: Deleting pod "simpletest-rc-to-be-deleted-k24nq" in namespace "gc-874"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 21 21:06:03.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-874" for this suite. 04/21/23 21:06:03.364
------------------------------
â€¢ [SLOW TEST] [19.380 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:05:43.99
    Apr 21 21:05:43.990: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename gc 04/21/23 21:05:43.991
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:05:44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:05:44.002
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 04/21/23 21:05:44.006
    STEP: create the rc2 04/21/23 21:05:44.009
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 04/21/23 21:05:49.041
    STEP: delete the rc simpletest-rc-to-be-deleted 04/21/23 21:05:50.558
    STEP: wait for the rc to be deleted 04/21/23 21:05:50.567
    Apr 21 21:05:55.663: INFO: 64 pods remaining
    Apr 21 21:05:55.663: INFO: 64 pods has nil DeletionTimestamp
    Apr 21 21:05:55.663: INFO: 
    STEP: Gathering metrics 04/21/23 21:06:00.58
    Apr 21 21:06:01.047: INFO: Waiting up to 5m0s for pod "kube-controller-manager-k8sconformance" in namespace "kube-system" to be "running and ready"
    Apr 21 21:06:01.050: INFO: Pod "kube-controller-manager-k8sconformance": Phase="Running", Reason="", readiness=true. Elapsed: 2.233141ms
    Apr 21 21:06:01.050: INFO: The phase of Pod kube-controller-manager-k8sconformance is Running (Ready = true)
    Apr 21 21:06:01.050: INFO: Pod "kube-controller-manager-k8sconformance" satisfied condition "running and ready"
    Apr 21 21:06:01.857: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Apr 21 21:06:01.863: INFO: Deleting pod "simpletest-rc-to-be-deleted-29vmn" in namespace "gc-874"
    Apr 21 21:06:01.944: INFO: Deleting pod "simpletest-rc-to-be-deleted-2fjwv" in namespace "gc-874"
    Apr 21 21:06:01.953: INFO: Deleting pod "simpletest-rc-to-be-deleted-2k9lz" in namespace "gc-874"
    Apr 21 21:06:02.037: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zqbb" in namespace "gc-874"
    Apr 21 21:06:02.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fl4t" in namespace "gc-874"
    Apr 21 21:06:02.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-4hlbr" in namespace "gc-874"
    Apr 21 21:06:02.143: INFO: Deleting pod "simpletest-rc-to-be-deleted-4klv5" in namespace "gc-874"
    Apr 21 21:06:02.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kwzh" in namespace "gc-874"
    Apr 21 21:06:02.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ncrl" in namespace "gc-874"
    Apr 21 21:06:02.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-4psdt" in namespace "gc-874"
    Apr 21 21:06:02.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xhst" in namespace "gc-874"
    Apr 21 21:06:02.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-5r8sq" in namespace "gc-874"
    Apr 21 21:06:02.281: INFO: Deleting pod "simpletest-rc-to-be-deleted-6p4v9" in namespace "gc-874"
    Apr 21 21:06:02.288: INFO: Deleting pod "simpletest-rc-to-be-deleted-6whzc" in namespace "gc-874"
    Apr 21 21:06:02.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q6t2" in namespace "gc-874"
    Apr 21 21:06:02.347: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mxrt" in namespace "gc-874"
    Apr 21 21:06:02.353: INFO: Deleting pod "simpletest-rc-to-be-deleted-94mpf" in namespace "gc-874"
    Apr 21 21:06:02.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-982rm" in namespace "gc-874"
    Apr 21 21:06:02.376: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p6fb" in namespace "gc-874"
    Apr 21 21:06:02.443: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pfvn" in namespace "gc-874"
    Apr 21 21:06:02.458: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pstn" in namespace "gc-874"
    Apr 21 21:06:02.479: INFO: Deleting pod "simpletest-rc-to-be-deleted-b68ms" in namespace "gc-874"
    Apr 21 21:06:02.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-b6w5h" in namespace "gc-874"
    Apr 21 21:06:02.566: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqzzm" in namespace "gc-874"
    Apr 21 21:06:02.634: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvqfl" in namespace "gc-874"
    Apr 21 21:06:02.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzhs6" in namespace "gc-874"
    Apr 21 21:06:02.675: INFO: Deleting pod "simpletest-rc-to-be-deleted-f2nwn" in namespace "gc-874"
    Apr 21 21:06:02.732: INFO: Deleting pod "simpletest-rc-to-be-deleted-f779c" in namespace "gc-874"
    Apr 21 21:06:02.743: INFO: Deleting pod "simpletest-rc-to-be-deleted-f7hkq" in namespace "gc-874"
    Apr 21 21:06:02.757: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8gj2" in namespace "gc-874"
    Apr 21 21:06:02.773: INFO: Deleting pod "simpletest-rc-to-be-deleted-f98qr" in namespace "gc-874"
    Apr 21 21:06:02.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9ddd" in namespace "gc-874"
    Apr 21 21:06:02.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-ff5cb" in namespace "gc-874"
    Apr 21 21:06:02.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgh6x" in namespace "gc-874"
    Apr 21 21:06:02.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-fml5k" in namespace "gc-874"
    Apr 21 21:06:02.932: INFO: Deleting pod "simpletest-rc-to-be-deleted-fr42n" in namespace "gc-874"
    Apr 21 21:06:02.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv87g" in namespace "gc-874"
    Apr 21 21:06:02.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwgrt" in namespace "gc-874"
    Apr 21 21:06:03.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggcll" in namespace "gc-874"
    Apr 21 21:06:03.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-gqbgd" in namespace "gc-874"
    Apr 21 21:06:03.062: INFO: Deleting pod "simpletest-rc-to-be-deleted-grtvl" in namespace "gc-874"
    Apr 21 21:06:03.072: INFO: Deleting pod "simpletest-rc-to-be-deleted-gwbc2" in namespace "gc-874"
    Apr 21 21:06:03.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4jwg" in namespace "gc-874"
    Apr 21 21:06:03.157: INFO: Deleting pod "simpletest-rc-to-be-deleted-hsdlk" in namespace "gc-874"
    Apr 21 21:06:03.170: INFO: Deleting pod "simpletest-rc-to-be-deleted-j75mk" in namespace "gc-874"
    Apr 21 21:06:03.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-j78fj" in namespace "gc-874"
    Apr 21 21:06:03.252: INFO: Deleting pod "simpletest-rc-to-be-deleted-jnlns" in namespace "gc-874"
    Apr 21 21:06:03.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-jt8gg" in namespace "gc-874"
    Apr 21 21:06:03.283: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtdlr" in namespace "gc-874"
    Apr 21 21:06:03.342: INFO: Deleting pod "simpletest-rc-to-be-deleted-k24nq" in namespace "gc-874"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:06:03.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-874" for this suite. 04/21/23 21:06:03.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:06:03.372
Apr 21 21:06:03.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:06:03.373
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:06:03.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:06:03.443
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-489716fa-890e-40f8-86de-6b27c6c46ec1 04/21/23 21:06:03.461
STEP: Creating secret with name s-test-opt-upd-472f5950-d5a4-4acc-bf1b-c9515c639ac9 04/21/23 21:06:03.466
STEP: Creating the pod 04/21/23 21:06:03.472
Apr 21 21:06:03.540: INFO: Waiting up to 5m0s for pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b" in namespace "secrets-8435" to be "running and ready"
Apr 21 21:06:03.548: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.380473ms
Apr 21 21:06:03.548: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:06:05.553: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013174877s
Apr 21 21:06:05.553: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:06:07.554: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014079295s
Apr 21 21:06:07.554: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:06:09.551: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011694387s
Apr 21 21:06:09.551: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:06:11.551: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Running", Reason="", readiness=true. Elapsed: 8.011818929s
Apr 21 21:06:11.551: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Running (Ready = true)
Apr 21 21:06:11.551: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-489716fa-890e-40f8-86de-6b27c6c46ec1 04/21/23 21:06:11.568
STEP: Updating secret s-test-opt-upd-472f5950-d5a4-4acc-bf1b-c9515c639ac9 04/21/23 21:06:11.572
STEP: Creating secret with name s-test-opt-create-55996ce1-51b6-4472-97c0-263ba55ed894 04/21/23 21:06:11.576
STEP: waiting to observe update in volume 04/21/23 21:06:11.579
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:07:27.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8435" for this suite. 04/21/23 21:07:27.875
------------------------------
â€¢ [SLOW TEST] [84.507 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:06:03.372
    Apr 21 21:06:03.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:06:03.373
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:06:03.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:06:03.443
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-489716fa-890e-40f8-86de-6b27c6c46ec1 04/21/23 21:06:03.461
    STEP: Creating secret with name s-test-opt-upd-472f5950-d5a4-4acc-bf1b-c9515c639ac9 04/21/23 21:06:03.466
    STEP: Creating the pod 04/21/23 21:06:03.472
    Apr 21 21:06:03.540: INFO: Waiting up to 5m0s for pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b" in namespace "secrets-8435" to be "running and ready"
    Apr 21 21:06:03.548: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.380473ms
    Apr 21 21:06:03.548: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:06:05.553: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013174877s
    Apr 21 21:06:05.553: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:06:07.554: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014079295s
    Apr 21 21:06:07.554: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:06:09.551: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.011694387s
    Apr 21 21:06:09.551: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:06:11.551: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b": Phase="Running", Reason="", readiness=true. Elapsed: 8.011818929s
    Apr 21 21:06:11.551: INFO: The phase of Pod pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b is Running (Ready = true)
    Apr 21 21:06:11.551: INFO: Pod "pod-secrets-dc6a950a-68fa-45fe-964c-b012cb46ea5b" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-489716fa-890e-40f8-86de-6b27c6c46ec1 04/21/23 21:06:11.568
    STEP: Updating secret s-test-opt-upd-472f5950-d5a4-4acc-bf1b-c9515c639ac9 04/21/23 21:06:11.572
    STEP: Creating secret with name s-test-opt-create-55996ce1-51b6-4472-97c0-263ba55ed894 04/21/23 21:06:11.576
    STEP: waiting to observe update in volume 04/21/23 21:06:11.579
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:07:27.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8435" for this suite. 04/21/23 21:07:27.875
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:07:27.879
Apr 21 21:07:27.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 21:07:27.88
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:07:27.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:07:27.889
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-8445 04/21/23 21:07:27.891
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/21/23 21:07:27.901
STEP: creating service externalsvc in namespace services-8445 04/21/23 21:07:27.901
STEP: creating replication controller externalsvc in namespace services-8445 04/21/23 21:07:27.91
I0421 21:07:27.915834      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8445, replica count: 2
I0421 21:07:30.967692      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 04/21/23 21:07:30.97
Apr 21 21:07:30.984: INFO: Creating new exec pod
Apr 21 21:07:30.991: INFO: Waiting up to 5m0s for pod "execpodwvzmz" in namespace "services-8445" to be "running"
Apr 21 21:07:30.993: INFO: Pod "execpodwvzmz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243244ms
Apr 21 21:07:32.995: INFO: Pod "execpodwvzmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004609265s
Apr 21 21:07:32.995: INFO: Pod "execpodwvzmz" satisfied condition "running"
Apr 21 21:07:32.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8445 exec execpodwvzmz -- /bin/sh -x -c nslookup nodeport-service.services-8445.svc.cluster.local'
Apr 21 21:07:33.186: INFO: stderr: "+ nslookup nodeport-service.services-8445.svc.cluster.local\n"
Apr 21 21:07:33.186: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8445.svc.cluster.local\tcanonical name = externalsvc.services-8445.svc.cluster.local.\nName:\texternalsvc.services-8445.svc.cluster.local\nAddress: 10.97.14.120\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-8445, will wait for the garbage collector to delete the pods 04/21/23 21:07:33.186
Apr 21 21:07:33.243: INFO: Deleting ReplicationController externalsvc took: 4.231533ms
Apr 21 21:07:33.343: INFO: Terminating ReplicationController externalsvc pods took: 100.64523ms
Apr 21 21:07:34.957: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 21:07:34.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8445" for this suite. 04/21/23 21:07:34.965
------------------------------
â€¢ [SLOW TEST] [7.091 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:07:27.879
    Apr 21 21:07:27.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 21:07:27.88
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:07:27.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:07:27.889
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-8445 04/21/23 21:07:27.891
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/21/23 21:07:27.901
    STEP: creating service externalsvc in namespace services-8445 04/21/23 21:07:27.901
    STEP: creating replication controller externalsvc in namespace services-8445 04/21/23 21:07:27.91
    I0421 21:07:27.915834      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-8445, replica count: 2
    I0421 21:07:30.967692      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 04/21/23 21:07:30.97
    Apr 21 21:07:30.984: INFO: Creating new exec pod
    Apr 21 21:07:30.991: INFO: Waiting up to 5m0s for pod "execpodwvzmz" in namespace "services-8445" to be "running"
    Apr 21 21:07:30.993: INFO: Pod "execpodwvzmz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243244ms
    Apr 21 21:07:32.995: INFO: Pod "execpodwvzmz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004609265s
    Apr 21 21:07:32.995: INFO: Pod "execpodwvzmz" satisfied condition "running"
    Apr 21 21:07:32.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8445 exec execpodwvzmz -- /bin/sh -x -c nslookup nodeport-service.services-8445.svc.cluster.local'
    Apr 21 21:07:33.186: INFO: stderr: "+ nslookup nodeport-service.services-8445.svc.cluster.local\n"
    Apr 21 21:07:33.186: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8445.svc.cluster.local\tcanonical name = externalsvc.services-8445.svc.cluster.local.\nName:\texternalsvc.services-8445.svc.cluster.local\nAddress: 10.97.14.120\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-8445, will wait for the garbage collector to delete the pods 04/21/23 21:07:33.186
    Apr 21 21:07:33.243: INFO: Deleting ReplicationController externalsvc took: 4.231533ms
    Apr 21 21:07:33.343: INFO: Terminating ReplicationController externalsvc pods took: 100.64523ms
    Apr 21 21:07:34.957: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:07:34.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8445" for this suite. 04/21/23 21:07:34.965
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:07:34.971
Apr 21 21:07:34.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 21:07:34.972
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:07:34.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:07:34.981
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 21:07:34.992
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:07:35.228
STEP: Deploying the webhook pod 04/21/23 21:07:35.235
STEP: Wait for the deployment to be ready 04/21/23 21:07:35.244
Apr 21 21:07:35.251: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 21:07:37.258
STEP: Verifying the service has paired with the endpoint 04/21/23 21:07:37.267
Apr 21 21:07:38.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 04/21/23 21:07:38.27
STEP: create a pod 04/21/23 21:07:38.281
Apr 21 21:07:38.286: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9929" to be "running"
Apr 21 21:07:38.288: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.981835ms
Apr 21 21:07:40.291: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004214094s
Apr 21 21:07:40.291: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 04/21/23 21:07:40.291
Apr 21 21:07:40.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=webhook-9929 attach --namespace=webhook-9929 to-be-attached-pod -i -c=container1'
Apr 21 21:07:40.354: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:07:40.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9929" for this suite. 04/21/23 21:07:40.386
STEP: Destroying namespace "webhook-9929-markers" for this suite. 04/21/23 21:07:40.39
------------------------------
â€¢ [SLOW TEST] [5.425 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:07:34.971
    Apr 21 21:07:34.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 21:07:34.972
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:07:34.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:07:34.981
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 21:07:34.992
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:07:35.228
    STEP: Deploying the webhook pod 04/21/23 21:07:35.235
    STEP: Wait for the deployment to be ready 04/21/23 21:07:35.244
    Apr 21 21:07:35.251: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 21:07:37.258
    STEP: Verifying the service has paired with the endpoint 04/21/23 21:07:37.267
    Apr 21 21:07:38.267: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 04/21/23 21:07:38.27
    STEP: create a pod 04/21/23 21:07:38.281
    Apr 21 21:07:38.286: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-9929" to be "running"
    Apr 21 21:07:38.288: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.981835ms
    Apr 21 21:07:40.291: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004214094s
    Apr 21 21:07:40.291: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 04/21/23 21:07:40.291
    Apr 21 21:07:40.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=webhook-9929 attach --namespace=webhook-9929 to-be-attached-pod -i -c=container1'
    Apr 21 21:07:40.354: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:07:40.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9929" for this suite. 04/21/23 21:07:40.386
    STEP: Destroying namespace "webhook-9929-markers" for this suite. 04/21/23 21:07:40.39
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:07:40.396
Apr 21 21:07:40.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replication-controller 04/21/23 21:07:40.396
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:07:40.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:07:40.408
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-rx5c9" 04/21/23 21:07:40.41
Apr 21 21:07:40.418: INFO: Get Replication Controller "e2e-rc-rx5c9" to confirm replicas
Apr 21 21:07:41.420: INFO: Get Replication Controller "e2e-rc-rx5c9" to confirm replicas
Apr 21 21:07:41.423: INFO: Found 1 replicas for "e2e-rc-rx5c9" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-rx5c9" 04/21/23 21:07:41.423
STEP: Updating a scale subresource 04/21/23 21:07:41.425
STEP: Verifying replicas where modified for replication controller "e2e-rc-rx5c9" 04/21/23 21:07:41.43
Apr 21 21:07:41.430: INFO: Get Replication Controller "e2e-rc-rx5c9" to confirm replicas
Apr 21 21:07:42.433: INFO: Get Replication Controller "e2e-rc-rx5c9" to confirm replicas
Apr 21 21:07:42.435: INFO: Found 2 replicas for "e2e-rc-rx5c9" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 21 21:07:42.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9528" for this suite. 04/21/23 21:07:42.438
------------------------------
â€¢ [2.046 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:07:40.396
    Apr 21 21:07:40.396: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replication-controller 04/21/23 21:07:40.396
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:07:40.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:07:40.408
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-rx5c9" 04/21/23 21:07:40.41
    Apr 21 21:07:40.418: INFO: Get Replication Controller "e2e-rc-rx5c9" to confirm replicas
    Apr 21 21:07:41.420: INFO: Get Replication Controller "e2e-rc-rx5c9" to confirm replicas
    Apr 21 21:07:41.423: INFO: Found 1 replicas for "e2e-rc-rx5c9" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-rx5c9" 04/21/23 21:07:41.423
    STEP: Updating a scale subresource 04/21/23 21:07:41.425
    STEP: Verifying replicas where modified for replication controller "e2e-rc-rx5c9" 04/21/23 21:07:41.43
    Apr 21 21:07:41.430: INFO: Get Replication Controller "e2e-rc-rx5c9" to confirm replicas
    Apr 21 21:07:42.433: INFO: Get Replication Controller "e2e-rc-rx5c9" to confirm replicas
    Apr 21 21:07:42.435: INFO: Found 2 replicas for "e2e-rc-rx5c9" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:07:42.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9528" for this suite. 04/21/23 21:07:42.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:07:42.442
Apr 21 21:07:42.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename dns 04/21/23 21:07:42.443
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:07:42.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:07:42.454
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 04/21/23 21:07:42.456
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7011 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7011;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7011 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7011;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7011.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7011.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7011.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7011.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7011.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7011.svc;check="$$(dig +notcp +noall +answer +search 54.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.54_tcp@PTR;sleep 1; done
 04/21/23 21:07:42.471
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7011 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7011;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7011 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7011;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7011.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7011.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7011.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7011.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7011.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7011.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7011.svc;check="$$(dig +notcp +noall +answer +search 54.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.54_tcp@PTR;sleep 1; done
 04/21/23 21:07:42.471
STEP: creating a pod to probe DNS 04/21/23 21:07:42.471
STEP: submitting the pod to kubernetes 04/21/23 21:07:42.472
Apr 21 21:07:42.479: INFO: Waiting up to 15m0s for pod "dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6" in namespace "dns-7011" to be "running"
Apr 21 21:07:42.482: INFO: Pod "dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.391685ms
Apr 21 21:07:44.484: INFO: Pod "dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004854764s
Apr 21 21:07:44.484: INFO: Pod "dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6" satisfied condition "running"
STEP: retrieving the pod 04/21/23 21:07:44.484
STEP: looking for the results for each expected name from probers 04/21/23 21:07:44.486
Apr 21 21:07:44.489: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.491: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.493: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.494: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.496: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.497: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.499: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.500: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.508: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.510: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.511: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.513: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.514: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.516: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.519: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.521: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:44.528: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

Apr 21 21:07:49.531: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.534: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.536: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.539: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.550: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.552: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.553: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.555: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.556: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.558: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.560: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.561: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:49.568: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

Apr 21 21:07:54.531: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.541: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.543: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.545: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.554: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.555: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.557: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.559: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.560: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.562: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.564: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.566: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:54.572: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

Apr 21 21:07:59.530: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.542: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.551: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.553: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.554: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.556: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.557: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.559: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.560: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.562: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:07:59.568: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

Apr 21 21:08:04.531: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.551: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.553: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.554: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.556: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.557: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.559: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.560: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.562: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:04.568: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

Apr 21 21:08:09.531: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.541: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.543: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.544: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.553: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.554: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.556: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.558: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.559: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.561: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.563: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.565: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
Apr 21 21:08:09.573: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

Apr 21 21:08:14.568: INFO: DNS probes using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 succeeded

STEP: deleting the pod 04/21/23 21:08:14.568
STEP: deleting the test service 04/21/23 21:08:14.577
STEP: deleting the test headless service 04/21/23 21:08:14.594
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 21 21:08:14.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7011" for this suite. 04/21/23 21:08:14.605
------------------------------
â€¢ [SLOW TEST] [32.166 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:07:42.442
    Apr 21 21:07:42.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename dns 04/21/23 21:07:42.443
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:07:42.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:07:42.454
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 04/21/23 21:07:42.456
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7011 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7011;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7011 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7011;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7011.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7011.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7011.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7011.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7011.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7011.svc;check="$$(dig +notcp +noall +answer +search 54.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.54_tcp@PTR;sleep 1; done
     04/21/23 21:07:42.471
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7011 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7011;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7011 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7011;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7011.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7011.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7011.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7011.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7011.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7011.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7011.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7011.svc;check="$$(dig +notcp +noall +answer +search 54.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.54_udp@PTR;check="$$(dig +tcp +noall +answer +search 54.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.54_tcp@PTR;sleep 1; done
     04/21/23 21:07:42.471
    STEP: creating a pod to probe DNS 04/21/23 21:07:42.471
    STEP: submitting the pod to kubernetes 04/21/23 21:07:42.472
    Apr 21 21:07:42.479: INFO: Waiting up to 15m0s for pod "dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6" in namespace "dns-7011" to be "running"
    Apr 21 21:07:42.482: INFO: Pod "dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.391685ms
    Apr 21 21:07:44.484: INFO: Pod "dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.004854764s
    Apr 21 21:07:44.484: INFO: Pod "dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6" satisfied condition "running"
    STEP: retrieving the pod 04/21/23 21:07:44.484
    STEP: looking for the results for each expected name from probers 04/21/23 21:07:44.486
    Apr 21 21:07:44.489: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.491: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.493: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.494: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.496: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.497: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.499: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.500: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.508: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.510: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.511: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.513: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.514: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.516: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.519: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.521: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:44.528: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

    Apr 21 21:07:49.531: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.534: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.536: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.539: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.550: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.552: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.553: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.555: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.556: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.558: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.560: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.561: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:49.568: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

    Apr 21 21:07:54.531: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.541: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.543: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.545: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.554: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.555: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.557: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.559: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.560: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.562: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.564: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.566: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:54.572: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

    Apr 21 21:07:59.530: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.542: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.551: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.553: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.554: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.556: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.557: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.559: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.560: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.562: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:07:59.568: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

    Apr 21 21:08:04.531: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.538: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.540: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.543: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.551: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.553: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.554: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.556: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.557: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.559: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.560: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.562: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:04.568: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

    Apr 21 21:08:09.531: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.533: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.537: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.539: INFO: Unable to read wheezy_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.541: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.543: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.544: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.553: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.554: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.556: INFO: Unable to read jessie_udp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.558: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011 from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.559: INFO: Unable to read jessie_udp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.561: INFO: Unable to read jessie_tcp@dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.563: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.565: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc from pod dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6: the server could not find the requested resource (get pods dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6)
    Apr 21 21:08:09.573: INFO: Lookups using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7011 wheezy_tcp@dns-test-service.dns-7011 wheezy_udp@dns-test-service.dns-7011.svc wheezy_tcp@dns-test-service.dns-7011.svc wheezy_udp@_http._tcp.dns-test-service.dns-7011.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7011.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7011 jessie_tcp@dns-test-service.dns-7011 jessie_udp@dns-test-service.dns-7011.svc jessie_tcp@dns-test-service.dns-7011.svc jessie_udp@_http._tcp.dns-test-service.dns-7011.svc jessie_tcp@_http._tcp.dns-test-service.dns-7011.svc]

    Apr 21 21:08:14.568: INFO: DNS probes using dns-7011/dns-test-ea1d5674-9355-40c3-9aeb-f1c766e627b6 succeeded

    STEP: deleting the pod 04/21/23 21:08:14.568
    STEP: deleting the test service 04/21/23 21:08:14.577
    STEP: deleting the test headless service 04/21/23 21:08:14.594
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:08:14.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7011" for this suite. 04/21/23 21:08:14.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:08:14.609
Apr 21 21:08:14.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename podtemplate 04/21/23 21:08:14.61
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:14.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:14.639
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 04/21/23 21:08:14.641
Apr 21 21:08:14.647: INFO: created test-podtemplate-1
Apr 21 21:08:14.650: INFO: created test-podtemplate-2
Apr 21 21:08:14.656: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 04/21/23 21:08:14.656
STEP: delete collection of pod templates 04/21/23 21:08:14.658
Apr 21 21:08:14.658: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 04/21/23 21:08:14.667
Apr 21 21:08:14.667: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Apr 21 21:08:14.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-4626" for this suite. 04/21/23 21:08:14.671
------------------------------
â€¢ [0.065 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:08:14.609
    Apr 21 21:08:14.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename podtemplate 04/21/23 21:08:14.61
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:14.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:14.639
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 04/21/23 21:08:14.641
    Apr 21 21:08:14.647: INFO: created test-podtemplate-1
    Apr 21 21:08:14.650: INFO: created test-podtemplate-2
    Apr 21 21:08:14.656: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 04/21/23 21:08:14.656
    STEP: delete collection of pod templates 04/21/23 21:08:14.658
    Apr 21 21:08:14.658: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 04/21/23 21:08:14.667
    Apr 21 21:08:14.667: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:08:14.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-4626" for this suite. 04/21/23 21:08:14.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:08:14.677
Apr 21 21:08:14.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:08:14.678
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:14.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:14.688
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 04/21/23 21:08:14.69
Apr 21 21:08:14.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4" in namespace "projected-9340" to be "Succeeded or Failed"
Apr 21 21:08:14.697: INFO: Pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.741969ms
Apr 21 21:08:16.701: INFO: Pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005245604s
Apr 21 21:08:18.701: INFO: Pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005474607s
STEP: Saw pod success 04/21/23 21:08:18.701
Apr 21 21:08:18.701: INFO: Pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4" satisfied condition "Succeeded or Failed"
Apr 21 21:08:18.703: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4 container client-container: <nil>
STEP: delete the pod 04/21/23 21:08:18.708
Apr 21 21:08:18.716: INFO: Waiting for pod downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4 to disappear
Apr 21 21:08:18.718: INFO: Pod downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 21:08:18.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9340" for this suite. 04/21/23 21:08:18.72
------------------------------
â€¢ [4.047 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:08:14.677
    Apr 21 21:08:14.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:08:14.678
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:14.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:14.688
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 04/21/23 21:08:14.69
    Apr 21 21:08:14.696: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4" in namespace "projected-9340" to be "Succeeded or Failed"
    Apr 21 21:08:14.697: INFO: Pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.741969ms
    Apr 21 21:08:16.701: INFO: Pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005245604s
    Apr 21 21:08:18.701: INFO: Pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005474607s
    STEP: Saw pod success 04/21/23 21:08:18.701
    Apr 21 21:08:18.701: INFO: Pod "downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4" satisfied condition "Succeeded or Failed"
    Apr 21 21:08:18.703: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4 container client-container: <nil>
    STEP: delete the pod 04/21/23 21:08:18.708
    Apr 21 21:08:18.716: INFO: Waiting for pod downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4 to disappear
    Apr 21 21:08:18.718: INFO: Pod downwardapi-volume-c80cf7d0-31d8-4f04-ad9b-5a4b9533d7c4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:08:18.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9340" for this suite. 04/21/23 21:08:18.72
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:08:18.724
Apr 21 21:08:18.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:08:18.725
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:18.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:18.738
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-239520fa-e465-4f66-8ae1-36a90f627e09 04/21/23 21:08:18.74
STEP: Creating a pod to test consume configMaps 04/21/23 21:08:18.743
Apr 21 21:08:18.750: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a" in namespace "projected-2915" to be "Succeeded or Failed"
Apr 21 21:08:18.752: INFO: Pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37299ms
Apr 21 21:08:20.755: INFO: Pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005262661s
Apr 21 21:08:22.756: INFO: Pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005751566s
STEP: Saw pod success 04/21/23 21:08:22.756
Apr 21 21:08:22.756: INFO: Pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a" satisfied condition "Succeeded or Failed"
Apr 21 21:08:22.758: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a container agnhost-container: <nil>
STEP: delete the pod 04/21/23 21:08:22.763
Apr 21 21:08:22.771: INFO: Waiting for pod pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a to disappear
Apr 21 21:08:22.772: INFO: Pod pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:08:22.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2915" for this suite. 04/21/23 21:08:22.774
------------------------------
â€¢ [4.054 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:08:18.724
    Apr 21 21:08:18.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:08:18.725
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:18.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:18.738
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-239520fa-e465-4f66-8ae1-36a90f627e09 04/21/23 21:08:18.74
    STEP: Creating a pod to test consume configMaps 04/21/23 21:08:18.743
    Apr 21 21:08:18.750: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a" in namespace "projected-2915" to be "Succeeded or Failed"
    Apr 21 21:08:18.752: INFO: Pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37299ms
    Apr 21 21:08:20.755: INFO: Pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005262661s
    Apr 21 21:08:22.756: INFO: Pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005751566s
    STEP: Saw pod success 04/21/23 21:08:22.756
    Apr 21 21:08:22.756: INFO: Pod "pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a" satisfied condition "Succeeded or Failed"
    Apr 21 21:08:22.758: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 21:08:22.763
    Apr 21 21:08:22.771: INFO: Waiting for pod pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a to disappear
    Apr 21 21:08:22.772: INFO: Pod pod-projected-configmaps-113128b9-5f6e-438f-92e1-a2a4aec3f34a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:08:22.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2915" for this suite. 04/21/23 21:08:22.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:08:22.778
Apr 21 21:08:22.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename containers 04/21/23 21:08:22.779
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:22.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:22.789
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 04/21/23 21:08:22.791
Apr 21 21:08:22.795: INFO: Waiting up to 5m0s for pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2" in namespace "containers-3012" to be "Succeeded or Failed"
Apr 21 21:08:22.797: INFO: Pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.682858ms
Apr 21 21:08:24.800: INFO: Pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005108444s
Apr 21 21:08:26.799: INFO: Pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003998461s
STEP: Saw pod success 04/21/23 21:08:26.799
Apr 21 21:08:26.799: INFO: Pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2" satisfied condition "Succeeded or Failed"
Apr 21 21:08:26.801: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 21:08:26.807
Apr 21 21:08:26.815: INFO: Waiting for pod client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2 to disappear
Apr 21 21:08:26.816: INFO: Pod client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 21 21:08:26.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3012" for this suite. 04/21/23 21:08:26.818
------------------------------
â€¢ [4.043 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:08:22.778
    Apr 21 21:08:22.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename containers 04/21/23 21:08:22.779
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:22.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:22.789
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 04/21/23 21:08:22.791
    Apr 21 21:08:22.795: INFO: Waiting up to 5m0s for pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2" in namespace "containers-3012" to be "Succeeded or Failed"
    Apr 21 21:08:22.797: INFO: Pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.682858ms
    Apr 21 21:08:24.800: INFO: Pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005108444s
    Apr 21 21:08:26.799: INFO: Pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003998461s
    STEP: Saw pod success 04/21/23 21:08:26.799
    Apr 21 21:08:26.799: INFO: Pod "client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2" satisfied condition "Succeeded or Failed"
    Apr 21 21:08:26.801: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 21:08:26.807
    Apr 21 21:08:26.815: INFO: Waiting for pod client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2 to disappear
    Apr 21 21:08:26.816: INFO: Pod client-containers-48ef450b-d62b-4a5d-94cd-df5d134593e2 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:08:26.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3012" for this suite. 04/21/23 21:08:26.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:08:26.822
Apr 21 21:08:26.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename subpath 04/21/23 21:08:26.822
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:26.831
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:26.832
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/21/23 21:08:26.834
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-pjnd 04/21/23 21:08:26.839
STEP: Creating a pod to test atomic-volume-subpath 04/21/23 21:08:26.84
Apr 21 21:08:26.844: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pjnd" in namespace "subpath-5512" to be "Succeeded or Failed"
Apr 21 21:08:26.846: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.968294ms
Apr 21 21:08:28.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005602555s
Apr 21 21:08:30.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 4.005255976s
Apr 21 21:08:32.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 6.005122593s
Apr 21 21:08:34.849: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 8.004904375s
Apr 21 21:08:36.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 10.005245257s
Apr 21 21:08:38.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 12.005046028s
Apr 21 21:08:40.849: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 14.004960677s
Apr 21 21:08:42.851: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 16.006058742s
Apr 21 21:08:44.849: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 18.004458063s
Apr 21 21:08:46.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 20.005818165s
Apr 21 21:08:48.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=false. Elapsed: 22.006000401s
Apr 21 21:08:50.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005460821s
STEP: Saw pod success 04/21/23 21:08:50.85
Apr 21 21:08:50.850: INFO: Pod "pod-subpath-test-projected-pjnd" satisfied condition "Succeeded or Failed"
Apr 21 21:08:50.852: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-projected-pjnd container test-container-subpath-projected-pjnd: <nil>
STEP: delete the pod 04/21/23 21:08:50.858
Apr 21 21:08:50.866: INFO: Waiting for pod pod-subpath-test-projected-pjnd to disappear
Apr 21 21:08:50.867: INFO: Pod pod-subpath-test-projected-pjnd no longer exists
STEP: Deleting pod pod-subpath-test-projected-pjnd 04/21/23 21:08:50.867
Apr 21 21:08:50.868: INFO: Deleting pod "pod-subpath-test-projected-pjnd" in namespace "subpath-5512"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 21 21:08:50.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5512" for this suite. 04/21/23 21:08:50.871
------------------------------
â€¢ [SLOW TEST] [24.053 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:08:26.822
    Apr 21 21:08:26.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename subpath 04/21/23 21:08:26.822
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:26.831
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:26.832
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/21/23 21:08:26.834
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-pjnd 04/21/23 21:08:26.839
    STEP: Creating a pod to test atomic-volume-subpath 04/21/23 21:08:26.84
    Apr 21 21:08:26.844: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-pjnd" in namespace "subpath-5512" to be "Succeeded or Failed"
    Apr 21 21:08:26.846: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.968294ms
    Apr 21 21:08:28.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005602555s
    Apr 21 21:08:30.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 4.005255976s
    Apr 21 21:08:32.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 6.005122593s
    Apr 21 21:08:34.849: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 8.004904375s
    Apr 21 21:08:36.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 10.005245257s
    Apr 21 21:08:38.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 12.005046028s
    Apr 21 21:08:40.849: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 14.004960677s
    Apr 21 21:08:42.851: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 16.006058742s
    Apr 21 21:08:44.849: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 18.004458063s
    Apr 21 21:08:46.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=true. Elapsed: 20.005818165s
    Apr 21 21:08:48.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Running", Reason="", readiness=false. Elapsed: 22.006000401s
    Apr 21 21:08:50.850: INFO: Pod "pod-subpath-test-projected-pjnd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005460821s
    STEP: Saw pod success 04/21/23 21:08:50.85
    Apr 21 21:08:50.850: INFO: Pod "pod-subpath-test-projected-pjnd" satisfied condition "Succeeded or Failed"
    Apr 21 21:08:50.852: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-projected-pjnd container test-container-subpath-projected-pjnd: <nil>
    STEP: delete the pod 04/21/23 21:08:50.858
    Apr 21 21:08:50.866: INFO: Waiting for pod pod-subpath-test-projected-pjnd to disappear
    Apr 21 21:08:50.867: INFO: Pod pod-subpath-test-projected-pjnd no longer exists
    STEP: Deleting pod pod-subpath-test-projected-pjnd 04/21/23 21:08:50.867
    Apr 21 21:08:50.868: INFO: Deleting pod "pod-subpath-test-projected-pjnd" in namespace "subpath-5512"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:08:50.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5512" for this suite. 04/21/23 21:08:50.871
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:08:50.875
Apr 21 21:08:50.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 21:08:50.876
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:50.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:50.885
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 04/21/23 21:08:50.887
Apr 21 21:08:50.892: INFO: Waiting up to 5m0s for pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a" in namespace "emptydir-6600" to be "Succeeded or Failed"
Apr 21 21:08:50.893: INFO: Pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518453ms
Apr 21 21:08:52.896: INFO: Pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003915507s
Apr 21 21:08:54.896: INFO: Pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00451617s
STEP: Saw pod success 04/21/23 21:08:54.896
Apr 21 21:08:54.896: INFO: Pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a" satisfied condition "Succeeded or Failed"
Apr 21 21:08:54.898: INFO: Trying to get logs from node k8sconformance-m02 pod pod-e381831d-3402-4171-bcb2-33e83d4d738a container test-container: <nil>
STEP: delete the pod 04/21/23 21:08:54.903
Apr 21 21:08:54.912: INFO: Waiting for pod pod-e381831d-3402-4171-bcb2-33e83d4d738a to disappear
Apr 21 21:08:54.913: INFO: Pod pod-e381831d-3402-4171-bcb2-33e83d4d738a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:08:54.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6600" for this suite. 04/21/23 21:08:54.916
------------------------------
â€¢ [4.044 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:08:50.875
    Apr 21 21:08:50.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 21:08:50.876
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:50.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:50.885
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 04/21/23 21:08:50.887
    Apr 21 21:08:50.892: INFO: Waiting up to 5m0s for pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a" in namespace "emptydir-6600" to be "Succeeded or Failed"
    Apr 21 21:08:50.893: INFO: Pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.518453ms
    Apr 21 21:08:52.896: INFO: Pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003915507s
    Apr 21 21:08:54.896: INFO: Pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00451617s
    STEP: Saw pod success 04/21/23 21:08:54.896
    Apr 21 21:08:54.896: INFO: Pod "pod-e381831d-3402-4171-bcb2-33e83d4d738a" satisfied condition "Succeeded or Failed"
    Apr 21 21:08:54.898: INFO: Trying to get logs from node k8sconformance-m02 pod pod-e381831d-3402-4171-bcb2-33e83d4d738a container test-container: <nil>
    STEP: delete the pod 04/21/23 21:08:54.903
    Apr 21 21:08:54.912: INFO: Waiting for pod pod-e381831d-3402-4171-bcb2-33e83d4d738a to disappear
    Apr 21 21:08:54.913: INFO: Pod pod-e381831d-3402-4171-bcb2-33e83d4d738a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:08:54.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6600" for this suite. 04/21/23 21:08:54.916
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:08:54.922
Apr 21 21:08:54.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:08:54.922
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:54.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:54.934
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-a4d9cbea-5ac5-4100-a93b-637ffed0463b 04/21/23 21:08:54.936
STEP: Creating a pod to test consume secrets 04/21/23 21:08:54.939
Apr 21 21:08:54.945: INFO: Waiting up to 5m0s for pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d" in namespace "secrets-7676" to be "Succeeded or Failed"
Apr 21 21:08:54.947: INFO: Pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.732863ms
Apr 21 21:08:56.949: INFO: Pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004326533s
Apr 21 21:08:58.950: INFO: Pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005474377s
STEP: Saw pod success 04/21/23 21:08:58.95
Apr 21 21:08:58.950: INFO: Pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d" satisfied condition "Succeeded or Failed"
Apr 21 21:08:58.952: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d container secret-volume-test: <nil>
STEP: delete the pod 04/21/23 21:08:58.957
Apr 21 21:08:58.967: INFO: Waiting for pod pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d to disappear
Apr 21 21:08:58.972: INFO: Pod pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:08:58.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7676" for this suite. 04/21/23 21:08:58.974
------------------------------
â€¢ [4.056 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:08:54.922
    Apr 21 21:08:54.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:08:54.922
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:54.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:54.934
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-a4d9cbea-5ac5-4100-a93b-637ffed0463b 04/21/23 21:08:54.936
    STEP: Creating a pod to test consume secrets 04/21/23 21:08:54.939
    Apr 21 21:08:54.945: INFO: Waiting up to 5m0s for pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d" in namespace "secrets-7676" to be "Succeeded or Failed"
    Apr 21 21:08:54.947: INFO: Pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.732863ms
    Apr 21 21:08:56.949: INFO: Pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004326533s
    Apr 21 21:08:58.950: INFO: Pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005474377s
    STEP: Saw pod success 04/21/23 21:08:58.95
    Apr 21 21:08:58.950: INFO: Pod "pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d" satisfied condition "Succeeded or Failed"
    Apr 21 21:08:58.952: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d container secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 21:08:58.957
    Apr 21 21:08:58.967: INFO: Waiting for pod pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d to disappear
    Apr 21 21:08:58.972: INFO: Pod pod-secrets-856f7687-bbdc-4860-ba5c-30d3690ffa7d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:08:58.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7676" for this suite. 04/21/23 21:08:58.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:08:58.979
Apr 21 21:08:58.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-lifecycle-hook 04/21/23 21:08:58.979
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:58.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:58.988
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/21/23 21:08:58.992
Apr 21 21:08:58.998: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-681" to be "running and ready"
Apr 21 21:08:59.000: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71581ms
Apr 21 21:08:59.000: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:09:01.003: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004862322s
Apr 21 21:09:01.003: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 21 21:09:01.003: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 04/21/23 21:09:01.005
Apr 21 21:09:01.010: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-681" to be "running and ready"
Apr 21 21:09:01.012: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691512ms
Apr 21 21:09:01.012: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:09:03.015: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004957094s
Apr 21 21:09:03.015: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Apr 21 21:09:03.015: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 04/21/23 21:09:03.017
Apr 21 21:09:03.023: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 21 21:09:03.025: INFO: Pod pod-with-prestop-http-hook still exists
Apr 21 21:09:05.025: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 21 21:09:05.027: INFO: Pod pod-with-prestop-http-hook still exists
Apr 21 21:09:07.026: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 21 21:09:07.029: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 04/21/23 21:09:07.029
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 21 21:09:07.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-681" for this suite. 04/21/23 21:09:07.044
------------------------------
â€¢ [SLOW TEST] [8.070 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:08:58.979
    Apr 21 21:08:58.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/21/23 21:08:58.979
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:08:58.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:08:58.988
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/21/23 21:08:58.992
    Apr 21 21:08:58.998: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-681" to be "running and ready"
    Apr 21 21:08:59.000: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.71581ms
    Apr 21 21:08:59.000: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:09:01.003: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004862322s
    Apr 21 21:09:01.003: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 21 21:09:01.003: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 04/21/23 21:09:01.005
    Apr 21 21:09:01.010: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-681" to be "running and ready"
    Apr 21 21:09:01.012: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.691512ms
    Apr 21 21:09:01.012: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:09:03.015: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004957094s
    Apr 21 21:09:03.015: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Apr 21 21:09:03.015: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 04/21/23 21:09:03.017
    Apr 21 21:09:03.023: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr 21 21:09:03.025: INFO: Pod pod-with-prestop-http-hook still exists
    Apr 21 21:09:05.025: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr 21 21:09:05.027: INFO: Pod pod-with-prestop-http-hook still exists
    Apr 21 21:09:07.026: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr 21 21:09:07.029: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 04/21/23 21:09:07.029
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:09:07.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-681" for this suite. 04/21/23 21:09:07.044
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:09:07.049
Apr 21 21:09:07.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-lifecycle-hook 04/21/23 21:09:07.05
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:07.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:07.059
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/21/23 21:09:07.063
Apr 21 21:09:07.069: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6239" to be "running and ready"
Apr 21 21:09:07.071: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573269ms
Apr 21 21:09:07.071: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:09:09.074: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004893948s
Apr 21 21:09:09.074: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 21 21:09:09.074: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 04/21/23 21:09:09.076
Apr 21 21:09:09.080: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6239" to be "running and ready"
Apr 21 21:09:09.081: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.777493ms
Apr 21 21:09:09.081: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:09:11.085: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004878014s
Apr 21 21:09:11.085: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Apr 21 21:09:11.085: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 04/21/23 21:09:11.086
Apr 21 21:09:11.092: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 21 21:09:11.094: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 21 21:09:13.095: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 21 21:09:13.098: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 21 21:09:15.095: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 21 21:09:15.097: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 04/21/23 21:09:15.098
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 21 21:09:15.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6239" for this suite. 04/21/23 21:09:15.106
------------------------------
â€¢ [SLOW TEST] [8.061 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:09:07.049
    Apr 21 21:09:07.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/21/23 21:09:07.05
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:07.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:07.059
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/21/23 21:09:07.063
    Apr 21 21:09:07.069: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6239" to be "running and ready"
    Apr 21 21:09:07.071: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.573269ms
    Apr 21 21:09:07.071: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:09:09.074: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004893948s
    Apr 21 21:09:09.074: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 21 21:09:09.074: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 04/21/23 21:09:09.076
    Apr 21 21:09:09.080: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-6239" to be "running and ready"
    Apr 21 21:09:09.081: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.777493ms
    Apr 21 21:09:09.081: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:09:11.085: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004878014s
    Apr 21 21:09:11.085: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Apr 21 21:09:11.085: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 04/21/23 21:09:11.086
    Apr 21 21:09:11.092: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr 21 21:09:11.094: INFO: Pod pod-with-prestop-exec-hook still exists
    Apr 21 21:09:13.095: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr 21 21:09:13.098: INFO: Pod pod-with-prestop-exec-hook still exists
    Apr 21 21:09:15.095: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr 21 21:09:15.097: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 04/21/23 21:09:15.098
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:09:15.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6239" for this suite. 04/21/23 21:09:15.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:09:15.11
Apr 21 21:09:15.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename tables 04/21/23 21:09:15.111
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:15.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:15.123
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Apr 21 21:09:15.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-1392" for this suite. 04/21/23 21:09:15.128
------------------------------
â€¢ [0.021 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:09:15.11
    Apr 21 21:09:15.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename tables 04/21/23 21:09:15.111
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:15.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:15.123
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:09:15.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-1392" for this suite. 04/21/23 21:09:15.128
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:09:15.132
Apr 21 21:09:15.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename prestop 04/21/23 21:09:15.133
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:15.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:15.141
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2183 04/21/23 21:09:15.144
STEP: Waiting for pods to come up. 04/21/23 21:09:15.148
Apr 21 21:09:15.148: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2183" to be "running"
Apr 21 21:09:15.150: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.601659ms
Apr 21 21:09:17.153: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.005089909s
Apr 21 21:09:17.153: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2183 04/21/23 21:09:17.155
Apr 21 21:09:17.160: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2183" to be "running"
Apr 21 21:09:17.161: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.709086ms
Apr 21 21:09:19.165: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.005019004s
Apr 21 21:09:19.165: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 04/21/23 21:09:19.165
Apr 21 21:09:24.172: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 04/21/23 21:09:24.172
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Apr 21 21:09:24.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2183" for this suite. 04/21/23 21:09:24.183
------------------------------
â€¢ [SLOW TEST] [9.054 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:09:15.132
    Apr 21 21:09:15.132: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename prestop 04/21/23 21:09:15.133
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:15.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:15.141
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2183 04/21/23 21:09:15.144
    STEP: Waiting for pods to come up. 04/21/23 21:09:15.148
    Apr 21 21:09:15.148: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2183" to be "running"
    Apr 21 21:09:15.150: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 1.601659ms
    Apr 21 21:09:17.153: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.005089909s
    Apr 21 21:09:17.153: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2183 04/21/23 21:09:17.155
    Apr 21 21:09:17.160: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2183" to be "running"
    Apr 21 21:09:17.161: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 1.709086ms
    Apr 21 21:09:19.165: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.005019004s
    Apr 21 21:09:19.165: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 04/21/23 21:09:19.165
    Apr 21 21:09:24.172: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 04/21/23 21:09:24.172
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:09:24.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2183" for this suite. 04/21/23 21:09:24.183
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:09:24.186
Apr 21 21:09:24.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 21:09:24.187
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:24.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:24.198
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 04/21/23 21:09:24.2
Apr 21 21:09:24.206: INFO: Waiting up to 5m0s for pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7" in namespace "emptydir-6448" to be "Succeeded or Failed"
Apr 21 21:09:24.208: INFO: Pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827946ms
Apr 21 21:09:26.210: INFO: Pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004465649s
Apr 21 21:09:28.212: INFO: Pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005764248s
STEP: Saw pod success 04/21/23 21:09:28.212
Apr 21 21:09:28.212: INFO: Pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7" satisfied condition "Succeeded or Failed"
Apr 21 21:09:28.214: INFO: Trying to get logs from node k8sconformance-m02 pod pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7 container test-container: <nil>
STEP: delete the pod 04/21/23 21:09:28.22
Apr 21 21:09:28.229: INFO: Waiting for pod pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7 to disappear
Apr 21 21:09:28.230: INFO: Pod pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:09:28.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6448" for this suite. 04/21/23 21:09:28.232
------------------------------
â€¢ [4.049 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:09:24.186
    Apr 21 21:09:24.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 21:09:24.187
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:24.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:24.198
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 04/21/23 21:09:24.2
    Apr 21 21:09:24.206: INFO: Waiting up to 5m0s for pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7" in namespace "emptydir-6448" to be "Succeeded or Failed"
    Apr 21 21:09:24.208: INFO: Pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.827946ms
    Apr 21 21:09:26.210: INFO: Pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004465649s
    Apr 21 21:09:28.212: INFO: Pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005764248s
    STEP: Saw pod success 04/21/23 21:09:28.212
    Apr 21 21:09:28.212: INFO: Pod "pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7" satisfied condition "Succeeded or Failed"
    Apr 21 21:09:28.214: INFO: Trying to get logs from node k8sconformance-m02 pod pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7 container test-container: <nil>
    STEP: delete the pod 04/21/23 21:09:28.22
    Apr 21 21:09:28.229: INFO: Waiting for pod pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7 to disappear
    Apr 21 21:09:28.230: INFO: Pod pod-85b50812-2d5b-4b8f-9bec-84f7f267e9d7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:09:28.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6448" for this suite. 04/21/23 21:09:28.232
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:09:28.236
Apr 21 21:09:28.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename daemonsets 04/21/23 21:09:28.237
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:28.245
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:28.246
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 04/21/23 21:09:28.256
STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:09:28.26
Apr 21 21:09:28.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:09:28.263: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:09:29.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:09:29.268: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:09:30.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 21 21:09:30.268: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 04/21/23 21:09:30.27
STEP: DeleteCollection of the DaemonSets 04/21/23 21:09:30.271
STEP: Verify that ReplicaSets have been deleted 04/21/23 21:09:30.275
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Apr 21 21:09:30.280: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16431"},"items":null}

Apr 21 21:09:30.282: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16431"},"items":[{"metadata":{"name":"daemon-set-6zgl4","generateName":"daemon-set-","namespace":"daemonsets-1382","uid":"97402583-3a7f-4b78-b5e5-73e46c020ad9","resourceVersion":"16429","creationTimestamp":"2023-04-21T21:09:28Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0eb35890-a34f-4e8c-930e-91386a57ec71","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-21T21:09:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0eb35890-a34f-4e8c-930e-91386a57ec71\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-21T21:09:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nqcw9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nqcw9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8sconformance","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8sconformance"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:29Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:29Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:28Z"}],"hostIP":"192.168.49.2","podIP":"10.244.0.181","podIPs":[{"ip":"10.244.0.181"}],"startTime":"2023-04-21T21:09:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-21T21:09:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"docker://f8d9ea3358004ca08d4cf624f741bdcf5439faf4a2da6d9aa906578a9cdd5be9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w49sd","generateName":"daemon-set-","namespace":"daemonsets-1382","uid":"3431703e-1890-4602-9877-51bf02ce7073","resourceVersion":"16426","creationTimestamp":"2023-04-21T21:09:28Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0eb35890-a34f-4e8c-930e-91386a57ec71","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-21T21:09:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0eb35890-a34f-4e8c-930e-91386a57ec71\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-21T21:09:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jskbp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jskbp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8sconformance-m02","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8sconformance-m02"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:29Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:29Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:28Z"}],"hostIP":"192.168.49.3","podIP":"10.244.1.104","podIPs":[{"ip":"10.244.1.104"}],"startTime":"2023-04-21T21:09:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-21T21:09:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"docker://342aef6a49cb16a2954ce2889a24454b83ca96210cf42c66b20591edb98ec5d8","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:09:30.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1382" for this suite. 04/21/23 21:09:30.291
------------------------------
â€¢ [2.058 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:09:28.236
    Apr 21 21:09:28.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename daemonsets 04/21/23 21:09:28.237
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:28.245
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:28.246
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 04/21/23 21:09:28.256
    STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:09:28.26
    Apr 21 21:09:28.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:09:28.263: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:09:29.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:09:29.268: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:09:30.268: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 21 21:09:30.268: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 04/21/23 21:09:30.27
    STEP: DeleteCollection of the DaemonSets 04/21/23 21:09:30.271
    STEP: Verify that ReplicaSets have been deleted 04/21/23 21:09:30.275
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Apr 21 21:09:30.280: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16431"},"items":null}

    Apr 21 21:09:30.282: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16431"},"items":[{"metadata":{"name":"daemon-set-6zgl4","generateName":"daemon-set-","namespace":"daemonsets-1382","uid":"97402583-3a7f-4b78-b5e5-73e46c020ad9","resourceVersion":"16429","creationTimestamp":"2023-04-21T21:09:28Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0eb35890-a34f-4e8c-930e-91386a57ec71","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-21T21:09:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0eb35890-a34f-4e8c-930e-91386a57ec71\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-21T21:09:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nqcw9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nqcw9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8sconformance","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8sconformance"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:29Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:29Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:28Z"}],"hostIP":"192.168.49.2","podIP":"10.244.0.181","podIPs":[{"ip":"10.244.0.181"}],"startTime":"2023-04-21T21:09:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-21T21:09:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"docker://f8d9ea3358004ca08d4cf624f741bdcf5439faf4a2da6d9aa906578a9cdd5be9","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w49sd","generateName":"daemon-set-","namespace":"daemonsets-1382","uid":"3431703e-1890-4602-9877-51bf02ce7073","resourceVersion":"16426","creationTimestamp":"2023-04-21T21:09:28Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0eb35890-a34f-4e8c-930e-91386a57ec71","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-21T21:09:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0eb35890-a34f-4e8c-930e-91386a57ec71\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-21T21:09:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-jskbp","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-jskbp","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8sconformance-m02","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8sconformance-m02"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:28Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:29Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:29Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-21T21:09:28Z"}],"hostIP":"192.168.49.3","podIP":"10.244.1.104","podIPs":[{"ip":"10.244.1.104"}],"startTime":"2023-04-21T21:09:28Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-21T21:09:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"docker://342aef6a49cb16a2954ce2889a24454b83ca96210cf42c66b20591edb98ec5d8","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:09:30.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1382" for this suite. 04/21/23 21:09:30.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:09:30.296
Apr 21 21:09:30.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 21:09:30.296
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:30.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:30.305
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 04/21/23 21:09:30.307
Apr 21 21:09:30.311: INFO: Waiting up to 5m0s for pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8" in namespace "downward-api-9076" to be "Succeeded or Failed"
Apr 21 21:09:30.313: INFO: Pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.66008ms
Apr 21 21:09:32.316: INFO: Pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004469403s
Apr 21 21:09:34.315: INFO: Pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004249234s
STEP: Saw pod success 04/21/23 21:09:34.315
Apr 21 21:09:34.316: INFO: Pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8" satisfied condition "Succeeded or Failed"
Apr 21 21:09:34.317: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8 container dapi-container: <nil>
STEP: delete the pod 04/21/23 21:09:34.325
Apr 21 21:09:34.333: INFO: Waiting for pod downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8 to disappear
Apr 21 21:09:34.335: INFO: Pod downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 21 21:09:34.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9076" for this suite. 04/21/23 21:09:34.337
------------------------------
â€¢ [4.045 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:09:30.296
    Apr 21 21:09:30.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 21:09:30.296
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:30.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:30.305
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 04/21/23 21:09:30.307
    Apr 21 21:09:30.311: INFO: Waiting up to 5m0s for pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8" in namespace "downward-api-9076" to be "Succeeded or Failed"
    Apr 21 21:09:30.313: INFO: Pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.66008ms
    Apr 21 21:09:32.316: INFO: Pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004469403s
    Apr 21 21:09:34.315: INFO: Pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004249234s
    STEP: Saw pod success 04/21/23 21:09:34.315
    Apr 21 21:09:34.316: INFO: Pod "downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8" satisfied condition "Succeeded or Failed"
    Apr 21 21:09:34.317: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8 container dapi-container: <nil>
    STEP: delete the pod 04/21/23 21:09:34.325
    Apr 21 21:09:34.333: INFO: Waiting for pod downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8 to disappear
    Apr 21 21:09:34.335: INFO: Pod downward-api-35b7decb-f881-482f-aa4f-1e7b068b8da8 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:09:34.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9076" for this suite. 04/21/23 21:09:34.337
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:09:34.34
Apr 21 21:09:34.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 21:09:34.341
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:34.35
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:34.352
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 04/21/23 21:09:34.354
Apr 21 21:09:34.359: INFO: Waiting up to 5m0s for pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a" in namespace "emptydir-3680" to be "Succeeded or Failed"
Apr 21 21:09:34.361: INFO: Pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.592675ms
Apr 21 21:09:36.364: INFO: Pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004706526s
Apr 21 21:09:38.364: INFO: Pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004324622s
STEP: Saw pod success 04/21/23 21:09:38.364
Apr 21 21:09:38.364: INFO: Pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a" satisfied condition "Succeeded or Failed"
Apr 21 21:09:38.365: INFO: Trying to get logs from node k8sconformance-m02 pod pod-0361454f-3dbd-499a-bb2b-e3fc5011618a container test-container: <nil>
STEP: delete the pod 04/21/23 21:09:38.371
Apr 21 21:09:38.380: INFO: Waiting for pod pod-0361454f-3dbd-499a-bb2b-e3fc5011618a to disappear
Apr 21 21:09:38.382: INFO: Pod pod-0361454f-3dbd-499a-bb2b-e3fc5011618a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:09:38.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3680" for this suite. 04/21/23 21:09:38.384
------------------------------
â€¢ [4.048 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:09:34.34
    Apr 21 21:09:34.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 21:09:34.341
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:34.35
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:34.352
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 04/21/23 21:09:34.354
    Apr 21 21:09:34.359: INFO: Waiting up to 5m0s for pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a" in namespace "emptydir-3680" to be "Succeeded or Failed"
    Apr 21 21:09:34.361: INFO: Pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.592675ms
    Apr 21 21:09:36.364: INFO: Pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004706526s
    Apr 21 21:09:38.364: INFO: Pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004324622s
    STEP: Saw pod success 04/21/23 21:09:38.364
    Apr 21 21:09:38.364: INFO: Pod "pod-0361454f-3dbd-499a-bb2b-e3fc5011618a" satisfied condition "Succeeded or Failed"
    Apr 21 21:09:38.365: INFO: Trying to get logs from node k8sconformance-m02 pod pod-0361454f-3dbd-499a-bb2b-e3fc5011618a container test-container: <nil>
    STEP: delete the pod 04/21/23 21:09:38.371
    Apr 21 21:09:38.380: INFO: Waiting for pod pod-0361454f-3dbd-499a-bb2b-e3fc5011618a to disappear
    Apr 21 21:09:38.382: INFO: Pod pod-0361454f-3dbd-499a-bb2b-e3fc5011618a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:09:38.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3680" for this suite. 04/21/23 21:09:38.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:09:38.394
Apr 21 21:09:38.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir-wrapper 04/21/23 21:09:38.395
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:38.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:38.404
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 04/21/23 21:09:38.406
STEP: Creating RC which spawns configmap-volume pods 04/21/23 21:09:38.648
Apr 21 21:09:38.758: INFO: Pod name wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377: Found 4 pods out of 5
Apr 21 21:09:43.765: INFO: Pod name wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/21/23 21:09:43.765
Apr 21 21:09:43.765: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:09:43.767: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.437481ms
Apr 21 21:09:45.771: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006006092s
Apr 21 21:09:47.772: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00752064s
Apr 21 21:09:49.770: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005354999s
Apr 21 21:09:51.835: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070687741s
Apr 21 21:09:53.771: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Running", Reason="", readiness=true. Elapsed: 10.00640197s
Apr 21 21:09:53.771: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5" satisfied condition "running"
Apr 21 21:09:53.771: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-9qqcc" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:09:53.774: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-9qqcc": Phase="Running", Reason="", readiness=true. Elapsed: 2.41931ms
Apr 21 21:09:53.774: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-9qqcc" satisfied condition "running"
Apr 21 21:09:53.774: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-lsrsf" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:09:53.783: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-lsrsf": Phase="Running", Reason="", readiness=true. Elapsed: 9.707396ms
Apr 21 21:09:53.784: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-lsrsf" satisfied condition "running"
Apr 21 21:09:53.784: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-v8l9w" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:09:53.787: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-v8l9w": Phase="Running", Reason="", readiness=true. Elapsed: 3.556481ms
Apr 21 21:09:53.787: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-v8l9w" satisfied condition "running"
Apr 21 21:09:53.787: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-vkpsr" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:09:53.790: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-vkpsr": Phase="Running", Reason="", readiness=true. Elapsed: 2.302301ms
Apr 21 21:09:53.790: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-vkpsr" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377 in namespace emptydir-wrapper-5704, will wait for the garbage collector to delete the pods 04/21/23 21:09:53.79
Apr 21 21:09:53.850: INFO: Deleting ReplicationController wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377 took: 7.069839ms
Apr 21 21:09:53.951: INFO: Terminating ReplicationController wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377 pods took: 101.103679ms
STEP: Creating RC which spawns configmap-volume pods 04/21/23 21:09:57.554
Apr 21 21:09:57.565: INFO: Pod name wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0: Found 0 pods out of 5
Apr 21 21:10:02.570: INFO: Pod name wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/21/23 21:10:02.57
Apr 21 21:10:02.570: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:02.572: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00176ms
Apr 21 21:10:04.575: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00536652s
Apr 21 21:10:06.575: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005440409s
Apr 21 21:10:08.575: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005476673s
Apr 21 21:10:10.634: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 8.063916244s
Apr 21 21:10:12.576: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Running", Reason="", readiness=true. Elapsed: 10.00581171s
Apr 21 21:10:12.576: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m" satisfied condition "running"
Apr 21 21:10:12.576: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-8h4ml" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:12.578: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-8h4ml": Phase="Running", Reason="", readiness=true. Elapsed: 2.245412ms
Apr 21 21:10:12.578: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-8h4ml" satisfied condition "running"
Apr 21 21:10:12.578: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-bkkkm" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:12.580: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-bkkkm": Phase="Running", Reason="", readiness=true. Elapsed: 2.154277ms
Apr 21 21:10:12.580: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-bkkkm" satisfied condition "running"
Apr 21 21:10:12.580: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-gr7q7" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:12.582: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-gr7q7": Phase="Running", Reason="", readiness=true. Elapsed: 2.096555ms
Apr 21 21:10:12.582: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-gr7q7" satisfied condition "running"
Apr 21 21:10:12.582: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-k9slw" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:12.584: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-k9slw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042894ms
Apr 21 21:10:14.587: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-k9slw": Phase="Running", Reason="", readiness=true. Elapsed: 2.005055443s
Apr 21 21:10:14.587: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-k9slw" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0 in namespace emptydir-wrapper-5704, will wait for the garbage collector to delete the pods 04/21/23 21:10:14.588
Apr 21 21:10:14.646: INFO: Deleting ReplicationController wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0 took: 4.510997ms
Apr 21 21:10:14.747: INFO: Terminating ReplicationController wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0 pods took: 101.052981ms
STEP: Creating RC which spawns configmap-volume pods 04/21/23 21:10:17.75
Apr 21 21:10:17.761: INFO: Pod name wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8: Found 0 pods out of 5
Apr 21 21:10:22.767: INFO: Pod name wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/21/23 21:10:22.767
Apr 21 21:10:22.767: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:22.769: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114268ms
Apr 21 21:10:24.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005214564s
Apr 21 21:10:26.773: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006104381s
Apr 21 21:10:28.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004891976s
Apr 21 21:10:30.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004923528s
Apr 21 21:10:32.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Running", Reason="", readiness=true. Elapsed: 10.005179088s
Apr 21 21:10:32.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj" satisfied condition "running"
Apr 21 21:10:32.772: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-bh7fg" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:32.774: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-bh7fg": Phase="Running", Reason="", readiness=true. Elapsed: 2.165737ms
Apr 21 21:10:32.774: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-bh7fg" satisfied condition "running"
Apr 21 21:10:32.774: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-j27qp" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:32.776: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-j27qp": Phase="Running", Reason="", readiness=true. Elapsed: 1.933502ms
Apr 21 21:10:32.776: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-j27qp" satisfied condition "running"
Apr 21 21:10:32.776: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-pxsfp" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:32.778: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-pxsfp": Phase="Running", Reason="", readiness=true. Elapsed: 2.033921ms
Apr 21 21:10:32.778: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-pxsfp" satisfied condition "running"
Apr 21 21:10:32.778: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-xxtg2" in namespace "emptydir-wrapper-5704" to be "running"
Apr 21 21:10:32.781: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-xxtg2": Phase="Running", Reason="", readiness=true. Elapsed: 2.273103ms
Apr 21 21:10:32.781: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-xxtg2" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8 in namespace emptydir-wrapper-5704, will wait for the garbage collector to delete the pods 04/21/23 21:10:32.781
Apr 21 21:10:32.838: INFO: Deleting ReplicationController wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8 took: 4.405703ms
Apr 21 21:10:32.939: INFO: Terminating ReplicationController wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8 pods took: 100.926049ms
STEP: Cleaning up the configMaps 04/21/23 21:10:36.639
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:10:36.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-5704" for this suite. 04/21/23 21:10:36.784
------------------------------
â€¢ [SLOW TEST] [58.393 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:09:38.394
    Apr 21 21:09:38.394: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir-wrapper 04/21/23 21:09:38.395
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:09:38.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:09:38.404
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 04/21/23 21:09:38.406
    STEP: Creating RC which spawns configmap-volume pods 04/21/23 21:09:38.648
    Apr 21 21:09:38.758: INFO: Pod name wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377: Found 4 pods out of 5
    Apr 21 21:09:43.765: INFO: Pod name wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/21/23 21:09:43.765
    Apr 21 21:09:43.765: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:09:43.767: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.437481ms
    Apr 21 21:09:45.771: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006006092s
    Apr 21 21:09:47.772: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00752064s
    Apr 21 21:09:49.770: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005354999s
    Apr 21 21:09:51.835: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070687741s
    Apr 21 21:09:53.771: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5": Phase="Running", Reason="", readiness=true. Elapsed: 10.00640197s
    Apr 21 21:09:53.771: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-94cc5" satisfied condition "running"
    Apr 21 21:09:53.771: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-9qqcc" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:09:53.774: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-9qqcc": Phase="Running", Reason="", readiness=true. Elapsed: 2.41931ms
    Apr 21 21:09:53.774: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-9qqcc" satisfied condition "running"
    Apr 21 21:09:53.774: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-lsrsf" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:09:53.783: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-lsrsf": Phase="Running", Reason="", readiness=true. Elapsed: 9.707396ms
    Apr 21 21:09:53.784: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-lsrsf" satisfied condition "running"
    Apr 21 21:09:53.784: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-v8l9w" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:09:53.787: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-v8l9w": Phase="Running", Reason="", readiness=true. Elapsed: 3.556481ms
    Apr 21 21:09:53.787: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-v8l9w" satisfied condition "running"
    Apr 21 21:09:53.787: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-vkpsr" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:09:53.790: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-vkpsr": Phase="Running", Reason="", readiness=true. Elapsed: 2.302301ms
    Apr 21 21:09:53.790: INFO: Pod "wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377-vkpsr" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377 in namespace emptydir-wrapper-5704, will wait for the garbage collector to delete the pods 04/21/23 21:09:53.79
    Apr 21 21:09:53.850: INFO: Deleting ReplicationController wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377 took: 7.069839ms
    Apr 21 21:09:53.951: INFO: Terminating ReplicationController wrapped-volume-race-f189e9c0-f288-46c3-970f-424bfe9de377 pods took: 101.103679ms
    STEP: Creating RC which spawns configmap-volume pods 04/21/23 21:09:57.554
    Apr 21 21:09:57.565: INFO: Pod name wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0: Found 0 pods out of 5
    Apr 21 21:10:02.570: INFO: Pod name wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/21/23 21:10:02.57
    Apr 21 21:10:02.570: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:02.572: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00176ms
    Apr 21 21:10:04.575: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00536652s
    Apr 21 21:10:06.575: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005440409s
    Apr 21 21:10:08.575: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 6.005476673s
    Apr 21 21:10:10.634: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Pending", Reason="", readiness=false. Elapsed: 8.063916244s
    Apr 21 21:10:12.576: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m": Phase="Running", Reason="", readiness=true. Elapsed: 10.00581171s
    Apr 21 21:10:12.576: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-2z68m" satisfied condition "running"
    Apr 21 21:10:12.576: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-8h4ml" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:12.578: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-8h4ml": Phase="Running", Reason="", readiness=true. Elapsed: 2.245412ms
    Apr 21 21:10:12.578: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-8h4ml" satisfied condition "running"
    Apr 21 21:10:12.578: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-bkkkm" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:12.580: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-bkkkm": Phase="Running", Reason="", readiness=true. Elapsed: 2.154277ms
    Apr 21 21:10:12.580: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-bkkkm" satisfied condition "running"
    Apr 21 21:10:12.580: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-gr7q7" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:12.582: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-gr7q7": Phase="Running", Reason="", readiness=true. Elapsed: 2.096555ms
    Apr 21 21:10:12.582: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-gr7q7" satisfied condition "running"
    Apr 21 21:10:12.582: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-k9slw" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:12.584: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-k9slw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042894ms
    Apr 21 21:10:14.587: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-k9slw": Phase="Running", Reason="", readiness=true. Elapsed: 2.005055443s
    Apr 21 21:10:14.587: INFO: Pod "wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0-k9slw" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0 in namespace emptydir-wrapper-5704, will wait for the garbage collector to delete the pods 04/21/23 21:10:14.588
    Apr 21 21:10:14.646: INFO: Deleting ReplicationController wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0 took: 4.510997ms
    Apr 21 21:10:14.747: INFO: Terminating ReplicationController wrapped-volume-race-e87e540e-626e-4a10-b12d-657fb04c78e0 pods took: 101.052981ms
    STEP: Creating RC which spawns configmap-volume pods 04/21/23 21:10:17.75
    Apr 21 21:10:17.761: INFO: Pod name wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8: Found 0 pods out of 5
    Apr 21 21:10:22.767: INFO: Pod name wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/21/23 21:10:22.767
    Apr 21 21:10:22.767: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:22.769: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.114268ms
    Apr 21 21:10:24.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005214564s
    Apr 21 21:10:26.773: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006104381s
    Apr 21 21:10:28.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004891976s
    Apr 21 21:10:30.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Pending", Reason="", readiness=false. Elapsed: 8.004923528s
    Apr 21 21:10:32.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj": Phase="Running", Reason="", readiness=true. Elapsed: 10.005179088s
    Apr 21 21:10:32.772: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-b9vtj" satisfied condition "running"
    Apr 21 21:10:32.772: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-bh7fg" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:32.774: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-bh7fg": Phase="Running", Reason="", readiness=true. Elapsed: 2.165737ms
    Apr 21 21:10:32.774: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-bh7fg" satisfied condition "running"
    Apr 21 21:10:32.774: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-j27qp" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:32.776: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-j27qp": Phase="Running", Reason="", readiness=true. Elapsed: 1.933502ms
    Apr 21 21:10:32.776: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-j27qp" satisfied condition "running"
    Apr 21 21:10:32.776: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-pxsfp" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:32.778: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-pxsfp": Phase="Running", Reason="", readiness=true. Elapsed: 2.033921ms
    Apr 21 21:10:32.778: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-pxsfp" satisfied condition "running"
    Apr 21 21:10:32.778: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-xxtg2" in namespace "emptydir-wrapper-5704" to be "running"
    Apr 21 21:10:32.781: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-xxtg2": Phase="Running", Reason="", readiness=true. Elapsed: 2.273103ms
    Apr 21 21:10:32.781: INFO: Pod "wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8-xxtg2" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8 in namespace emptydir-wrapper-5704, will wait for the garbage collector to delete the pods 04/21/23 21:10:32.781
    Apr 21 21:10:32.838: INFO: Deleting ReplicationController wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8 took: 4.405703ms
    Apr 21 21:10:32.939: INFO: Terminating ReplicationController wrapped-volume-race-7efee998-cd7f-487f-85d3-724ab266a8f8 pods took: 100.926049ms
    STEP: Cleaning up the configMaps 04/21/23 21:10:36.639
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:10:36.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-5704" for this suite. 04/21/23 21:10:36.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:10:36.787
Apr 21 21:10:36.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:10:36.788
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:10:36.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:10:36.798
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-c0698889-d0fe-4f7c-a1a8-a7c8fcb4602b 04/21/23 21:10:36.799
STEP: Creating a pod to test consume configMaps 04/21/23 21:10:36.802
Apr 21 21:10:36.806: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8" in namespace "projected-8168" to be "Succeeded or Failed"
Apr 21 21:10:36.807: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.416887ms
Apr 21 21:10:38.810: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004216925s
Apr 21 21:10:40.810: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003857789s
Apr 21 21:10:42.810: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00416095s
STEP: Saw pod success 04/21/23 21:10:42.81
Apr 21 21:10:42.810: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8" satisfied condition "Succeeded or Failed"
Apr 21 21:10:42.812: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 21:10:42.817
Apr 21 21:10:42.826: INFO: Waiting for pod pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8 to disappear
Apr 21 21:10:42.828: INFO: Pod pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:10:42.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8168" for this suite. 04/21/23 21:10:42.83
------------------------------
â€¢ [SLOW TEST] [6.047 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:10:36.787
    Apr 21 21:10:36.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:10:36.788
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:10:36.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:10:36.798
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-c0698889-d0fe-4f7c-a1a8-a7c8fcb4602b 04/21/23 21:10:36.799
    STEP: Creating a pod to test consume configMaps 04/21/23 21:10:36.802
    Apr 21 21:10:36.806: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8" in namespace "projected-8168" to be "Succeeded or Failed"
    Apr 21 21:10:36.807: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.416887ms
    Apr 21 21:10:38.810: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004216925s
    Apr 21 21:10:40.810: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.003857789s
    Apr 21 21:10:42.810: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.00416095s
    STEP: Saw pod success 04/21/23 21:10:42.81
    Apr 21 21:10:42.810: INFO: Pod "pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8" satisfied condition "Succeeded or Failed"
    Apr 21 21:10:42.812: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 21:10:42.817
    Apr 21 21:10:42.826: INFO: Waiting for pod pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8 to disappear
    Apr 21 21:10:42.828: INFO: Pod pod-projected-configmaps-8a9103f8-bd09-4a75-8259-8acfab0a2dd8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:10:42.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8168" for this suite. 04/21/23 21:10:42.83
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:10:42.835
Apr 21 21:10:42.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 21:10:42.836
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:10:42.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:10:42.844
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 04/21/23 21:10:42.85
STEP: waiting for available Endpoint 04/21/23 21:10:42.853
STEP: listing all Endpoints 04/21/23 21:10:42.853
STEP: updating the Endpoint 04/21/23 21:10:42.855
STEP: fetching the Endpoint 04/21/23 21:10:42.86
STEP: patching the Endpoint 04/21/23 21:10:42.861
STEP: fetching the Endpoint 04/21/23 21:10:42.867
STEP: deleting the Endpoint by Collection 04/21/23 21:10:42.869
STEP: waiting for Endpoint deletion 04/21/23 21:10:42.873
STEP: fetching the Endpoint 04/21/23 21:10:42.874
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 21:10:42.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-451" for this suite. 04/21/23 21:10:42.877
------------------------------
â€¢ [0.045 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:10:42.835
    Apr 21 21:10:42.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 21:10:42.836
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:10:42.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:10:42.844
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 04/21/23 21:10:42.85
    STEP: waiting for available Endpoint 04/21/23 21:10:42.853
    STEP: listing all Endpoints 04/21/23 21:10:42.853
    STEP: updating the Endpoint 04/21/23 21:10:42.855
    STEP: fetching the Endpoint 04/21/23 21:10:42.86
    STEP: patching the Endpoint 04/21/23 21:10:42.861
    STEP: fetching the Endpoint 04/21/23 21:10:42.867
    STEP: deleting the Endpoint by Collection 04/21/23 21:10:42.869
    STEP: waiting for Endpoint deletion 04/21/23 21:10:42.873
    STEP: fetching the Endpoint 04/21/23 21:10:42.874
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:10:42.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-451" for this suite. 04/21/23 21:10:42.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:10:42.881
Apr 21 21:10:42.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename job 04/21/23 21:10:42.882
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:10:42.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:10:42.89
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 04/21/23 21:10:42.892
STEP: Ensuring job reaches completions 04/21/23 21:10:42.895
STEP: Ensuring pods with index for job exist 04/21/23 21:10:52.898
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 21 21:10:52.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2480" for this suite. 04/21/23 21:10:52.902
------------------------------
â€¢ [SLOW TEST] [10.025 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:10:42.881
    Apr 21 21:10:42.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename job 04/21/23 21:10:42.882
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:10:42.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:10:42.89
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 04/21/23 21:10:42.892
    STEP: Ensuring job reaches completions 04/21/23 21:10:42.895
    STEP: Ensuring pods with index for job exist 04/21/23 21:10:52.898
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:10:52.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2480" for this suite. 04/21/23 21:10:52.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:10:52.907
Apr 21 21:10:52.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-probe 04/21/23 21:10:52.908
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:10:52.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:10:52.92
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de in namespace container-probe-2256 04/21/23 21:10:52.922
Apr 21 21:10:52.927: INFO: Waiting up to 5m0s for pod "liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de" in namespace "container-probe-2256" to be "not pending"
Apr 21 21:10:52.929: INFO: Pod "liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de": Phase="Pending", Reason="", readiness=false. Elapsed: 1.865295ms
Apr 21 21:10:54.932: INFO: Pod "liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de": Phase="Running", Reason="", readiness=true. Elapsed: 2.004765586s
Apr 21 21:10:54.932: INFO: Pod "liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de" satisfied condition "not pending"
Apr 21 21:10:54.932: INFO: Started pod liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de in namespace container-probe-2256
STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 21:10:54.932
Apr 21 21:10:54.934: INFO: Initial restart count of pod liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is 0
Apr 21 21:11:14.968: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 1 (20.033958318s elapsed)
Apr 21 21:11:35.001: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 2 (40.067171134s elapsed)
Apr 21 21:11:55.032: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 3 (1m0.098281334s elapsed)
Apr 21 21:12:15.063: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 4 (1m20.128855924s elapsed)
Apr 21 21:13:15.155: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 5 (2m20.220935222s elapsed)
STEP: deleting the pod 04/21/23 21:13:15.155
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 21 21:13:15.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2256" for this suite. 04/21/23 21:13:15.165
------------------------------
â€¢ [SLOW TEST] [142.261 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:10:52.907
    Apr 21 21:10:52.907: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-probe 04/21/23 21:10:52.908
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:10:52.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:10:52.92
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de in namespace container-probe-2256 04/21/23 21:10:52.922
    Apr 21 21:10:52.927: INFO: Waiting up to 5m0s for pod "liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de" in namespace "container-probe-2256" to be "not pending"
    Apr 21 21:10:52.929: INFO: Pod "liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de": Phase="Pending", Reason="", readiness=false. Elapsed: 1.865295ms
    Apr 21 21:10:54.932: INFO: Pod "liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de": Phase="Running", Reason="", readiness=true. Elapsed: 2.004765586s
    Apr 21 21:10:54.932: INFO: Pod "liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de" satisfied condition "not pending"
    Apr 21 21:10:54.932: INFO: Started pod liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de in namespace container-probe-2256
    STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 21:10:54.932
    Apr 21 21:10:54.934: INFO: Initial restart count of pod liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is 0
    Apr 21 21:11:14.968: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 1 (20.033958318s elapsed)
    Apr 21 21:11:35.001: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 2 (40.067171134s elapsed)
    Apr 21 21:11:55.032: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 3 (1m0.098281334s elapsed)
    Apr 21 21:12:15.063: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 4 (1m20.128855924s elapsed)
    Apr 21 21:13:15.155: INFO: Restart count of pod container-probe-2256/liveness-9f0c4886-a858-4136-bd4c-f0e856cdc5de is now 5 (2m20.220935222s elapsed)
    STEP: deleting the pod 04/21/23 21:13:15.155
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:13:15.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2256" for this suite. 04/21/23 21:13:15.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:13:15.169
Apr 21 21:13:15.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 21:13:15.17
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:13:15.179
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:13:15.181
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 04/21/23 21:13:15.183
STEP: Creating a ResourceQuota 04/21/23 21:13:20.185
STEP: Ensuring resource quota status is calculated 04/21/23 21:13:20.189
STEP: Creating a Pod that fits quota 04/21/23 21:13:22.192
STEP: Ensuring ResourceQuota status captures the pod usage 04/21/23 21:13:22.202
STEP: Not allowing a pod to be created that exceeds remaining quota 04/21/23 21:13:24.205
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 04/21/23 21:13:24.207
STEP: Ensuring a pod cannot update its resource requirements 04/21/23 21:13:24.209
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 04/21/23 21:13:24.212
STEP: Deleting the pod 04/21/23 21:13:26.215
STEP: Ensuring resource quota status released the pod usage 04/21/23 21:13:26.223
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 21:13:28.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7338" for this suite. 04/21/23 21:13:28.229
------------------------------
â€¢ [SLOW TEST] [13.063 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:13:15.169
    Apr 21 21:13:15.169: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 21:13:15.17
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:13:15.179
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:13:15.181
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 04/21/23 21:13:15.183
    STEP: Creating a ResourceQuota 04/21/23 21:13:20.185
    STEP: Ensuring resource quota status is calculated 04/21/23 21:13:20.189
    STEP: Creating a Pod that fits quota 04/21/23 21:13:22.192
    STEP: Ensuring ResourceQuota status captures the pod usage 04/21/23 21:13:22.202
    STEP: Not allowing a pod to be created that exceeds remaining quota 04/21/23 21:13:24.205
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 04/21/23 21:13:24.207
    STEP: Ensuring a pod cannot update its resource requirements 04/21/23 21:13:24.209
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 04/21/23 21:13:24.212
    STEP: Deleting the pod 04/21/23 21:13:26.215
    STEP: Ensuring resource quota status released the pod usage 04/21/23 21:13:26.223
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:13:28.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7338" for this suite. 04/21/23 21:13:28.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:13:28.234
Apr 21 21:13:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 21:13:28.234
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:13:28.243
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:13:28.245
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 04/21/23 21:13:28.247
Apr 21 21:13:28.252: INFO: Waiting up to 5m0s for pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c" in namespace "emptydir-8725" to be "Succeeded or Failed"
Apr 21 21:13:28.254: INFO: Pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.715569ms
Apr 21 21:13:30.257: INFO: Pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004722752s
Apr 21 21:13:32.257: INFO: Pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005127666s
STEP: Saw pod success 04/21/23 21:13:32.257
Apr 21 21:13:32.257: INFO: Pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c" satisfied condition "Succeeded or Failed"
Apr 21 21:13:32.259: INFO: Trying to get logs from node k8sconformance-m02 pod pod-8a9a8d0a-df50-4527-b254-50c39d164d1c container test-container: <nil>
STEP: delete the pod 04/21/23 21:13:32.269
Apr 21 21:13:32.278: INFO: Waiting for pod pod-8a9a8d0a-df50-4527-b254-50c39d164d1c to disappear
Apr 21 21:13:32.279: INFO: Pod pod-8a9a8d0a-df50-4527-b254-50c39d164d1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:13:32.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8725" for this suite. 04/21/23 21:13:32.281
------------------------------
â€¢ [4.051 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:13:28.234
    Apr 21 21:13:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 21:13:28.234
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:13:28.243
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:13:28.245
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 04/21/23 21:13:28.247
    Apr 21 21:13:28.252: INFO: Waiting up to 5m0s for pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c" in namespace "emptydir-8725" to be "Succeeded or Failed"
    Apr 21 21:13:28.254: INFO: Pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.715569ms
    Apr 21 21:13:30.257: INFO: Pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004722752s
    Apr 21 21:13:32.257: INFO: Pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005127666s
    STEP: Saw pod success 04/21/23 21:13:32.257
    Apr 21 21:13:32.257: INFO: Pod "pod-8a9a8d0a-df50-4527-b254-50c39d164d1c" satisfied condition "Succeeded or Failed"
    Apr 21 21:13:32.259: INFO: Trying to get logs from node k8sconformance-m02 pod pod-8a9a8d0a-df50-4527-b254-50c39d164d1c container test-container: <nil>
    STEP: delete the pod 04/21/23 21:13:32.269
    Apr 21 21:13:32.278: INFO: Waiting for pod pod-8a9a8d0a-df50-4527-b254-50c39d164d1c to disappear
    Apr 21 21:13:32.279: INFO: Pod pod-8a9a8d0a-df50-4527-b254-50c39d164d1c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:13:32.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8725" for this suite. 04/21/23 21:13:32.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:13:32.285
Apr 21 21:13:32.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename deployment 04/21/23 21:13:32.286
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:13:32.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:13:32.295
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 04/21/23 21:13:32.299
STEP: waiting for Deployment to be created 04/21/23 21:13:32.302
STEP: waiting for all Replicas to be Ready 04/21/23 21:13:32.303
Apr 21 21:13:32.304: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 21 21:13:32.304: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 21 21:13:32.311: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 21 21:13:32.311: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 21 21:13:32.319: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 21 21:13:32.319: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 21 21:13:32.336: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 21 21:13:32.336: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 21 21:13:33.610: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 21 21:13:33.610: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 21 21:13:33.928: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 04/21/23 21:13:33.928
W0421 21:13:33.940551      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr 21 21:13:33.941: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 04/21/23 21:13:33.941
Apr 21 21:13:33.942: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:33.950: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:33.950: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:33.958: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:33.958: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:33.970: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:33.970: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:33.976: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:33.976: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:34.949: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:34.949: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:34.963: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
STEP: listing Deployments 04/21/23 21:13:34.963
Apr 21 21:13:34.965: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 04/21/23 21:13:34.965
Apr 21 21:13:34.973: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 04/21/23 21:13:34.973
Apr 21 21:13:34.977: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:34.982: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:34.993: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:35.004: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:35.008: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:35.975: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:35.988: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:35.995: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:36.002: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:36.009: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 21 21:13:36.719: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 04/21/23 21:13:36.745
STEP: fetching the DeploymentStatus 04/21/23 21:13:36.749
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 3
STEP: deleting the Deployment 04/21/23 21:13:36.753
Apr 21 21:13:36.758: INFO: observed event type MODIFIED
Apr 21 21:13:36.758: INFO: observed event type MODIFIED
Apr 21 21:13:36.758: INFO: observed event type MODIFIED
Apr 21 21:13:36.758: INFO: observed event type MODIFIED
Apr 21 21:13:36.759: INFO: observed event type MODIFIED
Apr 21 21:13:36.759: INFO: observed event type MODIFIED
Apr 21 21:13:36.759: INFO: observed event type MODIFIED
Apr 21 21:13:36.759: INFO: observed event type MODIFIED
Apr 21 21:13:36.759: INFO: observed event type MODIFIED
Apr 21 21:13:36.759: INFO: observed event type MODIFIED
Apr 21 21:13:36.759: INFO: observed event type MODIFIED
Apr 21 21:13:36.759: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 21 21:13:36.761: INFO: Log out all the ReplicaSets if there is no deployment created
Apr 21 21:13:36.763: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8822  a4b41086-346b-48d0-8e5a-bc88d0bf7966 17922 2 2023-04-21 21:13:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d89575df-99c8-4bca-b965-73cf7c2d6431 0xc004e665c7 0xc004e665c8}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:13:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d89575df-99c8-4bca-b965-73cf7c2d6431\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e66650 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Apr 21 21:13:36.765: INFO: pod: "test-deployment-7b7876f9d6-h7w6m":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-h7w6m test-deployment-7b7876f9d6- deployment-8822  f1720bfa-3cdc-4dca-bd01-1d3d1e675edc 17894 0 2023-04-21 21:13:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 a4b41086-346b-48d0-8e5a-bc88d0bf7966 0xc0054f5a57 0xc0054f5a58}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4b41086-346b-48d0-8e5a-bc88d0bf7966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbdwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbdwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.121,StartTime:2023-04-21 21:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://60e7c5cfe4b2030ab15d6960b67a2f6f5f8d0d7f67f17086ff84b3ff559c094c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 21 21:13:36.765: INFO: pod: "test-deployment-7b7876f9d6-wktph":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-wktph test-deployment-7b7876f9d6- deployment-8822  e01fd815-1697-43cd-962d-093676646299 17921 0 2023-04-21 21:13:35 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 a4b41086-346b-48d0-8e5a-bc88d0bf7966 0xc0054f5cb7 0xc0054f5cb8}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4b41086-346b-48d0-8e5a-bc88d0bf7966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gk8k8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gk8k8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.194,StartTime:2023-04-21 21:13:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://b5e1544a686b7d4ed6495265766e40003f5814f9b1bb060772f503bf38ff9a17,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 21 21:13:36.765: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8822  1e995c15-f10f-4ae1-bac0-5ac1329ec37b 17932 4 2023-04-21 21:13:33 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d89575df-99c8-4bca-b965-73cf7c2d6431 0xc004e666b7 0xc004e666b8}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d89575df-99c8-4bca-b965-73cf7c2d6431\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e66740 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Apr 21 21:13:36.767: INFO: pod: "test-deployment-7df74c55ff-6xd67":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-6xd67 test-deployment-7df74c55ff- deployment-8822  83c7e0ec-f242-4e91-bcbe-45b16871aac4 17920 0 2023-04-21 21:13:34 +0000 UTC 2023-04-21 21:13:36 +0000 UTC 0xc004e66aa8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1e995c15-f10f-4ae1-bac0-5ac1329ec37b 0xc004e66ad7 0xc004e66ad8}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e995c15-f10f-4ae1-bac0-5ac1329ec37b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzwfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzwfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.193,StartTime:2023-04-21 21:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:docker-pullable://registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:docker://ad4546364a3d75616e5e23a3e75dd32b46a682baec06dca93511dd9cb347ff37,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 21 21:13:36.767: INFO: pod: "test-deployment-7df74c55ff-jsz2z":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-jsz2z test-deployment-7df74c55ff- deployment-8822  dd0146a0-73f4-447a-8ad8-b39d418996a2 17929 0 2023-04-21 21:13:33 +0000 UTC 2023-04-21 21:13:37 +0000 UTC 0xc004e66ca0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1e995c15-f10f-4ae1-bac0-5ac1329ec37b 0xc004e66cd7 0xc004e66cd8}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e995c15-f10f-4ae1-bac0-5ac1329ec37b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bl4v4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bl4v4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.120,StartTime:2023-04-21 21:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:docker-pullable://registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:docker://4312004eb1bcdccdfe93d40ecdec9de8752c30da3c8f6700db1c53ff7ca439c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 21 21:13:36.768: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8822  86c0cb61-21c4-4c54-9948-75e85b1bd1ea 17860 3 2023-04-21 21:13:32 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d89575df-99c8-4bca-b965-73cf7c2d6431 0xc004e667a7 0xc004e667a8}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d89575df-99c8-4bca-b965-73cf7c2d6431\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e66830 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Apr 21 21:13:36.770: INFO: pod: "test-deployment-f4dbc4647-kxdkz":
&Pod{ObjectMeta:{test-deployment-f4dbc4647-kxdkz test-deployment-f4dbc4647- deployment-8822  925be546-721e-4f90-a984-e4de27a2553c 17831 0 2023-04-21 21:13:32 +0000 UTC 2023-04-21 21:13:34 +0000 UTC 0xc003b5c0b8 map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-f4dbc4647 86c0cb61-21c4-4c54-9948-75e85b1bd1ea 0xc003b5c0e7 0xc003b5c0e8}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86c0cb61-21c4-4c54-9948-75e85b1bd1ea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2hzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2hzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.119,StartTime:2023-04-21 21:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:docker://f264dc937e3470ac3e4b5d3297f7c53ef9f901d2b4d398317e43e2d7feeceae2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 21 21:13:36.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8822" for this suite. 04/21/23 21:13:36.772
------------------------------
â€¢ [4.492 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:13:32.285
    Apr 21 21:13:32.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename deployment 04/21/23 21:13:32.286
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:13:32.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:13:32.295
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 04/21/23 21:13:32.299
    STEP: waiting for Deployment to be created 04/21/23 21:13:32.302
    STEP: waiting for all Replicas to be Ready 04/21/23 21:13:32.303
    Apr 21 21:13:32.304: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 21 21:13:32.304: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 21 21:13:32.311: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 21 21:13:32.311: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 21 21:13:32.319: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 21 21:13:32.319: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 21 21:13:32.336: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 21 21:13:32.336: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 21 21:13:33.610: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Apr 21 21:13:33.610: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Apr 21 21:13:33.928: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 04/21/23 21:13:33.928
    W0421 21:13:33.940551      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr 21 21:13:33.941: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 04/21/23 21:13:33.941
    Apr 21 21:13:33.942: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 0
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:33.943: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:33.950: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:33.950: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:33.958: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:33.958: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:33.970: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:33.970: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:33.976: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:33.976: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:34.949: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:34.949: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:34.963: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    STEP: listing Deployments 04/21/23 21:13:34.963
    Apr 21 21:13:34.965: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 04/21/23 21:13:34.965
    Apr 21 21:13:34.973: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 04/21/23 21:13:34.973
    Apr 21 21:13:34.977: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:34.982: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:34.993: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:35.004: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:35.008: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:35.975: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:35.988: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:35.995: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:36.002: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:36.009: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 21 21:13:36.719: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 04/21/23 21:13:36.745
    STEP: fetching the DeploymentStatus 04/21/23 21:13:36.749
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 1
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 2
    Apr 21 21:13:36.753: INFO: observed Deployment test-deployment in namespace deployment-8822 with ReadyReplicas 3
    STEP: deleting the Deployment 04/21/23 21:13:36.753
    Apr 21 21:13:36.758: INFO: observed event type MODIFIED
    Apr 21 21:13:36.758: INFO: observed event type MODIFIED
    Apr 21 21:13:36.758: INFO: observed event type MODIFIED
    Apr 21 21:13:36.758: INFO: observed event type MODIFIED
    Apr 21 21:13:36.759: INFO: observed event type MODIFIED
    Apr 21 21:13:36.759: INFO: observed event type MODIFIED
    Apr 21 21:13:36.759: INFO: observed event type MODIFIED
    Apr 21 21:13:36.759: INFO: observed event type MODIFIED
    Apr 21 21:13:36.759: INFO: observed event type MODIFIED
    Apr 21 21:13:36.759: INFO: observed event type MODIFIED
    Apr 21 21:13:36.759: INFO: observed event type MODIFIED
    Apr 21 21:13:36.759: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 21 21:13:36.761: INFO: Log out all the ReplicaSets if there is no deployment created
    Apr 21 21:13:36.763: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-8822  a4b41086-346b-48d0-8e5a-bc88d0bf7966 17922 2 2023-04-21 21:13:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment d89575df-99c8-4bca-b965-73cf7c2d6431 0xc004e665c7 0xc004e665c8}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:13:35 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d89575df-99c8-4bca-b965-73cf7c2d6431\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e66650 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Apr 21 21:13:36.765: INFO: pod: "test-deployment-7b7876f9d6-h7w6m":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-h7w6m test-deployment-7b7876f9d6- deployment-8822  f1720bfa-3cdc-4dca-bd01-1d3d1e675edc 17894 0 2023-04-21 21:13:34 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 a4b41086-346b-48d0-8e5a-bc88d0bf7966 0xc0054f5a57 0xc0054f5a58}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4b41086-346b-48d0-8e5a-bc88d0bf7966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fbdwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fbdwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.121,StartTime:2023-04-21 21:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://60e7c5cfe4b2030ab15d6960b67a2f6f5f8d0d7f67f17086ff84b3ff559c094c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.121,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 21 21:13:36.765: INFO: pod: "test-deployment-7b7876f9d6-wktph":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-wktph test-deployment-7b7876f9d6- deployment-8822  e01fd815-1697-43cd-962d-093676646299 17921 0 2023-04-21 21:13:35 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 a4b41086-346b-48d0-8e5a-bc88d0bf7966 0xc0054f5cb7 0xc0054f5cb8}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:35 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a4b41086-346b-48d0-8e5a-bc88d0bf7966\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gk8k8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gk8k8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:35 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.194,StartTime:2023-04-21 21:13:35 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:36 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://b5e1544a686b7d4ed6495265766e40003f5814f9b1bb060772f503bf38ff9a17,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 21 21:13:36.765: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-8822  1e995c15-f10f-4ae1-bac0-5ac1329ec37b 17932 4 2023-04-21 21:13:33 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment d89575df-99c8-4bca-b965-73cf7c2d6431 0xc004e666b7 0xc004e666b8}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d89575df-99c8-4bca-b965-73cf7c2d6431\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e66740 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Apr 21 21:13:36.767: INFO: pod: "test-deployment-7df74c55ff-6xd67":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-6xd67 test-deployment-7df74c55ff- deployment-8822  83c7e0ec-f242-4e91-bcbe-45b16871aac4 17920 0 2023-04-21 21:13:34 +0000 UTC 2023-04-21 21:13:36 +0000 UTC 0xc004e66aa8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1e995c15-f10f-4ae1-bac0-5ac1329ec37b 0xc004e66ad7 0xc004e66ad8}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e995c15-f10f-4ae1-bac0-5ac1329ec37b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.193\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gzwfq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gzwfq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.193,StartTime:2023-04-21 21:13:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:docker-pullable://registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:docker://ad4546364a3d75616e5e23a3e75dd32b46a682baec06dca93511dd9cb347ff37,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.193,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 21 21:13:36.767: INFO: pod: "test-deployment-7df74c55ff-jsz2z":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-jsz2z test-deployment-7df74c55ff- deployment-8822  dd0146a0-73f4-447a-8ad8-b39d418996a2 17929 0 2023-04-21 21:13:33 +0000 UTC 2023-04-21 21:13:37 +0000 UTC 0xc004e66ca0 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 1e995c15-f10f-4ae1-bac0-5ac1329ec37b 0xc004e66cd7 0xc004e66cd8}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e995c15-f10f-4ae1-bac0-5ac1329ec37b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bl4v4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bl4v4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.120,StartTime:2023-04-21 21:13:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:34 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:docker-pullable://registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:docker://4312004eb1bcdccdfe93d40ecdec9de8752c30da3c8f6700db1c53ff7ca439c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 21 21:13:36.768: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-8822  86c0cb61-21c4-4c54-9948-75e85b1bd1ea 17860 3 2023-04-21 21:13:32 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment d89575df-99c8-4bca-b965-73cf7c2d6431 0xc004e667a7 0xc004e667a8}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d89575df-99c8-4bca-b965-73cf7c2d6431\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:13:34 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004e66830 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Apr 21 21:13:36.770: INFO: pod: "test-deployment-f4dbc4647-kxdkz":
    &Pod{ObjectMeta:{test-deployment-f4dbc4647-kxdkz test-deployment-f4dbc4647- deployment-8822  925be546-721e-4f90-a984-e4de27a2553c 17831 0 2023-04-21 21:13:32 +0000 UTC 2023-04-21 21:13:34 +0000 UTC 0xc003b5c0b8 map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-f4dbc4647 86c0cb61-21c4-4c54-9948-75e85b1bd1ea 0xc003b5c0e7 0xc003b5c0e8}] [] [{kube-controller-manager Update v1 2023-04-21 21:13:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"86c0cb61-21c4-4c54-9948-75e85b1bd1ea\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:13:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.119\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2hzw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2hzw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:13:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.119,StartTime:2023-04-21 21:13:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:13:33 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:docker://f264dc937e3470ac3e4b5d3297f7c53ef9f901d2b4d398317e43e2d7feeceae2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.119,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:13:36.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8822" for this suite. 04/21/23 21:13:36.772
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:13:36.778
Apr 21 21:13:36.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename statefulset 04/21/23 21:13:36.779
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:13:36.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:13:36.789
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1235 04/21/23 21:13:36.791
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 04/21/23 21:13:36.794
STEP: Creating stateful set ss in namespace statefulset-1235 04/21/23 21:13:36.796
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1235 04/21/23 21:13:36.8
Apr 21 21:13:36.802: INFO: Found 0 stateful pods, waiting for 1
Apr 21 21:13:46.805: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 04/21/23 21:13:46.805
Apr 21 21:13:46.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 21:13:46.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 21:13:46.938: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 21:13:46.938: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 21 21:13:46.940: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 21 21:13:56.943: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 21 21:13:56.943: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 21:13:56.952: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999787s
Apr 21 21:13:57.954: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998104485s
Apr 21 21:13:58.957: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995730378s
Apr 21 21:13:59.960: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.9920725s
Apr 21 21:14:00.963: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989827806s
Apr 21 21:14:01.965: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987352826s
Apr 21 21:14:02.968: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.983720442s
Apr 21 21:14:03.971: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.981434428s
Apr 21 21:14:04.974: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978141412s
Apr 21 21:14:05.977: INFO: Verifying statefulset ss doesn't scale past 1 for another 975.681619ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1235 04/21/23 21:14:06.977
Apr 21 21:14:06.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 21 21:14:07.099: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 21 21:14:07.099: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 21 21:14:07.099: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 21 21:14:07.101: INFO: Found 1 stateful pods, waiting for 3
Apr 21 21:14:17.105: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 21:14:17.105: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 21:14:17.105: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 04/21/23 21:14:17.105
STEP: Scale down will halt with unhealthy stateful pod 04/21/23 21:14:17.105
Apr 21 21:14:17.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 21:14:17.243: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 21:14:17.243: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 21:14:17.243: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 21 21:14:17.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 21:14:17.369: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 21:14:17.369: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 21:14:17.369: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 21 21:14:17.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 21:14:17.494: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 21:14:17.494: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 21:14:17.494: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 21 21:14:17.494: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 21:14:17.496: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 21 21:14:27.502: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 21 21:14:27.502: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 21 21:14:27.502: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 21 21:14:27.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999687s
Apr 21 21:14:28.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996735638s
Apr 21 21:14:29.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994297002s
Apr 21 21:14:30.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991853482s
Apr 21 21:14:31.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989018212s
Apr 21 21:14:32.525: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986163211s
Apr 21 21:14:33.528: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983492165s
Apr 21 21:14:34.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.979996353s
Apr 21 21:14:35.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977088988s
Apr 21 21:14:36.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 974.332502ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1235 04/21/23 21:14:37.537
Apr 21 21:14:37.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 21 21:14:37.675: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 21 21:14:37.675: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 21 21:14:37.675: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 21 21:14:37.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 21 21:14:37.791: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 21 21:14:37.791: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 21 21:14:37.791: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 21 21:14:37.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 21 21:14:37.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 21 21:14:37.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 21 21:14:37.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 21 21:14:37.903: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 04/21/23 21:14:47.916
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 21 21:14:47.917: INFO: Deleting all statefulset in ns statefulset-1235
Apr 21 21:14:47.918: INFO: Scaling statefulset ss to 0
Apr 21 21:14:47.924: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 21:14:47.926: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 21 21:14:47.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1235" for this suite. 04/21/23 21:14:47.934
------------------------------
â€¢ [SLOW TEST] [71.159 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:13:36.778
    Apr 21 21:13:36.778: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename statefulset 04/21/23 21:13:36.779
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:13:36.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:13:36.789
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1235 04/21/23 21:13:36.791
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 04/21/23 21:13:36.794
    STEP: Creating stateful set ss in namespace statefulset-1235 04/21/23 21:13:36.796
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1235 04/21/23 21:13:36.8
    Apr 21 21:13:36.802: INFO: Found 0 stateful pods, waiting for 1
    Apr 21 21:13:46.805: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 04/21/23 21:13:46.805
    Apr 21 21:13:46.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 21:13:46.938: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 21:13:46.938: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 21:13:46.938: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 21 21:13:46.940: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Apr 21 21:13:56.943: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 21 21:13:56.943: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 21:13:56.952: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999787s
    Apr 21 21:13:57.954: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998104485s
    Apr 21 21:13:58.957: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995730378s
    Apr 21 21:13:59.960: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.9920725s
    Apr 21 21:14:00.963: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.989827806s
    Apr 21 21:14:01.965: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.987352826s
    Apr 21 21:14:02.968: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.983720442s
    Apr 21 21:14:03.971: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.981434428s
    Apr 21 21:14:04.974: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.978141412s
    Apr 21 21:14:05.977: INFO: Verifying statefulset ss doesn't scale past 1 for another 975.681619ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1235 04/21/23 21:14:06.977
    Apr 21 21:14:06.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 21 21:14:07.099: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 21 21:14:07.099: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 21 21:14:07.099: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 21 21:14:07.101: INFO: Found 1 stateful pods, waiting for 3
    Apr 21 21:14:17.105: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 21:14:17.105: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 21:14:17.105: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 04/21/23 21:14:17.105
    STEP: Scale down will halt with unhealthy stateful pod 04/21/23 21:14:17.105
    Apr 21 21:14:17.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 21:14:17.243: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 21:14:17.243: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 21:14:17.243: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 21 21:14:17.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 21:14:17.369: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 21:14:17.369: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 21:14:17.369: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 21 21:14:17.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 21:14:17.494: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 21:14:17.494: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 21:14:17.494: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 21 21:14:17.494: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 21:14:17.496: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Apr 21 21:14:27.502: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 21 21:14:27.502: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Apr 21 21:14:27.502: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Apr 21 21:14:27.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999687s
    Apr 21 21:14:28.514: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996735638s
    Apr 21 21:14:29.517: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994297002s
    Apr 21 21:14:30.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991853482s
    Apr 21 21:14:31.522: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989018212s
    Apr 21 21:14:32.525: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986163211s
    Apr 21 21:14:33.528: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983492165s
    Apr 21 21:14:34.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.979996353s
    Apr 21 21:14:35.534: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.977088988s
    Apr 21 21:14:36.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 974.332502ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1235 04/21/23 21:14:37.537
    Apr 21 21:14:37.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 21 21:14:37.675: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 21 21:14:37.675: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 21 21:14:37.675: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 21 21:14:37.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 21 21:14:37.791: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 21 21:14:37.791: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 21 21:14:37.791: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 21 21:14:37.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-1235 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 21 21:14:37.903: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 21 21:14:37.903: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 21 21:14:37.903: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 21 21:14:37.903: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 04/21/23 21:14:47.916
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 21 21:14:47.917: INFO: Deleting all statefulset in ns statefulset-1235
    Apr 21 21:14:47.918: INFO: Scaling statefulset ss to 0
    Apr 21 21:14:47.924: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 21:14:47.926: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:14:47.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1235" for this suite. 04/21/23 21:14:47.934
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:14:47.938
Apr 21 21:14:47.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 21:14:47.939
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:14:47.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:14:47.947
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 04/21/23 21:14:47.949
STEP: submitting the pod to kubernetes 04/21/23 21:14:47.949
Apr 21 21:14:47.953: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518" in namespace "pods-4365" to be "running and ready"
Apr 21 21:14:47.954: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555607ms
Apr 21 21:14:47.954: INFO: The phase of Pod pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:14:49.957: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Running", Reason="", readiness=true. Elapsed: 2.004163734s
Apr 21 21:14:49.957: INFO: The phase of Pod pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518 is Running (Ready = true)
Apr 21 21:14:49.957: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 04/21/23 21:14:49.959
STEP: updating the pod 04/21/23 21:14:49.961
Apr 21 21:14:50.470: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518"
Apr 21 21:14:50.470: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518" in namespace "pods-4365" to be "terminated with reason DeadlineExceeded"
Apr 21 21:14:50.472: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Running", Reason="", readiness=true. Elapsed: 1.614345ms
Apr 21 21:14:52.475: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Running", Reason="", readiness=true. Elapsed: 2.005035263s
Apr 21 21:14:54.475: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00459482s
Apr 21 21:14:54.475: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 21:14:54.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4365" for this suite. 04/21/23 21:14:54.477
------------------------------
â€¢ [SLOW TEST] [6.543 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:14:47.938
    Apr 21 21:14:47.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 21:14:47.939
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:14:47.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:14:47.947
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 04/21/23 21:14:47.949
    STEP: submitting the pod to kubernetes 04/21/23 21:14:47.949
    Apr 21 21:14:47.953: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518" in namespace "pods-4365" to be "running and ready"
    Apr 21 21:14:47.954: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555607ms
    Apr 21 21:14:47.954: INFO: The phase of Pod pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:14:49.957: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Running", Reason="", readiness=true. Elapsed: 2.004163734s
    Apr 21 21:14:49.957: INFO: The phase of Pod pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518 is Running (Ready = true)
    Apr 21 21:14:49.957: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 04/21/23 21:14:49.959
    STEP: updating the pod 04/21/23 21:14:49.961
    Apr 21 21:14:50.470: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518"
    Apr 21 21:14:50.470: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518" in namespace "pods-4365" to be "terminated with reason DeadlineExceeded"
    Apr 21 21:14:50.472: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Running", Reason="", readiness=true. Elapsed: 1.614345ms
    Apr 21 21:14:52.475: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Running", Reason="", readiness=true. Elapsed: 2.005035263s
    Apr 21 21:14:54.475: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.00459482s
    Apr 21 21:14:54.475: INFO: Pod "pod-update-activedeadlineseconds-9d51b6ce-2265-48e9-9bdd-4dee96715518" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:14:54.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4365" for this suite. 04/21/23 21:14:54.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:14:54.482
Apr 21 21:14:54.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-lifecycle-hook 04/21/23 21:14:54.483
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:14:54.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:14:54.492
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/21/23 21:14:54.495
Apr 21 21:14:54.500: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3678" to be "running and ready"
Apr 21 21:14:54.502: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.73803ms
Apr 21 21:14:54.502: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:14:56.504: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004659456s
Apr 21 21:14:56.504: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 21 21:14:56.504: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 04/21/23 21:14:56.506
Apr 21 21:14:56.510: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3678" to be "running and ready"
Apr 21 21:14:56.512: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.91623ms
Apr 21 21:14:56.512: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:14:58.515: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004632776s
Apr 21 21:14:58.515: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Apr 21 21:14:58.515: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 04/21/23 21:14:58.516
STEP: delete the pod with lifecycle hook 04/21/23 21:14:58.523
Apr 21 21:14:58.529: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 21 21:14:58.531: INFO: Pod pod-with-poststart-http-hook still exists
Apr 21 21:15:00.532: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 21 21:15:00.534: INFO: Pod pod-with-poststart-http-hook still exists
Apr 21 21:15:02.532: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 21 21:15:02.535: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 21 21:15:02.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-3678" for this suite. 04/21/23 21:15:02.537
------------------------------
â€¢ [SLOW TEST] [8.059 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:14:54.482
    Apr 21 21:14:54.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/21/23 21:14:54.483
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:14:54.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:14:54.492
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/21/23 21:14:54.495
    Apr 21 21:14:54.500: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-3678" to be "running and ready"
    Apr 21 21:14:54.502: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 1.73803ms
    Apr 21 21:14:54.502: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:14:56.504: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.004659456s
    Apr 21 21:14:56.504: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 21 21:14:56.504: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 04/21/23 21:14:56.506
    Apr 21 21:14:56.510: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-3678" to be "running and ready"
    Apr 21 21:14:56.512: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 1.91623ms
    Apr 21 21:14:56.512: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:14:58.515: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.004632776s
    Apr 21 21:14:58.515: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Apr 21 21:14:58.515: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 04/21/23 21:14:58.516
    STEP: delete the pod with lifecycle hook 04/21/23 21:14:58.523
    Apr 21 21:14:58.529: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr 21 21:14:58.531: INFO: Pod pod-with-poststart-http-hook still exists
    Apr 21 21:15:00.532: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr 21 21:15:00.534: INFO: Pod pod-with-poststart-http-hook still exists
    Apr 21 21:15:02.532: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr 21 21:15:02.535: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:15:02.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-3678" for this suite. 04/21/23 21:15:02.537
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:15:02.542
Apr 21 21:15:02.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 21:15:02.543
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:15:02.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:15:02.554
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 04/21/23 21:15:02.556
Apr 21 21:15:02.561: INFO: Waiting up to 5m0s for pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb" in namespace "emptydir-7952" to be "Succeeded or Failed"
Apr 21 21:15:02.563: INFO: Pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873384ms
Apr 21 21:15:04.566: INFO: Pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005051189s
Apr 21 21:15:06.565: INFO: Pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00437922s
STEP: Saw pod success 04/21/23 21:15:06.565
Apr 21 21:15:06.565: INFO: Pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb" satisfied condition "Succeeded or Failed"
Apr 21 21:15:06.567: INFO: Trying to get logs from node k8sconformance-m02 pod pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb container test-container: <nil>
STEP: delete the pod 04/21/23 21:15:06.572
Apr 21 21:15:06.580: INFO: Waiting for pod pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb to disappear
Apr 21 21:15:06.581: INFO: Pod pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:15:06.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7952" for this suite. 04/21/23 21:15:06.583
------------------------------
â€¢ [4.044 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:15:02.542
    Apr 21 21:15:02.542: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 21:15:02.543
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:15:02.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:15:02.554
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 04/21/23 21:15:02.556
    Apr 21 21:15:02.561: INFO: Waiting up to 5m0s for pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb" in namespace "emptydir-7952" to be "Succeeded or Failed"
    Apr 21 21:15:02.563: INFO: Pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.873384ms
    Apr 21 21:15:04.566: INFO: Pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005051189s
    Apr 21 21:15:06.565: INFO: Pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00437922s
    STEP: Saw pod success 04/21/23 21:15:06.565
    Apr 21 21:15:06.565: INFO: Pod "pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb" satisfied condition "Succeeded or Failed"
    Apr 21 21:15:06.567: INFO: Trying to get logs from node k8sconformance-m02 pod pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb container test-container: <nil>
    STEP: delete the pod 04/21/23 21:15:06.572
    Apr 21 21:15:06.580: INFO: Waiting for pod pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb to disappear
    Apr 21 21:15:06.581: INFO: Pod pod-0bea2d88-8372-4d63-92a6-cc5abefe63eb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:15:06.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7952" for this suite. 04/21/23 21:15:06.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:15:06.588
Apr 21 21:15:06.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-preemption 04/21/23 21:15:06.589
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:15:06.598
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:15:06.6
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 21 21:15:06.610: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 21 21:16:06.621: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:06.623
Apr 21 21:16:06.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-preemption-path 04/21/23 21:16:06.624
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:06.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:06.633
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 04/21/23 21:16:06.635
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/21/23 21:16:06.635
Apr 21 21:16:06.640: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3142" to be "running"
Apr 21 21:16:06.642: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5548ms
Apr 21 21:16:08.644: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00400795s
Apr 21 21:16:08.644: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/21/23 21:16:08.646
Apr 21 21:16:08.653: INFO: found a healthy node: k8sconformance-m02
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Apr 21 21:16:14.703: INFO: pods created so far: [1 1 1]
Apr 21 21:16:14.703: INFO: length of pods created so far: 3
Apr 21 21:16:16.711: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:23.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-3142" for this suite. 04/21/23 21:16:23.751
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1108" for this suite. 04/21/23 21:16:23.755
------------------------------
â€¢ [SLOW TEST] [77.170 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:15:06.588
    Apr 21 21:15:06.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-preemption 04/21/23 21:15:06.589
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:15:06.598
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:15:06.6
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 21 21:15:06.610: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 21 21:16:06.621: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:06.623
    Apr 21 21:16:06.623: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-preemption-path 04/21/23 21:16:06.624
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:06.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:06.633
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 04/21/23 21:16:06.635
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/21/23 21:16:06.635
    Apr 21 21:16:06.640: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3142" to be "running"
    Apr 21 21:16:06.642: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.5548ms
    Apr 21 21:16:08.644: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.00400795s
    Apr 21 21:16:08.644: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/21/23 21:16:08.646
    Apr 21 21:16:08.653: INFO: found a healthy node: k8sconformance-m02
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Apr 21 21:16:14.703: INFO: pods created so far: [1 1 1]
    Apr 21 21:16:14.703: INFO: length of pods created so far: 3
    Apr 21 21:16:16.711: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:23.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:23.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-3142" for this suite. 04/21/23 21:16:23.751
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1108" for this suite. 04/21/23 21:16:23.755
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:23.758
Apr 21 21:16:23.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 21:16:23.759
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:23.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:23.77
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 04/21/23 21:16:23.772
STEP: Creating a ResourceQuota 04/21/23 21:16:28.773
STEP: Ensuring resource quota status is calculated 04/21/23 21:16:28.779
STEP: Creating a ReplicaSet 04/21/23 21:16:30.782
STEP: Ensuring resource quota status captures replicaset creation 04/21/23 21:16:30.791
STEP: Deleting a ReplicaSet 04/21/23 21:16:32.795
STEP: Ensuring resource quota status released usage 04/21/23 21:16:32.799
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:34.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6480" for this suite. 04/21/23 21:16:34.804
------------------------------
â€¢ [SLOW TEST] [11.051 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:23.758
    Apr 21 21:16:23.758: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 21:16:23.759
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:23.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:23.77
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 04/21/23 21:16:23.772
    STEP: Creating a ResourceQuota 04/21/23 21:16:28.773
    STEP: Ensuring resource quota status is calculated 04/21/23 21:16:28.779
    STEP: Creating a ReplicaSet 04/21/23 21:16:30.782
    STEP: Ensuring resource quota status captures replicaset creation 04/21/23 21:16:30.791
    STEP: Deleting a ReplicaSet 04/21/23 21:16:32.795
    STEP: Ensuring resource quota status released usage 04/21/23 21:16:32.799
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:34.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6480" for this suite. 04/21/23 21:16:34.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:34.81
Apr 21 21:16:34.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 21:16:34.81
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:34.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:34.819
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 04/21/23 21:16:34.821
Apr 21 21:16:34.827: INFO: Waiting up to 5m0s for pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b" in namespace "emptydir-5014" to be "Succeeded or Failed"
Apr 21 21:16:34.829: INFO: Pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.693221ms
Apr 21 21:16:36.832: INFO: Pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00454109s
Apr 21 21:16:38.832: INFO: Pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004525677s
STEP: Saw pod success 04/21/23 21:16:38.832
Apr 21 21:16:38.832: INFO: Pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b" satisfied condition "Succeeded or Failed"
Apr 21 21:16:38.834: INFO: Trying to get logs from node k8sconformance-m02 pod pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b container test-container: <nil>
STEP: delete the pod 04/21/23 21:16:38.843
Apr 21 21:16:38.851: INFO: Waiting for pod pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b to disappear
Apr 21 21:16:38.855: INFO: Pod pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:38.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5014" for this suite. 04/21/23 21:16:38.857
------------------------------
â€¢ [4.053 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:34.81
    Apr 21 21:16:34.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 21:16:34.81
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:34.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:34.819
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 04/21/23 21:16:34.821
    Apr 21 21:16:34.827: INFO: Waiting up to 5m0s for pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b" in namespace "emptydir-5014" to be "Succeeded or Failed"
    Apr 21 21:16:34.829: INFO: Pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 1.693221ms
    Apr 21 21:16:36.832: INFO: Pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00454109s
    Apr 21 21:16:38.832: INFO: Pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004525677s
    STEP: Saw pod success 04/21/23 21:16:38.832
    Apr 21 21:16:38.832: INFO: Pod "pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b" satisfied condition "Succeeded or Failed"
    Apr 21 21:16:38.834: INFO: Trying to get logs from node k8sconformance-m02 pod pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b container test-container: <nil>
    STEP: delete the pod 04/21/23 21:16:38.843
    Apr 21 21:16:38.851: INFO: Waiting for pod pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b to disappear
    Apr 21 21:16:38.855: INFO: Pod pod-d1f5d295-ebd0-4112-b80d-7ae17ed27e4b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:38.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5014" for this suite. 04/21/23 21:16:38.857
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:38.862
Apr 21 21:16:38.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename daemonsets 04/21/23 21:16:38.863
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:38.87
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:38.872
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 04/21/23 21:16:38.885
STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:16:38.888
Apr 21 21:16:38.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:16:38.892: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:16:39.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:16:39.897: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:16:40.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 21 21:16:40.896: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 04/21/23 21:16:40.898
Apr 21 21:16:40.900: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 04/21/23 21:16:40.9
Apr 21 21:16:40.905: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 04/21/23 21:16:40.905
Apr 21 21:16:40.906: INFO: Observed &DaemonSet event: ADDED
Apr 21 21:16:40.906: INFO: Observed &DaemonSet event: MODIFIED
Apr 21 21:16:40.907: INFO: Observed &DaemonSet event: MODIFIED
Apr 21 21:16:40.907: INFO: Observed &DaemonSet event: MODIFIED
Apr 21 21:16:40.907: INFO: Found daemon set daemon-set in namespace daemonsets-6273 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 21 21:16:40.907: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 04/21/23 21:16:40.907
STEP: watching for the daemon set status to be patched 04/21/23 21:16:40.912
Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: ADDED
Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: MODIFIED
Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: MODIFIED
Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: MODIFIED
Apr 21 21:16:40.913: INFO: Observed daemon set daemon-set in namespace daemonsets-6273 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: MODIFIED
Apr 21 21:16:40.914: INFO: Found daemon set daemon-set in namespace daemonsets-6273 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Apr 21 21:16:40.914: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:16:40.915
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6273, will wait for the garbage collector to delete the pods 04/21/23 21:16:40.915
Apr 21 21:16:40.971: INFO: Deleting DaemonSet.extensions daemon-set took: 4.238891ms
Apr 21 21:16:41.072: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.127777ms
Apr 21 21:16:43.174: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:16:43.174: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 21 21:16:43.176: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18761"},"items":null}

Apr 21 21:16:43.177: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18761"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:43.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-6273" for this suite. 04/21/23 21:16:43.184
------------------------------
â€¢ [4.325 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:38.862
    Apr 21 21:16:38.862: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename daemonsets 04/21/23 21:16:38.863
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:38.87
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:38.872
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 04/21/23 21:16:38.885
    STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:16:38.888
    Apr 21 21:16:38.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:16:38.892: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:16:39.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:16:39.897: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:16:40.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 21 21:16:40.896: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 04/21/23 21:16:40.898
    Apr 21 21:16:40.900: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 04/21/23 21:16:40.9
    Apr 21 21:16:40.905: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 04/21/23 21:16:40.905
    Apr 21 21:16:40.906: INFO: Observed &DaemonSet event: ADDED
    Apr 21 21:16:40.906: INFO: Observed &DaemonSet event: MODIFIED
    Apr 21 21:16:40.907: INFO: Observed &DaemonSet event: MODIFIED
    Apr 21 21:16:40.907: INFO: Observed &DaemonSet event: MODIFIED
    Apr 21 21:16:40.907: INFO: Found daemon set daemon-set in namespace daemonsets-6273 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 21 21:16:40.907: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 04/21/23 21:16:40.907
    STEP: watching for the daemon set status to be patched 04/21/23 21:16:40.912
    Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: ADDED
    Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: MODIFIED
    Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: MODIFIED
    Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: MODIFIED
    Apr 21 21:16:40.913: INFO: Observed daemon set daemon-set in namespace daemonsets-6273 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 21 21:16:40.913: INFO: Observed &DaemonSet event: MODIFIED
    Apr 21 21:16:40.914: INFO: Found daemon set daemon-set in namespace daemonsets-6273 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Apr 21 21:16:40.914: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:16:40.915
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6273, will wait for the garbage collector to delete the pods 04/21/23 21:16:40.915
    Apr 21 21:16:40.971: INFO: Deleting DaemonSet.extensions daemon-set took: 4.238891ms
    Apr 21 21:16:41.072: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.127777ms
    Apr 21 21:16:43.174: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:16:43.174: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 21 21:16:43.176: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18761"},"items":null}

    Apr 21 21:16:43.177: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18761"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:43.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-6273" for this suite. 04/21/23 21:16:43.184
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:43.188
Apr 21 21:16:43.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 21:16:43.189
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:43.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:43.2
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:43.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4784" for this suite. 04/21/23 21:16:43.224
------------------------------
â€¢ [0.040 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:43.188
    Apr 21 21:16:43.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 21:16:43.189
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:43.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:43.2
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:43.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4784" for this suite. 04/21/23 21:16:43.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:43.229
Apr 21 21:16:43.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 21:16:43.23
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:43.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:43.238
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-c32df1c7-ef54-4a20-b608-c8f45162b093 04/21/23 21:16:43.242
STEP: Creating the pod 04/21/23 21:16:43.246
Apr 21 21:16:43.251: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d44853c-410f-4a98-b25a-e0e2f1e10130" in namespace "configmap-8347" to be "running"
Apr 21 21:16:43.253: INFO: Pod "pod-configmaps-5d44853c-410f-4a98-b25a-e0e2f1e10130": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702586ms
Apr 21 21:16:45.255: INFO: Pod "pod-configmaps-5d44853c-410f-4a98-b25a-e0e2f1e10130": Phase="Running", Reason="", readiness=false. Elapsed: 2.004508228s
Apr 21 21:16:45.256: INFO: Pod "pod-configmaps-5d44853c-410f-4a98-b25a-e0e2f1e10130" satisfied condition "running"
STEP: Waiting for pod with text data 04/21/23 21:16:45.256
STEP: Waiting for pod with binary data 04/21/23 21:16:45.261
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:45.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8347" for this suite. 04/21/23 21:16:45.269
------------------------------
â€¢ [2.044 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:43.229
    Apr 21 21:16:43.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 21:16:43.23
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:43.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:43.238
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-c32df1c7-ef54-4a20-b608-c8f45162b093 04/21/23 21:16:43.242
    STEP: Creating the pod 04/21/23 21:16:43.246
    Apr 21 21:16:43.251: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d44853c-410f-4a98-b25a-e0e2f1e10130" in namespace "configmap-8347" to be "running"
    Apr 21 21:16:43.253: INFO: Pod "pod-configmaps-5d44853c-410f-4a98-b25a-e0e2f1e10130": Phase="Pending", Reason="", readiness=false. Elapsed: 1.702586ms
    Apr 21 21:16:45.255: INFO: Pod "pod-configmaps-5d44853c-410f-4a98-b25a-e0e2f1e10130": Phase="Running", Reason="", readiness=false. Elapsed: 2.004508228s
    Apr 21 21:16:45.256: INFO: Pod "pod-configmaps-5d44853c-410f-4a98-b25a-e0e2f1e10130" satisfied condition "running"
    STEP: Waiting for pod with text data 04/21/23 21:16:45.256
    STEP: Waiting for pod with binary data 04/21/23 21:16:45.261
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:45.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8347" for this suite. 04/21/23 21:16:45.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:45.275
Apr 21 21:16:45.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:16:45.275
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:45.284
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:45.286
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Apr 21 21:16:45.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1656 version'
Apr 21 21:16:45.338: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Apr 21 21:16:45.338: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:12Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1656" for this suite. 04/21/23 21:16:45.34
------------------------------
â€¢ [0.071 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:45.275
    Apr 21 21:16:45.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:16:45.275
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:45.284
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:45.286
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Apr 21 21:16:45.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1656 version'
    Apr 21 21:16:45.338: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Apr 21 21:16:45.338: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:12Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1656" for this suite. 04/21/23 21:16:45.34
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:45.346
Apr 21 21:16:45.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename namespaces 04/21/23 21:16:45.347
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:45.356
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:45.358
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 04/21/23 21:16:45.36
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:45.367
STEP: Creating a service in the namespace 04/21/23 21:16:45.368
STEP: Deleting the namespace 04/21/23 21:16:45.377
STEP: Waiting for the namespace to be removed. 04/21/23 21:16:45.381
STEP: Recreating the namespace 04/21/23 21:16:51.383
STEP: Verifying there is no service in the namespace 04/21/23 21:16:51.393
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5000" for this suite. 04/21/23 21:16:51.396
STEP: Destroying namespace "nsdeletetest-8901" for this suite. 04/21/23 21:16:51.399
Apr 21 21:16:51.403: INFO: Namespace nsdeletetest-8901 was already deleted
STEP: Destroying namespace "nsdeletetest-9447" for this suite. 04/21/23 21:16:51.403
------------------------------
â€¢ [SLOW TEST] [6.060 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:45.346
    Apr 21 21:16:45.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename namespaces 04/21/23 21:16:45.347
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:45.356
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:45.358
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 04/21/23 21:16:45.36
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:45.367
    STEP: Creating a service in the namespace 04/21/23 21:16:45.368
    STEP: Deleting the namespace 04/21/23 21:16:45.377
    STEP: Waiting for the namespace to be removed. 04/21/23 21:16:45.381
    STEP: Recreating the namespace 04/21/23 21:16:51.383
    STEP: Verifying there is no service in the namespace 04/21/23 21:16:51.393
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:51.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5000" for this suite. 04/21/23 21:16:51.396
    STEP: Destroying namespace "nsdeletetest-8901" for this suite. 04/21/23 21:16:51.399
    Apr 21 21:16:51.403: INFO: Namespace nsdeletetest-8901 was already deleted
    STEP: Destroying namespace "nsdeletetest-9447" for this suite. 04/21/23 21:16:51.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:51.407
Apr 21 21:16:51.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 21:16:51.408
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:51.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:51.419
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 04/21/23 21:16:51.42
Apr 21 21:16:51.427: INFO: Waiting up to 5m0s for pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04" in namespace "downward-api-3682" to be "running and ready"
Apr 21 21:16:51.429: INFO: Pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09964ms
Apr 21 21:16:51.429: INFO: The phase of Pod labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:16:53.432: INFO: Pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04": Phase="Running", Reason="", readiness=true. Elapsed: 2.005327468s
Apr 21 21:16:53.432: INFO: The phase of Pod labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04 is Running (Ready = true)
Apr 21 21:16:53.432: INFO: Pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04" satisfied condition "running and ready"
Apr 21 21:16:53.948: INFO: Successfully updated pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 21:16:57.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3682" for this suite. 04/21/23 21:16:57.969
------------------------------
â€¢ [SLOW TEST] [6.566 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:51.407
    Apr 21 21:16:51.407: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 21:16:51.408
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:51.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:51.419
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 04/21/23 21:16:51.42
    Apr 21 21:16:51.427: INFO: Waiting up to 5m0s for pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04" in namespace "downward-api-3682" to be "running and ready"
    Apr 21 21:16:51.429: INFO: Pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.09964ms
    Apr 21 21:16:51.429: INFO: The phase of Pod labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:16:53.432: INFO: Pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04": Phase="Running", Reason="", readiness=true. Elapsed: 2.005327468s
    Apr 21 21:16:53.432: INFO: The phase of Pod labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04 is Running (Ready = true)
    Apr 21 21:16:53.432: INFO: Pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04" satisfied condition "running and ready"
    Apr 21 21:16:53.948: INFO: Successfully updated pod "labelsupdate60f54f5d-ce60-4eae-98c6-f1fe4ee88d04"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:16:57.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3682" for this suite. 04/21/23 21:16:57.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:16:57.974
Apr 21 21:16:57.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-webhook 04/21/23 21:16:57.975
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:57.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:57.986
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 04/21/23 21:16:57.988
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/21/23 21:16:58.456
STEP: Deploying the custom resource conversion webhook pod 04/21/23 21:16:58.461
STEP: Wait for the deployment to be ready 04/21/23 21:16:58.472
Apr 21 21:16:58.477: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 21:17:00.484
STEP: Verifying the service has paired with the endpoint 04/21/23 21:17:00.492
Apr 21 21:17:01.493: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Apr 21 21:17:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Creating a v1 custom resource 04/21/23 21:17:04.055
STEP: v2 custom resource should be converted 04/21/23 21:17:04.058
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:17:04.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7712" for this suite. 04/21/23 21:17:04.603
------------------------------
â€¢ [SLOW TEST] [6.633 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:16:57.974
    Apr 21 21:16:57.974: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-webhook 04/21/23 21:16:57.975
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:16:57.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:16:57.986
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 04/21/23 21:16:57.988
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/21/23 21:16:58.456
    STEP: Deploying the custom resource conversion webhook pod 04/21/23 21:16:58.461
    STEP: Wait for the deployment to be ready 04/21/23 21:16:58.472
    Apr 21 21:16:58.477: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 21:17:00.484
    STEP: Verifying the service has paired with the endpoint 04/21/23 21:17:00.492
    Apr 21 21:17:01.493: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Apr 21 21:17:01.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Creating a v1 custom resource 04/21/23 21:17:04.055
    STEP: v2 custom resource should be converted 04/21/23 21:17:04.058
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:17:04.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7712" for this suite. 04/21/23 21:17:04.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:17:04.608
Apr 21 21:17:04.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:17:04.609
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:04.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:04.624
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 04/21/23 21:17:04.626
STEP: watching for the ServiceAccount to be added 04/21/23 21:17:04.635
STEP: patching the ServiceAccount 04/21/23 21:17:04.636
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 04/21/23 21:17:04.643
STEP: deleting the ServiceAccount 04/21/23 21:17:04.645
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 21 21:17:04.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2701" for this suite. 04/21/23 21:17:04.654
------------------------------
â€¢ [0.050 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:17:04.608
    Apr 21 21:17:04.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:17:04.609
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:04.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:04.624
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 04/21/23 21:17:04.626
    STEP: watching for the ServiceAccount to be added 04/21/23 21:17:04.635
    STEP: patching the ServiceAccount 04/21/23 21:17:04.636
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 04/21/23 21:17:04.643
    STEP: deleting the ServiceAccount 04/21/23 21:17:04.645
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:17:04.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2701" for this suite. 04/21/23 21:17:04.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:17:04.659
Apr 21 21:17:04.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 21:17:04.659
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:04.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:04.673
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Apr 21 21:17:04.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: creating the pod 04/21/23 21:17:04.676
STEP: submitting the pod to kubernetes 04/21/23 21:17:04.676
Apr 21 21:17:04.682: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc" in namespace "pods-8964" to be "running and ready"
Apr 21 21:17:04.684: INFO: Pod "pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90177ms
Apr 21 21:17:04.684: INFO: The phase of Pod pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:17:06.687: INFO: Pod "pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc": Phase="Running", Reason="", readiness=true. Elapsed: 2.005204799s
Apr 21 21:17:06.687: INFO: The phase of Pod pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc is Running (Ready = true)
Apr 21 21:17:06.687: INFO: Pod "pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 21:17:06.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8964" for this suite. 04/21/23 21:17:06.767
------------------------------
â€¢ [2.115 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:17:04.659
    Apr 21 21:17:04.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 21:17:04.659
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:04.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:04.673
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Apr 21 21:17:04.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: creating the pod 04/21/23 21:17:04.676
    STEP: submitting the pod to kubernetes 04/21/23 21:17:04.676
    Apr 21 21:17:04.682: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc" in namespace "pods-8964" to be "running and ready"
    Apr 21 21:17:04.684: INFO: Pod "pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.90177ms
    Apr 21 21:17:04.684: INFO: The phase of Pod pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:17:06.687: INFO: Pod "pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc": Phase="Running", Reason="", readiness=true. Elapsed: 2.005204799s
    Apr 21 21:17:06.687: INFO: The phase of Pod pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc is Running (Ready = true)
    Apr 21 21:17:06.687: INFO: Pod "pod-exec-websocket-f5a9864d-5c93-4223-8459-71f09c00addc" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:17:06.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8964" for this suite. 04/21/23 21:17:06.767
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:17:06.774
Apr 21 21:17:06.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename var-expansion 04/21/23 21:17:06.774
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:06.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:06.783
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 04/21/23 21:17:06.785
Apr 21 21:17:06.791: INFO: Waiting up to 5m0s for pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3" in namespace "var-expansion-7798" to be "Succeeded or Failed"
Apr 21 21:17:06.793: INFO: Pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597247ms
Apr 21 21:17:08.795: INFO: Pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004297306s
Apr 21 21:17:10.796: INFO: Pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005000436s
STEP: Saw pod success 04/21/23 21:17:10.796
Apr 21 21:17:10.796: INFO: Pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3" satisfied condition "Succeeded or Failed"
Apr 21 21:17:10.798: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3 container dapi-container: <nil>
STEP: delete the pod 04/21/23 21:17:10.803
Apr 21 21:17:10.812: INFO: Waiting for pod var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3 to disappear
Apr 21 21:17:10.813: INFO: Pod var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 21 21:17:10.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7798" for this suite. 04/21/23 21:17:10.815
------------------------------
â€¢ [4.047 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:17:06.774
    Apr 21 21:17:06.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename var-expansion 04/21/23 21:17:06.774
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:06.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:06.783
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 04/21/23 21:17:06.785
    Apr 21 21:17:06.791: INFO: Waiting up to 5m0s for pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3" in namespace "var-expansion-7798" to be "Succeeded or Failed"
    Apr 21 21:17:06.793: INFO: Pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597247ms
    Apr 21 21:17:08.795: INFO: Pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004297306s
    Apr 21 21:17:10.796: INFO: Pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005000436s
    STEP: Saw pod success 04/21/23 21:17:10.796
    Apr 21 21:17:10.796: INFO: Pod "var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3" satisfied condition "Succeeded or Failed"
    Apr 21 21:17:10.798: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3 container dapi-container: <nil>
    STEP: delete the pod 04/21/23 21:17:10.803
    Apr 21 21:17:10.812: INFO: Waiting for pod var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3 to disappear
    Apr 21 21:17:10.813: INFO: Pod var-expansion-9ab54c46-3327-4c5c-9ae7-49fa612766c3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:17:10.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7798" for this suite. 04/21/23 21:17:10.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:17:10.821
Apr 21 21:17:10.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:17:10.822
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:10.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:10.832
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 04/21/23 21:17:10.834
Apr 21 21:17:10.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 create -f -'
Apr 21 21:17:11.408: INFO: stderr: ""
Apr 21 21:17:11.408: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/21/23 21:17:11.408
Apr 21 21:17:11.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 21 21:17:11.470: INFO: stderr: ""
Apr 21 21:17:11.470: INFO: stdout: "update-demo-nautilus-4m7xh update-demo-nautilus-zt8wh "
Apr 21 21:17:11.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-4m7xh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 21 21:17:11.525: INFO: stderr: ""
Apr 21 21:17:11.525: INFO: stdout: ""
Apr 21 21:17:11.525: INFO: update-demo-nautilus-4m7xh is created but not running
Apr 21 21:17:16.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 21 21:17:16.585: INFO: stderr: ""
Apr 21 21:17:16.585: INFO: stdout: "update-demo-nautilus-4m7xh update-demo-nautilus-zt8wh "
Apr 21 21:17:16.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-4m7xh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 21 21:17:16.640: INFO: stderr: ""
Apr 21 21:17:16.640: INFO: stdout: "true"
Apr 21 21:17:16.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-4m7xh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 21 21:17:16.693: INFO: stderr: ""
Apr 21 21:17:16.693: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 21 21:17:16.693: INFO: validating pod update-demo-nautilus-4m7xh
Apr 21 21:17:16.696: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 21 21:17:16.696: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 21 21:17:16.696: INFO: update-demo-nautilus-4m7xh is verified up and running
Apr 21 21:17:16.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-zt8wh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 21 21:17:16.752: INFO: stderr: ""
Apr 21 21:17:16.752: INFO: stdout: "true"
Apr 21 21:17:16.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-zt8wh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 21 21:17:16.807: INFO: stderr: ""
Apr 21 21:17:16.807: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 21 21:17:16.807: INFO: validating pod update-demo-nautilus-zt8wh
Apr 21 21:17:16.810: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 21 21:17:16.810: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 21 21:17:16.810: INFO: update-demo-nautilus-zt8wh is verified up and running
STEP: using delete to clean up resources 04/21/23 21:17:16.81
Apr 21 21:17:16.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 delete --grace-period=0 --force -f -'
Apr 21 21:17:16.867: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 21 21:17:16.867: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 21 21:17:16.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get rc,svc -l name=update-demo --no-headers'
Apr 21 21:17:16.930: INFO: stderr: "No resources found in kubectl-838 namespace.\n"
Apr 21 21:17:16.930: INFO: stdout: ""
Apr 21 21:17:16.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 21 21:17:16.997: INFO: stderr: ""
Apr 21 21:17:16.997: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:17:16.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-838" for this suite. 04/21/23 21:17:17
------------------------------
â€¢ [SLOW TEST] [6.183 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:17:10.821
    Apr 21 21:17:10.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:17:10.822
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:10.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:10.832
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 04/21/23 21:17:10.834
    Apr 21 21:17:10.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 create -f -'
    Apr 21 21:17:11.408: INFO: stderr: ""
    Apr 21 21:17:11.408: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/21/23 21:17:11.408
    Apr 21 21:17:11.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 21 21:17:11.470: INFO: stderr: ""
    Apr 21 21:17:11.470: INFO: stdout: "update-demo-nautilus-4m7xh update-demo-nautilus-zt8wh "
    Apr 21 21:17:11.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-4m7xh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 21 21:17:11.525: INFO: stderr: ""
    Apr 21 21:17:11.525: INFO: stdout: ""
    Apr 21 21:17:11.525: INFO: update-demo-nautilus-4m7xh is created but not running
    Apr 21 21:17:16.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 21 21:17:16.585: INFO: stderr: ""
    Apr 21 21:17:16.585: INFO: stdout: "update-demo-nautilus-4m7xh update-demo-nautilus-zt8wh "
    Apr 21 21:17:16.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-4m7xh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 21 21:17:16.640: INFO: stderr: ""
    Apr 21 21:17:16.640: INFO: stdout: "true"
    Apr 21 21:17:16.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-4m7xh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 21 21:17:16.693: INFO: stderr: ""
    Apr 21 21:17:16.693: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 21 21:17:16.693: INFO: validating pod update-demo-nautilus-4m7xh
    Apr 21 21:17:16.696: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 21 21:17:16.696: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 21 21:17:16.696: INFO: update-demo-nautilus-4m7xh is verified up and running
    Apr 21 21:17:16.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-zt8wh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 21 21:17:16.752: INFO: stderr: ""
    Apr 21 21:17:16.752: INFO: stdout: "true"
    Apr 21 21:17:16.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods update-demo-nautilus-zt8wh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 21 21:17:16.807: INFO: stderr: ""
    Apr 21 21:17:16.807: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 21 21:17:16.807: INFO: validating pod update-demo-nautilus-zt8wh
    Apr 21 21:17:16.810: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 21 21:17:16.810: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 21 21:17:16.810: INFO: update-demo-nautilus-zt8wh is verified up and running
    STEP: using delete to clean up resources 04/21/23 21:17:16.81
    Apr 21 21:17:16.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 delete --grace-period=0 --force -f -'
    Apr 21 21:17:16.867: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 21 21:17:16.867: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Apr 21 21:17:16.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get rc,svc -l name=update-demo --no-headers'
    Apr 21 21:17:16.930: INFO: stderr: "No resources found in kubectl-838 namespace.\n"
    Apr 21 21:17:16.930: INFO: stdout: ""
    Apr 21 21:17:16.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-838 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr 21 21:17:16.997: INFO: stderr: ""
    Apr 21 21:17:16.997: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:17:16.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-838" for this suite. 04/21/23 21:17:17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:17:17.004
Apr 21 21:17:17.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:17:17.005
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:17.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:17.014
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-870a34e0-a549-4a6a-8531-e7779bb26ee9 04/21/23 21:17:17.016
STEP: Creating a pod to test consume secrets 04/21/23 21:17:17.02
Apr 21 21:17:17.025: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e" in namespace "projected-3796" to be "Succeeded or Failed"
Apr 21 21:17:17.027: INFO: Pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.768187ms
Apr 21 21:17:19.030: INFO: Pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005194663s
Apr 21 21:17:21.030: INFO: Pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004900939s
STEP: Saw pod success 04/21/23 21:17:21.03
Apr 21 21:17:21.030: INFO: Pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e" satisfied condition "Succeeded or Failed"
Apr 21 21:17:21.032: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e container projected-secret-volume-test: <nil>
STEP: delete the pod 04/21/23 21:17:21.037
Apr 21 21:17:21.047: INFO: Waiting for pod pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e to disappear
Apr 21 21:17:21.048: INFO: Pod pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 21 21:17:21.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3796" for this suite. 04/21/23 21:17:21.051
------------------------------
â€¢ [4.050 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:17:17.004
    Apr 21 21:17:17.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:17:17.005
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:17.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:17.014
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-870a34e0-a549-4a6a-8531-e7779bb26ee9 04/21/23 21:17:17.016
    STEP: Creating a pod to test consume secrets 04/21/23 21:17:17.02
    Apr 21 21:17:17.025: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e" in namespace "projected-3796" to be "Succeeded or Failed"
    Apr 21 21:17:17.027: INFO: Pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.768187ms
    Apr 21 21:17:19.030: INFO: Pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005194663s
    Apr 21 21:17:21.030: INFO: Pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004900939s
    STEP: Saw pod success 04/21/23 21:17:21.03
    Apr 21 21:17:21.030: INFO: Pod "pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e" satisfied condition "Succeeded or Failed"
    Apr 21 21:17:21.032: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 21:17:21.037
    Apr 21 21:17:21.047: INFO: Waiting for pod pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e to disappear
    Apr 21 21:17:21.048: INFO: Pod pod-projected-secrets-916a2853-4e42-4b8d-8477-82a594aca92e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:17:21.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3796" for this suite. 04/21/23 21:17:21.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:17:21.054
Apr 21 21:17:21.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename var-expansion 04/21/23 21:17:21.055
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:21.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:21.066
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 04/21/23 21:17:21.067
Apr 21 21:17:21.072: INFO: Waiting up to 2m0s for pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" in namespace "var-expansion-2813" to be "running"
Apr 21 21:17:21.074: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.665ms
Apr 21 21:17:23.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004652595s
Apr 21 21:17:25.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004459172s
Apr 21 21:17:27.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004822805s
Apr 21 21:17:29.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006188429s
Apr 21 21:17:31.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004566333s
Apr 21 21:17:33.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005211457s
Apr 21 21:17:35.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004272525s
Apr 21 21:17:37.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006001843s
Apr 21 21:17:39.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005207279s
Apr 21 21:17:41.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004444926s
Apr 21 21:17:43.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005743101s
Apr 21 21:17:45.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004209884s
Apr 21 21:17:47.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.0057721s
Apr 21 21:17:49.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005580336s
Apr 21 21:17:51.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004445962s
Apr 21 21:17:53.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005726691s
Apr 21 21:17:55.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004182728s
Apr 21 21:17:57.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004593707s
Apr 21 21:17:59.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004160452s
Apr 21 21:18:01.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004475313s
Apr 21 21:18:03.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 42.0055565s
Apr 21 21:18:05.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005370404s
Apr 21 21:18:07.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005152326s
Apr 21 21:18:09.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004076312s
Apr 21 21:18:11.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005440649s
Apr 21 21:18:13.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005732124s
Apr 21 21:18:15.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 54.0040462s
Apr 21 21:18:17.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004589936s
Apr 21 21:18:19.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005740729s
Apr 21 21:18:21.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005507216s
Apr 21 21:18:23.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005923298s
Apr 21 21:18:25.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.003948115s
Apr 21 21:18:27.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004591043s
Apr 21 21:18:29.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005723975s
Apr 21 21:18:31.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005603427s
Apr 21 21:18:33.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005765396s
Apr 21 21:18:35.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004173669s
Apr 21 21:18:37.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004617504s
Apr 21 21:18:39.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005763376s
Apr 21 21:18:41.079: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006613748s
Apr 21 21:18:43.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004407976s
Apr 21 21:18:45.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004247038s
Apr 21 21:18:47.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004161462s
Apr 21 21:18:49.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004604863s
Apr 21 21:18:51.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004462831s
Apr 21 21:18:53.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004970707s
Apr 21 21:18:55.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004680988s
Apr 21 21:18:57.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005912706s
Apr 21 21:18:59.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004242387s
Apr 21 21:19:01.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004413339s
Apr 21 21:19:03.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.003944603s
Apr 21 21:19:05.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.003877398s
Apr 21 21:19:07.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005318568s
Apr 21 21:19:09.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00564723s
Apr 21 21:19:11.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004181943s
Apr 21 21:19:13.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005690863s
Apr 21 21:19:15.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004096431s
Apr 21 21:19:17.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004961495s
Apr 21 21:19:19.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005392553s
Apr 21 21:19:21.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004595118s
Apr 21 21:19:21.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006412682s
STEP: updating the pod 04/21/23 21:19:21.079
Apr 21 21:19:21.587: INFO: Successfully updated pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9"
STEP: waiting for pod running 04/21/23 21:19:21.587
Apr 21 21:19:21.587: INFO: Waiting up to 2m0s for pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" in namespace "var-expansion-2813" to be "running"
Apr 21 21:19:21.589: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.821095ms
Apr 21 21:19:23.592: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004414101s
Apr 21 21:19:23.592: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" satisfied condition "running"
STEP: deleting the pod gracefully 04/21/23 21:19:23.592
Apr 21 21:19:23.592: INFO: Deleting pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" in namespace "var-expansion-2813"
Apr 21 21:19:23.597: INFO: Wait up to 5m0s for pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 21 21:19:55.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2813" for this suite. 04/21/23 21:19:55.605
------------------------------
â€¢ [SLOW TEST] [154.555 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:17:21.054
    Apr 21 21:17:21.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename var-expansion 04/21/23 21:17:21.055
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:17:21.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:17:21.066
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 04/21/23 21:17:21.067
    Apr 21 21:17:21.072: INFO: Waiting up to 2m0s for pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" in namespace "var-expansion-2813" to be "running"
    Apr 21 21:17:21.074: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.665ms
    Apr 21 21:17:23.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004652595s
    Apr 21 21:17:25.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004459172s
    Apr 21 21:17:27.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004822805s
    Apr 21 21:17:29.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006188429s
    Apr 21 21:17:31.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004566333s
    Apr 21 21:17:33.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.005211457s
    Apr 21 21:17:35.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.004272525s
    Apr 21 21:17:37.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006001843s
    Apr 21 21:17:39.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005207279s
    Apr 21 21:17:41.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.004444926s
    Apr 21 21:17:43.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.005743101s
    Apr 21 21:17:45.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.004209884s
    Apr 21 21:17:47.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.0057721s
    Apr 21 21:17:49.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005580336s
    Apr 21 21:17:51.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.004445962s
    Apr 21 21:17:53.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.005726691s
    Apr 21 21:17:55.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.004182728s
    Apr 21 21:17:57.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 36.004593707s
    Apr 21 21:17:59.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.004160452s
    Apr 21 21:18:01.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 40.004475313s
    Apr 21 21:18:03.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 42.0055565s
    Apr 21 21:18:05.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005370404s
    Apr 21 21:18:07.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005152326s
    Apr 21 21:18:09.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004076312s
    Apr 21 21:18:11.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.005440649s
    Apr 21 21:18:13.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 52.005732124s
    Apr 21 21:18:15.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 54.0040462s
    Apr 21 21:18:17.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004589936s
    Apr 21 21:18:19.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005740729s
    Apr 21 21:18:21.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.005507216s
    Apr 21 21:18:23.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005923298s
    Apr 21 21:18:25.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.003948115s
    Apr 21 21:18:27.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.004591043s
    Apr 21 21:18:29.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005723975s
    Apr 21 21:18:31.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.005603427s
    Apr 21 21:18:33.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005765396s
    Apr 21 21:18:35.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.004173669s
    Apr 21 21:18:37.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.004617504s
    Apr 21 21:18:39.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005763376s
    Apr 21 21:18:41.079: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006613748s
    Apr 21 21:18:43.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.004407976s
    Apr 21 21:18:45.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.004247038s
    Apr 21 21:18:47.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.004161462s
    Apr 21 21:18:49.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004604863s
    Apr 21 21:18:51.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004462831s
    Apr 21 21:18:53.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.004970707s
    Apr 21 21:18:55.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.004680988s
    Apr 21 21:18:57.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.005912706s
    Apr 21 21:18:59.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.004242387s
    Apr 21 21:19:01.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.004413339s
    Apr 21 21:19:03.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.003944603s
    Apr 21 21:19:05.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.003877398s
    Apr 21 21:19:07.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005318568s
    Apr 21 21:19:09.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00564723s
    Apr 21 21:19:11.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.004181943s
    Apr 21 21:19:13.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005690863s
    Apr 21 21:19:15.076: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.004096431s
    Apr 21 21:19:17.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.004961495s
    Apr 21 21:19:19.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.005392553s
    Apr 21 21:19:21.077: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.004595118s
    Apr 21 21:19:21.078: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.006412682s
    STEP: updating the pod 04/21/23 21:19:21.079
    Apr 21 21:19:21.587: INFO: Successfully updated pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9"
    STEP: waiting for pod running 04/21/23 21:19:21.587
    Apr 21 21:19:21.587: INFO: Waiting up to 2m0s for pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" in namespace "var-expansion-2813" to be "running"
    Apr 21 21:19:21.589: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.821095ms
    Apr 21 21:19:23.592: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.004414101s
    Apr 21 21:19:23.592: INFO: Pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" satisfied condition "running"
    STEP: deleting the pod gracefully 04/21/23 21:19:23.592
    Apr 21 21:19:23.592: INFO: Deleting pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" in namespace "var-expansion-2813"
    Apr 21 21:19:23.597: INFO: Wait up to 5m0s for pod "var-expansion-7e28c91d-262e-497c-843f-6ac2ddeccdf9" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:19:55.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2813" for this suite. 04/21/23 21:19:55.605
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:19:55.614
Apr 21 21:19:55.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:19:55.614
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:19:55.625
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:19:55.627
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 04/21/23 21:19:55.63
Apr 21 21:19:55.636: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf" in namespace "projected-4301" to be "Succeeded or Failed"
Apr 21 21:19:55.639: INFO: Pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.817864ms
Apr 21 21:19:57.642: INFO: Pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006202232s
Apr 21 21:19:59.641: INFO: Pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005212923s
STEP: Saw pod success 04/21/23 21:19:59.641
Apr 21 21:19:59.641: INFO: Pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf" satisfied condition "Succeeded or Failed"
Apr 21 21:19:59.643: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf container client-container: <nil>
STEP: delete the pod 04/21/23 21:19:59.655
Apr 21 21:19:59.662: INFO: Waiting for pod downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf to disappear
Apr 21 21:19:59.663: INFO: Pod downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 21:19:59.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4301" for this suite. 04/21/23 21:19:59.665
------------------------------
â€¢ [4.055 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:19:55.614
    Apr 21 21:19:55.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:19:55.614
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:19:55.625
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:19:55.627
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 04/21/23 21:19:55.63
    Apr 21 21:19:55.636: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf" in namespace "projected-4301" to be "Succeeded or Failed"
    Apr 21 21:19:55.639: INFO: Pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.817864ms
    Apr 21 21:19:57.642: INFO: Pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006202232s
    Apr 21 21:19:59.641: INFO: Pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005212923s
    STEP: Saw pod success 04/21/23 21:19:59.641
    Apr 21 21:19:59.641: INFO: Pod "downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf" satisfied condition "Succeeded or Failed"
    Apr 21 21:19:59.643: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf container client-container: <nil>
    STEP: delete the pod 04/21/23 21:19:59.655
    Apr 21 21:19:59.662: INFO: Waiting for pod downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf to disappear
    Apr 21 21:19:59.663: INFO: Pod downwardapi-volume-31758c5c-a897-4a23-b768-0b6f028f97bf no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:19:59.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4301" for this suite. 04/21/23 21:19:59.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:19:59.67
Apr 21 21:19:59.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replicaset 04/21/23 21:19:59.671
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:19:59.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:19:59.681
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Apr 21 21:19:59.689: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 21 21:20:04.692: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/21/23 21:20:04.692
STEP: Scaling up "test-rs" replicaset  04/21/23 21:20:04.692
Apr 21 21:20:04.697: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 04/21/23 21:20:04.697
W0421 21:20:04.702919      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr 21 21:20:04.704: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 1, AvailableReplicas 1
Apr 21 21:20:04.712: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 1, AvailableReplicas 1
Apr 21 21:20:04.722: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 1, AvailableReplicas 1
Apr 21 21:20:04.733: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 1, AvailableReplicas 1
Apr 21 21:20:05.544: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 2, AvailableReplicas 2
Apr 21 21:20:06.548: INFO: observed Replicaset test-rs in namespace replicaset-3657 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:06.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3657" for this suite. 04/21/23 21:20:06.55
------------------------------
â€¢ [SLOW TEST] [6.884 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:19:59.67
    Apr 21 21:19:59.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replicaset 04/21/23 21:19:59.671
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:19:59.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:19:59.681
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Apr 21 21:19:59.689: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 21 21:20:04.692: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/21/23 21:20:04.692
    STEP: Scaling up "test-rs" replicaset  04/21/23 21:20:04.692
    Apr 21 21:20:04.697: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 04/21/23 21:20:04.697
    W0421 21:20:04.702919      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr 21 21:20:04.704: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 1, AvailableReplicas 1
    Apr 21 21:20:04.712: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 1, AvailableReplicas 1
    Apr 21 21:20:04.722: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 1, AvailableReplicas 1
    Apr 21 21:20:04.733: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 1, AvailableReplicas 1
    Apr 21 21:20:05.544: INFO: observed ReplicaSet test-rs in namespace replicaset-3657 with ReadyReplicas 2, AvailableReplicas 2
    Apr 21 21:20:06.548: INFO: observed Replicaset test-rs in namespace replicaset-3657 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:06.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3657" for this suite. 04/21/23 21:20:06.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:06.555
Apr 21 21:20:06.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sysctl 04/21/23 21:20:06.556
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:06.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:06.566
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 04/21/23 21:20:06.568
STEP: Watching for error events or started pod 04/21/23 21:20:06.574
STEP: Waiting for pod completion 04/21/23 21:20:08.577
Apr 21 21:20:08.577: INFO: Waiting up to 3m0s for pod "sysctl-0a2ebe1f-67a5-49a9-b90e-df2327c58434" in namespace "sysctl-5442" to be "completed"
Apr 21 21:20:08.579: INFO: Pod "sysctl-0a2ebe1f-67a5-49a9-b90e-df2327c58434": Phase="Pending", Reason="", readiness=false. Elapsed: 1.94563ms
Apr 21 21:20:10.582: INFO: Pod "sysctl-0a2ebe1f-67a5-49a9-b90e-df2327c58434": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005221506s
Apr 21 21:20:10.582: INFO: Pod "sysctl-0a2ebe1f-67a5-49a9-b90e-df2327c58434" satisfied condition "completed"
STEP: Checking that the pod succeeded 04/21/23 21:20:10.584
STEP: Getting logs from the pod 04/21/23 21:20:10.584
STEP: Checking that the sysctl is actually updated 04/21/23 21:20:10.589
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:10.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-5442" for this suite. 04/21/23 21:20:10.592
------------------------------
â€¢ [4.041 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:06.555
    Apr 21 21:20:06.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sysctl 04/21/23 21:20:06.556
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:06.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:06.566
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 04/21/23 21:20:06.568
    STEP: Watching for error events or started pod 04/21/23 21:20:06.574
    STEP: Waiting for pod completion 04/21/23 21:20:08.577
    Apr 21 21:20:08.577: INFO: Waiting up to 3m0s for pod "sysctl-0a2ebe1f-67a5-49a9-b90e-df2327c58434" in namespace "sysctl-5442" to be "completed"
    Apr 21 21:20:08.579: INFO: Pod "sysctl-0a2ebe1f-67a5-49a9-b90e-df2327c58434": Phase="Pending", Reason="", readiness=false. Elapsed: 1.94563ms
    Apr 21 21:20:10.582: INFO: Pod "sysctl-0a2ebe1f-67a5-49a9-b90e-df2327c58434": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005221506s
    Apr 21 21:20:10.582: INFO: Pod "sysctl-0a2ebe1f-67a5-49a9-b90e-df2327c58434" satisfied condition "completed"
    STEP: Checking that the pod succeeded 04/21/23 21:20:10.584
    STEP: Getting logs from the pod 04/21/23 21:20:10.584
    STEP: Checking that the sysctl is actually updated 04/21/23 21:20:10.589
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:10.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-5442" for this suite. 04/21/23 21:20:10.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:10.596
Apr 21 21:20:10.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:20:10.597
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:10.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:10.606
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 04/21/23 21:20:10.608
Apr 21 21:20:10.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-706 create -f -'
Apr 21 21:20:10.758: INFO: stderr: ""
Apr 21 21:20:10.758: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 04/21/23 21:20:10.758
Apr 21 21:20:10.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-706 diff -f -'
Apr 21 21:20:10.917: INFO: rc: 1
Apr 21 21:20:10.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-706 delete -f -'
Apr 21 21:20:10.977: INFO: stderr: ""
Apr 21 21:20:10.977: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:10.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-706" for this suite. 04/21/23 21:20:10.98
------------------------------
â€¢ [0.388 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:10.596
    Apr 21 21:20:10.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:20:10.597
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:10.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:10.606
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 04/21/23 21:20:10.608
    Apr 21 21:20:10.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-706 create -f -'
    Apr 21 21:20:10.758: INFO: stderr: ""
    Apr 21 21:20:10.758: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 04/21/23 21:20:10.758
    Apr 21 21:20:10.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-706 diff -f -'
    Apr 21 21:20:10.917: INFO: rc: 1
    Apr 21 21:20:10.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-706 delete -f -'
    Apr 21 21:20:10.977: INFO: stderr: ""
    Apr 21 21:20:10.977: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:10.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-706" for this suite. 04/21/23 21:20:10.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:10.984
Apr 21 21:20:10.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-runtime 04/21/23 21:20:10.985
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:10.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:10.995
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 04/21/23 21:20:11.004
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 04/21/23 21:20:28.05
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 04/21/23 21:20:28.052
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 04/21/23 21:20:28.055
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 04/21/23 21:20:28.055
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 04/21/23 21:20:28.068
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 04/21/23 21:20:30.075
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 04/21/23 21:20:32.082
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 04/21/23 21:20:32.085
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 04/21/23 21:20:32.086
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 04/21/23 21:20:32.099
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 04/21/23 21:20:33.103
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 04/21/23 21:20:35.11
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 04/21/23 21:20:35.114
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 04/21/23 21:20:35.114
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:35.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7002" for this suite. 04/21/23 21:20:35.131
------------------------------
â€¢ [SLOW TEST] [24.150 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:10.984
    Apr 21 21:20:10.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-runtime 04/21/23 21:20:10.985
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:10.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:10.995
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 04/21/23 21:20:11.004
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 04/21/23 21:20:28.05
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 04/21/23 21:20:28.052
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 04/21/23 21:20:28.055
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 04/21/23 21:20:28.055
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 04/21/23 21:20:28.068
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 04/21/23 21:20:30.075
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 04/21/23 21:20:32.082
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 04/21/23 21:20:32.085
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 04/21/23 21:20:32.086
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 04/21/23 21:20:32.099
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 04/21/23 21:20:33.103
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 04/21/23 21:20:35.11
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 04/21/23 21:20:35.114
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 04/21/23 21:20:35.114
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:35.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7002" for this suite. 04/21/23 21:20:35.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:35.135
Apr 21 21:20:35.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:20:35.136
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:35.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:35.146
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 04/21/23 21:20:35.148
Apr 21 21:20:35.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: rename a version 04/21/23 21:20:38.46
STEP: check the new version name is served 04/21/23 21:20:38.471
STEP: check the old version name is removed 04/21/23 21:20:40.294
STEP: check the other version is not changed 04/21/23 21:20:40.948
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:43.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8400" for this suite. 04/21/23 21:20:43.546
------------------------------
â€¢ [SLOW TEST] [8.415 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:35.135
    Apr 21 21:20:35.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:20:35.136
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:35.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:35.146
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 04/21/23 21:20:35.148
    Apr 21 21:20:35.148: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: rename a version 04/21/23 21:20:38.46
    STEP: check the new version name is served 04/21/23 21:20:38.471
    STEP: check the old version name is removed 04/21/23 21:20:40.294
    STEP: check the other version is not changed 04/21/23 21:20:40.948
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:43.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8400" for this suite. 04/21/23 21:20:43.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:43.551
Apr 21 21:20:43.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename disruption 04/21/23 21:20:43.551
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:43.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:43.562
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:43.564
Apr 21 21:20:43.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename disruption-2 04/21/23 21:20:43.565
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:43.573
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:43.575
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 04/21/23 21:20:43.58
STEP: Waiting for the pdb to be processed 04/21/23 21:20:45.588
STEP: Waiting for the pdb to be processed 04/21/23 21:20:47.596
STEP: listing a collection of PDBs across all namespaces 04/21/23 21:20:49.6
STEP: listing a collection of PDBs in namespace disruption-8748 04/21/23 21:20:49.602
STEP: deleting a collection of PDBs 04/21/23 21:20:49.604
STEP: Waiting for the PDB collection to be deleted 04/21/23 21:20:49.61
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:49.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:49.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-6951" for this suite. 04/21/23 21:20:49.616
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8748" for this suite. 04/21/23 21:20:49.62
------------------------------
â€¢ [SLOW TEST] [6.074 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:43.551
    Apr 21 21:20:43.551: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename disruption 04/21/23 21:20:43.551
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:43.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:43.562
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:43.564
    Apr 21 21:20:43.564: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename disruption-2 04/21/23 21:20:43.565
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:43.573
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:43.575
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 04/21/23 21:20:43.58
    STEP: Waiting for the pdb to be processed 04/21/23 21:20:45.588
    STEP: Waiting for the pdb to be processed 04/21/23 21:20:47.596
    STEP: listing a collection of PDBs across all namespaces 04/21/23 21:20:49.6
    STEP: listing a collection of PDBs in namespace disruption-8748 04/21/23 21:20:49.602
    STEP: deleting a collection of PDBs 04/21/23 21:20:49.604
    STEP: Waiting for the PDB collection to be deleted 04/21/23 21:20:49.61
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:49.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:49.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-6951" for this suite. 04/21/23 21:20:49.616
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8748" for this suite. 04/21/23 21:20:49.62
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:49.625
Apr 21 21:20:49.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename watch 04/21/23 21:20:49.626
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:49.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:49.636
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 04/21/23 21:20:49.637
STEP: modifying the configmap once 04/21/23 21:20:49.64
STEP: modifying the configmap a second time 04/21/23 21:20:49.645
STEP: deleting the configmap 04/21/23 21:20:49.649
STEP: creating a watch on configmaps from the resource version returned by the first update 04/21/23 21:20:49.652
STEP: Expecting to observe notifications for all changes to the configmap after the first update 04/21/23 21:20:49.653
Apr 21 21:20:49.653: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3595  415c5817-2e90-4971-a275-b37af362a9d3 19651 0 2023-04-21 21:20:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-21 21:20:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:20:49.653: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3595  415c5817-2e90-4971-a275-b37af362a9d3 19652 0 2023-04-21 21:20:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-21 21:20:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:49.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3595" for this suite. 04/21/23 21:20:49.657
------------------------------
â€¢ [0.036 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:49.625
    Apr 21 21:20:49.625: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename watch 04/21/23 21:20:49.626
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:49.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:49.636
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 04/21/23 21:20:49.637
    STEP: modifying the configmap once 04/21/23 21:20:49.64
    STEP: modifying the configmap a second time 04/21/23 21:20:49.645
    STEP: deleting the configmap 04/21/23 21:20:49.649
    STEP: creating a watch on configmaps from the resource version returned by the first update 04/21/23 21:20:49.652
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 04/21/23 21:20:49.653
    Apr 21 21:20:49.653: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3595  415c5817-2e90-4971-a275-b37af362a9d3 19651 0 2023-04-21 21:20:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-21 21:20:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:20:49.653: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3595  415c5817-2e90-4971-a275-b37af362a9d3 19652 0 2023-04-21 21:20:49 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-21 21:20:49 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:49.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3595" for this suite. 04/21/23 21:20:49.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:49.662
Apr 21 21:20:49.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 21:20:49.662
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:49.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:49.671
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-a82ff825-0c2b-4266-b920-1f431597ac21 04/21/23 21:20:49.673
STEP: Creating a pod to test consume configMaps 04/21/23 21:20:49.676
Apr 21 21:20:49.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2" in namespace "configmap-3205" to be "Succeeded or Failed"
Apr 21 21:20:49.682: INFO: Pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.598575ms
Apr 21 21:20:51.685: INFO: Pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004106682s
Apr 21 21:20:53.684: INFO: Pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004016169s
STEP: Saw pod success 04/21/23 21:20:53.685
Apr 21 21:20:53.685: INFO: Pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2" satisfied condition "Succeeded or Failed"
Apr 21 21:20:53.687: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 21:20:53.692
Apr 21 21:20:53.700: INFO: Waiting for pod pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2 to disappear
Apr 21 21:20:53.702: INFO: Pod pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:53.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3205" for this suite. 04/21/23 21:20:53.704
------------------------------
â€¢ [4.047 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:49.662
    Apr 21 21:20:49.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 21:20:49.662
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:49.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:49.671
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-a82ff825-0c2b-4266-b920-1f431597ac21 04/21/23 21:20:49.673
    STEP: Creating a pod to test consume configMaps 04/21/23 21:20:49.676
    Apr 21 21:20:49.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2" in namespace "configmap-3205" to be "Succeeded or Failed"
    Apr 21 21:20:49.682: INFO: Pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.598575ms
    Apr 21 21:20:51.685: INFO: Pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004106682s
    Apr 21 21:20:53.684: INFO: Pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004016169s
    STEP: Saw pod success 04/21/23 21:20:53.685
    Apr 21 21:20:53.685: INFO: Pod "pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2" satisfied condition "Succeeded or Failed"
    Apr 21 21:20:53.687: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 21:20:53.692
    Apr 21 21:20:53.700: INFO: Waiting for pod pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2 to disappear
    Apr 21 21:20:53.702: INFO: Pod pod-configmaps-11fce4ae-a7d0-4378-89ee-9a7a3d614eb2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:53.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3205" for this suite. 04/21/23 21:20:53.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:53.709
Apr 21 21:20:53.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:20:53.71
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:53.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:53.719
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-992f5"  04/21/23 21:20:53.721
Apr 21 21:20:53.724: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-992f5"  04/21/23 21:20:53.724
Apr 21 21:20:53.729: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 21 21:20:53.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9443" for this suite. 04/21/23 21:20:53.731
------------------------------
â€¢ [0.025 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:53.709
    Apr 21 21:20:53.709: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:20:53.71
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:53.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:53.719
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-992f5"  04/21/23 21:20:53.721
    Apr 21 21:20:53.724: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-992f5"  04/21/23 21:20:53.724
    Apr 21 21:20:53.729: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:20:53.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9443" for this suite. 04/21/23 21:20:53.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:20:53.735
Apr 21 21:20:53.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-pred 04/21/23 21:20:53.735
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:53.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:53.746
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 21 21:20:53.748: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 21 21:20:53.751: INFO: Waiting for terminating namespaces to be deleted...
Apr 21 21:20:53.753: INFO: 
Logging pods the apiserver thinks is on node k8sconformance before test
Apr 21 21:20:53.756: INFO: coredns-787d4945fb-bp78j from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.756: INFO: 	Container coredns ready: true, restart count 1
Apr 21 21:20:53.756: INFO: etcd-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.756: INFO: 	Container etcd ready: true, restart count 0
Apr 21 21:20:53.756: INFO: kindnet-6bcq2 from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.756: INFO: 	Container kindnet-cni ready: true, restart count 0
Apr 21 21:20:53.756: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.756: INFO: 	Container kube-apiserver ready: true, restart count 0
Apr 21 21:20:53.756: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.756: INFO: 	Container kube-controller-manager ready: true, restart count 0
Apr 21 21:20:53.756: INFO: kube-proxy-k9znz from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.756: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 21 21:20:53.756: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.756: INFO: 	Container kube-scheduler ready: true, restart count 0
Apr 21 21:20:53.756: INFO: storage-provisioner from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.756: INFO: 	Container storage-provisioner ready: true, restart count 0
Apr 21 21:20:53.756: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 21:20:53.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 21:20:53.756: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 21 21:20:53.756: INFO: 
Logging pods the apiserver thinks is on node k8sconformance-m02 before test
Apr 21 21:20:53.759: INFO: kindnet-znkt6 from kube-system started at 2023-04-21 20:47:53 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.759: INFO: 	Container kindnet-cni ready: true, restart count 0
Apr 21 21:20:53.759: INFO: kube-proxy-fvpz4 from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.759: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 21 21:20:53.759: INFO: sonobuoy from sonobuoy started at 2023-04-21 20:13:43 +0000 UTC (1 container statuses recorded)
Apr 21 21:20:53.759: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 21 21:20:53.759: INFO: sonobuoy-e2e-job-ddbedf54a58f4885 from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 21:20:53.759: INFO: 	Container e2e ready: true, restart count 0
Apr 21 21:20:53.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 21:20:53.759: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
Apr 21 21:20:53.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 21 21:20:53.759: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/21/23 21:20:53.759
Apr 21 21:20:53.764: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4835" to be "running"
Apr 21 21:20:53.765: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.631061ms
Apr 21 21:20:55.769: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005055201s
Apr 21 21:20:55.769: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/21/23 21:20:55.771
STEP: Trying to apply a random label on the found node. 04/21/23 21:20:55.781
STEP: verifying the node has the label kubernetes.io/e2e-dbf2327c-926c-4265-88ed-d36a61a126d8 95 04/21/23 21:20:55.788
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 04/21/23 21:20:55.79
Apr 21 21:20:55.794: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4835" to be "not pending"
Apr 21 21:20:55.796: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.992465ms
Apr 21 21:20:57.799: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004435785s
Apr 21 21:20:57.799: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.49.3 on the node which pod4 resides and expect not scheduled 04/21/23 21:20:57.799
Apr 21 21:20:57.804: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4835" to be "not pending"
Apr 21 21:20:57.806: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096636ms
Apr 21 21:20:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004510937s
Apr 21 21:21:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005750328s
Apr 21 21:21:03.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004744038s
Apr 21 21:21:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005160765s
Apr 21 21:21:07.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004964423s
Apr 21 21:21:09.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004465558s
Apr 21 21:21:11.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005019855s
Apr 21 21:21:13.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006277853s
Apr 21 21:21:15.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005963774s
Apr 21 21:21:17.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006080944s
Apr 21 21:21:19.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004550541s
Apr 21 21:21:21.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005631273s
Apr 21 21:21:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005465844s
Apr 21 21:21:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005049954s
Apr 21 21:21:27.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005284158s
Apr 21 21:21:29.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004657914s
Apr 21 21:21:31.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00584614s
Apr 21 21:21:33.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005999051s
Apr 21 21:21:35.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00527386s
Apr 21 21:21:37.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006624514s
Apr 21 21:21:39.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005322077s
Apr 21 21:21:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005368104s
Apr 21 21:21:43.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005739075s
Apr 21 21:21:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004918943s
Apr 21 21:21:47.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006278461s
Apr 21 21:21:49.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004880817s
Apr 21 21:21:51.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006207444s
Apr 21 21:21:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004810456s
Apr 21 21:21:55.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005372778s
Apr 21 21:21:57.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00504866s
Apr 21 21:21:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005150547s
Apr 21 21:22:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006281237s
Apr 21 21:22:03.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006562603s
Apr 21 21:22:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004868544s
Apr 21 21:22:07.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006128856s
Apr 21 21:22:09.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004739871s
Apr 21 21:22:11.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.005508036s
Apr 21 21:22:13.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00531063s
Apr 21 21:22:15.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005441495s
Apr 21 21:22:17.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005609039s
Apr 21 21:22:19.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005596863s
Apr 21 21:22:21.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005526593s
Apr 21 21:22:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00545277s
Apr 21 21:22:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004942688s
Apr 21 21:22:27.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004742191s
Apr 21 21:22:29.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005305134s
Apr 21 21:22:31.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005007154s
Apr 21 21:22:33.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004578626s
Apr 21 21:22:35.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005107853s
Apr 21 21:22:37.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005619189s
Apr 21 21:22:39.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005311724s
Apr 21 21:22:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005273668s
Apr 21 21:22:43.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005447789s
Apr 21 21:22:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005075369s
Apr 21 21:22:47.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006229934s
Apr 21 21:22:49.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005464058s
Apr 21 21:22:51.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005867386s
Apr 21 21:22:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005426047s
Apr 21 21:22:55.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004641734s
Apr 21 21:22:57.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005598775s
Apr 21 21:22:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005237572s
Apr 21 21:23:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006425545s
Apr 21 21:23:03.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.005789594s
Apr 21 21:23:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.00491902s
Apr 21 21:23:07.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006214024s
Apr 21 21:23:09.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004809579s
Apr 21 21:23:11.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005424638s
Apr 21 21:23:13.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005717566s
Apr 21 21:23:15.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.004900559s
Apr 21 21:23:17.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.005991619s
Apr 21 21:23:19.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005513145s
Apr 21 21:23:21.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005473573s
Apr 21 21:23:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.005106503s
Apr 21 21:23:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004730943s
Apr 21 21:23:27.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005074057s
Apr 21 21:23:29.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.004416092s
Apr 21 21:23:31.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005791543s
Apr 21 21:23:33.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.004570074s
Apr 21 21:23:35.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.00512364s
Apr 21 21:23:37.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005719918s
Apr 21 21:23:39.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005186105s
Apr 21 21:23:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005460207s
Apr 21 21:23:43.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005692208s
Apr 21 21:23:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005032671s
Apr 21 21:23:47.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.005067213s
Apr 21 21:23:49.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004573342s
Apr 21 21:23:51.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005839395s
Apr 21 21:23:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.00549109s
Apr 21 21:23:55.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.004626847s
Apr 21 21:23:57.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006040948s
Apr 21 21:23:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004580836s
Apr 21 21:24:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.005620406s
Apr 21 21:24:03.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005918957s
Apr 21 21:24:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004586693s
Apr 21 21:24:07.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005895012s
Apr 21 21:24:09.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005330025s
Apr 21 21:24:11.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006189943s
Apr 21 21:24:13.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006246338s
Apr 21 21:24:15.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005681884s
Apr 21 21:24:17.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004804018s
Apr 21 21:24:19.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005927837s
Apr 21 21:24:21.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005258553s
Apr 21 21:24:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004557946s
Apr 21 21:24:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005115772s
Apr 21 21:24:27.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005231736s
Apr 21 21:24:29.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005337501s
Apr 21 21:24:31.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005915292s
Apr 21 21:24:33.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.005990769s
Apr 21 21:24:35.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005732673s
Apr 21 21:24:37.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.005775142s
Apr 21 21:24:39.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.005474028s
Apr 21 21:24:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004619849s
Apr 21 21:24:43.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00613121s
Apr 21 21:24:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.004911718s
Apr 21 21:24:47.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005894663s
Apr 21 21:24:49.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005308924s
Apr 21 21:24:51.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005541451s
Apr 21 21:24:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004605805s
Apr 21 21:24:55.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004491863s
Apr 21 21:24:57.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.004702272s
Apr 21 21:24:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.005081941s
Apr 21 21:25:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005630065s
Apr 21 21:25:03.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005758178s
Apr 21 21:25:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.004961282s
Apr 21 21:25:07.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006263324s
Apr 21 21:25:09.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004636952s
Apr 21 21:25:11.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.00537112s
Apr 21 21:25:13.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005612049s
Apr 21 21:25:15.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005717862s
Apr 21 21:25:17.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004436173s
Apr 21 21:25:19.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005287024s
Apr 21 21:25:21.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004345579s
Apr 21 21:25:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.004830971s
Apr 21 21:25:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005254946s
Apr 21 21:25:27.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005749757s
Apr 21 21:25:29.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.004630339s
Apr 21 21:25:31.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005960864s
Apr 21 21:25:33.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004362478s
Apr 21 21:25:35.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00465315s
Apr 21 21:25:37.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004962856s
Apr 21 21:25:39.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005610668s
Apr 21 21:25:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.004993383s
Apr 21 21:25:43.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006246086s
Apr 21 21:25:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005397478s
Apr 21 21:25:47.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00556298s
Apr 21 21:25:49.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005532217s
Apr 21 21:25:51.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.004814729s
Apr 21 21:25:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004553327s
Apr 21 21:25:55.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.004995306s
Apr 21 21:25:57.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005820857s
Apr 21 21:25:57.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007820559s
STEP: removing the label kubernetes.io/e2e-dbf2327c-926c-4265-88ed-d36a61a126d8 off the node k8sconformance-m02 04/21/23 21:25:57.812
STEP: verifying the node doesn't have the label kubernetes.io/e2e-dbf2327c-926c-4265-88ed-d36a61a126d8 04/21/23 21:25:57.82
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:25:57.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4835" for this suite. 04/21/23 21:25:57.825
------------------------------
â€¢ [SLOW TEST] [304.095 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:20:53.735
    Apr 21 21:20:53.735: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-pred 04/21/23 21:20:53.735
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:20:53.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:20:53.746
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 21 21:20:53.748: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 21 21:20:53.751: INFO: Waiting for terminating namespaces to be deleted...
    Apr 21 21:20:53.753: INFO: 
    Logging pods the apiserver thinks is on node k8sconformance before test
    Apr 21 21:20:53.756: INFO: coredns-787d4945fb-bp78j from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.756: INFO: 	Container coredns ready: true, restart count 1
    Apr 21 21:20:53.756: INFO: etcd-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.756: INFO: 	Container etcd ready: true, restart count 0
    Apr 21 21:20:53.756: INFO: kindnet-6bcq2 from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.756: INFO: 	Container kindnet-cni ready: true, restart count 0
    Apr 21 21:20:53.756: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.756: INFO: 	Container kube-apiserver ready: true, restart count 0
    Apr 21 21:20:53.756: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-04-21 20:12:51 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.756: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Apr 21 21:20:53.756: INFO: kube-proxy-k9znz from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.756: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 21 21:20:53.756: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-04-21 20:12:52 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.756: INFO: 	Container kube-scheduler ready: true, restart count 0
    Apr 21 21:20:53.756: INFO: storage-provisioner from kube-system started at 2023-04-21 20:13:04 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.756: INFO: 	Container storage-provisioner ready: true, restart count 0
    Apr 21 21:20:53.756: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-pknxt from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 21:20:53.756: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 21:20:53.756: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 21 21:20:53.756: INFO: 
    Logging pods the apiserver thinks is on node k8sconformance-m02 before test
    Apr 21 21:20:53.759: INFO: kindnet-znkt6 from kube-system started at 2023-04-21 20:47:53 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.759: INFO: 	Container kindnet-cni ready: true, restart count 0
    Apr 21 21:20:53.759: INFO: kube-proxy-fvpz4 from kube-system started at 2023-04-21 20:13:34 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.759: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 21 21:20:53.759: INFO: sonobuoy from sonobuoy started at 2023-04-21 20:13:43 +0000 UTC (1 container statuses recorded)
    Apr 21 21:20:53.759: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 21 21:20:53.759: INFO: sonobuoy-e2e-job-ddbedf54a58f4885 from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 21:20:53.759: INFO: 	Container e2e ready: true, restart count 0
    Apr 21 21:20:53.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 21:20:53.759: INFO: sonobuoy-systemd-logs-daemon-set-82c3e1e08d394cbf-h7cwf from sonobuoy started at 2023-04-21 20:13:46 +0000 UTC (2 container statuses recorded)
    Apr 21 21:20:53.759: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 21 21:20:53.759: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/21/23 21:20:53.759
    Apr 21 21:20:53.764: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-4835" to be "running"
    Apr 21 21:20:53.765: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 1.631061ms
    Apr 21 21:20:55.769: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005055201s
    Apr 21 21:20:55.769: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/21/23 21:20:55.771
    STEP: Trying to apply a random label on the found node. 04/21/23 21:20:55.781
    STEP: verifying the node has the label kubernetes.io/e2e-dbf2327c-926c-4265-88ed-d36a61a126d8 95 04/21/23 21:20:55.788
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 04/21/23 21:20:55.79
    Apr 21 21:20:55.794: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-4835" to be "not pending"
    Apr 21 21:20:55.796: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.992465ms
    Apr 21 21:20:57.799: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004435785s
    Apr 21 21:20:57.799: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.49.3 on the node which pod4 resides and expect not scheduled 04/21/23 21:20:57.799
    Apr 21 21:20:57.804: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-4835" to be "not pending"
    Apr 21 21:20:57.806: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.096636ms
    Apr 21 21:20:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004510937s
    Apr 21 21:21:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005750328s
    Apr 21 21:21:03.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.004744038s
    Apr 21 21:21:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.005160765s
    Apr 21 21:21:07.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.004964423s
    Apr 21 21:21:09.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.004465558s
    Apr 21 21:21:11.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.005019855s
    Apr 21 21:21:13.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006277853s
    Apr 21 21:21:15.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.005963774s
    Apr 21 21:21:17.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006080944s
    Apr 21 21:21:19.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.004550541s
    Apr 21 21:21:21.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.005631273s
    Apr 21 21:21:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.005465844s
    Apr 21 21:21:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.005049954s
    Apr 21 21:21:27.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005284158s
    Apr 21 21:21:29.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.004657914s
    Apr 21 21:21:31.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.00584614s
    Apr 21 21:21:33.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.005999051s
    Apr 21 21:21:35.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00527386s
    Apr 21 21:21:37.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.006624514s
    Apr 21 21:21:39.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.005322077s
    Apr 21 21:21:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.005368104s
    Apr 21 21:21:43.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005739075s
    Apr 21 21:21:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.004918943s
    Apr 21 21:21:47.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006278461s
    Apr 21 21:21:49.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.004880817s
    Apr 21 21:21:51.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.006207444s
    Apr 21 21:21:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.004810456s
    Apr 21 21:21:55.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.005372778s
    Apr 21 21:21:57.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00504866s
    Apr 21 21:21:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.005150547s
    Apr 21 21:22:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006281237s
    Apr 21 21:22:03.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.006562603s
    Apr 21 21:22:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.004868544s
    Apr 21 21:22:07.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006128856s
    Apr 21 21:22:09.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.004739871s
    Apr 21 21:22:11.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.005508036s
    Apr 21 21:22:13.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00531063s
    Apr 21 21:22:15.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.005441495s
    Apr 21 21:22:17.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.005609039s
    Apr 21 21:22:19.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.005596863s
    Apr 21 21:22:21.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.005526593s
    Apr 21 21:22:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00545277s
    Apr 21 21:22:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.004942688s
    Apr 21 21:22:27.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.004742191s
    Apr 21 21:22:29.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005305134s
    Apr 21 21:22:31.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.005007154s
    Apr 21 21:22:33.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.004578626s
    Apr 21 21:22:35.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005107853s
    Apr 21 21:22:37.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.005619189s
    Apr 21 21:22:39.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.005311724s
    Apr 21 21:22:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.005273668s
    Apr 21 21:22:43.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.005447789s
    Apr 21 21:22:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.005075369s
    Apr 21 21:22:47.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.006229934s
    Apr 21 21:22:49.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.005464058s
    Apr 21 21:22:51.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.005867386s
    Apr 21 21:22:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.005426047s
    Apr 21 21:22:55.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.004641734s
    Apr 21 21:22:57.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005598775s
    Apr 21 21:22:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.005237572s
    Apr 21 21:23:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.006425545s
    Apr 21 21:23:03.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.005789594s
    Apr 21 21:23:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.00491902s
    Apr 21 21:23:07.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.006214024s
    Apr 21 21:23:09.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.004809579s
    Apr 21 21:23:11.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.005424638s
    Apr 21 21:23:13.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.005717566s
    Apr 21 21:23:15.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.004900559s
    Apr 21 21:23:17.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.005991619s
    Apr 21 21:23:19.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.005513145s
    Apr 21 21:23:21.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.005473573s
    Apr 21 21:23:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.005106503s
    Apr 21 21:23:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.004730943s
    Apr 21 21:23:27.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.005074057s
    Apr 21 21:23:29.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.004416092s
    Apr 21 21:23:31.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.005791543s
    Apr 21 21:23:33.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.004570074s
    Apr 21 21:23:35.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.00512364s
    Apr 21 21:23:37.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.005719918s
    Apr 21 21:23:39.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.005186105s
    Apr 21 21:23:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.005460207s
    Apr 21 21:23:43.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.005692208s
    Apr 21 21:23:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.005032671s
    Apr 21 21:23:47.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.005067213s
    Apr 21 21:23:49.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.004573342s
    Apr 21 21:23:51.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.005839395s
    Apr 21 21:23:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.00549109s
    Apr 21 21:23:55.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.004626847s
    Apr 21 21:23:57.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.006040948s
    Apr 21 21:23:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.004580836s
    Apr 21 21:24:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.005620406s
    Apr 21 21:24:03.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.005918957s
    Apr 21 21:24:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.004586693s
    Apr 21 21:24:07.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.005895012s
    Apr 21 21:24:09.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.005330025s
    Apr 21 21:24:11.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.006189943s
    Apr 21 21:24:13.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.006246338s
    Apr 21 21:24:15.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.005681884s
    Apr 21 21:24:17.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.004804018s
    Apr 21 21:24:19.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.005927837s
    Apr 21 21:24:21.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.005258553s
    Apr 21 21:24:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.004557946s
    Apr 21 21:24:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.005115772s
    Apr 21 21:24:27.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.005231736s
    Apr 21 21:24:29.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.005337501s
    Apr 21 21:24:31.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.005915292s
    Apr 21 21:24:33.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.005990769s
    Apr 21 21:24:35.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.005732673s
    Apr 21 21:24:37.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.005775142s
    Apr 21 21:24:39.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.005474028s
    Apr 21 21:24:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.004619849s
    Apr 21 21:24:43.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.00613121s
    Apr 21 21:24:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.004911718s
    Apr 21 21:24:47.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.005894663s
    Apr 21 21:24:49.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.005308924s
    Apr 21 21:24:51.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.005541451s
    Apr 21 21:24:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.004605805s
    Apr 21 21:24:55.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.004491863s
    Apr 21 21:24:57.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.004702272s
    Apr 21 21:24:59.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.005081941s
    Apr 21 21:25:01.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.005630065s
    Apr 21 21:25:03.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.005758178s
    Apr 21 21:25:05.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.004961282s
    Apr 21 21:25:07.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.006263324s
    Apr 21 21:25:09.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.004636952s
    Apr 21 21:25:11.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.00537112s
    Apr 21 21:25:13.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.005612049s
    Apr 21 21:25:15.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.005717862s
    Apr 21 21:25:17.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.004436173s
    Apr 21 21:25:19.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.005287024s
    Apr 21 21:25:21.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.004345579s
    Apr 21 21:25:23.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.004830971s
    Apr 21 21:25:25.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.005254946s
    Apr 21 21:25:27.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.005749757s
    Apr 21 21:25:29.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.004630339s
    Apr 21 21:25:31.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.005960864s
    Apr 21 21:25:33.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.004362478s
    Apr 21 21:25:35.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.00465315s
    Apr 21 21:25:37.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.004962856s
    Apr 21 21:25:39.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.005610668s
    Apr 21 21:25:41.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.004993383s
    Apr 21 21:25:43.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.006246086s
    Apr 21 21:25:45.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.005397478s
    Apr 21 21:25:47.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00556298s
    Apr 21 21:25:49.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.005532217s
    Apr 21 21:25:51.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.004814729s
    Apr 21 21:25:53.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.004553327s
    Apr 21 21:25:55.809: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.004995306s
    Apr 21 21:25:57.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.005820857s
    Apr 21 21:25:57.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007820559s
    STEP: removing the label kubernetes.io/e2e-dbf2327c-926c-4265-88ed-d36a61a126d8 off the node k8sconformance-m02 04/21/23 21:25:57.812
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-dbf2327c-926c-4265-88ed-d36a61a126d8 04/21/23 21:25:57.82
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:25:57.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4835" for this suite. 04/21/23 21:25:57.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:25:57.831
Apr 21 21:25:57.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replication-controller 04/21/23 21:25:57.832
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:25:57.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:25:57.843
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 04/21/23 21:25:57.845
STEP: When the matched label of one of its pods change 04/21/23 21:25:57.849
Apr 21 21:25:57.851: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 21 21:26:02.856: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 04/21/23 21:26:02.863
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:03.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2679" for this suite. 04/21/23 21:26:03.87
------------------------------
â€¢ [SLOW TEST] [6.043 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:25:57.831
    Apr 21 21:25:57.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replication-controller 04/21/23 21:25:57.832
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:25:57.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:25:57.843
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 04/21/23 21:25:57.845
    STEP: When the matched label of one of its pods change 04/21/23 21:25:57.849
    Apr 21 21:25:57.851: INFO: Pod name pod-release: Found 0 pods out of 1
    Apr 21 21:26:02.856: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 04/21/23 21:26:02.863
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:03.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2679" for this suite. 04/21/23 21:26:03.87
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:03.875
Apr 21 21:26:03.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename daemonsets 04/21/23 21:26:03.876
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:03.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:03.885
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Apr 21 21:26:03.897: INFO: Create a RollingUpdate DaemonSet
Apr 21 21:26:03.901: INFO: Check that daemon pods launch on every node of the cluster
Apr 21 21:26:03.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:26:03.905: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:26:04.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:26:04.910: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:26:05.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 21 21:26:05.910: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Apr 21 21:26:05.910: INFO: Update the DaemonSet to trigger a rollout
Apr 21 21:26:05.916: INFO: Updating DaemonSet daemon-set
Apr 21 21:26:08.925: INFO: Roll back the DaemonSet before rollout is complete
Apr 21 21:26:08.930: INFO: Updating DaemonSet daemon-set
Apr 21 21:26:08.930: INFO: Make sure DaemonSet rollback is complete
Apr 21 21:26:08.932: INFO: Wrong image for pod: daemon-set-kn42b. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Apr 21 21:26:08.932: INFO: Pod daemon-set-kn42b is not available
Apr 21 21:26:11.938: INFO: Pod daemon-set-ndkrp is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:26:11.943
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8239, will wait for the garbage collector to delete the pods 04/21/23 21:26:11.943
Apr 21 21:26:12.000: INFO: Deleting DaemonSet.extensions daemon-set took: 4.120765ms
Apr 21 21:26:12.101: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.049236ms
Apr 21 21:26:13.304: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:26:13.304: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 21 21:26:13.305: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20180"},"items":null}

Apr 21 21:26:13.307: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20180"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-8239" for this suite. 04/21/23 21:26:13.316
------------------------------
â€¢ [SLOW TEST] [9.447 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:03.875
    Apr 21 21:26:03.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename daemonsets 04/21/23 21:26:03.876
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:03.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:03.885
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Apr 21 21:26:03.897: INFO: Create a RollingUpdate DaemonSet
    Apr 21 21:26:03.901: INFO: Check that daemon pods launch on every node of the cluster
    Apr 21 21:26:03.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:26:03.905: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:26:04.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:26:04.910: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:26:05.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 21 21:26:05.910: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Apr 21 21:26:05.910: INFO: Update the DaemonSet to trigger a rollout
    Apr 21 21:26:05.916: INFO: Updating DaemonSet daemon-set
    Apr 21 21:26:08.925: INFO: Roll back the DaemonSet before rollout is complete
    Apr 21 21:26:08.930: INFO: Updating DaemonSet daemon-set
    Apr 21 21:26:08.930: INFO: Make sure DaemonSet rollback is complete
    Apr 21 21:26:08.932: INFO: Wrong image for pod: daemon-set-kn42b. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Apr 21 21:26:08.932: INFO: Pod daemon-set-kn42b is not available
    Apr 21 21:26:11.938: INFO: Pod daemon-set-ndkrp is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:26:11.943
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8239, will wait for the garbage collector to delete the pods 04/21/23 21:26:11.943
    Apr 21 21:26:12.000: INFO: Deleting DaemonSet.extensions daemon-set took: 4.120765ms
    Apr 21 21:26:12.101: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.049236ms
    Apr 21 21:26:13.304: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:26:13.304: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 21 21:26:13.305: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20180"},"items":null}

    Apr 21 21:26:13.307: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20180"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:13.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-8239" for this suite. 04/21/23 21:26:13.316
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:13.322
Apr 21 21:26:13.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename runtimeclass 04/21/23 21:26:13.323
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:13.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:13.334
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Apr 21 21:26:13.344: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7932 to be scheduled
Apr 21 21:26:13.345: INFO: 1 pods are not scheduled: [runtimeclass-7932/test-runtimeclass-runtimeclass-7932-preconfigured-handler-npt6g(3160e429-887b-4b22-a9fb-824d85be1893)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:15.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7932" for this suite. 04/21/23 21:26:15.354
------------------------------
â€¢ [2.038 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:13.322
    Apr 21 21:26:13.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename runtimeclass 04/21/23 21:26:13.323
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:13.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:13.334
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Apr 21 21:26:13.344: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7932 to be scheduled
    Apr 21 21:26:13.345: INFO: 1 pods are not scheduled: [runtimeclass-7932/test-runtimeclass-runtimeclass-7932-preconfigured-handler-npt6g(3160e429-887b-4b22-a9fb-824d85be1893)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:15.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7932" for this suite. 04/21/23 21:26:15.354
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:15.36
Apr 21 21:26:15.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename var-expansion 04/21/23 21:26:15.361
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:15.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:15.37
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 04/21/23 21:26:15.372
Apr 21 21:26:15.378: INFO: Waiting up to 5m0s for pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25" in namespace "var-expansion-5450" to be "Succeeded or Failed"
Apr 21 21:26:15.380: INFO: Pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094447ms
Apr 21 21:26:17.383: INFO: Pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005001199s
Apr 21 21:26:19.384: INFO: Pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005961439s
STEP: Saw pod success 04/21/23 21:26:19.384
Apr 21 21:26:19.384: INFO: Pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25" satisfied condition "Succeeded or Failed"
Apr 21 21:26:19.386: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25 container dapi-container: <nil>
STEP: delete the pod 04/21/23 21:26:19.396
Apr 21 21:26:19.405: INFO: Waiting for pod var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25 to disappear
Apr 21 21:26:19.407: INFO: Pod var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:19.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5450" for this suite. 04/21/23 21:26:19.409
------------------------------
â€¢ [4.052 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:15.36
    Apr 21 21:26:15.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename var-expansion 04/21/23 21:26:15.361
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:15.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:15.37
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 04/21/23 21:26:15.372
    Apr 21 21:26:15.378: INFO: Waiting up to 5m0s for pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25" in namespace "var-expansion-5450" to be "Succeeded or Failed"
    Apr 21 21:26:15.380: INFO: Pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.094447ms
    Apr 21 21:26:17.383: INFO: Pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005001199s
    Apr 21 21:26:19.384: INFO: Pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005961439s
    STEP: Saw pod success 04/21/23 21:26:19.384
    Apr 21 21:26:19.384: INFO: Pod "var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25" satisfied condition "Succeeded or Failed"
    Apr 21 21:26:19.386: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25 container dapi-container: <nil>
    STEP: delete the pod 04/21/23 21:26:19.396
    Apr 21 21:26:19.405: INFO: Waiting for pod var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25 to disappear
    Apr 21 21:26:19.407: INFO: Pod var-expansion-34ba7fca-90ea-49df-ab0e-a03900daef25 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:19.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5450" for this suite. 04/21/23 21:26:19.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:19.413
Apr 21 21:26:19.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:26:19.414
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:19.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:19.423
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 04/21/23 21:26:19.425
Apr 21 21:26:19.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 21 21:26:19.485: INFO: stderr: ""
Apr 21 21:26:19.485: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 04/21/23 21:26:19.485
Apr 21 21:26:19.485: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 21 21:26:19.485: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5146" to be "running and ready, or succeeded"
Apr 21 21:26:19.487: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.732178ms
Apr 21 21:26:19.487: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8sconformance-m02' to be 'Running' but was 'Pending'
Apr 21 21:26:21.489: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004073981s
Apr 21 21:26:21.489: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 21 21:26:21.489: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 04/21/23 21:26:21.489
Apr 21 21:26:21.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator'
Apr 21 21:26:21.549: INFO: stderr: ""
Apr 21 21:26:21.549: INFO: stdout: "I0421 21:26:20.141145       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/6mwf 378\nI0421 21:26:20.341278       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/v45 515\nI0421 21:26:20.541875       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vxz 293\nI0421 21:26:20.742218       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/4m2r 406\nI0421 21:26:20.941558       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/zrk 397\nI0421 21:26:21.141739       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/bb2 207\nI0421 21:26:21.342082       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/kzjk 518\nI0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
STEP: limiting log lines 04/21/23 21:26:21.549
Apr 21 21:26:21.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --tail=1'
Apr 21 21:26:21.612: INFO: stderr: ""
Apr 21 21:26:21.612: INFO: stdout: "I0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
Apr 21 21:26:21.612: INFO: got output "I0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
STEP: limiting log bytes 04/21/23 21:26:21.612
Apr 21 21:26:21.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --limit-bytes=1'
Apr 21 21:26:21.671: INFO: stderr: ""
Apr 21 21:26:21.671: INFO: stdout: "I"
Apr 21 21:26:21.671: INFO: got output "I"
STEP: exposing timestamps 04/21/23 21:26:21.671
Apr 21 21:26:21.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --tail=1 --timestamps'
Apr 21 21:26:21.730: INFO: stderr: ""
Apr 21 21:26:21.730: INFO: stdout: "2023-04-21T21:26:21.541479725Z I0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
Apr 21 21:26:21.730: INFO: got output "2023-04-21T21:26:21.541479725Z I0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
STEP: restricting to a time range 04/21/23 21:26:21.731
Apr 21 21:26:24.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --since=1s'
Apr 21 21:26:24.341: INFO: stderr: ""
Apr 21 21:26:24.341: INFO: stdout: "I0421 21:26:23.342096       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/bq4t 533\nI0421 21:26:23.541371       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ptwc 374\nI0421 21:26:23.741738       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/rs7x 312\nI0421 21:26:23.942064       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/n9ct 453\nI0421 21:26:24.141348       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/lc2 246\n"
Apr 21 21:26:24.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --since=24h'
Apr 21 21:26:24.404: INFO: stderr: ""
Apr 21 21:26:24.404: INFO: stdout: "I0421 21:26:20.141145       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/6mwf 378\nI0421 21:26:20.341278       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/v45 515\nI0421 21:26:20.541875       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vxz 293\nI0421 21:26:20.742218       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/4m2r 406\nI0421 21:26:20.941558       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/zrk 397\nI0421 21:26:21.141739       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/bb2 207\nI0421 21:26:21.342082       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/kzjk 518\nI0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\nI0421 21:26:21.741704       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/pjg8 215\nI0421 21:26:21.942037       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/gcnv 385\nI0421 21:26:22.141321       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/7km 503\nI0421 21:26:22.341652       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/zjvr 529\nI0421 21:26:22.541988       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/9bz 449\nI0421 21:26:22.741246       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/n6v4 361\nI0421 21:26:22.941415       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/ckc 227\nI0421 21:26:23.141741       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/nws 454\nI0421 21:26:23.342096       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/bq4t 533\nI0421 21:26:23.541371       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ptwc 374\nI0421 21:26:23.741738       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/rs7x 312\nI0421 21:26:23.942064       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/n9ct 453\nI0421 21:26:24.141348       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/lc2 246\nI0421 21:26:24.341746       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/4vql 318\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Apr 21 21:26:24.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 delete pod logs-generator'
Apr 21 21:26:25.103: INFO: stderr: ""
Apr 21 21:26:25.103: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:25.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5146" for this suite. 04/21/23 21:26:25.106
------------------------------
â€¢ [SLOW TEST] [5.696 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:19.413
    Apr 21 21:26:19.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:26:19.414
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:19.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:19.423
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 04/21/23 21:26:19.425
    Apr 21 21:26:19.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Apr 21 21:26:19.485: INFO: stderr: ""
    Apr 21 21:26:19.485: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 04/21/23 21:26:19.485
    Apr 21 21:26:19.485: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Apr 21 21:26:19.485: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-5146" to be "running and ready, or succeeded"
    Apr 21 21:26:19.487: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.732178ms
    Apr 21 21:26:19.487: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'k8sconformance-m02' to be 'Running' but was 'Pending'
    Apr 21 21:26:21.489: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004073981s
    Apr 21 21:26:21.489: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Apr 21 21:26:21.489: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 04/21/23 21:26:21.489
    Apr 21 21:26:21.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator'
    Apr 21 21:26:21.549: INFO: stderr: ""
    Apr 21 21:26:21.549: INFO: stdout: "I0421 21:26:20.141145       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/6mwf 378\nI0421 21:26:20.341278       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/v45 515\nI0421 21:26:20.541875       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vxz 293\nI0421 21:26:20.742218       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/4m2r 406\nI0421 21:26:20.941558       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/zrk 397\nI0421 21:26:21.141739       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/bb2 207\nI0421 21:26:21.342082       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/kzjk 518\nI0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
    STEP: limiting log lines 04/21/23 21:26:21.549
    Apr 21 21:26:21.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --tail=1'
    Apr 21 21:26:21.612: INFO: stderr: ""
    Apr 21 21:26:21.612: INFO: stdout: "I0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
    Apr 21 21:26:21.612: INFO: got output "I0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
    STEP: limiting log bytes 04/21/23 21:26:21.612
    Apr 21 21:26:21.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --limit-bytes=1'
    Apr 21 21:26:21.671: INFO: stderr: ""
    Apr 21 21:26:21.671: INFO: stdout: "I"
    Apr 21 21:26:21.671: INFO: got output "I"
    STEP: exposing timestamps 04/21/23 21:26:21.671
    Apr 21 21:26:21.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --tail=1 --timestamps'
    Apr 21 21:26:21.730: INFO: stderr: ""
    Apr 21 21:26:21.730: INFO: stdout: "2023-04-21T21:26:21.541479725Z I0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
    Apr 21 21:26:21.730: INFO: got output "2023-04-21T21:26:21.541479725Z I0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\n"
    STEP: restricting to a time range 04/21/23 21:26:21.731
    Apr 21 21:26:24.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --since=1s'
    Apr 21 21:26:24.341: INFO: stderr: ""
    Apr 21 21:26:24.341: INFO: stdout: "I0421 21:26:23.342096       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/bq4t 533\nI0421 21:26:23.541371       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ptwc 374\nI0421 21:26:23.741738       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/rs7x 312\nI0421 21:26:23.942064       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/n9ct 453\nI0421 21:26:24.141348       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/lc2 246\n"
    Apr 21 21:26:24.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 logs logs-generator logs-generator --since=24h'
    Apr 21 21:26:24.404: INFO: stderr: ""
    Apr 21 21:26:24.404: INFO: stdout: "I0421 21:26:20.141145       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/6mwf 378\nI0421 21:26:20.341278       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/kube-system/pods/v45 515\nI0421 21:26:20.541875       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vxz 293\nI0421 21:26:20.742218       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/4m2r 406\nI0421 21:26:20.941558       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/zrk 397\nI0421 21:26:21.141739       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/bb2 207\nI0421 21:26:21.342082       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/kzjk 518\nI0421 21:26:21.541349       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/n2qd 505\nI0421 21:26:21.741704       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/kube-system/pods/pjg8 215\nI0421 21:26:21.942037       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/default/pods/gcnv 385\nI0421 21:26:22.141321       1 logs_generator.go:76] 10 GET /api/v1/namespaces/ns/pods/7km 503\nI0421 21:26:22.341652       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/zjvr 529\nI0421 21:26:22.541988       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/9bz 449\nI0421 21:26:22.741246       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/n6v4 361\nI0421 21:26:22.941415       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/ckc 227\nI0421 21:26:23.141741       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/nws 454\nI0421 21:26:23.342096       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/bq4t 533\nI0421 21:26:23.541371       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/ptwc 374\nI0421 21:26:23.741738       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/rs7x 312\nI0421 21:26:23.942064       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/ns/pods/n9ct 453\nI0421 21:26:24.141348       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/lc2 246\nI0421 21:26:24.341746       1 logs_generator.go:76] 21 GET /api/v1/namespaces/default/pods/4vql 318\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Apr 21 21:26:24.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-5146 delete pod logs-generator'
    Apr 21 21:26:25.103: INFO: stderr: ""
    Apr 21 21:26:25.103: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:25.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5146" for this suite. 04/21/23 21:26:25.106
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:25.109
Apr 21 21:26:25.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:26:25.11
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:25.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:25.122
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 04/21/23 21:26:25.124
Apr 21 21:26:25.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 create -f -'
Apr 21 21:26:25.571: INFO: stderr: ""
Apr 21 21:26:25.571: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/21/23 21:26:25.571
Apr 21 21:26:25.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 21 21:26:25.637: INFO: stderr: ""
Apr 21 21:26:25.637: INFO: stdout: "update-demo-nautilus-ggpqx update-demo-nautilus-q5pzr "
Apr 21 21:26:25.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 21 21:26:25.690: INFO: stderr: ""
Apr 21 21:26:25.690: INFO: stdout: ""
Apr 21 21:26:25.690: INFO: update-demo-nautilus-ggpqx is created but not running
Apr 21 21:26:30.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 21 21:26:30.748: INFO: stderr: ""
Apr 21 21:26:30.748: INFO: stdout: "update-demo-nautilus-ggpqx update-demo-nautilus-q5pzr "
Apr 21 21:26:30.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 21 21:26:30.803: INFO: stderr: ""
Apr 21 21:26:30.803: INFO: stdout: "true"
Apr 21 21:26:30.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 21 21:26:30.858: INFO: stderr: ""
Apr 21 21:26:30.858: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 21 21:26:30.858: INFO: validating pod update-demo-nautilus-ggpqx
Apr 21 21:26:30.860: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 21 21:26:30.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 21 21:26:30.860: INFO: update-demo-nautilus-ggpqx is verified up and running
Apr 21 21:26:30.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-q5pzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 21 21:26:30.914: INFO: stderr: ""
Apr 21 21:26:30.914: INFO: stdout: "true"
Apr 21 21:26:30.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-q5pzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 21 21:26:30.968: INFO: stderr: ""
Apr 21 21:26:30.968: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 21 21:26:30.968: INFO: validating pod update-demo-nautilus-q5pzr
Apr 21 21:26:30.971: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 21 21:26:30.971: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 21 21:26:30.971: INFO: update-demo-nautilus-q5pzr is verified up and running
STEP: scaling down the replication controller 04/21/23 21:26:30.971
Apr 21 21:26:30.972: INFO: scanned /root for discovery docs: <nil>
Apr 21 21:26:30.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr 21 21:26:32.038: INFO: stderr: ""
Apr 21 21:26:32.038: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/21/23 21:26:32.038
Apr 21 21:26:32.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 21 21:26:32.093: INFO: stderr: ""
Apr 21 21:26:32.093: INFO: stdout: "update-demo-nautilus-ggpqx "
Apr 21 21:26:32.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 21 21:26:32.148: INFO: stderr: ""
Apr 21 21:26:32.148: INFO: stdout: "true"
Apr 21 21:26:32.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 21 21:26:32.202: INFO: stderr: ""
Apr 21 21:26:32.202: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 21 21:26:32.202: INFO: validating pod update-demo-nautilus-ggpqx
Apr 21 21:26:32.204: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 21 21:26:32.204: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 21 21:26:32.204: INFO: update-demo-nautilus-ggpqx is verified up and running
STEP: scaling up the replication controller 04/21/23 21:26:32.204
Apr 21 21:26:32.205: INFO: scanned /root for discovery docs: <nil>
Apr 21 21:26:32.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr 21 21:26:33.276: INFO: stderr: ""
Apr 21 21:26:33.276: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/21/23 21:26:33.276
Apr 21 21:26:33.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 21 21:26:33.334: INFO: stderr: ""
Apr 21 21:26:33.334: INFO: stdout: "update-demo-nautilus-ggpqx update-demo-nautilus-v6cc2 "
Apr 21 21:26:33.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 21 21:26:33.390: INFO: stderr: ""
Apr 21 21:26:33.390: INFO: stdout: "true"
Apr 21 21:26:33.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 21 21:26:33.443: INFO: stderr: ""
Apr 21 21:26:33.443: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 21 21:26:33.443: INFO: validating pod update-demo-nautilus-ggpqx
Apr 21 21:26:33.445: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 21 21:26:33.445: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 21 21:26:33.445: INFO: update-demo-nautilus-ggpqx is verified up and running
Apr 21 21:26:33.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-v6cc2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 21 21:26:33.499: INFO: stderr: ""
Apr 21 21:26:33.499: INFO: stdout: "true"
Apr 21 21:26:33.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-v6cc2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 21 21:26:33.552: INFO: stderr: ""
Apr 21 21:26:33.552: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 21 21:26:33.552: INFO: validating pod update-demo-nautilus-v6cc2
Apr 21 21:26:33.555: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 21 21:26:33.555: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 21 21:26:33.555: INFO: update-demo-nautilus-v6cc2 is verified up and running
STEP: using delete to clean up resources 04/21/23 21:26:33.555
Apr 21 21:26:33.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 delete --grace-period=0 --force -f -'
Apr 21 21:26:33.614: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 21 21:26:33.614: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 21 21:26:33.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get rc,svc -l name=update-demo --no-headers'
Apr 21 21:26:33.681: INFO: stderr: "No resources found in kubectl-3018 namespace.\n"
Apr 21 21:26:33.681: INFO: stdout: ""
Apr 21 21:26:33.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 21 21:26:33.742: INFO: stderr: ""
Apr 21 21:26:33.742: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:33.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3018" for this suite. 04/21/23 21:26:33.744
------------------------------
â€¢ [SLOW TEST] [8.639 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:25.109
    Apr 21 21:26:25.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:26:25.11
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:25.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:25.122
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 04/21/23 21:26:25.124
    Apr 21 21:26:25.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 create -f -'
    Apr 21 21:26:25.571: INFO: stderr: ""
    Apr 21 21:26:25.571: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/21/23 21:26:25.571
    Apr 21 21:26:25.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 21 21:26:25.637: INFO: stderr: ""
    Apr 21 21:26:25.637: INFO: stdout: "update-demo-nautilus-ggpqx update-demo-nautilus-q5pzr "
    Apr 21 21:26:25.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 21 21:26:25.690: INFO: stderr: ""
    Apr 21 21:26:25.690: INFO: stdout: ""
    Apr 21 21:26:25.690: INFO: update-demo-nautilus-ggpqx is created but not running
    Apr 21 21:26:30.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 21 21:26:30.748: INFO: stderr: ""
    Apr 21 21:26:30.748: INFO: stdout: "update-demo-nautilus-ggpqx update-demo-nautilus-q5pzr "
    Apr 21 21:26:30.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 21 21:26:30.803: INFO: stderr: ""
    Apr 21 21:26:30.803: INFO: stdout: "true"
    Apr 21 21:26:30.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 21 21:26:30.858: INFO: stderr: ""
    Apr 21 21:26:30.858: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 21 21:26:30.858: INFO: validating pod update-demo-nautilus-ggpqx
    Apr 21 21:26:30.860: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 21 21:26:30.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 21 21:26:30.860: INFO: update-demo-nautilus-ggpqx is verified up and running
    Apr 21 21:26:30.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-q5pzr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 21 21:26:30.914: INFO: stderr: ""
    Apr 21 21:26:30.914: INFO: stdout: "true"
    Apr 21 21:26:30.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-q5pzr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 21 21:26:30.968: INFO: stderr: ""
    Apr 21 21:26:30.968: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 21 21:26:30.968: INFO: validating pod update-demo-nautilus-q5pzr
    Apr 21 21:26:30.971: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 21 21:26:30.971: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 21 21:26:30.971: INFO: update-demo-nautilus-q5pzr is verified up and running
    STEP: scaling down the replication controller 04/21/23 21:26:30.971
    Apr 21 21:26:30.972: INFO: scanned /root for discovery docs: <nil>
    Apr 21 21:26:30.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Apr 21 21:26:32.038: INFO: stderr: ""
    Apr 21 21:26:32.038: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/21/23 21:26:32.038
    Apr 21 21:26:32.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 21 21:26:32.093: INFO: stderr: ""
    Apr 21 21:26:32.093: INFO: stdout: "update-demo-nautilus-ggpqx "
    Apr 21 21:26:32.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 21 21:26:32.148: INFO: stderr: ""
    Apr 21 21:26:32.148: INFO: stdout: "true"
    Apr 21 21:26:32.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 21 21:26:32.202: INFO: stderr: ""
    Apr 21 21:26:32.202: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 21 21:26:32.202: INFO: validating pod update-demo-nautilus-ggpqx
    Apr 21 21:26:32.204: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 21 21:26:32.204: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 21 21:26:32.204: INFO: update-demo-nautilus-ggpqx is verified up and running
    STEP: scaling up the replication controller 04/21/23 21:26:32.204
    Apr 21 21:26:32.205: INFO: scanned /root for discovery docs: <nil>
    Apr 21 21:26:32.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Apr 21 21:26:33.276: INFO: stderr: ""
    Apr 21 21:26:33.276: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/21/23 21:26:33.276
    Apr 21 21:26:33.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 21 21:26:33.334: INFO: stderr: ""
    Apr 21 21:26:33.334: INFO: stdout: "update-demo-nautilus-ggpqx update-demo-nautilus-v6cc2 "
    Apr 21 21:26:33.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 21 21:26:33.390: INFO: stderr: ""
    Apr 21 21:26:33.390: INFO: stdout: "true"
    Apr 21 21:26:33.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-ggpqx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 21 21:26:33.443: INFO: stderr: ""
    Apr 21 21:26:33.443: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 21 21:26:33.443: INFO: validating pod update-demo-nautilus-ggpqx
    Apr 21 21:26:33.445: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 21 21:26:33.445: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 21 21:26:33.445: INFO: update-demo-nautilus-ggpqx is verified up and running
    Apr 21 21:26:33.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-v6cc2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 21 21:26:33.499: INFO: stderr: ""
    Apr 21 21:26:33.499: INFO: stdout: "true"
    Apr 21 21:26:33.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods update-demo-nautilus-v6cc2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 21 21:26:33.552: INFO: stderr: ""
    Apr 21 21:26:33.552: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 21 21:26:33.552: INFO: validating pod update-demo-nautilus-v6cc2
    Apr 21 21:26:33.555: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 21 21:26:33.555: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 21 21:26:33.555: INFO: update-demo-nautilus-v6cc2 is verified up and running
    STEP: using delete to clean up resources 04/21/23 21:26:33.555
    Apr 21 21:26:33.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 delete --grace-period=0 --force -f -'
    Apr 21 21:26:33.614: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 21 21:26:33.614: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Apr 21 21:26:33.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get rc,svc -l name=update-demo --no-headers'
    Apr 21 21:26:33.681: INFO: stderr: "No resources found in kubectl-3018 namespace.\n"
    Apr 21 21:26:33.681: INFO: stdout: ""
    Apr 21 21:26:33.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3018 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr 21 21:26:33.742: INFO: stderr: ""
    Apr 21 21:26:33.742: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:33.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3018" for this suite. 04/21/23 21:26:33.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:33.749
Apr 21 21:26:33.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:26:33.75
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:33.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:33.76
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Apr 21 21:26:33.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/21/23 21:26:35.668
Apr 21 21:26:35.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 --namespace=crd-publish-openapi-6137 create -f -'
Apr 21 21:26:36.101: INFO: stderr: ""
Apr 21 21:26:36.101: INFO: stdout: "e2e-test-crd-publish-openapi-4507-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 21 21:26:36.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 --namespace=crd-publish-openapi-6137 delete e2e-test-crd-publish-openapi-4507-crds test-cr'
Apr 21 21:26:36.160: INFO: stderr: ""
Apr 21 21:26:36.160: INFO: stdout: "e2e-test-crd-publish-openapi-4507-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 21 21:26:36.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 --namespace=crd-publish-openapi-6137 apply -f -'
Apr 21 21:26:36.315: INFO: stderr: ""
Apr 21 21:26:36.315: INFO: stdout: "e2e-test-crd-publish-openapi-4507-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 21 21:26:36.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 --namespace=crd-publish-openapi-6137 delete e2e-test-crd-publish-openapi-4507-crds test-cr'
Apr 21 21:26:36.383: INFO: stderr: ""
Apr 21 21:26:36.383: INFO: stdout: "e2e-test-crd-publish-openapi-4507-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 04/21/23 21:26:36.383
Apr 21 21:26:36.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 explain e2e-test-crd-publish-openapi-4507-crds'
Apr 21 21:26:36.536: INFO: stderr: ""
Apr 21 21:26:36.536: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4507-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:37.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6137" for this suite. 04/21/23 21:26:37.84
------------------------------
â€¢ [4.097 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:33.749
    Apr 21 21:26:33.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:26:33.75
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:33.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:33.76
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Apr 21 21:26:33.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/21/23 21:26:35.668
    Apr 21 21:26:35.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 --namespace=crd-publish-openapi-6137 create -f -'
    Apr 21 21:26:36.101: INFO: stderr: ""
    Apr 21 21:26:36.101: INFO: stdout: "e2e-test-crd-publish-openapi-4507-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Apr 21 21:26:36.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 --namespace=crd-publish-openapi-6137 delete e2e-test-crd-publish-openapi-4507-crds test-cr'
    Apr 21 21:26:36.160: INFO: stderr: ""
    Apr 21 21:26:36.160: INFO: stdout: "e2e-test-crd-publish-openapi-4507-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Apr 21 21:26:36.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 --namespace=crd-publish-openapi-6137 apply -f -'
    Apr 21 21:26:36.315: INFO: stderr: ""
    Apr 21 21:26:36.315: INFO: stdout: "e2e-test-crd-publish-openapi-4507-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Apr 21 21:26:36.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 --namespace=crd-publish-openapi-6137 delete e2e-test-crd-publish-openapi-4507-crds test-cr'
    Apr 21 21:26:36.383: INFO: stderr: ""
    Apr 21 21:26:36.383: INFO: stdout: "e2e-test-crd-publish-openapi-4507-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 04/21/23 21:26:36.383
    Apr 21 21:26:36.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6137 explain e2e-test-crd-publish-openapi-4507-crds'
    Apr 21 21:26:36.536: INFO: stderr: ""
    Apr 21 21:26:36.536: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4507-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:37.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6137" for this suite. 04/21/23 21:26:37.84
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:37.846
Apr 21 21:26:37.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename endpointslice 04/21/23 21:26:37.847
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:37.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:37.857
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 04/21/23 21:26:37.859
STEP: getting /apis/discovery.k8s.io 04/21/23 21:26:37.86
STEP: getting /apis/discovery.k8s.iov1 04/21/23 21:26:37.861
STEP: creating 04/21/23 21:26:37.862
STEP: getting 04/21/23 21:26:37.872
STEP: listing 04/21/23 21:26:37.873
STEP: watching 04/21/23 21:26:37.875
Apr 21 21:26:37.875: INFO: starting watch
STEP: cluster-wide listing 04/21/23 21:26:37.875
STEP: cluster-wide watching 04/21/23 21:26:37.877
Apr 21 21:26:37.877: INFO: starting watch
STEP: patching 04/21/23 21:26:37.878
STEP: updating 04/21/23 21:26:37.881
Apr 21 21:26:37.885: INFO: waiting for watch events with expected annotations
Apr 21 21:26:37.885: INFO: saw patched and updated annotations
STEP: deleting 04/21/23 21:26:37.885
STEP: deleting a collection 04/21/23 21:26:37.891
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:37.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-5024" for this suite. 04/21/23 21:26:37.899
------------------------------
â€¢ [0.058 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:37.846
    Apr 21 21:26:37.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename endpointslice 04/21/23 21:26:37.847
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:37.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:37.857
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 04/21/23 21:26:37.859
    STEP: getting /apis/discovery.k8s.io 04/21/23 21:26:37.86
    STEP: getting /apis/discovery.k8s.iov1 04/21/23 21:26:37.861
    STEP: creating 04/21/23 21:26:37.862
    STEP: getting 04/21/23 21:26:37.872
    STEP: listing 04/21/23 21:26:37.873
    STEP: watching 04/21/23 21:26:37.875
    Apr 21 21:26:37.875: INFO: starting watch
    STEP: cluster-wide listing 04/21/23 21:26:37.875
    STEP: cluster-wide watching 04/21/23 21:26:37.877
    Apr 21 21:26:37.877: INFO: starting watch
    STEP: patching 04/21/23 21:26:37.878
    STEP: updating 04/21/23 21:26:37.881
    Apr 21 21:26:37.885: INFO: waiting for watch events with expected annotations
    Apr 21 21:26:37.885: INFO: saw patched and updated annotations
    STEP: deleting 04/21/23 21:26:37.885
    STEP: deleting a collection 04/21/23 21:26:37.891
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:37.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-5024" for this suite. 04/21/23 21:26:37.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:37.904
Apr 21 21:26:37.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-runtime 04/21/23 21:26:37.905
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:37.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:37.914
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 04/21/23 21:26:37.916
STEP: wait for the container to reach Succeeded 04/21/23 21:26:37.922
STEP: get the container status 04/21/23 21:26:41.935
STEP: the container should be terminated 04/21/23 21:26:41.936
STEP: the termination message should be set 04/21/23 21:26:41.936
Apr 21 21:26:41.936: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 04/21/23 21:26:41.936
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:41.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1277" for this suite. 04/21/23 21:26:41.949
------------------------------
â€¢ [4.048 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:37.904
    Apr 21 21:26:37.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-runtime 04/21/23 21:26:37.905
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:37.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:37.914
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 04/21/23 21:26:37.916
    STEP: wait for the container to reach Succeeded 04/21/23 21:26:37.922
    STEP: get the container status 04/21/23 21:26:41.935
    STEP: the container should be terminated 04/21/23 21:26:41.936
    STEP: the termination message should be set 04/21/23 21:26:41.936
    Apr 21 21:26:41.936: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 04/21/23 21:26:41.936
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:41.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1277" for this suite. 04/21/23 21:26:41.949
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:41.952
Apr 21 21:26:41.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 21:26:41.953
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:41.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:41.961
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 04/21/23 21:26:41.963
Apr 21 21:26:41.967: INFO: Waiting up to 5m0s for pod "pod-9hsgv" in namespace "pods-7422" to be "running"
Apr 21 21:26:41.969: INFO: Pod "pod-9hsgv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.459034ms
Apr 21 21:26:43.971: INFO: Pod "pod-9hsgv": Phase="Running", Reason="", readiness=true. Elapsed: 2.003914778s
Apr 21 21:26:43.971: INFO: Pod "pod-9hsgv" satisfied condition "running"
STEP: patching /status 04/21/23 21:26:43.971
Apr 21 21:26:43.980: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:43.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7422" for this suite. 04/21/23 21:26:43.982
------------------------------
â€¢ [2.032 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:41.952
    Apr 21 21:26:41.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 21:26:41.953
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:41.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:41.961
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 04/21/23 21:26:41.963
    Apr 21 21:26:41.967: INFO: Waiting up to 5m0s for pod "pod-9hsgv" in namespace "pods-7422" to be "running"
    Apr 21 21:26:41.969: INFO: Pod "pod-9hsgv": Phase="Pending", Reason="", readiness=false. Elapsed: 1.459034ms
    Apr 21 21:26:43.971: INFO: Pod "pod-9hsgv": Phase="Running", Reason="", readiness=true. Elapsed: 2.003914778s
    Apr 21 21:26:43.971: INFO: Pod "pod-9hsgv" satisfied condition "running"
    STEP: patching /status 04/21/23 21:26:43.971
    Apr 21 21:26:43.980: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:43.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7422" for this suite. 04/21/23 21:26:43.982
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:43.985
Apr 21 21:26:43.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename daemonsets 04/21/23 21:26:43.986
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:43.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:43.996
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 04/21/23 21:26:44.004
STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:26:44.008
Apr 21 21:26:44.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:26:44.012: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:26:45.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:26:45.017: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:26:46.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 21 21:26:46.017: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 04/21/23 21:26:46.019
Apr 21 21:26:46.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:26:46.028: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:26:47.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:26:47.033: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:26:48.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:26:48.033: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:26:49.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:26:49.032: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:26:50.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 21 21:26:50.033: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:26:50.035
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-803, will wait for the garbage collector to delete the pods 04/21/23 21:26:50.035
Apr 21 21:26:50.090: INFO: Deleting DaemonSet.extensions daemon-set took: 3.745603ms
Apr 21 21:26:50.190: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.165412ms
Apr 21 21:26:52.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:26:52.892: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 21 21:26:52.894: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20550"},"items":null}

Apr 21 21:26:52.895: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20550"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:26:52.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-803" for this suite. 04/21/23 21:26:52.902
------------------------------
â€¢ [SLOW TEST] [8.920 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:43.985
    Apr 21 21:26:43.985: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename daemonsets 04/21/23 21:26:43.986
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:43.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:43.996
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 04/21/23 21:26:44.004
    STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:26:44.008
    Apr 21 21:26:44.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:26:44.012: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:26:45.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:26:45.017: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:26:46.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 21 21:26:46.017: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 04/21/23 21:26:46.019
    Apr 21 21:26:46.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:26:46.028: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:26:47.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:26:47.033: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:26:48.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:26:48.033: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:26:49.032: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:26:49.032: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:26:50.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 21 21:26:50.033: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:26:50.035
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-803, will wait for the garbage collector to delete the pods 04/21/23 21:26:50.035
    Apr 21 21:26:50.090: INFO: Deleting DaemonSet.extensions daemon-set took: 3.745603ms
    Apr 21 21:26:50.190: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.165412ms
    Apr 21 21:26:52.892: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:26:52.892: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 21 21:26:52.894: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20550"},"items":null}

    Apr 21 21:26:52.895: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20550"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:26:52.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-803" for this suite. 04/21/23 21:26:52.902
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:26:52.905
Apr 21 21:26:52.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 21:26:52.906
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:52.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:52.915
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-9822 04/21/23 21:26:52.917
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[] 04/21/23 21:26:52.924
Apr 21 21:26:52.926: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Apr 21 21:26:53.931: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9822 04/21/23 21:26:53.931
Apr 21 21:26:53.936: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9822" to be "running and ready"
Apr 21 21:26:53.937: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542273ms
Apr 21 21:26:53.937: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:26:55.940: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004655383s
Apr 21 21:26:55.940: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr 21 21:26:55.940: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[pod1:[100]] 04/21/23 21:26:55.942
Apr 21 21:26:55.947: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9822 04/21/23 21:26:55.947
Apr 21 21:26:55.950: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9822" to be "running and ready"
Apr 21 21:26:55.952: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558591ms
Apr 21 21:26:55.952: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:26:57.954: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003683374s
Apr 21 21:26:57.954: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr 21 21:26:57.954: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[pod1:[100] pod2:[101]] 04/21/23 21:26:57.956
Apr 21 21:26:57.962: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 04/21/23 21:26:57.962
Apr 21 21:26:57.963: INFO: Creating new exec pod
Apr 21 21:26:57.966: INFO: Waiting up to 5m0s for pod "execpodkp4mq" in namespace "services-9822" to be "running"
Apr 21 21:26:57.967: INFO: Pod "execpodkp4mq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3639ms
Apr 21 21:26:59.970: INFO: Pod "execpodkp4mq": Phase="Running", Reason="", readiness=true. Elapsed: 2.004297179s
Apr 21 21:26:59.970: INFO: Pod "execpodkp4mq" satisfied condition "running"
Apr 21 21:27:00.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-9822 exec execpodkp4mq -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Apr 21 21:27:01.087: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Apr 21 21:27:01.087: INFO: stdout: ""
Apr 21 21:27:01.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-9822 exec execpodkp4mq -- /bin/sh -x -c nc -v -z -w 2 10.98.244.90 80'
Apr 21 21:27:01.210: INFO: stderr: "+ nc -v -z -w 2 10.98.244.90 80\nConnection to 10.98.244.90 80 port [tcp/http] succeeded!\n"
Apr 21 21:27:01.210: INFO: stdout: ""
Apr 21 21:27:01.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-9822 exec execpodkp4mq -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Apr 21 21:27:01.344: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Apr 21 21:27:01.344: INFO: stdout: ""
Apr 21 21:27:01.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-9822 exec execpodkp4mq -- /bin/sh -x -c nc -v -z -w 2 10.98.244.90 81'
Apr 21 21:27:01.459: INFO: stderr: "+ nc -v -z -w 2 10.98.244.90 81\nConnection to 10.98.244.90 81 port [tcp/*] succeeded!\n"
Apr 21 21:27:01.460: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9822 04/21/23 21:27:01.46
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[pod2:[101]] 04/21/23 21:27:01.469
Apr 21 21:27:01.477: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9822 04/21/23 21:27:01.477
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[] 04/21/23 21:27:01.487
Apr 21 21:27:01.494: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 21:27:01.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9822" for this suite. 04/21/23 21:27:01.512
------------------------------
â€¢ [SLOW TEST] [8.610 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:26:52.905
    Apr 21 21:26:52.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 21:26:52.906
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:26:52.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:26:52.915
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-9822 04/21/23 21:26:52.917
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[] 04/21/23 21:26:52.924
    Apr 21 21:26:52.926: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Apr 21 21:26:53.931: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9822 04/21/23 21:26:53.931
    Apr 21 21:26:53.936: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9822" to be "running and ready"
    Apr 21 21:26:53.937: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.542273ms
    Apr 21 21:26:53.937: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:26:55.940: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.004655383s
    Apr 21 21:26:55.940: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr 21 21:26:55.940: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[pod1:[100]] 04/21/23 21:26:55.942
    Apr 21 21:26:55.947: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9822 04/21/23 21:26:55.947
    Apr 21 21:26:55.950: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9822" to be "running and ready"
    Apr 21 21:26:55.952: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.558591ms
    Apr 21 21:26:55.952: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:26:57.954: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.003683374s
    Apr 21 21:26:57.954: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr 21 21:26:57.954: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[pod1:[100] pod2:[101]] 04/21/23 21:26:57.956
    Apr 21 21:26:57.962: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 04/21/23 21:26:57.962
    Apr 21 21:26:57.963: INFO: Creating new exec pod
    Apr 21 21:26:57.966: INFO: Waiting up to 5m0s for pod "execpodkp4mq" in namespace "services-9822" to be "running"
    Apr 21 21:26:57.967: INFO: Pod "execpodkp4mq": Phase="Pending", Reason="", readiness=false. Elapsed: 1.3639ms
    Apr 21 21:26:59.970: INFO: Pod "execpodkp4mq": Phase="Running", Reason="", readiness=true. Elapsed: 2.004297179s
    Apr 21 21:26:59.970: INFO: Pod "execpodkp4mq" satisfied condition "running"
    Apr 21 21:27:00.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-9822 exec execpodkp4mq -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Apr 21 21:27:01.087: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Apr 21 21:27:01.087: INFO: stdout: ""
    Apr 21 21:27:01.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-9822 exec execpodkp4mq -- /bin/sh -x -c nc -v -z -w 2 10.98.244.90 80'
    Apr 21 21:27:01.210: INFO: stderr: "+ nc -v -z -w 2 10.98.244.90 80\nConnection to 10.98.244.90 80 port [tcp/http] succeeded!\n"
    Apr 21 21:27:01.210: INFO: stdout: ""
    Apr 21 21:27:01.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-9822 exec execpodkp4mq -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Apr 21 21:27:01.344: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Apr 21 21:27:01.344: INFO: stdout: ""
    Apr 21 21:27:01.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-9822 exec execpodkp4mq -- /bin/sh -x -c nc -v -z -w 2 10.98.244.90 81'
    Apr 21 21:27:01.459: INFO: stderr: "+ nc -v -z -w 2 10.98.244.90 81\nConnection to 10.98.244.90 81 port [tcp/*] succeeded!\n"
    Apr 21 21:27:01.460: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9822 04/21/23 21:27:01.46
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[pod2:[101]] 04/21/23 21:27:01.469
    Apr 21 21:27:01.477: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9822 04/21/23 21:27:01.477
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9822 to expose endpoints map[] 04/21/23 21:27:01.487
    Apr 21 21:27:01.494: INFO: successfully validated that service multi-endpoint-test in namespace services-9822 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:27:01.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9822" for this suite. 04/21/23 21:27:01.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:27:01.516
Apr 21 21:27:01.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:27:01.517
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:01.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:01.538
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:27:01.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3713" for this suite. 04/21/23 21:27:01.571
------------------------------
â€¢ [0.059 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:27:01.516
    Apr 21 21:27:01.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:27:01.517
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:01.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:01.538
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:27:01.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3713" for this suite. 04/21/23 21:27:01.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:27:01.575
Apr 21 21:27:01.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 21:27:01.576
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:01.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:01.588
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Apr 21 21:27:01.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:27:04.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5076" for this suite. 04/21/23 21:27:04.677
------------------------------
â€¢ [3.105 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:27:01.575
    Apr 21 21:27:01.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename custom-resource-definition 04/21/23 21:27:01.576
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:01.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:01.588
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Apr 21 21:27:01.590: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:27:04.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5076" for this suite. 04/21/23 21:27:04.677
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:27:04.68
Apr 21 21:27:04.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename statefulset 04/21/23 21:27:04.681
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:04.689
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:04.691
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4377 04/21/23 21:27:04.693
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 04/21/23 21:27:04.695
STEP: Creating pod with conflicting port in namespace statefulset-4377 04/21/23 21:27:04.697
STEP: Waiting until pod test-pod will start running in namespace statefulset-4377 04/21/23 21:27:04.702
Apr 21 21:27:04.702: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4377" to be "running"
Apr 21 21:27:04.704: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.516625ms
Apr 21 21:27:06.706: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00376069s
Apr 21 21:27:06.706: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-4377 04/21/23 21:27:06.706
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4377 04/21/23 21:27:06.71
Apr 21 21:27:06.717: INFO: Observed stateful pod in namespace: statefulset-4377, name: ss-0, uid: bb4bfb5d-6a42-4428-b847-e8ad7e8aa325, status phase: Pending. Waiting for statefulset controller to delete.
Apr 21 21:27:06.736: INFO: Observed stateful pod in namespace: statefulset-4377, name: ss-0, uid: bb4bfb5d-6a42-4428-b847-e8ad7e8aa325, status phase: Failed. Waiting for statefulset controller to delete.
Apr 21 21:27:06.740: INFO: Observed stateful pod in namespace: statefulset-4377, name: ss-0, uid: bb4bfb5d-6a42-4428-b847-e8ad7e8aa325, status phase: Failed. Waiting for statefulset controller to delete.
Apr 21 21:27:06.743: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4377
STEP: Removing pod with conflicting port in namespace statefulset-4377 04/21/23 21:27:06.743
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4377 and will be in running state 04/21/23 21:27:06.75
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 21 21:27:08.755: INFO: Deleting all statefulset in ns statefulset-4377
Apr 21 21:27:08.756: INFO: Scaling statefulset ss to 0
Apr 21 21:27:18.769: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 21:27:18.771: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 21 21:27:18.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4377" for this suite. 04/21/23 21:27:18.779
------------------------------
â€¢ [SLOW TEST] [14.104 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:27:04.68
    Apr 21 21:27:04.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename statefulset 04/21/23 21:27:04.681
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:04.689
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:04.691
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4377 04/21/23 21:27:04.693
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 04/21/23 21:27:04.695
    STEP: Creating pod with conflicting port in namespace statefulset-4377 04/21/23 21:27:04.697
    STEP: Waiting until pod test-pod will start running in namespace statefulset-4377 04/21/23 21:27:04.702
    Apr 21 21:27:04.702: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4377" to be "running"
    Apr 21 21:27:04.704: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.516625ms
    Apr 21 21:27:06.706: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00376069s
    Apr 21 21:27:06.706: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-4377 04/21/23 21:27:06.706
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4377 04/21/23 21:27:06.71
    Apr 21 21:27:06.717: INFO: Observed stateful pod in namespace: statefulset-4377, name: ss-0, uid: bb4bfb5d-6a42-4428-b847-e8ad7e8aa325, status phase: Pending. Waiting for statefulset controller to delete.
    Apr 21 21:27:06.736: INFO: Observed stateful pod in namespace: statefulset-4377, name: ss-0, uid: bb4bfb5d-6a42-4428-b847-e8ad7e8aa325, status phase: Failed. Waiting for statefulset controller to delete.
    Apr 21 21:27:06.740: INFO: Observed stateful pod in namespace: statefulset-4377, name: ss-0, uid: bb4bfb5d-6a42-4428-b847-e8ad7e8aa325, status phase: Failed. Waiting for statefulset controller to delete.
    Apr 21 21:27:06.743: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4377
    STEP: Removing pod with conflicting port in namespace statefulset-4377 04/21/23 21:27:06.743
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4377 and will be in running state 04/21/23 21:27:06.75
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 21 21:27:08.755: INFO: Deleting all statefulset in ns statefulset-4377
    Apr 21 21:27:08.756: INFO: Scaling statefulset ss to 0
    Apr 21 21:27:18.769: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 21:27:18.771: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:27:18.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4377" for this suite. 04/21/23 21:27:18.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:27:18.785
Apr 21 21:27:18.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename discovery 04/21/23 21:27:18.786
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:18.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:18.794
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 04/21/23 21:27:18.796
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Apr 21 21:27:19.138: INFO: Checking APIGroup: apiregistration.k8s.io
Apr 21 21:27:19.138: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr 21 21:27:19.138: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Apr 21 21:27:19.138: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr 21 21:27:19.138: INFO: Checking APIGroup: apps
Apr 21 21:27:19.139: INFO: PreferredVersion.GroupVersion: apps/v1
Apr 21 21:27:19.139: INFO: Versions found [{apps/v1 v1}]
Apr 21 21:27:19.139: INFO: apps/v1 matches apps/v1
Apr 21 21:27:19.139: INFO: Checking APIGroup: events.k8s.io
Apr 21 21:27:19.140: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr 21 21:27:19.140: INFO: Versions found [{events.k8s.io/v1 v1}]
Apr 21 21:27:19.140: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr 21 21:27:19.140: INFO: Checking APIGroup: authentication.k8s.io
Apr 21 21:27:19.141: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr 21 21:27:19.141: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Apr 21 21:27:19.141: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr 21 21:27:19.141: INFO: Checking APIGroup: authorization.k8s.io
Apr 21 21:27:19.141: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr 21 21:27:19.141: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Apr 21 21:27:19.141: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr 21 21:27:19.141: INFO: Checking APIGroup: autoscaling
Apr 21 21:27:19.142: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Apr 21 21:27:19.142: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Apr 21 21:27:19.142: INFO: autoscaling/v2 matches autoscaling/v2
Apr 21 21:27:19.142: INFO: Checking APIGroup: batch
Apr 21 21:27:19.143: INFO: PreferredVersion.GroupVersion: batch/v1
Apr 21 21:27:19.143: INFO: Versions found [{batch/v1 v1}]
Apr 21 21:27:19.143: INFO: batch/v1 matches batch/v1
Apr 21 21:27:19.143: INFO: Checking APIGroup: certificates.k8s.io
Apr 21 21:27:19.143: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr 21 21:27:19.143: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Apr 21 21:27:19.143: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr 21 21:27:19.143: INFO: Checking APIGroup: networking.k8s.io
Apr 21 21:27:19.144: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr 21 21:27:19.144: INFO: Versions found [{networking.k8s.io/v1 v1}]
Apr 21 21:27:19.144: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr 21 21:27:19.144: INFO: Checking APIGroup: policy
Apr 21 21:27:19.144: INFO: PreferredVersion.GroupVersion: policy/v1
Apr 21 21:27:19.144: INFO: Versions found [{policy/v1 v1}]
Apr 21 21:27:19.144: INFO: policy/v1 matches policy/v1
Apr 21 21:27:19.144: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr 21 21:27:19.145: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr 21 21:27:19.145: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Apr 21 21:27:19.145: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr 21 21:27:19.145: INFO: Checking APIGroup: storage.k8s.io
Apr 21 21:27:19.146: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr 21 21:27:19.146: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr 21 21:27:19.146: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr 21 21:27:19.146: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr 21 21:27:19.146: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr 21 21:27:19.146: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Apr 21 21:27:19.146: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr 21 21:27:19.146: INFO: Checking APIGroup: apiextensions.k8s.io
Apr 21 21:27:19.147: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr 21 21:27:19.147: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Apr 21 21:27:19.147: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr 21 21:27:19.147: INFO: Checking APIGroup: scheduling.k8s.io
Apr 21 21:27:19.148: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr 21 21:27:19.148: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Apr 21 21:27:19.148: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr 21 21:27:19.148: INFO: Checking APIGroup: coordination.k8s.io
Apr 21 21:27:19.148: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr 21 21:27:19.148: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Apr 21 21:27:19.148: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr 21 21:27:19.148: INFO: Checking APIGroup: node.k8s.io
Apr 21 21:27:19.149: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr 21 21:27:19.149: INFO: Versions found [{node.k8s.io/v1 v1}]
Apr 21 21:27:19.149: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr 21 21:27:19.149: INFO: Checking APIGroup: discovery.k8s.io
Apr 21 21:27:19.150: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Apr 21 21:27:19.150: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Apr 21 21:27:19.150: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Apr 21 21:27:19.150: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr 21 21:27:19.150: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Apr 21 21:27:19.150: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Apr 21 21:27:19.150: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Apr 21 21:27:19.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-8490" for this suite. 04/21/23 21:27:19.153
------------------------------
â€¢ [0.371 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:27:18.785
    Apr 21 21:27:18.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename discovery 04/21/23 21:27:18.786
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:18.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:18.794
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 04/21/23 21:27:18.796
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Apr 21 21:27:19.138: INFO: Checking APIGroup: apiregistration.k8s.io
    Apr 21 21:27:19.138: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Apr 21 21:27:19.138: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Apr 21 21:27:19.138: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Apr 21 21:27:19.138: INFO: Checking APIGroup: apps
    Apr 21 21:27:19.139: INFO: PreferredVersion.GroupVersion: apps/v1
    Apr 21 21:27:19.139: INFO: Versions found [{apps/v1 v1}]
    Apr 21 21:27:19.139: INFO: apps/v1 matches apps/v1
    Apr 21 21:27:19.139: INFO: Checking APIGroup: events.k8s.io
    Apr 21 21:27:19.140: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Apr 21 21:27:19.140: INFO: Versions found [{events.k8s.io/v1 v1}]
    Apr 21 21:27:19.140: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Apr 21 21:27:19.140: INFO: Checking APIGroup: authentication.k8s.io
    Apr 21 21:27:19.141: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Apr 21 21:27:19.141: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Apr 21 21:27:19.141: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Apr 21 21:27:19.141: INFO: Checking APIGroup: authorization.k8s.io
    Apr 21 21:27:19.141: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Apr 21 21:27:19.141: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Apr 21 21:27:19.141: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Apr 21 21:27:19.141: INFO: Checking APIGroup: autoscaling
    Apr 21 21:27:19.142: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Apr 21 21:27:19.142: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Apr 21 21:27:19.142: INFO: autoscaling/v2 matches autoscaling/v2
    Apr 21 21:27:19.142: INFO: Checking APIGroup: batch
    Apr 21 21:27:19.143: INFO: PreferredVersion.GroupVersion: batch/v1
    Apr 21 21:27:19.143: INFO: Versions found [{batch/v1 v1}]
    Apr 21 21:27:19.143: INFO: batch/v1 matches batch/v1
    Apr 21 21:27:19.143: INFO: Checking APIGroup: certificates.k8s.io
    Apr 21 21:27:19.143: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Apr 21 21:27:19.143: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Apr 21 21:27:19.143: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Apr 21 21:27:19.143: INFO: Checking APIGroup: networking.k8s.io
    Apr 21 21:27:19.144: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Apr 21 21:27:19.144: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Apr 21 21:27:19.144: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Apr 21 21:27:19.144: INFO: Checking APIGroup: policy
    Apr 21 21:27:19.144: INFO: PreferredVersion.GroupVersion: policy/v1
    Apr 21 21:27:19.144: INFO: Versions found [{policy/v1 v1}]
    Apr 21 21:27:19.144: INFO: policy/v1 matches policy/v1
    Apr 21 21:27:19.144: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Apr 21 21:27:19.145: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Apr 21 21:27:19.145: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Apr 21 21:27:19.145: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Apr 21 21:27:19.145: INFO: Checking APIGroup: storage.k8s.io
    Apr 21 21:27:19.146: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Apr 21 21:27:19.146: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Apr 21 21:27:19.146: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Apr 21 21:27:19.146: INFO: Checking APIGroup: admissionregistration.k8s.io
    Apr 21 21:27:19.146: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Apr 21 21:27:19.146: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Apr 21 21:27:19.146: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Apr 21 21:27:19.146: INFO: Checking APIGroup: apiextensions.k8s.io
    Apr 21 21:27:19.147: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Apr 21 21:27:19.147: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Apr 21 21:27:19.147: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Apr 21 21:27:19.147: INFO: Checking APIGroup: scheduling.k8s.io
    Apr 21 21:27:19.148: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Apr 21 21:27:19.148: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Apr 21 21:27:19.148: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Apr 21 21:27:19.148: INFO: Checking APIGroup: coordination.k8s.io
    Apr 21 21:27:19.148: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Apr 21 21:27:19.148: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Apr 21 21:27:19.148: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Apr 21 21:27:19.148: INFO: Checking APIGroup: node.k8s.io
    Apr 21 21:27:19.149: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Apr 21 21:27:19.149: INFO: Versions found [{node.k8s.io/v1 v1}]
    Apr 21 21:27:19.149: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Apr 21 21:27:19.149: INFO: Checking APIGroup: discovery.k8s.io
    Apr 21 21:27:19.150: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Apr 21 21:27:19.150: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Apr 21 21:27:19.150: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Apr 21 21:27:19.150: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Apr 21 21:27:19.150: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Apr 21 21:27:19.150: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Apr 21 21:27:19.150: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:27:19.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-8490" for this suite. 04/21/23 21:27:19.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:27:19.157
Apr 21 21:27:19.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename container-probe 04/21/23 21:27:19.157
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:19.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:19.167
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3 in namespace container-probe-9807 04/21/23 21:27:19.169
Apr 21 21:27:19.173: INFO: Waiting up to 5m0s for pod "test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3" in namespace "container-probe-9807" to be "not pending"
Apr 21 21:27:19.175: INFO: Pod "test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505682ms
Apr 21 21:27:21.178: INFO: Pod "test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004677211s
Apr 21 21:27:21.178: INFO: Pod "test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3" satisfied condition "not pending"
Apr 21 21:27:21.178: INFO: Started pod test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3 in namespace container-probe-9807
STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 21:27:21.178
Apr 21 21:27:21.180: INFO: Initial restart count of pod test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3 is 0
STEP: deleting the pod 04/21/23 21:31:21.537
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 21 21:31:21.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9807" for this suite. 04/21/23 21:31:21.549
------------------------------
â€¢ [SLOW TEST] [242.396 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:27:19.157
    Apr 21 21:27:19.157: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename container-probe 04/21/23 21:27:19.157
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:27:19.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:27:19.167
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3 in namespace container-probe-9807 04/21/23 21:27:19.169
    Apr 21 21:27:19.173: INFO: Waiting up to 5m0s for pod "test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3" in namespace "container-probe-9807" to be "not pending"
    Apr 21 21:27:19.175: INFO: Pod "test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505682ms
    Apr 21 21:27:21.178: INFO: Pod "test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004677211s
    Apr 21 21:27:21.178: INFO: Pod "test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3" satisfied condition "not pending"
    Apr 21 21:27:21.178: INFO: Started pod test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3 in namespace container-probe-9807
    STEP: checking the pod's current state and verifying that restartCount is present 04/21/23 21:27:21.178
    Apr 21 21:27:21.180: INFO: Initial restart count of pod test-webserver-74961661-cbc5-4912-a81d-4532facf4fa3 is 0
    STEP: deleting the pod 04/21/23 21:31:21.537
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:31:21.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9807" for this suite. 04/21/23 21:31:21.549
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:31:21.554
Apr 21 21:31:21.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 21:31:21.555
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:31:21.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:31:21.564
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 04/21/23 21:31:21.566
Apr 21 21:31:21.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6" in namespace "downward-api-3515" to be "Succeeded or Failed"
Apr 21 21:31:21.575: INFO: Pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511703ms
Apr 21 21:31:23.578: INFO: Pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005743797s
Apr 21 21:31:25.577: INFO: Pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004846525s
STEP: Saw pod success 04/21/23 21:31:25.577
Apr 21 21:31:25.577: INFO: Pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6" satisfied condition "Succeeded or Failed"
Apr 21 21:31:25.579: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6 container client-container: <nil>
STEP: delete the pod 04/21/23 21:31:25.59
Apr 21 21:31:25.598: INFO: Waiting for pod downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6 to disappear
Apr 21 21:31:25.599: INFO: Pod downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 21:31:25.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3515" for this suite. 04/21/23 21:31:25.601
------------------------------
â€¢ [4.051 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:31:21.554
    Apr 21 21:31:21.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 21:31:21.555
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:31:21.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:31:21.564
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 04/21/23 21:31:21.566
    Apr 21 21:31:21.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6" in namespace "downward-api-3515" to be "Succeeded or Failed"
    Apr 21 21:31:21.575: INFO: Pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.511703ms
    Apr 21 21:31:23.578: INFO: Pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005743797s
    Apr 21 21:31:25.577: INFO: Pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004846525s
    STEP: Saw pod success 04/21/23 21:31:25.577
    Apr 21 21:31:25.577: INFO: Pod "downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6" satisfied condition "Succeeded or Failed"
    Apr 21 21:31:25.579: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6 container client-container: <nil>
    STEP: delete the pod 04/21/23 21:31:25.59
    Apr 21 21:31:25.598: INFO: Waiting for pod downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6 to disappear
    Apr 21 21:31:25.599: INFO: Pod downwardapi-volume-dba31aa1-073f-4463-a91a-4a11e98d5cd6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:31:25.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3515" for this suite. 04/21/23 21:31:25.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:31:25.605
Apr 21 21:31:25.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename security-context 04/21/23 21:31:25.606
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:31:25.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:31:25.614
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/21/23 21:31:25.616
Apr 21 21:31:25.621: INFO: Waiting up to 5m0s for pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f" in namespace "security-context-2079" to be "Succeeded or Failed"
Apr 21 21:31:25.622: INFO: Pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.404606ms
Apr 21 21:31:27.625: INFO: Pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004058762s
Apr 21 21:31:29.625: INFO: Pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00431504s
STEP: Saw pod success 04/21/23 21:31:29.625
Apr 21 21:31:29.625: INFO: Pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f" satisfied condition "Succeeded or Failed"
Apr 21 21:31:29.627: INFO: Trying to get logs from node k8sconformance-m02 pod security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f container test-container: <nil>
STEP: delete the pod 04/21/23 21:31:29.632
Apr 21 21:31:29.638: INFO: Waiting for pod security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f to disappear
Apr 21 21:31:29.640: INFO: Pod security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 21 21:31:29.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-2079" for this suite. 04/21/23 21:31:29.642
------------------------------
â€¢ [4.040 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:31:25.605
    Apr 21 21:31:25.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename security-context 04/21/23 21:31:25.606
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:31:25.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:31:25.614
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/21/23 21:31:25.616
    Apr 21 21:31:25.621: INFO: Waiting up to 5m0s for pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f" in namespace "security-context-2079" to be "Succeeded or Failed"
    Apr 21 21:31:25.622: INFO: Pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.404606ms
    Apr 21 21:31:27.625: INFO: Pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004058762s
    Apr 21 21:31:29.625: INFO: Pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00431504s
    STEP: Saw pod success 04/21/23 21:31:29.625
    Apr 21 21:31:29.625: INFO: Pod "security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f" satisfied condition "Succeeded or Failed"
    Apr 21 21:31:29.627: INFO: Trying to get logs from node k8sconformance-m02 pod security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f container test-container: <nil>
    STEP: delete the pod 04/21/23 21:31:29.632
    Apr 21 21:31:29.638: INFO: Waiting for pod security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f to disappear
    Apr 21 21:31:29.640: INFO: Pod security-context-0e83a8ea-a7a8-49be-bf63-382f1a95402f no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:31:29.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-2079" for this suite. 04/21/23 21:31:29.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:31:29.645
Apr 21 21:31:29.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename cronjob 04/21/23 21:31:29.646
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:31:29.652
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:31:29.654
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 04/21/23 21:31:29.656
STEP: Ensuring a job is scheduled 04/21/23 21:31:29.659
STEP: Ensuring exactly one is scheduled 04/21/23 21:32:01.662
STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/21/23 21:32:01.664
STEP: Ensuring the job is replaced with a new one 04/21/23 21:32:01.666
STEP: Removing cronjob 04/21/23 21:33:01.668
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 21 21:33:01.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7083" for this suite. 04/21/23 21:33:01.674
------------------------------
â€¢ [SLOW TEST] [92.032 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:31:29.645
    Apr 21 21:31:29.645: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename cronjob 04/21/23 21:31:29.646
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:31:29.652
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:31:29.654
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 04/21/23 21:31:29.656
    STEP: Ensuring a job is scheduled 04/21/23 21:31:29.659
    STEP: Ensuring exactly one is scheduled 04/21/23 21:32:01.662
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/21/23 21:32:01.664
    STEP: Ensuring the job is replaced with a new one 04/21/23 21:32:01.666
    STEP: Removing cronjob 04/21/23 21:33:01.668
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:33:01.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7083" for this suite. 04/21/23 21:33:01.674
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:33:01.68
Apr 21 21:33:01.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 21:33:01.681
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:01.69
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:01.692
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 21:33:01.7
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:33:01.951
STEP: Deploying the webhook pod 04/21/23 21:33:01.957
STEP: Wait for the deployment to be ready 04/21/23 21:33:01.966
Apr 21 21:33:01.971: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 21:33:03.977
STEP: Verifying the service has paired with the endpoint 04/21/23 21:33:03.985
Apr 21 21:33:04.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Apr 21 21:33:04.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7789-crds.webhook.example.com via the AdmissionRegistration API 04/21/23 21:33:05.495
STEP: Creating a custom resource that should be mutated by the webhook 04/21/23 21:33:05.506
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:33:08.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2218" for this suite. 04/21/23 21:33:08.05
STEP: Destroying namespace "webhook-2218-markers" for this suite. 04/21/23 21:33:08.053
------------------------------
â€¢ [SLOW TEST] [6.376 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:33:01.68
    Apr 21 21:33:01.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 21:33:01.681
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:01.69
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:01.692
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 21:33:01.7
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:33:01.951
    STEP: Deploying the webhook pod 04/21/23 21:33:01.957
    STEP: Wait for the deployment to be ready 04/21/23 21:33:01.966
    Apr 21 21:33:01.971: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 21:33:03.977
    STEP: Verifying the service has paired with the endpoint 04/21/23 21:33:03.985
    Apr 21 21:33:04.986: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Apr 21 21:33:04.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7789-crds.webhook.example.com via the AdmissionRegistration API 04/21/23 21:33:05.495
    STEP: Creating a custom resource that should be mutated by the webhook 04/21/23 21:33:05.506
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:33:08.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2218" for this suite. 04/21/23 21:33:08.05
    STEP: Destroying namespace "webhook-2218-markers" for this suite. 04/21/23 21:33:08.053
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:33:08.057
Apr 21 21:33:08.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename daemonsets 04/21/23 21:33:08.058
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:08.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:08.068
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Apr 21 21:33:08.079: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:33:08.083
Apr 21 21:33:08.088: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:33:08.088: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:33:09.092: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:33:09.092: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:33:10.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 21 21:33:10.093: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 04/21/23 21:33:10.099
STEP: Check that daemon pods images are updated. 04/21/23 21:33:10.105
Apr 21 21:33:10.108: INFO: Wrong image for pod: daemon-set-n6fxr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 21 21:33:10.108: INFO: Wrong image for pod: daemon-set-sfrxx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 21 21:33:11.112: INFO: Wrong image for pod: daemon-set-n6fxr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 21 21:33:12.112: INFO: Wrong image for pod: daemon-set-n6fxr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 21 21:33:13.135: INFO: Pod daemon-set-jz677 is not available
Apr 21 21:33:13.135: INFO: Wrong image for pod: daemon-set-n6fxr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 21 21:33:14.113: INFO: Pod daemon-set-fcsmd is not available
STEP: Check that daemon pods are still running on every node of the cluster. 04/21/23 21:33:14.115
Apr 21 21:33:14.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:33:14.118: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:33:15.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 21 21:33:15.123: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:33:15.132
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2646, will wait for the garbage collector to delete the pods 04/21/23 21:33:15.132
Apr 21 21:33:15.188: INFO: Deleting DaemonSet.extensions daemon-set took: 3.566938ms
Apr 21 21:33:15.288: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.424366ms
Apr 21 21:33:18.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:33:18.192: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 21 21:33:18.194: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21414"},"items":null}

Apr 21 21:33:18.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21414"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:33:18.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2646" for this suite. 04/21/23 21:33:18.203
------------------------------
â€¢ [SLOW TEST] [10.151 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:33:08.057
    Apr 21 21:33:08.057: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename daemonsets 04/21/23 21:33:08.058
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:08.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:08.068
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Apr 21 21:33:08.079: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:33:08.083
    Apr 21 21:33:08.088: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:33:08.088: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:33:09.092: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:33:09.092: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:33:10.093: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 21 21:33:10.093: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 04/21/23 21:33:10.099
    STEP: Check that daemon pods images are updated. 04/21/23 21:33:10.105
    Apr 21 21:33:10.108: INFO: Wrong image for pod: daemon-set-n6fxr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 21 21:33:10.108: INFO: Wrong image for pod: daemon-set-sfrxx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 21 21:33:11.112: INFO: Wrong image for pod: daemon-set-n6fxr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 21 21:33:12.112: INFO: Wrong image for pod: daemon-set-n6fxr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 21 21:33:13.135: INFO: Pod daemon-set-jz677 is not available
    Apr 21 21:33:13.135: INFO: Wrong image for pod: daemon-set-n6fxr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 21 21:33:14.113: INFO: Pod daemon-set-fcsmd is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 04/21/23 21:33:14.115
    Apr 21 21:33:14.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:33:14.118: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:33:15.123: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 21 21:33:15.123: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:33:15.132
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2646, will wait for the garbage collector to delete the pods 04/21/23 21:33:15.132
    Apr 21 21:33:15.188: INFO: Deleting DaemonSet.extensions daemon-set took: 3.566938ms
    Apr 21 21:33:15.288: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.424366ms
    Apr 21 21:33:18.192: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:33:18.192: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 21 21:33:18.194: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"21414"},"items":null}

    Apr 21 21:33:18.196: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"21414"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:33:18.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2646" for this suite. 04/21/23 21:33:18.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:33:18.208
Apr 21 21:33:18.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:33:18.209
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:18.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:18.217
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-c4b99d80-a963-436c-b8f8-f5bdabbc5ed2 04/21/23 21:33:18.219
STEP: Creating a pod to test consume secrets 04/21/23 21:33:18.222
Apr 21 21:33:18.226: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82" in namespace "projected-6865" to be "Succeeded or Failed"
Apr 21 21:33:18.228: INFO: Pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82": Phase="Pending", Reason="", readiness=false. Elapsed: 1.380062ms
Apr 21 21:33:20.231: INFO: Pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004297377s
Apr 21 21:33:22.230: INFO: Pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00372785s
STEP: Saw pod success 04/21/23 21:33:22.23
Apr 21 21:33:22.230: INFO: Pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82" satisfied condition "Succeeded or Failed"
Apr 21 21:33:22.232: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/21/23 21:33:22.242
Apr 21 21:33:22.250: INFO: Waiting for pod pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82 to disappear
Apr 21 21:33:22.252: INFO: Pod pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 21 21:33:22.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6865" for this suite. 04/21/23 21:33:22.254
------------------------------
â€¢ [4.050 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:33:18.208
    Apr 21 21:33:18.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:33:18.209
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:18.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:18.217
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-c4b99d80-a963-436c-b8f8-f5bdabbc5ed2 04/21/23 21:33:18.219
    STEP: Creating a pod to test consume secrets 04/21/23 21:33:18.222
    Apr 21 21:33:18.226: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82" in namespace "projected-6865" to be "Succeeded or Failed"
    Apr 21 21:33:18.228: INFO: Pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82": Phase="Pending", Reason="", readiness=false. Elapsed: 1.380062ms
    Apr 21 21:33:20.231: INFO: Pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004297377s
    Apr 21 21:33:22.230: INFO: Pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00372785s
    STEP: Saw pod success 04/21/23 21:33:22.23
    Apr 21 21:33:22.230: INFO: Pod "pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82" satisfied condition "Succeeded or Failed"
    Apr 21 21:33:22.232: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 21:33:22.242
    Apr 21 21:33:22.250: INFO: Waiting for pod pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82 to disappear
    Apr 21 21:33:22.252: INFO: Pod pod-projected-secrets-5c8c0619-12d2-40db-a3c6-2c1043c4bf82 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:33:22.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6865" for this suite. 04/21/23 21:33:22.254
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:33:22.259
Apr 21 21:33:22.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:33:22.259
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:22.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:22.267
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-bb81eb72-bc4e-4f2c-a4ea-33df005e2431 04/21/23 21:33:22.269
STEP: Creating a pod to test consume secrets 04/21/23 21:33:22.271
Apr 21 21:33:22.276: INFO: Waiting up to 5m0s for pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1" in namespace "secrets-798" to be "Succeeded or Failed"
Apr 21 21:33:22.278: INFO: Pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463676ms
Apr 21 21:33:24.280: INFO: Pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004184085s
Apr 21 21:33:26.282: INFO: Pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005580241s
STEP: Saw pod success 04/21/23 21:33:26.282
Apr 21 21:33:26.282: INFO: Pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1" satisfied condition "Succeeded or Failed"
Apr 21 21:33:26.284: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1 container secret-volume-test: <nil>
STEP: delete the pod 04/21/23 21:33:26.289
Apr 21 21:33:26.295: INFO: Waiting for pod pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1 to disappear
Apr 21 21:33:26.297: INFO: Pod pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:33:26.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-798" for this suite. 04/21/23 21:33:26.299
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:33:22.259
    Apr 21 21:33:22.259: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:33:22.259
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:22.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:22.267
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-bb81eb72-bc4e-4f2c-a4ea-33df005e2431 04/21/23 21:33:22.269
    STEP: Creating a pod to test consume secrets 04/21/23 21:33:22.271
    Apr 21 21:33:22.276: INFO: Waiting up to 5m0s for pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1" in namespace "secrets-798" to be "Succeeded or Failed"
    Apr 21 21:33:22.278: INFO: Pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.463676ms
    Apr 21 21:33:24.280: INFO: Pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004184085s
    Apr 21 21:33:26.282: INFO: Pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005580241s
    STEP: Saw pod success 04/21/23 21:33:26.282
    Apr 21 21:33:26.282: INFO: Pod "pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1" satisfied condition "Succeeded or Failed"
    Apr 21 21:33:26.284: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1 container secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 21:33:26.289
    Apr 21 21:33:26.295: INFO: Waiting for pod pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1 to disappear
    Apr 21 21:33:26.297: INFO: Pod pod-secrets-85aef17a-5ad8-4b5f-9799-aa2cacb02ad1 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:33:26.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-798" for this suite. 04/21/23 21:33:26.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:33:26.303
Apr 21 21:33:26.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename deployment 04/21/23 21:33:26.304
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:26.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:26.314
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Apr 21 21:33:26.320: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 21 21:33:31.323: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/21/23 21:33:31.323
Apr 21 21:33:31.323: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 04/21/23 21:33:31.33
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 21 21:33:31.335: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-167  f2068fab-a57c-4235-9e7e-fb3d4d551dee 21523 1 2023-04-21 21:33:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-21 21:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004124b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr 21 21:33:31.338: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Apr 21 21:33:31.338: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr 21 21:33:31.338: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-167  2877119e-7b81-4b38-a341-dfb86eebcbc3 21526 1 2023-04-21 21:33:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f2068fab-a57c-4235-9e7e-fb3d4d551dee 0xc004124e57 0xc004124e58}] [] [{e2e.test Update apps/v1 2023-04-21 21:33:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:33:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-21 21:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"f2068fab-a57c-4235-9e7e-fb3d4d551dee\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004124f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 21 21:33:31.340: INFO: Pod "test-cleanup-controller-qj74b" is available:
&Pod{ObjectMeta:{test-cleanup-controller-qj74b test-cleanup-controller- deployment-167  36b1d98b-2201-4b01-832c-f1d6a02b44a3 21501 0 2023-04-21 21:33:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 2877119e-7b81-4b38-a341-dfb86eebcbc3 0xc004125237 0xc004125238}] [] [{kube-controller-manager Update v1 2023-04-21 21:33:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2877119e-7b81-4b38-a341-dfb86eebcbc3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:33:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dltqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dltqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:33:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:33:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:33:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.177,StartTime:2023-04-21 21:33:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:33:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://00b2ce03947ff7068652fada254ecf512161489770df4f1eb2d62fe788251172,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 21 21:33:31.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-167" for this suite. 04/21/23 21:33:31.344
------------------------------
â€¢ [SLOW TEST] [5.047 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:33:26.303
    Apr 21 21:33:26.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename deployment 04/21/23 21:33:26.304
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:26.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:26.314
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Apr 21 21:33:26.320: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Apr 21 21:33:31.323: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/21/23 21:33:31.323
    Apr 21 21:33:31.323: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 04/21/23 21:33:31.33
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 21 21:33:31.335: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-167  f2068fab-a57c-4235-9e7e-fb3d4d551dee 21523 1 2023-04-21 21:33:31 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-21 21:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004124b28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Apr 21 21:33:31.338: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Apr 21 21:33:31.338: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Apr 21 21:33:31.338: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-167  2877119e-7b81-4b38-a341-dfb86eebcbc3 21526 1 2023-04-21 21:33:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment f2068fab-a57c-4235-9e7e-fb3d4d551dee 0xc004124e57 0xc004124e58}] [] [{e2e.test Update apps/v1 2023-04-21 21:33:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:33:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-21 21:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"f2068fab-a57c-4235-9e7e-fb3d4d551dee\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004124f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 21:33:31.340: INFO: Pod "test-cleanup-controller-qj74b" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-qj74b test-cleanup-controller- deployment-167  36b1d98b-2201-4b01-832c-f1d6a02b44a3 21501 0 2023-04-21 21:33:26 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 2877119e-7b81-4b38-a341-dfb86eebcbc3 0xc004125237 0xc004125238}] [] [{kube-controller-manager Update v1 2023-04-21 21:33:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2877119e-7b81-4b38-a341-dfb86eebcbc3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:33:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dltqc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dltqc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:33:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:33:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:33:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:33:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.177,StartTime:2023-04-21 21:33:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:33:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://00b2ce03947ff7068652fada254ecf512161489770df4f1eb2d62fe788251172,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:33:31.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-167" for this suite. 04/21/23 21:33:31.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:33:31.351
Apr 21 21:33:31.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename subpath 04/21/23 21:33:31.352
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:31.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:31.366
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/21/23 21:33:31.368
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-jqwz 04/21/23 21:33:31.373
STEP: Creating a pod to test atomic-volume-subpath 04/21/23 21:33:31.373
Apr 21 21:33:31.377: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jqwz" in namespace "subpath-181" to be "Succeeded or Failed"
Apr 21 21:33:31.379: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.835884ms
Apr 21 21:33:33.381: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004220443s
Apr 21 21:33:35.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 4.005017082s
Apr 21 21:33:37.381: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 6.004126769s
Apr 21 21:33:39.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 8.004807959s
Apr 21 21:33:41.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 10.004868285s
Apr 21 21:33:43.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 12.004984142s
Apr 21 21:33:45.383: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 14.005254625s
Apr 21 21:33:47.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 16.00467643s
Apr 21 21:33:49.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 18.005199874s
Apr 21 21:33:51.383: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 20.005950786s
Apr 21 21:33:53.383: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=false. Elapsed: 22.0058544s
Apr 21 21:33:55.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004525083s
STEP: Saw pod success 04/21/23 21:33:55.382
Apr 21 21:33:55.382: INFO: Pod "pod-subpath-test-configmap-jqwz" satisfied condition "Succeeded or Failed"
Apr 21 21:33:55.384: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-jqwz container test-container-subpath-configmap-jqwz: <nil>
STEP: delete the pod 04/21/23 21:33:55.389
Apr 21 21:33:55.396: INFO: Waiting for pod pod-subpath-test-configmap-jqwz to disappear
Apr 21 21:33:55.397: INFO: Pod pod-subpath-test-configmap-jqwz no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jqwz 04/21/23 21:33:55.397
Apr 21 21:33:55.397: INFO: Deleting pod "pod-subpath-test-configmap-jqwz" in namespace "subpath-181"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 21 21:33:55.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-181" for this suite. 04/21/23 21:33:55.401
------------------------------
â€¢ [SLOW TEST] [24.054 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:33:31.351
    Apr 21 21:33:31.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename subpath 04/21/23 21:33:31.352
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:31.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:31.366
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/21/23 21:33:31.368
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-jqwz 04/21/23 21:33:31.373
    STEP: Creating a pod to test atomic-volume-subpath 04/21/23 21:33:31.373
    Apr 21 21:33:31.377: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jqwz" in namespace "subpath-181" to be "Succeeded or Failed"
    Apr 21 21:33:31.379: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Pending", Reason="", readiness=false. Elapsed: 1.835884ms
    Apr 21 21:33:33.381: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 2.004220443s
    Apr 21 21:33:35.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 4.005017082s
    Apr 21 21:33:37.381: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 6.004126769s
    Apr 21 21:33:39.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 8.004807959s
    Apr 21 21:33:41.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 10.004868285s
    Apr 21 21:33:43.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 12.004984142s
    Apr 21 21:33:45.383: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 14.005254625s
    Apr 21 21:33:47.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 16.00467643s
    Apr 21 21:33:49.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 18.005199874s
    Apr 21 21:33:51.383: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=true. Elapsed: 20.005950786s
    Apr 21 21:33:53.383: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Running", Reason="", readiness=false. Elapsed: 22.0058544s
    Apr 21 21:33:55.382: INFO: Pod "pod-subpath-test-configmap-jqwz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.004525083s
    STEP: Saw pod success 04/21/23 21:33:55.382
    Apr 21 21:33:55.382: INFO: Pod "pod-subpath-test-configmap-jqwz" satisfied condition "Succeeded or Failed"
    Apr 21 21:33:55.384: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-jqwz container test-container-subpath-configmap-jqwz: <nil>
    STEP: delete the pod 04/21/23 21:33:55.389
    Apr 21 21:33:55.396: INFO: Waiting for pod pod-subpath-test-configmap-jqwz to disappear
    Apr 21 21:33:55.397: INFO: Pod pod-subpath-test-configmap-jqwz no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-jqwz 04/21/23 21:33:55.397
    Apr 21 21:33:55.397: INFO: Deleting pod "pod-subpath-test-configmap-jqwz" in namespace "subpath-181"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:33:55.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-181" for this suite. 04/21/23 21:33:55.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:33:55.413
Apr 21 21:33:55.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sysctl 04/21/23 21:33:55.418
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:55.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:55.441
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 04/21/23 21:33:55.443
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:33:55.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-7709" for this suite. 04/21/23 21:33:55.448
------------------------------
â€¢ [0.037 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:33:55.413
    Apr 21 21:33:55.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sysctl 04/21/23 21:33:55.418
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:55.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:55.441
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 04/21/23 21:33:55.443
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:33:55.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-7709" for this suite. 04/21/23 21:33:55.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:33:55.451
Apr 21 21:33:55.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replicaset 04/21/23 21:33:55.452
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:55.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:55.46
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Apr 21 21:33:55.461: INFO: Creating ReplicaSet my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f
Apr 21 21:33:55.466: INFO: Pod name my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f: Found 0 pods out of 1
Apr 21 21:34:00.469: INFO: Pod name my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f: Found 1 pods out of 1
Apr 21 21:34:00.469: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f" is running
Apr 21 21:34:00.469: INFO: Waiting up to 5m0s for pod "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5" in namespace "replicaset-739" to be "running"
Apr 21 21:34:00.471: INFO: Pod "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5": Phase="Running", Reason="", readiness=true. Elapsed: 1.569285ms
Apr 21 21:34:00.471: INFO: Pod "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5" satisfied condition "running"
Apr 21 21:34:00.471: INFO: Pod "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 21:33:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 21:33:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 21:33:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 21:33:55 +0000 UTC Reason: Message:}])
Apr 21 21:34:00.471: INFO: Trying to dial the pod
Apr 21 21:34:05.477: INFO: Controller my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f: Got expected result from replica 1 [my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5]: "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:05.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-739" for this suite. 04/21/23 21:34:05.479
------------------------------
â€¢ [SLOW TEST] [10.031 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:33:55.451
    Apr 21 21:33:55.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replicaset 04/21/23 21:33:55.452
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:33:55.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:33:55.46
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Apr 21 21:33:55.461: INFO: Creating ReplicaSet my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f
    Apr 21 21:33:55.466: INFO: Pod name my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f: Found 0 pods out of 1
    Apr 21 21:34:00.469: INFO: Pod name my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f: Found 1 pods out of 1
    Apr 21 21:34:00.469: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f" is running
    Apr 21 21:34:00.469: INFO: Waiting up to 5m0s for pod "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5" in namespace "replicaset-739" to be "running"
    Apr 21 21:34:00.471: INFO: Pod "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5": Phase="Running", Reason="", readiness=true. Elapsed: 1.569285ms
    Apr 21 21:34:00.471: INFO: Pod "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5" satisfied condition "running"
    Apr 21 21:34:00.471: INFO: Pod "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 21:33:55 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 21:33:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 21:33:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-21 21:33:55 +0000 UTC Reason: Message:}])
    Apr 21 21:34:00.471: INFO: Trying to dial the pod
    Apr 21 21:34:05.477: INFO: Controller my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f: Got expected result from replica 1 [my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5]: "my-hostname-basic-dfa511c7-7557-47a2-8d1f-c78fb8e98d4f-xqhz5", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:05.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-739" for this suite. 04/21/23 21:34:05.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:05.483
Apr 21 21:34:05.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:34:05.483
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:05.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:05.493
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Apr 21 21:34:05.501: INFO: Waiting up to 5m0s for pod "pod-service-account-444dbde7-deca-452c-ae6b-72005062d319" in namespace "svcaccounts-6757" to be "running"
Apr 21 21:34:05.503: INFO: Pod "pod-service-account-444dbde7-deca-452c-ae6b-72005062d319": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38252ms
Apr 21 21:34:07.506: INFO: Pod "pod-service-account-444dbde7-deca-452c-ae6b-72005062d319": Phase="Running", Reason="", readiness=true. Elapsed: 2.004450588s
Apr 21 21:34:07.506: INFO: Pod "pod-service-account-444dbde7-deca-452c-ae6b-72005062d319" satisfied condition "running"
STEP: reading a file in the container 04/21/23 21:34:07.506
Apr 21 21:34:07.506: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6757 pod-service-account-444dbde7-deca-452c-ae6b-72005062d319 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 04/21/23 21:34:07.623
Apr 21 21:34:07.623: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6757 pod-service-account-444dbde7-deca-452c-ae6b-72005062d319 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 04/21/23 21:34:07.746
Apr 21 21:34:07.746: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6757 pod-service-account-444dbde7-deca-452c-ae6b-72005062d319 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Apr 21 21:34:07.873: INFO: Got root ca configmap in namespace "svcaccounts-6757"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:07.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6757" for this suite. 04/21/23 21:34:07.877
------------------------------
â€¢ [2.398 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:05.483
    Apr 21 21:34:05.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:34:05.483
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:05.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:05.493
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Apr 21 21:34:05.501: INFO: Waiting up to 5m0s for pod "pod-service-account-444dbde7-deca-452c-ae6b-72005062d319" in namespace "svcaccounts-6757" to be "running"
    Apr 21 21:34:05.503: INFO: Pod "pod-service-account-444dbde7-deca-452c-ae6b-72005062d319": Phase="Pending", Reason="", readiness=false. Elapsed: 1.38252ms
    Apr 21 21:34:07.506: INFO: Pod "pod-service-account-444dbde7-deca-452c-ae6b-72005062d319": Phase="Running", Reason="", readiness=true. Elapsed: 2.004450588s
    Apr 21 21:34:07.506: INFO: Pod "pod-service-account-444dbde7-deca-452c-ae6b-72005062d319" satisfied condition "running"
    STEP: reading a file in the container 04/21/23 21:34:07.506
    Apr 21 21:34:07.506: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6757 pod-service-account-444dbde7-deca-452c-ae6b-72005062d319 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 04/21/23 21:34:07.623
    Apr 21 21:34:07.623: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6757 pod-service-account-444dbde7-deca-452c-ae6b-72005062d319 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 04/21/23 21:34:07.746
    Apr 21 21:34:07.746: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6757 pod-service-account-444dbde7-deca-452c-ae6b-72005062d319 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Apr 21 21:34:07.873: INFO: Got root ca configmap in namespace "svcaccounts-6757"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:07.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6757" for this suite. 04/21/23 21:34:07.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:07.881
Apr 21 21:34:07.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 21:34:07.882
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:07.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:07.892
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 21:34:07.9
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:34:08.242
STEP: Deploying the webhook pod 04/21/23 21:34:08.246
STEP: Wait for the deployment to be ready 04/21/23 21:34:08.254
Apr 21 21:34:08.260: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 21:34:10.265
STEP: Verifying the service has paired with the endpoint 04/21/23 21:34:10.275
Apr 21 21:34:11.275: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Apr 21 21:34:11.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Registering the custom resource webhook via the AdmissionRegistration API 04/21/23 21:34:11.786
STEP: Creating a custom resource that should be denied by the webhook 04/21/23 21:34:11.798
STEP: Creating a custom resource whose deletion would be denied by the webhook 04/21/23 21:34:13.823
STEP: Updating the custom resource with disallowed data should be denied 04/21/23 21:34:13.827
STEP: Deleting the custom resource should be denied 04/21/23 21:34:13.832
STEP: Remove the offending key and value from the custom resource data 04/21/23 21:34:13.836
STEP: Deleting the updated custom resource should be successful 04/21/23 21:34:13.841
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:14.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7215" for this suite. 04/21/23 21:34:14.384
STEP: Destroying namespace "webhook-7215-markers" for this suite. 04/21/23 21:34:14.387
------------------------------
â€¢ [SLOW TEST] [6.558 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:07.881
    Apr 21 21:34:07.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 21:34:07.882
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:07.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:07.892
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 21:34:07.9
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:34:08.242
    STEP: Deploying the webhook pod 04/21/23 21:34:08.246
    STEP: Wait for the deployment to be ready 04/21/23 21:34:08.254
    Apr 21 21:34:08.260: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 21:34:10.265
    STEP: Verifying the service has paired with the endpoint 04/21/23 21:34:10.275
    Apr 21 21:34:11.275: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Apr 21 21:34:11.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 04/21/23 21:34:11.786
    STEP: Creating a custom resource that should be denied by the webhook 04/21/23 21:34:11.798
    STEP: Creating a custom resource whose deletion would be denied by the webhook 04/21/23 21:34:13.823
    STEP: Updating the custom resource with disallowed data should be denied 04/21/23 21:34:13.827
    STEP: Deleting the custom resource should be denied 04/21/23 21:34:13.832
    STEP: Remove the offending key and value from the custom resource data 04/21/23 21:34:13.836
    STEP: Deleting the updated custom resource should be successful 04/21/23 21:34:13.841
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:14.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7215" for this suite. 04/21/23 21:34:14.384
    STEP: Destroying namespace "webhook-7215-markers" for this suite. 04/21/23 21:34:14.387
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:14.439
Apr 21 21:34:14.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename dns 04/21/23 21:34:14.44
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:14.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:14.453
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 04/21/23 21:34:14.455
Apr 21 21:34:14.460: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1467  2e507a21-2d09-4df8-81fa-c8599b3c2edc 21760 0 2023-04-21 21:34:14 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-21 21:34:14 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fklxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fklxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 21 21:34:14.460: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1467" to be "running and ready"
Apr 21 21:34:14.462: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.77373ms
Apr 21 21:34:14.462: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:34:16.465: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004447634s
Apr 21 21:34:16.465: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Apr 21 21:34:16.465: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 04/21/23 21:34:16.465
Apr 21 21:34:16.465: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1467 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:34:16.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:34:16.466: INFO: ExecWithOptions: Clientset creation
Apr 21 21:34:16.466: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1467/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 04/21/23 21:34:16.533
Apr 21 21:34:16.533: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1467 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:34:16.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:34:16.534: INFO: ExecWithOptions: Clientset creation
Apr 21 21:34:16.534: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1467/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 21 21:34:16.610: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:16.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1467" for this suite. 04/21/23 21:34:16.621
------------------------------
â€¢ [2.185 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:14.439
    Apr 21 21:34:14.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename dns 04/21/23 21:34:14.44
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:14.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:14.453
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 04/21/23 21:34:14.455
    Apr 21 21:34:14.460: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1467  2e507a21-2d09-4df8-81fa-c8599b3c2edc 21760 0 2023-04-21 21:34:14 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-21 21:34:14 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fklxc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fklxc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 21 21:34:14.460: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1467" to be "running and ready"
    Apr 21 21:34:14.462: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 1.77373ms
    Apr 21 21:34:14.462: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:34:16.465: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.004447634s
    Apr 21 21:34:16.465: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Apr 21 21:34:16.465: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 04/21/23 21:34:16.465
    Apr 21 21:34:16.465: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1467 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:34:16.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:34:16.466: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:34:16.466: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1467/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 04/21/23 21:34:16.533
    Apr 21 21:34:16.533: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1467 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:34:16.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:34:16.534: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:34:16.534: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1467/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 21 21:34:16.610: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:16.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1467" for this suite. 04/21/23 21:34:16.621
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:16.624
Apr 21 21:34:16.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:34:16.625
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:16.632
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:16.634
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Apr 21 21:34:16.644: INFO: created pod pod-service-account-defaultsa
Apr 21 21:34:16.644: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 21 21:34:16.647: INFO: created pod pod-service-account-mountsa
Apr 21 21:34:16.648: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 21 21:34:16.653: INFO: created pod pod-service-account-nomountsa
Apr 21 21:34:16.653: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 21 21:34:16.656: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 21 21:34:16.656: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 21 21:34:16.662: INFO: created pod pod-service-account-mountsa-mountspec
Apr 21 21:34:16.662: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 21 21:34:16.667: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 21 21:34:16.667: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 21 21:34:16.671: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 21 21:34:16.671: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 21 21:34:16.675: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 21 21:34:16.675: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 21 21:34:16.679: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 21 21:34:16.679: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:16.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8030" for this suite. 04/21/23 21:34:16.681
------------------------------
â€¢ [0.063 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:16.624
    Apr 21 21:34:16.624: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:34:16.625
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:16.632
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:16.634
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Apr 21 21:34:16.644: INFO: created pod pod-service-account-defaultsa
    Apr 21 21:34:16.644: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Apr 21 21:34:16.647: INFO: created pod pod-service-account-mountsa
    Apr 21 21:34:16.648: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Apr 21 21:34:16.653: INFO: created pod pod-service-account-nomountsa
    Apr 21 21:34:16.653: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Apr 21 21:34:16.656: INFO: created pod pod-service-account-defaultsa-mountspec
    Apr 21 21:34:16.656: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Apr 21 21:34:16.662: INFO: created pod pod-service-account-mountsa-mountspec
    Apr 21 21:34:16.662: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Apr 21 21:34:16.667: INFO: created pod pod-service-account-nomountsa-mountspec
    Apr 21 21:34:16.667: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Apr 21 21:34:16.671: INFO: created pod pod-service-account-defaultsa-nomountspec
    Apr 21 21:34:16.671: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Apr 21 21:34:16.675: INFO: created pod pod-service-account-mountsa-nomountspec
    Apr 21 21:34:16.675: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Apr 21 21:34:16.679: INFO: created pod pod-service-account-nomountsa-nomountspec
    Apr 21 21:34:16.679: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:16.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8030" for this suite. 04/21/23 21:34:16.681
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:16.689
Apr 21 21:34:16.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:34:16.69
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:16.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:16.699
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-a6ba9bf4-c745-485f-b8e1-278818303093 04/21/23 21:34:16.702
STEP: Creating secret with name secret-projected-all-test-volume-91aa24ee-e5a8-4880-902d-6719376d5da1 04/21/23 21:34:16.704
STEP: Creating a pod to test Check all projections for projected volume plugin 04/21/23 21:34:16.711
Apr 21 21:34:16.716: INFO: Waiting up to 5m0s for pod "projected-volume-986d3068-89de-443f-9a46-47164440d193" in namespace "projected-8520" to be "Succeeded or Failed"
Apr 21 21:34:16.717: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193": Phase="Pending", Reason="", readiness=false. Elapsed: 1.70116ms
Apr 21 21:34:18.720: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004778194s
Apr 21 21:34:20.720: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004678844s
Apr 21 21:34:22.722: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005890159s
STEP: Saw pod success 04/21/23 21:34:22.722
Apr 21 21:34:22.722: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193" satisfied condition "Succeeded or Failed"
Apr 21 21:34:22.723: INFO: Trying to get logs from node k8sconformance pod projected-volume-986d3068-89de-443f-9a46-47164440d193 container projected-all-volume-test: <nil>
STEP: delete the pod 04/21/23 21:34:22.734
Apr 21 21:34:22.743: INFO: Waiting for pod projected-volume-986d3068-89de-443f-9a46-47164440d193 to disappear
Apr 21 21:34:22.744: INFO: Pod projected-volume-986d3068-89de-443f-9a46-47164440d193 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:22.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8520" for this suite. 04/21/23 21:34:22.746
------------------------------
â€¢ [SLOW TEST] [6.060 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:16.689
    Apr 21 21:34:16.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:34:16.69
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:16.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:16.699
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-a6ba9bf4-c745-485f-b8e1-278818303093 04/21/23 21:34:16.702
    STEP: Creating secret with name secret-projected-all-test-volume-91aa24ee-e5a8-4880-902d-6719376d5da1 04/21/23 21:34:16.704
    STEP: Creating a pod to test Check all projections for projected volume plugin 04/21/23 21:34:16.711
    Apr 21 21:34:16.716: INFO: Waiting up to 5m0s for pod "projected-volume-986d3068-89de-443f-9a46-47164440d193" in namespace "projected-8520" to be "Succeeded or Failed"
    Apr 21 21:34:16.717: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193": Phase="Pending", Reason="", readiness=false. Elapsed: 1.70116ms
    Apr 21 21:34:18.720: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004778194s
    Apr 21 21:34:20.720: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004678844s
    Apr 21 21:34:22.722: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005890159s
    STEP: Saw pod success 04/21/23 21:34:22.722
    Apr 21 21:34:22.722: INFO: Pod "projected-volume-986d3068-89de-443f-9a46-47164440d193" satisfied condition "Succeeded or Failed"
    Apr 21 21:34:22.723: INFO: Trying to get logs from node k8sconformance pod projected-volume-986d3068-89de-443f-9a46-47164440d193 container projected-all-volume-test: <nil>
    STEP: delete the pod 04/21/23 21:34:22.734
    Apr 21 21:34:22.743: INFO: Waiting for pod projected-volume-986d3068-89de-443f-9a46-47164440d193 to disappear
    Apr 21 21:34:22.744: INFO: Pod projected-volume-986d3068-89de-443f-9a46-47164440d193 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:22.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8520" for this suite. 04/21/23 21:34:22.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:22.749
Apr 21 21:34:22.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename limitrange 04/21/23 21:34:22.75
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:22.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:22.759
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 04/21/23 21:34:22.761
STEP: Setting up watch 04/21/23 21:34:22.761
STEP: Submitting a LimitRange 04/21/23 21:34:22.863
STEP: Verifying LimitRange creation was observed 04/21/23 21:34:22.867
STEP: Fetching the LimitRange to ensure it has proper values 04/21/23 21:34:22.867
Apr 21 21:34:22.869: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 21 21:34:22.869: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 04/21/23 21:34:22.869
STEP: Ensuring Pod has resource requirements applied from LimitRange 04/21/23 21:34:22.873
Apr 21 21:34:22.875: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 21 21:34:22.875: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 04/21/23 21:34:22.875
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 04/21/23 21:34:22.879
Apr 21 21:34:22.881: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr 21 21:34:22.881: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 04/21/23 21:34:22.881
STEP: Failing to create a Pod with more than max resources 04/21/23 21:34:22.883
STEP: Updating a LimitRange 04/21/23 21:34:22.884
STEP: Verifying LimitRange updating is effective 04/21/23 21:34:22.887
STEP: Creating a Pod with less than former min resources 04/21/23 21:34:24.889
STEP: Failing to create a Pod with more than max resources 04/21/23 21:34:24.894
STEP: Deleting a LimitRange 04/21/23 21:34:24.896
STEP: Verifying the LimitRange was deleted 04/21/23 21:34:24.899
Apr 21 21:34:29.902: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 04/21/23 21:34:29.902
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:29.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8703" for this suite. 04/21/23 21:34:29.909
------------------------------
â€¢ [SLOW TEST] [7.164 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:22.749
    Apr 21 21:34:22.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename limitrange 04/21/23 21:34:22.75
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:22.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:22.759
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 04/21/23 21:34:22.761
    STEP: Setting up watch 04/21/23 21:34:22.761
    STEP: Submitting a LimitRange 04/21/23 21:34:22.863
    STEP: Verifying LimitRange creation was observed 04/21/23 21:34:22.867
    STEP: Fetching the LimitRange to ensure it has proper values 04/21/23 21:34:22.867
    Apr 21 21:34:22.869: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Apr 21 21:34:22.869: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 04/21/23 21:34:22.869
    STEP: Ensuring Pod has resource requirements applied from LimitRange 04/21/23 21:34:22.873
    Apr 21 21:34:22.875: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Apr 21 21:34:22.875: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 04/21/23 21:34:22.875
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 04/21/23 21:34:22.879
    Apr 21 21:34:22.881: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Apr 21 21:34:22.881: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 04/21/23 21:34:22.881
    STEP: Failing to create a Pod with more than max resources 04/21/23 21:34:22.883
    STEP: Updating a LimitRange 04/21/23 21:34:22.884
    STEP: Verifying LimitRange updating is effective 04/21/23 21:34:22.887
    STEP: Creating a Pod with less than former min resources 04/21/23 21:34:24.889
    STEP: Failing to create a Pod with more than max resources 04/21/23 21:34:24.894
    STEP: Deleting a LimitRange 04/21/23 21:34:24.896
    STEP: Verifying the LimitRange was deleted 04/21/23 21:34:24.899
    Apr 21 21:34:29.902: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 04/21/23 21:34:29.902
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:29.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8703" for this suite. 04/21/23 21:34:29.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:29.914
Apr 21 21:34:29.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:34:29.915
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:29.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:29.925
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Apr 21 21:34:29.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 04/21/23 21:34:31.312
Apr 21 21:34:31.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 create -f -'
Apr 21 21:34:31.809: INFO: stderr: ""
Apr 21 21:34:31.809: INFO: stdout: "e2e-test-crd-publish-openapi-4682-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 21 21:34:31.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 delete e2e-test-crd-publish-openapi-4682-crds test-foo'
Apr 21 21:34:31.866: INFO: stderr: ""
Apr 21 21:34:31.866: INFO: stdout: "e2e-test-crd-publish-openapi-4682-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 21 21:34:31.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 apply -f -'
Apr 21 21:34:32.031: INFO: stderr: ""
Apr 21 21:34:32.031: INFO: stdout: "e2e-test-crd-publish-openapi-4682-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 21 21:34:32.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 delete e2e-test-crd-publish-openapi-4682-crds test-foo'
Apr 21 21:34:32.090: INFO: stderr: ""
Apr 21 21:34:32.090: INFO: stdout: "e2e-test-crd-publish-openapi-4682-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 04/21/23 21:34:32.09
Apr 21 21:34:32.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 create -f -'
Apr 21 21:34:32.265: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 04/21/23 21:34:32.265
Apr 21 21:34:32.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 create -f -'
Apr 21 21:34:32.406: INFO: rc: 1
Apr 21 21:34:32.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 apply -f -'
Apr 21 21:34:32.551: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 04/21/23 21:34:32.551
Apr 21 21:34:32.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 create -f -'
Apr 21 21:34:32.692: INFO: rc: 1
Apr 21 21:34:32.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 apply -f -'
Apr 21 21:34:32.834: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 04/21/23 21:34:32.834
Apr 21 21:34:32.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds'
Apr 21 21:34:32.976: INFO: stderr: ""
Apr 21 21:34:32.976: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4682-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 04/21/23 21:34:32.977
Apr 21 21:34:32.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds.metadata'
Apr 21 21:34:33.116: INFO: stderr: ""
Apr 21 21:34:33.116: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4682-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 21 21:34:33.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds.spec'
Apr 21 21:34:33.252: INFO: stderr: ""
Apr 21 21:34:33.252: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4682-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 21 21:34:33.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds.spec.bars'
Apr 21 21:34:33.391: INFO: stderr: ""
Apr 21 21:34:33.391: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4682-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 04/21/23 21:34:33.392
Apr 21 21:34:33.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds.spec.bars2'
Apr 21 21:34:33.532: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:34.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6121" for this suite. 04/21/23 21:34:34.907
------------------------------
â€¢ [4.996 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:29.914
    Apr 21 21:34:29.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename crd-publish-openapi 04/21/23 21:34:29.915
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:29.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:29.925
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Apr 21 21:34:29.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 04/21/23 21:34:31.312
    Apr 21 21:34:31.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 create -f -'
    Apr 21 21:34:31.809: INFO: stderr: ""
    Apr 21 21:34:31.809: INFO: stdout: "e2e-test-crd-publish-openapi-4682-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Apr 21 21:34:31.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 delete e2e-test-crd-publish-openapi-4682-crds test-foo'
    Apr 21 21:34:31.866: INFO: stderr: ""
    Apr 21 21:34:31.866: INFO: stdout: "e2e-test-crd-publish-openapi-4682-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Apr 21 21:34:31.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 apply -f -'
    Apr 21 21:34:32.031: INFO: stderr: ""
    Apr 21 21:34:32.031: INFO: stdout: "e2e-test-crd-publish-openapi-4682-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Apr 21 21:34:32.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 delete e2e-test-crd-publish-openapi-4682-crds test-foo'
    Apr 21 21:34:32.090: INFO: stderr: ""
    Apr 21 21:34:32.090: INFO: stdout: "e2e-test-crd-publish-openapi-4682-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 04/21/23 21:34:32.09
    Apr 21 21:34:32.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 create -f -'
    Apr 21 21:34:32.265: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 04/21/23 21:34:32.265
    Apr 21 21:34:32.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 create -f -'
    Apr 21 21:34:32.406: INFO: rc: 1
    Apr 21 21:34:32.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 apply -f -'
    Apr 21 21:34:32.551: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 04/21/23 21:34:32.551
    Apr 21 21:34:32.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 create -f -'
    Apr 21 21:34:32.692: INFO: rc: 1
    Apr 21 21:34:32.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 --namespace=crd-publish-openapi-6121 apply -f -'
    Apr 21 21:34:32.834: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 04/21/23 21:34:32.834
    Apr 21 21:34:32.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds'
    Apr 21 21:34:32.976: INFO: stderr: ""
    Apr 21 21:34:32.976: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4682-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 04/21/23 21:34:32.977
    Apr 21 21:34:32.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds.metadata'
    Apr 21 21:34:33.116: INFO: stderr: ""
    Apr 21 21:34:33.116: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4682-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Apr 21 21:34:33.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds.spec'
    Apr 21 21:34:33.252: INFO: stderr: ""
    Apr 21 21:34:33.252: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4682-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Apr 21 21:34:33.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds.spec.bars'
    Apr 21 21:34:33.391: INFO: stderr: ""
    Apr 21 21:34:33.391: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4682-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 04/21/23 21:34:33.392
    Apr 21 21:34:33.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=crd-publish-openapi-6121 explain e2e-test-crd-publish-openapi-4682-crds.spec.bars2'
    Apr 21 21:34:33.532: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:34.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6121" for this suite. 04/21/23 21:34:34.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:34.913
Apr 21 21:34:34.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 21:34:34.913
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:34.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:34.926
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 04/21/23 21:34:34.928
STEP: submitting the pod to kubernetes 04/21/23 21:34:34.928
STEP: verifying QOS class is set on the pod 04/21/23 21:34:34.935
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:34.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4817" for this suite. 04/21/23 21:34:34.939
------------------------------
â€¢ [0.031 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:34.913
    Apr 21 21:34:34.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 21:34:34.913
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:34.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:34.926
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 04/21/23 21:34:34.928
    STEP: submitting the pod to kubernetes 04/21/23 21:34:34.928
    STEP: verifying QOS class is set on the pod 04/21/23 21:34:34.935
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:34.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4817" for this suite. 04/21/23 21:34:34.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:34.944
Apr 21 21:34:34.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:34:34.945
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:34.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:34.955
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-4274435b-0d4a-4fea-ac27-4a398791309b 04/21/23 21:34:34.957
STEP: Creating a pod to test consume secrets 04/21/23 21:34:34.96
Apr 21 21:34:34.964: INFO: Waiting up to 5m0s for pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7" in namespace "secrets-7465" to be "Succeeded or Failed"
Apr 21 21:34:34.967: INFO: Pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.365429ms
Apr 21 21:34:36.969: INFO: Pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005410047s
Apr 21 21:34:38.971: INFO: Pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006816225s
STEP: Saw pod success 04/21/23 21:34:38.971
Apr 21 21:34:38.971: INFO: Pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7" satisfied condition "Succeeded or Failed"
Apr 21 21:34:38.972: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7 container secret-volume-test: <nil>
STEP: delete the pod 04/21/23 21:34:38.977
Apr 21 21:34:38.985: INFO: Waiting for pod pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7 to disappear
Apr 21 21:34:38.986: INFO: Pod pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:38.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7465" for this suite. 04/21/23 21:34:38.988
------------------------------
â€¢ [4.047 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:34.944
    Apr 21 21:34:34.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:34:34.945
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:34.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:34.955
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-4274435b-0d4a-4fea-ac27-4a398791309b 04/21/23 21:34:34.957
    STEP: Creating a pod to test consume secrets 04/21/23 21:34:34.96
    Apr 21 21:34:34.964: INFO: Waiting up to 5m0s for pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7" in namespace "secrets-7465" to be "Succeeded or Failed"
    Apr 21 21:34:34.967: INFO: Pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.365429ms
    Apr 21 21:34:36.969: INFO: Pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005410047s
    Apr 21 21:34:38.971: INFO: Pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006816225s
    STEP: Saw pod success 04/21/23 21:34:38.971
    Apr 21 21:34:38.971: INFO: Pod "pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7" satisfied condition "Succeeded or Failed"
    Apr 21 21:34:38.972: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7 container secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 21:34:38.977
    Apr 21 21:34:38.985: INFO: Waiting for pod pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7 to disappear
    Apr 21 21:34:38.986: INFO: Pod pod-secrets-5e590639-fc05-45d2-9d94-ee6be7b6f1e7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:38.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7465" for this suite. 04/21/23 21:34:38.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:38.992
Apr 21 21:34:38.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename lease-test 04/21/23 21:34:38.993
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:38.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:39.001
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:39.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-6333" for this suite. 04/21/23 21:34:39.029
------------------------------
â€¢ [0.040 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:38.992
    Apr 21 21:34:38.992: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename lease-test 04/21/23 21:34:38.993
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:38.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:39.001
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:39.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-6333" for this suite. 04/21/23 21:34:39.029
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:39.033
Apr 21 21:34:39.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:34:39.034
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:39.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:39.042
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Apr 21 21:34:39.045: INFO: Got root ca configmap in namespace "svcaccounts-1447"
Apr 21 21:34:39.048: INFO: Deleted root ca configmap in namespace "svcaccounts-1447"
STEP: waiting for a new root ca configmap created 04/21/23 21:34:39.548
Apr 21 21:34:39.551: INFO: Recreated root ca configmap in namespace "svcaccounts-1447"
Apr 21 21:34:39.554: INFO: Updated root ca configmap in namespace "svcaccounts-1447"
STEP: waiting for the root ca configmap reconciled 04/21/23 21:34:40.055
Apr 21 21:34:40.057: INFO: Reconciled root ca configmap in namespace "svcaccounts-1447"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:40.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1447" for this suite. 04/21/23 21:34:40.059
------------------------------
â€¢ [1.029 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:39.033
    Apr 21 21:34:39.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:34:39.034
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:39.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:39.042
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Apr 21 21:34:39.045: INFO: Got root ca configmap in namespace "svcaccounts-1447"
    Apr 21 21:34:39.048: INFO: Deleted root ca configmap in namespace "svcaccounts-1447"
    STEP: waiting for a new root ca configmap created 04/21/23 21:34:39.548
    Apr 21 21:34:39.551: INFO: Recreated root ca configmap in namespace "svcaccounts-1447"
    Apr 21 21:34:39.554: INFO: Updated root ca configmap in namespace "svcaccounts-1447"
    STEP: waiting for the root ca configmap reconciled 04/21/23 21:34:40.055
    Apr 21 21:34:40.057: INFO: Reconciled root ca configmap in namespace "svcaccounts-1447"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:40.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1447" for this suite. 04/21/23 21:34:40.059
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:40.063
Apr 21 21:34:40.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:34:40.064
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:40.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:40.072
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 04/21/23 21:34:40.074
Apr 21 21:34:40.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-7336 api-versions'
Apr 21 21:34:40.142: INFO: stderr: ""
Apr 21 21:34:40.142: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:40.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7336" for this suite. 04/21/23 21:34:40.144
------------------------------
â€¢ [0.084 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:40.063
    Apr 21 21:34:40.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:34:40.064
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:40.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:40.072
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 04/21/23 21:34:40.074
    Apr 21 21:34:40.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-7336 api-versions'
    Apr 21 21:34:40.142: INFO: stderr: ""
    Apr 21 21:34:40.142: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:40.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7336" for this suite. 04/21/23 21:34:40.144
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:40.147
Apr 21 21:34:40.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 21:34:40.148
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:40.157
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:40.159
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 04/21/23 21:34:40.161
Apr 21 21:34:40.166: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164" in namespace "downward-api-5335" to be "Succeeded or Failed"
Apr 21 21:34:40.167: INFO: Pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164": Phase="Pending", Reason="", readiness=false. Elapsed: 1.436774ms
Apr 21 21:34:42.170: INFO: Pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004354055s
Apr 21 21:34:44.171: INFO: Pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004932827s
STEP: Saw pod success 04/21/23 21:34:44.171
Apr 21 21:34:44.171: INFO: Pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164" satisfied condition "Succeeded or Failed"
Apr 21 21:34:44.172: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164 container client-container: <nil>
STEP: delete the pod 04/21/23 21:34:44.177
Apr 21 21:34:44.183: INFO: Waiting for pod downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164 to disappear
Apr 21 21:34:44.184: INFO: Pod downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:44.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5335" for this suite. 04/21/23 21:34:44.186
------------------------------
â€¢ [4.042 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:40.147
    Apr 21 21:34:40.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 21:34:40.148
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:40.157
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:40.159
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 04/21/23 21:34:40.161
    Apr 21 21:34:40.166: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164" in namespace "downward-api-5335" to be "Succeeded or Failed"
    Apr 21 21:34:40.167: INFO: Pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164": Phase="Pending", Reason="", readiness=false. Elapsed: 1.436774ms
    Apr 21 21:34:42.170: INFO: Pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004354055s
    Apr 21 21:34:44.171: INFO: Pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004932827s
    STEP: Saw pod success 04/21/23 21:34:44.171
    Apr 21 21:34:44.171: INFO: Pod "downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164" satisfied condition "Succeeded or Failed"
    Apr 21 21:34:44.172: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164 container client-container: <nil>
    STEP: delete the pod 04/21/23 21:34:44.177
    Apr 21 21:34:44.183: INFO: Waiting for pod downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164 to disappear
    Apr 21 21:34:44.184: INFO: Pod downwardapi-volume-aca8e1db-f619-4771-a9a1-7af695f90164 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:44.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5335" for this suite. 04/21/23 21:34:44.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:44.189
Apr 21 21:34:44.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 21:34:44.19
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:44.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:44.199
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-7217 04/21/23 21:34:44.2
STEP: creating service affinity-clusterip in namespace services-7217 04/21/23 21:34:44.2
STEP: creating replication controller affinity-clusterip in namespace services-7217 04/21/23 21:34:44.208
I0421 21:34:44.213854      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7217, replica count: 3
I0421 21:34:47.265036      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 21 21:34:47.268: INFO: Creating new exec pod
Apr 21 21:34:47.271: INFO: Waiting up to 5m0s for pod "execpod-affinitylt8zx" in namespace "services-7217" to be "running"
Apr 21 21:34:47.273: INFO: Pod "execpod-affinitylt8zx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.576057ms
Apr 21 21:34:49.335: INFO: Pod "execpod-affinitylt8zx": Phase="Running", Reason="", readiness=true. Elapsed: 2.063160471s
Apr 21 21:34:49.335: INFO: Pod "execpod-affinitylt8zx" satisfied condition "running"
Apr 21 21:34:50.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-7217 exec execpod-affinitylt8zx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Apr 21 21:34:50.452: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr 21 21:34:50.452: INFO: stdout: ""
Apr 21 21:34:50.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-7217 exec execpod-affinitylt8zx -- /bin/sh -x -c nc -v -z -w 2 10.107.11.114 80'
Apr 21 21:34:50.575: INFO: stderr: "+ nc -v -z -w 2 10.107.11.114 80\nConnection to 10.107.11.114 80 port [tcp/http] succeeded!\n"
Apr 21 21:34:50.575: INFO: stdout: ""
Apr 21 21:34:50.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-7217 exec execpod-affinitylt8zx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.11.114:80/ ; done'
Apr 21 21:34:50.741: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n"
Apr 21 21:34:50.741: INFO: stdout: "\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv"
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
Apr 21 21:34:50.741: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-7217, will wait for the garbage collector to delete the pods 04/21/23 21:34:50.75
Apr 21 21:34:50.806: INFO: Deleting ReplicationController affinity-clusterip took: 4.221002ms
Apr 21 21:34:50.907: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.597688ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:52.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7217" for this suite. 04/21/23 21:34:52.419
------------------------------
â€¢ [SLOW TEST] [8.233 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:44.189
    Apr 21 21:34:44.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 21:34:44.19
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:44.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:44.199
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-7217 04/21/23 21:34:44.2
    STEP: creating service affinity-clusterip in namespace services-7217 04/21/23 21:34:44.2
    STEP: creating replication controller affinity-clusterip in namespace services-7217 04/21/23 21:34:44.208
    I0421 21:34:44.213854      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-7217, replica count: 3
    I0421 21:34:47.265036      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 21 21:34:47.268: INFO: Creating new exec pod
    Apr 21 21:34:47.271: INFO: Waiting up to 5m0s for pod "execpod-affinitylt8zx" in namespace "services-7217" to be "running"
    Apr 21 21:34:47.273: INFO: Pod "execpod-affinitylt8zx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.576057ms
    Apr 21 21:34:49.335: INFO: Pod "execpod-affinitylt8zx": Phase="Running", Reason="", readiness=true. Elapsed: 2.063160471s
    Apr 21 21:34:49.335: INFO: Pod "execpod-affinitylt8zx" satisfied condition "running"
    Apr 21 21:34:50.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-7217 exec execpod-affinitylt8zx -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Apr 21 21:34:50.452: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Apr 21 21:34:50.452: INFO: stdout: ""
    Apr 21 21:34:50.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-7217 exec execpod-affinitylt8zx -- /bin/sh -x -c nc -v -z -w 2 10.107.11.114 80'
    Apr 21 21:34:50.575: INFO: stderr: "+ nc -v -z -w 2 10.107.11.114 80\nConnection to 10.107.11.114 80 port [tcp/http] succeeded!\n"
    Apr 21 21:34:50.575: INFO: stdout: ""
    Apr 21 21:34:50.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-7217 exec execpod-affinitylt8zx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.11.114:80/ ; done'
    Apr 21 21:34:50.741: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.11.114:80/\n"
    Apr 21 21:34:50.741: INFO: stdout: "\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv\naffinity-clusterip-rhcbv"
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Received response from host: affinity-clusterip-rhcbv
    Apr 21 21:34:50.741: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-7217, will wait for the garbage collector to delete the pods 04/21/23 21:34:50.75
    Apr 21 21:34:50.806: INFO: Deleting ReplicationController affinity-clusterip took: 4.221002ms
    Apr 21 21:34:50.907: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.597688ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:52.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7217" for this suite. 04/21/23 21:34:52.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:52.423
Apr 21 21:34:52.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename controllerrevisions 04/21/23 21:34:52.423
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:52.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:52.433
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-rhgdn-daemon-set" 04/21/23 21:34:52.442
STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:34:52.446
Apr 21 21:34:52.450: INFO: Number of nodes with available pods controlled by daemonset e2e-rhgdn-daemon-set: 0
Apr 21 21:34:52.450: INFO: Node k8sconformance is running 0 daemon pod, expected 1
Apr 21 21:34:53.453: INFO: Number of nodes with available pods controlled by daemonset e2e-rhgdn-daemon-set: 1
Apr 21 21:34:53.453: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:34:54.454: INFO: Number of nodes with available pods controlled by daemonset e2e-rhgdn-daemon-set: 2
Apr 21 21:34:54.454: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-rhgdn-daemon-set
STEP: Confirm DaemonSet "e2e-rhgdn-daemon-set" successfully created with "daemonset-name=e2e-rhgdn-daemon-set" label 04/21/23 21:34:54.456
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-rhgdn-daemon-set" 04/21/23 21:34:54.459
Apr 21 21:34:54.461: INFO: Located ControllerRevision: "e2e-rhgdn-daemon-set-99594d946"
STEP: Patching ControllerRevision "e2e-rhgdn-daemon-set-99594d946" 04/21/23 21:34:54.462
Apr 21 21:34:54.466: INFO: e2e-rhgdn-daemon-set-99594d946 has been patched
STEP: Create a new ControllerRevision 04/21/23 21:34:54.466
Apr 21 21:34:54.469: INFO: Created ControllerRevision: e2e-rhgdn-daemon-set-65bd8b4c44
STEP: Confirm that there are two ControllerRevisions 04/21/23 21:34:54.469
Apr 21 21:34:54.469: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 21 21:34:54.471: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-rhgdn-daemon-set-99594d946" 04/21/23 21:34:54.471
STEP: Confirm that there is only one ControllerRevision 04/21/23 21:34:54.475
Apr 21 21:34:54.475: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 21 21:34:54.476: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-rhgdn-daemon-set-65bd8b4c44" 04/21/23 21:34:54.477
Apr 21 21:34:54.481: INFO: e2e-rhgdn-daemon-set-65bd8b4c44 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 04/21/23 21:34:54.481
W0421 21:34:54.485806      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 04/21/23 21:34:54.485
Apr 21 21:34:54.485: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 21 21:34:55.487: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 21 21:34:55.490: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-rhgdn-daemon-set-65bd8b4c44=updated" 04/21/23 21:34:55.49
STEP: Confirm that there is only one ControllerRevision 04/21/23 21:34:55.494
Apr 21 21:34:55.494: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 21 21:34:55.496: INFO: Found 1 ControllerRevisions
Apr 21 21:34:55.498: INFO: ControllerRevision "e2e-rhgdn-daemon-set-6db9484889" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-rhgdn-daemon-set" 04/21/23 21:34:55.499
STEP: deleting DaemonSet.extensions e2e-rhgdn-daemon-set in namespace controllerrevisions-3684, will wait for the garbage collector to delete the pods 04/21/23 21:34:55.499
Apr 21 21:34:55.556: INFO: Deleting DaemonSet.extensions e2e-rhgdn-daemon-set took: 4.168078ms
Apr 21 21:34:55.657: INFO: Terminating DaemonSet.extensions e2e-rhgdn-daemon-set pods took: 100.892034ms
Apr 21 21:34:57.358: INFO: Number of nodes with available pods controlled by daemonset e2e-rhgdn-daemon-set: 0
Apr 21 21:34:57.358: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-rhgdn-daemon-set
Apr 21 21:34:57.360: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22265"},"items":null}

Apr 21 21:34:57.361: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22265"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:34:57.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-3684" for this suite. 04/21/23 21:34:57.368
------------------------------
â€¢ [4.948 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:52.423
    Apr 21 21:34:52.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename controllerrevisions 04/21/23 21:34:52.423
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:52.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:52.433
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-rhgdn-daemon-set" 04/21/23 21:34:52.442
    STEP: Check that daemon pods launch on every node of the cluster. 04/21/23 21:34:52.446
    Apr 21 21:34:52.450: INFO: Number of nodes with available pods controlled by daemonset e2e-rhgdn-daemon-set: 0
    Apr 21 21:34:52.450: INFO: Node k8sconformance is running 0 daemon pod, expected 1
    Apr 21 21:34:53.453: INFO: Number of nodes with available pods controlled by daemonset e2e-rhgdn-daemon-set: 1
    Apr 21 21:34:53.453: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:34:54.454: INFO: Number of nodes with available pods controlled by daemonset e2e-rhgdn-daemon-set: 2
    Apr 21 21:34:54.454: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-rhgdn-daemon-set
    STEP: Confirm DaemonSet "e2e-rhgdn-daemon-set" successfully created with "daemonset-name=e2e-rhgdn-daemon-set" label 04/21/23 21:34:54.456
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-rhgdn-daemon-set" 04/21/23 21:34:54.459
    Apr 21 21:34:54.461: INFO: Located ControllerRevision: "e2e-rhgdn-daemon-set-99594d946"
    STEP: Patching ControllerRevision "e2e-rhgdn-daemon-set-99594d946" 04/21/23 21:34:54.462
    Apr 21 21:34:54.466: INFO: e2e-rhgdn-daemon-set-99594d946 has been patched
    STEP: Create a new ControllerRevision 04/21/23 21:34:54.466
    Apr 21 21:34:54.469: INFO: Created ControllerRevision: e2e-rhgdn-daemon-set-65bd8b4c44
    STEP: Confirm that there are two ControllerRevisions 04/21/23 21:34:54.469
    Apr 21 21:34:54.469: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 21 21:34:54.471: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-rhgdn-daemon-set-99594d946" 04/21/23 21:34:54.471
    STEP: Confirm that there is only one ControllerRevision 04/21/23 21:34:54.475
    Apr 21 21:34:54.475: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 21 21:34:54.476: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-rhgdn-daemon-set-65bd8b4c44" 04/21/23 21:34:54.477
    Apr 21 21:34:54.481: INFO: e2e-rhgdn-daemon-set-65bd8b4c44 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 04/21/23 21:34:54.481
    W0421 21:34:54.485806      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 04/21/23 21:34:54.485
    Apr 21 21:34:54.485: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 21 21:34:55.487: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 21 21:34:55.490: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-rhgdn-daemon-set-65bd8b4c44=updated" 04/21/23 21:34:55.49
    STEP: Confirm that there is only one ControllerRevision 04/21/23 21:34:55.494
    Apr 21 21:34:55.494: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 21 21:34:55.496: INFO: Found 1 ControllerRevisions
    Apr 21 21:34:55.498: INFO: ControllerRevision "e2e-rhgdn-daemon-set-6db9484889" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-rhgdn-daemon-set" 04/21/23 21:34:55.499
    STEP: deleting DaemonSet.extensions e2e-rhgdn-daemon-set in namespace controllerrevisions-3684, will wait for the garbage collector to delete the pods 04/21/23 21:34:55.499
    Apr 21 21:34:55.556: INFO: Deleting DaemonSet.extensions e2e-rhgdn-daemon-set took: 4.168078ms
    Apr 21 21:34:55.657: INFO: Terminating DaemonSet.extensions e2e-rhgdn-daemon-set pods took: 100.892034ms
    Apr 21 21:34:57.358: INFO: Number of nodes with available pods controlled by daemonset e2e-rhgdn-daemon-set: 0
    Apr 21 21:34:57.358: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-rhgdn-daemon-set
    Apr 21 21:34:57.360: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22265"},"items":null}

    Apr 21 21:34:57.361: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22265"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:34:57.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-3684" for this suite. 04/21/23 21:34:57.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:34:57.372
Apr 21 21:34:57.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 21:34:57.372
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:57.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:57.382
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 04/21/23 21:34:57.384
Apr 21 21:34:57.390: INFO: Waiting up to 5m0s for pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4" in namespace "downward-api-9536" to be "Succeeded or Failed"
Apr 21 21:34:57.392: INFO: Pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.493281ms
Apr 21 21:34:59.395: INFO: Pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004982686s
Apr 21 21:35:01.394: INFO: Pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004270968s
STEP: Saw pod success 04/21/23 21:35:01.394
Apr 21 21:35:01.394: INFO: Pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4" satisfied condition "Succeeded or Failed"
Apr 21 21:35:01.396: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4 container dapi-container: <nil>
STEP: delete the pod 04/21/23 21:35:01.401
Apr 21 21:35:01.409: INFO: Waiting for pod downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4 to disappear
Apr 21 21:35:01.411: INFO: Pod downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 21 21:35:01.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9536" for this suite. 04/21/23 21:35:01.413
------------------------------
â€¢ [4.044 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:34:57.372
    Apr 21 21:34:57.372: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 21:34:57.372
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:34:57.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:34:57.382
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 04/21/23 21:34:57.384
    Apr 21 21:34:57.390: INFO: Waiting up to 5m0s for pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4" in namespace "downward-api-9536" to be "Succeeded or Failed"
    Apr 21 21:34:57.392: INFO: Pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.493281ms
    Apr 21 21:34:59.395: INFO: Pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004982686s
    Apr 21 21:35:01.394: INFO: Pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004270968s
    STEP: Saw pod success 04/21/23 21:35:01.394
    Apr 21 21:35:01.394: INFO: Pod "downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4" satisfied condition "Succeeded or Failed"
    Apr 21 21:35:01.396: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4 container dapi-container: <nil>
    STEP: delete the pod 04/21/23 21:35:01.401
    Apr 21 21:35:01.409: INFO: Waiting for pod downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4 to disappear
    Apr 21 21:35:01.411: INFO: Pod downward-api-349a367c-f8c9-4ccd-8fd6-2ca3eb0585d4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:35:01.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9536" for this suite. 04/21/23 21:35:01.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:35:01.416
Apr 21 21:35:01.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename subpath 04/21/23 21:35:01.417
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:01.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:01.426
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/21/23 21:35:01.428
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-rk46 04/21/23 21:35:01.432
STEP: Creating a pod to test atomic-volume-subpath 04/21/23 21:35:01.432
Apr 21 21:35:01.438: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rk46" in namespace "subpath-5083" to be "Succeeded or Failed"
Apr 21 21:35:01.439: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505015ms
Apr 21 21:35:03.441: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 2.003634327s
Apr 21 21:35:05.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 4.004659764s
Apr 21 21:35:07.441: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 6.003865122s
Apr 21 21:35:09.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 8.004277786s
Apr 21 21:35:11.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 10.004180164s
Apr 21 21:35:13.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 12.00449717s
Apr 21 21:35:15.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 14.003975035s
Apr 21 21:35:17.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 16.003944538s
Apr 21 21:35:19.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 18.004641899s
Apr 21 21:35:21.443: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 20.004957463s
Apr 21 21:35:23.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=false. Elapsed: 22.004798999s
Apr 21 21:35:25.441: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003685232s
STEP: Saw pod success 04/21/23 21:35:25.441
Apr 21 21:35:25.441: INFO: Pod "pod-subpath-test-downwardapi-rk46" satisfied condition "Succeeded or Failed"
Apr 21 21:35:25.443: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-downwardapi-rk46 container test-container-subpath-downwardapi-rk46: <nil>
STEP: delete the pod 04/21/23 21:35:25.449
Apr 21 21:35:25.457: INFO: Waiting for pod pod-subpath-test-downwardapi-rk46 to disappear
Apr 21 21:35:25.458: INFO: Pod pod-subpath-test-downwardapi-rk46 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rk46 04/21/23 21:35:25.458
Apr 21 21:35:25.458: INFO: Deleting pod "pod-subpath-test-downwardapi-rk46" in namespace "subpath-5083"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 21 21:35:25.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5083" for this suite. 04/21/23 21:35:25.462
------------------------------
â€¢ [SLOW TEST] [24.049 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:35:01.416
    Apr 21 21:35:01.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename subpath 04/21/23 21:35:01.417
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:01.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:01.426
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/21/23 21:35:01.428
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-rk46 04/21/23 21:35:01.432
    STEP: Creating a pod to test atomic-volume-subpath 04/21/23 21:35:01.432
    Apr 21 21:35:01.438: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rk46" in namespace "subpath-5083" to be "Succeeded or Failed"
    Apr 21 21:35:01.439: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505015ms
    Apr 21 21:35:03.441: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 2.003634327s
    Apr 21 21:35:05.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 4.004659764s
    Apr 21 21:35:07.441: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 6.003865122s
    Apr 21 21:35:09.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 8.004277786s
    Apr 21 21:35:11.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 10.004180164s
    Apr 21 21:35:13.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 12.00449717s
    Apr 21 21:35:15.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 14.003975035s
    Apr 21 21:35:17.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 16.003944538s
    Apr 21 21:35:19.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 18.004641899s
    Apr 21 21:35:21.443: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=true. Elapsed: 20.004957463s
    Apr 21 21:35:23.442: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Running", Reason="", readiness=false. Elapsed: 22.004798999s
    Apr 21 21:35:25.441: INFO: Pod "pod-subpath-test-downwardapi-rk46": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.003685232s
    STEP: Saw pod success 04/21/23 21:35:25.441
    Apr 21 21:35:25.441: INFO: Pod "pod-subpath-test-downwardapi-rk46" satisfied condition "Succeeded or Failed"
    Apr 21 21:35:25.443: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-downwardapi-rk46 container test-container-subpath-downwardapi-rk46: <nil>
    STEP: delete the pod 04/21/23 21:35:25.449
    Apr 21 21:35:25.457: INFO: Waiting for pod pod-subpath-test-downwardapi-rk46 to disappear
    Apr 21 21:35:25.458: INFO: Pod pod-subpath-test-downwardapi-rk46 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-rk46 04/21/23 21:35:25.458
    Apr 21 21:35:25.458: INFO: Deleting pod "pod-subpath-test-downwardapi-rk46" in namespace "subpath-5083"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:35:25.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5083" for this suite. 04/21/23 21:35:25.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:35:25.465
Apr 21 21:35:25.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:35:25.466
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:25.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:25.474
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 04/21/23 21:35:25.476
Apr 21 21:35:25.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0" in namespace "projected-8216" to be "Succeeded or Failed"
Apr 21 21:35:25.482: INFO: Pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.504521ms
Apr 21 21:35:27.485: INFO: Pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004692674s
Apr 21 21:35:29.485: INFO: Pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004176035s
STEP: Saw pod success 04/21/23 21:35:29.485
Apr 21 21:35:29.485: INFO: Pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0" satisfied condition "Succeeded or Failed"
Apr 21 21:35:29.487: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0 container client-container: <nil>
STEP: delete the pod 04/21/23 21:35:29.492
Apr 21 21:35:29.500: INFO: Waiting for pod downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0 to disappear
Apr 21 21:35:29.501: INFO: Pod downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 21 21:35:29.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8216" for this suite. 04/21/23 21:35:29.503
------------------------------
â€¢ [4.040 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:35:25.465
    Apr 21 21:35:25.465: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:35:25.466
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:25.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:25.474
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 04/21/23 21:35:25.476
    Apr 21 21:35:25.481: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0" in namespace "projected-8216" to be "Succeeded or Failed"
    Apr 21 21:35:25.482: INFO: Pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.504521ms
    Apr 21 21:35:27.485: INFO: Pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004692674s
    Apr 21 21:35:29.485: INFO: Pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004176035s
    STEP: Saw pod success 04/21/23 21:35:29.485
    Apr 21 21:35:29.485: INFO: Pod "downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0" satisfied condition "Succeeded or Failed"
    Apr 21 21:35:29.487: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0 container client-container: <nil>
    STEP: delete the pod 04/21/23 21:35:29.492
    Apr 21 21:35:29.500: INFO: Waiting for pod downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0 to disappear
    Apr 21 21:35:29.501: INFO: Pod downwardapi-volume-5be326f7-ce0d-4ac4-b40a-1c77204cbee0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:35:29.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8216" for this suite. 04/21/23 21:35:29.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:35:29.506
Apr 21 21:35:29.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename job 04/21/23 21:35:29.507
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:29.514
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:29.516
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 04/21/23 21:35:29.519
STEP: Patching the Job 04/21/23 21:35:29.522
STEP: Watching for Job to be patched 04/21/23 21:35:29.534
Apr 21 21:35:29.535: INFO: Event ADDED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj] and annotations: map[batch.kubernetes.io/job-tracking:]
Apr 21 21:35:29.535: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj] and annotations: map[batch.kubernetes.io/job-tracking:]
Apr 21 21:35:29.535: INFO: Event MODIFIED found for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 04/21/23 21:35:29.535
STEP: Watching for Job to be updated 04/21/23 21:35:29.54
Apr 21 21:35:29.541: INFO: Event MODIFIED found for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 21 21:35:29.541: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 04/21/23 21:35:29.541
Apr 21 21:35:29.542: INFO: Job: e2e-nrdxj as labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched]
STEP: Waiting for job to complete 04/21/23 21:35:29.542
STEP: Delete a job collection with a labelselector 04/21/23 21:35:39.547
STEP: Watching for Job to be deleted 04/21/23 21:35:39.552
Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 21 21:35:39.553: INFO: Event DELETED found for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 04/21/23 21:35:39.553
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 21 21:35:39.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4723" for this suite. 04/21/23 21:35:39.559
------------------------------
â€¢ [SLOW TEST] [10.056 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:35:29.506
    Apr 21 21:35:29.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename job 04/21/23 21:35:29.507
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:29.514
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:29.516
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 04/21/23 21:35:29.519
    STEP: Patching the Job 04/21/23 21:35:29.522
    STEP: Watching for Job to be patched 04/21/23 21:35:29.534
    Apr 21 21:35:29.535: INFO: Event ADDED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj] and annotations: map[batch.kubernetes.io/job-tracking:]
    Apr 21 21:35:29.535: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj] and annotations: map[batch.kubernetes.io/job-tracking:]
    Apr 21 21:35:29.535: INFO: Event MODIFIED found for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 04/21/23 21:35:29.535
    STEP: Watching for Job to be updated 04/21/23 21:35:29.54
    Apr 21 21:35:29.541: INFO: Event MODIFIED found for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 21 21:35:29.541: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 04/21/23 21:35:29.541
    Apr 21 21:35:29.542: INFO: Job: e2e-nrdxj as labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched]
    STEP: Waiting for job to complete 04/21/23 21:35:29.542
    STEP: Delete a job collection with a labelselector 04/21/23 21:35:39.547
    STEP: Watching for Job to be deleted 04/21/23 21:35:39.552
    Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 21 21:35:39.553: INFO: Event MODIFIED observed for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 21 21:35:39.553: INFO: Event DELETED found for Job e2e-nrdxj in namespace job-4723 with labels: map[e2e-job-label:e2e-nrdxj e2e-nrdxj:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 04/21/23 21:35:39.553
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:35:39.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4723" for this suite. 04/21/23 21:35:39.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:35:39.563
Apr 21 21:35:39.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir-wrapper 04/21/23 21:35:39.564
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:39.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:39.575
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Apr 21 21:35:39.586: INFO: Waiting up to 5m0s for pod "pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115" in namespace "emptydir-wrapper-1596" to be "running and ready"
Apr 21 21:35:39.587: INFO: Pod "pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46422ms
Apr 21 21:35:39.587: INFO: The phase of Pod pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:35:41.590: INFO: Pod "pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115": Phase="Running", Reason="", readiness=true. Elapsed: 2.003983027s
Apr 21 21:35:41.590: INFO: The phase of Pod pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115 is Running (Ready = true)
Apr 21 21:35:41.590: INFO: Pod "pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115" satisfied condition "running and ready"
STEP: Cleaning up the secret 04/21/23 21:35:41.592
STEP: Cleaning up the configmap 04/21/23 21:35:41.595
STEP: Cleaning up the pod 04/21/23 21:35:41.6
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:35:41.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1596" for this suite. 04/21/23 21:35:41.606
------------------------------
â€¢ [2.048 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:35:39.563
    Apr 21 21:35:39.563: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir-wrapper 04/21/23 21:35:39.564
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:39.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:39.575
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Apr 21 21:35:39.586: INFO: Waiting up to 5m0s for pod "pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115" in namespace "emptydir-wrapper-1596" to be "running and ready"
    Apr 21 21:35:39.587: INFO: Pod "pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115": Phase="Pending", Reason="", readiness=false. Elapsed: 1.46422ms
    Apr 21 21:35:39.587: INFO: The phase of Pod pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:35:41.590: INFO: Pod "pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115": Phase="Running", Reason="", readiness=true. Elapsed: 2.003983027s
    Apr 21 21:35:41.590: INFO: The phase of Pod pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115 is Running (Ready = true)
    Apr 21 21:35:41.590: INFO: Pod "pod-secrets-34d1e087-a2ac-4699-a928-51ef7db78115" satisfied condition "running and ready"
    STEP: Cleaning up the secret 04/21/23 21:35:41.592
    STEP: Cleaning up the configmap 04/21/23 21:35:41.595
    STEP: Cleaning up the pod 04/21/23 21:35:41.6
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:35:41.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1596" for this suite. 04/21/23 21:35:41.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:35:41.612
Apr 21 21:35:41.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename statefulset 04/21/23 21:35:41.612
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:41.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:41.623
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-9516 04/21/23 21:35:41.624
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 04/21/23 21:35:41.628
Apr 21 21:35:41.633: INFO: Found 0 stateful pods, waiting for 3
Apr 21 21:35:51.636: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 21:35:51.636: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 21:35:51.636: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 21:35:51.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9516 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 21:35:51.770: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 21:35:51.770: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 21:35:51.770: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/21/23 21:36:01.78
Apr 21 21:36:01.796: INFO: Updating stateful set ss2
STEP: Creating a new revision 04/21/23 21:36:01.796
STEP: Updating Pods in reverse ordinal order 04/21/23 21:36:11.806
Apr 21 21:36:11.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9516 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 21 21:36:11.931: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 21 21:36:11.931: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 21 21:36:11.931: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 04/21/23 21:36:21.943
Apr 21 21:36:21.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9516 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 21 21:36:22.062: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 21 21:36:22.062: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 21 21:36:22.062: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 21 21:36:32.089: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 04/21/23 21:36:42.099
Apr 21 21:36:42.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9516 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 21 21:36:42.223: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 21 21:36:42.223: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 21 21:36:42.223: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 21 21:36:52.234: INFO: Deleting all statefulset in ns statefulset-9516
Apr 21 21:36:52.236: INFO: Scaling statefulset ss2 to 0
Apr 21 21:37:02.247: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 21:37:02.248: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 21 21:37:02.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-9516" for this suite. 04/21/23 21:37:02.256
------------------------------
â€¢ [SLOW TEST] [80.649 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:35:41.612
    Apr 21 21:35:41.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename statefulset 04/21/23 21:35:41.612
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:35:41.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:35:41.623
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-9516 04/21/23 21:35:41.624
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 04/21/23 21:35:41.628
    Apr 21 21:35:41.633: INFO: Found 0 stateful pods, waiting for 3
    Apr 21 21:35:51.636: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 21:35:51.636: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 21:35:51.636: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 21:35:51.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9516 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 21:35:51.770: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 21:35:51.770: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 21:35:51.770: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/21/23 21:36:01.78
    Apr 21 21:36:01.796: INFO: Updating stateful set ss2
    STEP: Creating a new revision 04/21/23 21:36:01.796
    STEP: Updating Pods in reverse ordinal order 04/21/23 21:36:11.806
    Apr 21 21:36:11.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9516 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 21 21:36:11.931: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 21 21:36:11.931: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 21 21:36:11.931: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 04/21/23 21:36:21.943
    Apr 21 21:36:21.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9516 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 21 21:36:22.062: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 21 21:36:22.062: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 21 21:36:22.062: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 21 21:36:32.089: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 04/21/23 21:36:42.099
    Apr 21 21:36:42.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=statefulset-9516 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 21 21:36:42.223: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 21 21:36:42.223: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 21 21:36:42.223: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 21 21:36:52.234: INFO: Deleting all statefulset in ns statefulset-9516
    Apr 21 21:36:52.236: INFO: Scaling statefulset ss2 to 0
    Apr 21 21:37:02.247: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 21:37:02.248: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:37:02.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-9516" for this suite. 04/21/23 21:37:02.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:37:02.262
Apr 21 21:37:02.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename statefulset 04/21/23 21:37:02.263
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:02.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:02.271
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-2280 04/21/23 21:37:02.273
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-2280 04/21/23 21:37:02.276
Apr 21 21:37:02.281: INFO: Found 0 stateful pods, waiting for 1
Apr 21 21:37:12.284: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 04/21/23 21:37:12.287
STEP: updating a scale subresource 04/21/23 21:37:12.288
STEP: verifying the statefulset Spec.Replicas was modified 04/21/23 21:37:12.292
STEP: Patch a scale subresource 04/21/23 21:37:12.293
STEP: verifying the statefulset Spec.Replicas was modified 04/21/23 21:37:12.298
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 21 21:37:12.300: INFO: Deleting all statefulset in ns statefulset-2280
Apr 21 21:37:12.301: INFO: Scaling statefulset ss to 0
Apr 21 21:37:22.312: INFO: Waiting for statefulset status.replicas updated to 0
Apr 21 21:37:22.313: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 21 21:37:22.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-2280" for this suite. 04/21/23 21:37:22.322
------------------------------
â€¢ [SLOW TEST] [20.064 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:37:02.262
    Apr 21 21:37:02.262: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename statefulset 04/21/23 21:37:02.263
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:02.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:02.271
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-2280 04/21/23 21:37:02.273
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-2280 04/21/23 21:37:02.276
    Apr 21 21:37:02.281: INFO: Found 0 stateful pods, waiting for 1
    Apr 21 21:37:12.284: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 04/21/23 21:37:12.287
    STEP: updating a scale subresource 04/21/23 21:37:12.288
    STEP: verifying the statefulset Spec.Replicas was modified 04/21/23 21:37:12.292
    STEP: Patch a scale subresource 04/21/23 21:37:12.293
    STEP: verifying the statefulset Spec.Replicas was modified 04/21/23 21:37:12.298
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 21 21:37:12.300: INFO: Deleting all statefulset in ns statefulset-2280
    Apr 21 21:37:12.301: INFO: Scaling statefulset ss to 0
    Apr 21 21:37:22.312: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 21 21:37:22.313: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:37:22.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-2280" for this suite. 04/21/23 21:37:22.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:37:22.326
Apr 21 21:37:22.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 21:37:22.327
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:22.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:22.335
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 04/21/23 21:37:22.337
Apr 21 21:37:22.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6" in namespace "downward-api-8643" to be "Succeeded or Failed"
Apr 21 21:37:22.342: INFO: Pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.479222ms
Apr 21 21:37:24.345: INFO: Pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004144569s
Apr 21 21:37:26.346: INFO: Pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005273937s
STEP: Saw pod success 04/21/23 21:37:26.346
Apr 21 21:37:26.346: INFO: Pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6" satisfied condition "Succeeded or Failed"
Apr 21 21:37:26.348: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6 container client-container: <nil>
STEP: delete the pod 04/21/23 21:37:26.359
Apr 21 21:37:26.369: INFO: Waiting for pod downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6 to disappear
Apr 21 21:37:26.371: INFO: Pod downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 21:37:26.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8643" for this suite. 04/21/23 21:37:26.373
------------------------------
â€¢ [4.050 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:37:22.326
    Apr 21 21:37:22.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 21:37:22.327
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:22.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:22.335
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 04/21/23 21:37:22.337
    Apr 21 21:37:22.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6" in namespace "downward-api-8643" to be "Succeeded or Failed"
    Apr 21 21:37:22.342: INFO: Pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.479222ms
    Apr 21 21:37:24.345: INFO: Pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004144569s
    Apr 21 21:37:26.346: INFO: Pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005273937s
    STEP: Saw pod success 04/21/23 21:37:26.346
    Apr 21 21:37:26.346: INFO: Pod "downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6" satisfied condition "Succeeded or Failed"
    Apr 21 21:37:26.348: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6 container client-container: <nil>
    STEP: delete the pod 04/21/23 21:37:26.359
    Apr 21 21:37:26.369: INFO: Waiting for pod downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6 to disappear
    Apr 21 21:37:26.371: INFO: Pod downwardapi-volume-8faef804-a07f-4f22-a897-f53d4fa867e6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:37:26.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8643" for this suite. 04/21/23 21:37:26.373
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:37:26.376
Apr 21 21:37:26.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename ingressclass 04/21/23 21:37:26.377
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:26.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:26.386
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 04/21/23 21:37:26.388
STEP: getting /apis/networking.k8s.io 04/21/23 21:37:26.39
STEP: getting /apis/networking.k8s.iov1 04/21/23 21:37:26.39
STEP: creating 04/21/23 21:37:26.391
STEP: getting 04/21/23 21:37:26.4
STEP: listing 04/21/23 21:37:26.401
STEP: watching 04/21/23 21:37:26.402
Apr 21 21:37:26.402: INFO: starting watch
STEP: patching 04/21/23 21:37:26.403
STEP: updating 04/21/23 21:37:26.406
Apr 21 21:37:26.408: INFO: waiting for watch events with expected annotations
Apr 21 21:37:26.408: INFO: saw patched and updated annotations
STEP: deleting 04/21/23 21:37:26.408
STEP: deleting a collection 04/21/23 21:37:26.413
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Apr 21 21:37:26.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-9944" for this suite. 04/21/23 21:37:26.423
------------------------------
â€¢ [0.050 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:37:26.376
    Apr 21 21:37:26.376: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename ingressclass 04/21/23 21:37:26.377
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:26.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:26.386
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 04/21/23 21:37:26.388
    STEP: getting /apis/networking.k8s.io 04/21/23 21:37:26.39
    STEP: getting /apis/networking.k8s.iov1 04/21/23 21:37:26.39
    STEP: creating 04/21/23 21:37:26.391
    STEP: getting 04/21/23 21:37:26.4
    STEP: listing 04/21/23 21:37:26.401
    STEP: watching 04/21/23 21:37:26.402
    Apr 21 21:37:26.402: INFO: starting watch
    STEP: patching 04/21/23 21:37:26.403
    STEP: updating 04/21/23 21:37:26.406
    Apr 21 21:37:26.408: INFO: waiting for watch events with expected annotations
    Apr 21 21:37:26.408: INFO: saw patched and updated annotations
    STEP: deleting 04/21/23 21:37:26.408
    STEP: deleting a collection 04/21/23 21:37:26.413
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:37:26.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-9944" for this suite. 04/21/23 21:37:26.423
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:37:26.426
Apr 21 21:37:26.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename downward-api 04/21/23 21:37:26.427
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:26.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:26.435
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 04/21/23 21:37:26.436
Apr 21 21:37:26.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9" in namespace "downward-api-5293" to be "Succeeded or Failed"
Apr 21 21:37:26.442: INFO: Pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618013ms
Apr 21 21:37:28.445: INFO: Pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004306543s
Apr 21 21:37:30.445: INFO: Pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004615464s
STEP: Saw pod success 04/21/23 21:37:30.445
Apr 21 21:37:30.445: INFO: Pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9" satisfied condition "Succeeded or Failed"
Apr 21 21:37:30.447: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9 container client-container: <nil>
STEP: delete the pod 04/21/23 21:37:30.452
Apr 21 21:37:30.458: INFO: Waiting for pod downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9 to disappear
Apr 21 21:37:30.460: INFO: Pod downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 21 21:37:30.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5293" for this suite. 04/21/23 21:37:30.462
------------------------------
â€¢ [4.039 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:37:26.426
    Apr 21 21:37:26.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename downward-api 04/21/23 21:37:26.427
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:26.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:26.435
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 04/21/23 21:37:26.436
    Apr 21 21:37:26.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9" in namespace "downward-api-5293" to be "Succeeded or Failed"
    Apr 21 21:37:26.442: INFO: Pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.618013ms
    Apr 21 21:37:28.445: INFO: Pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004306543s
    Apr 21 21:37:30.445: INFO: Pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004615464s
    STEP: Saw pod success 04/21/23 21:37:30.445
    Apr 21 21:37:30.445: INFO: Pod "downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9" satisfied condition "Succeeded or Failed"
    Apr 21 21:37:30.447: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9 container client-container: <nil>
    STEP: delete the pod 04/21/23 21:37:30.452
    Apr 21 21:37:30.458: INFO: Waiting for pod downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9 to disappear
    Apr 21 21:37:30.460: INFO: Pod downwardapi-volume-3f66142e-c582-4c85-a6d8-d4bb1b0767c9 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:37:30.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5293" for this suite. 04/21/23 21:37:30.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:37:30.466
Apr 21 21:37:30.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:37:30.466
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:30.473
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:30.475
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-7d2fc37c-e8d2-4c7d-8730-152b246dff88 04/21/23 21:37:30.477
STEP: Creating a pod to test consume configMaps 04/21/23 21:37:30.479
Apr 21 21:37:30.484: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6" in namespace "projected-9123" to be "Succeeded or Failed"
Apr 21 21:37:30.485: INFO: Pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.513565ms
Apr 21 21:37:32.488: INFO: Pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004661873s
Apr 21 21:37:34.488: INFO: Pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003866601s
STEP: Saw pod success 04/21/23 21:37:34.488
Apr 21 21:37:34.488: INFO: Pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6" satisfied condition "Succeeded or Failed"
Apr 21 21:37:34.490: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 21:37:34.494
Apr 21 21:37:34.502: INFO: Waiting for pod pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6 to disappear
Apr 21 21:37:34.504: INFO: Pod pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:37:34.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9123" for this suite. 04/21/23 21:37:34.506
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:37:30.466
    Apr 21 21:37:30.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:37:30.466
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:30.473
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:30.475
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-7d2fc37c-e8d2-4c7d-8730-152b246dff88 04/21/23 21:37:30.477
    STEP: Creating a pod to test consume configMaps 04/21/23 21:37:30.479
    Apr 21 21:37:30.484: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6" in namespace "projected-9123" to be "Succeeded or Failed"
    Apr 21 21:37:30.485: INFO: Pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.513565ms
    Apr 21 21:37:32.488: INFO: Pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004661873s
    Apr 21 21:37:34.488: INFO: Pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003866601s
    STEP: Saw pod success 04/21/23 21:37:34.488
    Apr 21 21:37:34.488: INFO: Pod "pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6" satisfied condition "Succeeded or Failed"
    Apr 21 21:37:34.490: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 21:37:34.494
    Apr 21 21:37:34.502: INFO: Waiting for pod pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6 to disappear
    Apr 21 21:37:34.504: INFO: Pod pod-projected-configmaps-c990bb4e-6469-4282-b87f-5c83dc1abfa6 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:37:34.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9123" for this suite. 04/21/23 21:37:34.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:37:34.51
Apr 21 21:37:34.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 21:37:34.511
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:34.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:34.52
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 21:37:34.528
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:37:34.83
STEP: Deploying the webhook pod 04/21/23 21:37:34.834
STEP: Wait for the deployment to be ready 04/21/23 21:37:34.843
Apr 21 21:37:34.847: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 21:37:36.852
STEP: Verifying the service has paired with the endpoint 04/21/23 21:37:36.859
Apr 21 21:37:37.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 04/21/23 21:37:37.861
STEP: create a pod that should be denied by the webhook 04/21/23 21:37:37.872
STEP: create a pod that causes the webhook to hang 04/21/23 21:37:37.88
STEP: create a configmap that should be denied by the webhook 04/21/23 21:37:47.885
STEP: create a configmap that should be admitted by the webhook 04/21/23 21:37:47.891
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 04/21/23 21:37:47.897
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 04/21/23 21:37:47.901
STEP: create a namespace that bypass the webhook 04/21/23 21:37:47.904
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 04/21/23 21:37:47.908
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:37:47.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-996" for this suite. 04/21/23 21:37:47.946
STEP: Destroying namespace "webhook-996-markers" for this suite. 04/21/23 21:37:47.949
------------------------------
â€¢ [SLOW TEST] [13.443 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:37:34.51
    Apr 21 21:37:34.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 21:37:34.511
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:34.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:34.52
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 21:37:34.528
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:37:34.83
    STEP: Deploying the webhook pod 04/21/23 21:37:34.834
    STEP: Wait for the deployment to be ready 04/21/23 21:37:34.843
    Apr 21 21:37:34.847: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 21:37:36.852
    STEP: Verifying the service has paired with the endpoint 04/21/23 21:37:36.859
    Apr 21 21:37:37.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 04/21/23 21:37:37.861
    STEP: create a pod that should be denied by the webhook 04/21/23 21:37:37.872
    STEP: create a pod that causes the webhook to hang 04/21/23 21:37:37.88
    STEP: create a configmap that should be denied by the webhook 04/21/23 21:37:47.885
    STEP: create a configmap that should be admitted by the webhook 04/21/23 21:37:47.891
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 04/21/23 21:37:47.897
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 04/21/23 21:37:47.901
    STEP: create a namespace that bypass the webhook 04/21/23 21:37:47.904
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 04/21/23 21:37:47.908
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:37:47.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-996" for this suite. 04/21/23 21:37:47.946
    STEP: Destroying namespace "webhook-996-markers" for this suite. 04/21/23 21:37:47.949
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:37:47.956
Apr 21 21:37:47.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename watch 04/21/23 21:37:47.956
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:47.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:47.966
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 04/21/23 21:37:47.968
STEP: creating a new configmap 04/21/23 21:37:47.969
STEP: modifying the configmap once 04/21/23 21:37:47.972
STEP: closing the watch once it receives two notifications 04/21/23 21:37:47.977
Apr 21 21:37:47.977: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2980  62fdd73d-e952-4964-a1a0-46d4abf26647 23309 0 2023-04-21 21:37:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-21 21:37:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:37:47.977: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2980  62fdd73d-e952-4964-a1a0-46d4abf26647 23310 0 2023-04-21 21:37:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-21 21:37:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 04/21/23 21:37:47.978
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 04/21/23 21:37:47.982
STEP: deleting the configmap 04/21/23 21:37:47.984
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 04/21/23 21:37:47.989
Apr 21 21:37:47.989: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2980  62fdd73d-e952-4964-a1a0-46d4abf26647 23311 0 2023-04-21 21:37:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-21 21:37:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:37:47.989: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2980  62fdd73d-e952-4964-a1a0-46d4abf26647 23312 0 2023-04-21 21:37:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-21 21:37:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 21 21:37:47.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2980" for this suite. 04/21/23 21:37:47.992
------------------------------
â€¢ [0.040 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:37:47.956
    Apr 21 21:37:47.956: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename watch 04/21/23 21:37:47.956
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:47.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:47.966
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 04/21/23 21:37:47.968
    STEP: creating a new configmap 04/21/23 21:37:47.969
    STEP: modifying the configmap once 04/21/23 21:37:47.972
    STEP: closing the watch once it receives two notifications 04/21/23 21:37:47.977
    Apr 21 21:37:47.977: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2980  62fdd73d-e952-4964-a1a0-46d4abf26647 23309 0 2023-04-21 21:37:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-21 21:37:47 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:37:47.977: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2980  62fdd73d-e952-4964-a1a0-46d4abf26647 23310 0 2023-04-21 21:37:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-21 21:37:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 04/21/23 21:37:47.978
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 04/21/23 21:37:47.982
    STEP: deleting the configmap 04/21/23 21:37:47.984
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 04/21/23 21:37:47.989
    Apr 21 21:37:47.989: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2980  62fdd73d-e952-4964-a1a0-46d4abf26647 23311 0 2023-04-21 21:37:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-21 21:37:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:37:47.989: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2980  62fdd73d-e952-4964-a1a0-46d4abf26647 23312 0 2023-04-21 21:37:47 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-21 21:37:47 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:37:47.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2980" for this suite. 04/21/23 21:37:47.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:37:47.997
Apr 21 21:37:47.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename dns 04/21/23 21:37:47.998
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:48.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:48.011
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5817.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5817.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 04/21/23 21:37:48.033
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5817.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5817.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 04/21/23 21:37:48.033
STEP: creating a pod to probe /etc/hosts 04/21/23 21:37:48.033
STEP: submitting the pod to kubernetes 04/21/23 21:37:48.033
Apr 21 21:37:48.041: INFO: Waiting up to 15m0s for pod "dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52" in namespace "dns-5817" to be "running"
Apr 21 21:37:48.044: INFO: Pod "dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.591019ms
Apr 21 21:37:50.047: INFO: Pod "dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52": Phase="Running", Reason="", readiness=true. Elapsed: 2.005806186s
Apr 21 21:37:50.047: INFO: Pod "dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52" satisfied condition "running"
STEP: retrieving the pod 04/21/23 21:37:50.047
STEP: looking for the results for each expected name from probers 04/21/23 21:37:50.049
Apr 21 21:37:50.056: INFO: DNS probes using dns-5817/dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52 succeeded

STEP: deleting the pod 04/21/23 21:37:50.056
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 21 21:37:50.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5817" for this suite. 04/21/23 21:37:50.067
------------------------------
â€¢ [2.073 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:37:47.997
    Apr 21 21:37:47.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename dns 04/21/23 21:37:47.998
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:48.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:48.011
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5817.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5817.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     04/21/23 21:37:48.033
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5817.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5817.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     04/21/23 21:37:48.033
    STEP: creating a pod to probe /etc/hosts 04/21/23 21:37:48.033
    STEP: submitting the pod to kubernetes 04/21/23 21:37:48.033
    Apr 21 21:37:48.041: INFO: Waiting up to 15m0s for pod "dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52" in namespace "dns-5817" to be "running"
    Apr 21 21:37:48.044: INFO: Pod "dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.591019ms
    Apr 21 21:37:50.047: INFO: Pod "dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52": Phase="Running", Reason="", readiness=true. Elapsed: 2.005806186s
    Apr 21 21:37:50.047: INFO: Pod "dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52" satisfied condition "running"
    STEP: retrieving the pod 04/21/23 21:37:50.047
    STEP: looking for the results for each expected name from probers 04/21/23 21:37:50.049
    Apr 21 21:37:50.056: INFO: DNS probes using dns-5817/dns-test-df03cc4d-a779-4c06-ac66-73e72d250e52 succeeded

    STEP: deleting the pod 04/21/23 21:37:50.056
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:37:50.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5817" for this suite. 04/21/23 21:37:50.067
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:37:50.07
Apr 21 21:37:50.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename statefulset 04/21/23 21:37:50.071
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:50.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:50.079
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8725 04/21/23 21:37:50.08
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Apr 21 21:37:50.089: INFO: Found 0 stateful pods, waiting for 1
Apr 21 21:38:00.092: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 04/21/23 21:38:00.095
W0421 21:38:00.100456      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr 21 21:38:00.104: INFO: Found 1 stateful pods, waiting for 2
Apr 21 21:38:10.107: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 21 21:38:10.107: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 04/21/23 21:38:10.11
STEP: Delete all of the StatefulSets 04/21/23 21:38:10.112
STEP: Verify that StatefulSets have been deleted 04/21/23 21:38:10.115
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 21 21:38:10.117: INFO: Deleting all statefulset in ns statefulset-8725
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 21 21:38:10.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8725" for this suite. 04/21/23 21:38:10.124
------------------------------
â€¢ [SLOW TEST] [20.058 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:37:50.07
    Apr 21 21:37:50.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename statefulset 04/21/23 21:37:50.071
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:37:50.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:37:50.079
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8725 04/21/23 21:37:50.08
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Apr 21 21:37:50.089: INFO: Found 0 stateful pods, waiting for 1
    Apr 21 21:38:00.092: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 04/21/23 21:38:00.095
    W0421 21:38:00.100456      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr 21 21:38:00.104: INFO: Found 1 stateful pods, waiting for 2
    Apr 21 21:38:10.107: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 21 21:38:10.107: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 04/21/23 21:38:10.11
    STEP: Delete all of the StatefulSets 04/21/23 21:38:10.112
    STEP: Verify that StatefulSets have been deleted 04/21/23 21:38:10.115
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 21 21:38:10.117: INFO: Deleting all statefulset in ns statefulset-8725
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:38:10.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8725" for this suite. 04/21/23 21:38:10.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:38:10.129
Apr 21 21:38:10.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 21:38:10.13
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:10.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:10.144
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7215 04/21/23 21:38:10.147
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/21/23 21:38:10.164
STEP: creating service externalsvc in namespace services-7215 04/21/23 21:38:10.164
STEP: creating replication controller externalsvc in namespace services-7215 04/21/23 21:38:10.176
I0421 21:38:10.181510      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7215, replica count: 2
I0421 21:38:13.234411      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 04/21/23 21:38:13.236
Apr 21 21:38:13.247: INFO: Creating new exec pod
Apr 21 21:38:13.252: INFO: Waiting up to 5m0s for pod "execpodclx98" in namespace "services-7215" to be "running"
Apr 21 21:38:13.254: INFO: Pod "execpodclx98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.831927ms
Apr 21 21:38:15.257: INFO: Pod "execpodclx98": Phase="Running", Reason="", readiness=true. Elapsed: 2.004252838s
Apr 21 21:38:15.257: INFO: Pod "execpodclx98" satisfied condition "running"
Apr 21 21:38:15.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-7215 exec execpodclx98 -- /bin/sh -x -c nslookup clusterip-service.services-7215.svc.cluster.local'
Apr 21 21:38:15.413: INFO: stderr: "+ nslookup clusterip-service.services-7215.svc.cluster.local\n"
Apr 21 21:38:15.414: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7215.svc.cluster.local\tcanonical name = externalsvc.services-7215.svc.cluster.local.\nName:\texternalsvc.services-7215.svc.cluster.local\nAddress: 10.106.200.152\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7215, will wait for the garbage collector to delete the pods 04/21/23 21:38:15.414
Apr 21 21:38:15.470: INFO: Deleting ReplicationController externalsvc took: 3.868014ms
Apr 21 21:38:15.571: INFO: Terminating ReplicationController externalsvc pods took: 100.431619ms
Apr 21 21:38:17.283: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 21:38:17.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7215" for this suite. 04/21/23 21:38:17.293
------------------------------
â€¢ [SLOW TEST] [7.166 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:38:10.129
    Apr 21 21:38:10.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 21:38:10.13
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:10.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:10.144
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7215 04/21/23 21:38:10.147
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/21/23 21:38:10.164
    STEP: creating service externalsvc in namespace services-7215 04/21/23 21:38:10.164
    STEP: creating replication controller externalsvc in namespace services-7215 04/21/23 21:38:10.176
    I0421 21:38:10.181510      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7215, replica count: 2
    I0421 21:38:13.234411      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 04/21/23 21:38:13.236
    Apr 21 21:38:13.247: INFO: Creating new exec pod
    Apr 21 21:38:13.252: INFO: Waiting up to 5m0s for pod "execpodclx98" in namespace "services-7215" to be "running"
    Apr 21 21:38:13.254: INFO: Pod "execpodclx98": Phase="Pending", Reason="", readiness=false. Elapsed: 1.831927ms
    Apr 21 21:38:15.257: INFO: Pod "execpodclx98": Phase="Running", Reason="", readiness=true. Elapsed: 2.004252838s
    Apr 21 21:38:15.257: INFO: Pod "execpodclx98" satisfied condition "running"
    Apr 21 21:38:15.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-7215 exec execpodclx98 -- /bin/sh -x -c nslookup clusterip-service.services-7215.svc.cluster.local'
    Apr 21 21:38:15.413: INFO: stderr: "+ nslookup clusterip-service.services-7215.svc.cluster.local\n"
    Apr 21 21:38:15.414: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7215.svc.cluster.local\tcanonical name = externalsvc.services-7215.svc.cluster.local.\nName:\texternalsvc.services-7215.svc.cluster.local\nAddress: 10.106.200.152\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7215, will wait for the garbage collector to delete the pods 04/21/23 21:38:15.414
    Apr 21 21:38:15.470: INFO: Deleting ReplicationController externalsvc took: 3.868014ms
    Apr 21 21:38:15.571: INFO: Terminating ReplicationController externalsvc pods took: 100.431619ms
    Apr 21 21:38:17.283: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:38:17.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7215" for this suite. 04/21/23 21:38:17.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:38:17.296
Apr 21 21:38:17.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename deployment 04/21/23 21:38:17.297
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:17.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:17.305
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Apr 21 21:38:17.311: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 21 21:38:22.315: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/21/23 21:38:22.315
Apr 21 21:38:22.315: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 21 21:38:24.321: INFO: Creating deployment "test-rollover-deployment"
Apr 21 21:38:24.326: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 21 21:38:26.331: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 21 21:38:26.334: INFO: Ensure that both replica sets have 1 created replica
Apr 21 21:38:26.337: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 21 21:38:26.341: INFO: Updating deployment test-rollover-deployment
Apr 21 21:38:26.341: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 21 21:38:28.345: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 21 21:38:28.349: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 21 21:38:28.352: INFO: all replica sets need to contain the pod-template-hash label
Apr 21 21:38:28.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:38:30.357: INFO: all replica sets need to contain the pod-template-hash label
Apr 21 21:38:30.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:38:32.358: INFO: all replica sets need to contain the pod-template-hash label
Apr 21 21:38:32.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:38:34.358: INFO: all replica sets need to contain the pod-template-hash label
Apr 21 21:38:34.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:38:36.357: INFO: all replica sets need to contain the pod-template-hash label
Apr 21 21:38:36.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:38:38.358: INFO: 
Apr 21 21:38:38.358: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 21 21:38:38.362: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9750  0276755a-c922-449d-85fe-eb23954c2171 23696 2 2023-04-21 21:38:24 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b8d538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-21 21:38:24 +0000 UTC,LastTransitionTime:2023-04-21 21:38:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-04-21 21:38:37 +0000 UTC,LastTransitionTime:2023-04-21 21:38:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 21 21:38:38.364: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9750  63765a24-0213-4103-b856-0f7cccfb1a1a 23686 2 2023-04-21 21:38:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0276755a-c922-449d-85fe-eb23954c2171 0xc004386067 0xc004386068}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0276755a-c922-449d-85fe-eb23954c2171\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004386118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 21 21:38:38.364: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 21 21:38:38.364: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9750  140449a9-6e0f-469b-86a8-1d9e51f8cddc 23695 2 2023-04-21 21:38:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0276755a-c922-449d-85fe-eb23954c2171 0xc00300e347 0xc00300e348}] [] [{e2e.test Update apps/v1 2023-04-21 21:38:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0276755a-c922-449d-85fe-eb23954c2171\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00300e428 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 21 21:38:38.364: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9750  9e8ceb81-2467-4403-b29f-d3229ae0a2bd 23660 2 2023-04-21 21:38:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0276755a-c922-449d-85fe-eb23954c2171 0xc004386187 0xc004386188}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0276755a-c922-449d-85fe-eb23954c2171\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004386238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 21 21:38:38.366: INFO: Pod "test-rollover-deployment-6c6df9974f-2df4l" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-2df4l test-rollover-deployment-6c6df9974f- deployment-9750  5220f44d-56ab-4700-8bd8-7b43468f8fbb 23674 0 2023-04-21 21:38:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 63765a24-0213-4103-b856-0f7cccfb1a1a 0xc004386787 0xc004386788}] [] [{kube-controller-manager Update v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63765a24-0213-4103-b856-0f7cccfb1a1a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:38:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r245g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r245g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:38:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:38:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:38:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:38:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.221,StartTime:2023-04-21 21:38:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:38:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:docker://e8740aa5098f184cd176bbeec2a11702d5a1b378a9ba7cd60d1accc1cd6da52b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 21 21:38:38.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9750" for this suite. 04/21/23 21:38:38.368
------------------------------
â€¢ [SLOW TEST] [21.077 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:38:17.296
    Apr 21 21:38:17.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename deployment 04/21/23 21:38:17.297
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:17.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:17.305
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Apr 21 21:38:17.311: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Apr 21 21:38:22.315: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/21/23 21:38:22.315
    Apr 21 21:38:22.315: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Apr 21 21:38:24.321: INFO: Creating deployment "test-rollover-deployment"
    Apr 21 21:38:24.326: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Apr 21 21:38:26.331: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Apr 21 21:38:26.334: INFO: Ensure that both replica sets have 1 created replica
    Apr 21 21:38:26.337: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Apr 21 21:38:26.341: INFO: Updating deployment test-rollover-deployment
    Apr 21 21:38:26.341: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Apr 21 21:38:28.345: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Apr 21 21:38:28.349: INFO: Make sure deployment "test-rollover-deployment" is complete
    Apr 21 21:38:28.352: INFO: all replica sets need to contain the pod-template-hash label
    Apr 21 21:38:28.352: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:38:30.357: INFO: all replica sets need to contain the pod-template-hash label
    Apr 21 21:38:30.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:38:32.358: INFO: all replica sets need to contain the pod-template-hash label
    Apr 21 21:38:32.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:38:34.358: INFO: all replica sets need to contain the pod-template-hash label
    Apr 21 21:38:34.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:38:36.357: INFO: all replica sets need to contain the pod-template-hash label
    Apr 21 21:38:36.357: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 38, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 38, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:38:38.358: INFO: 
    Apr 21 21:38:38.358: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 21 21:38:38.362: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9750  0276755a-c922-449d-85fe-eb23954c2171 23696 2 2023-04-21 21:38:24 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b8d538 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-21 21:38:24 +0000 UTC,LastTransitionTime:2023-04-21 21:38:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-04-21 21:38:37 +0000 UTC,LastTransitionTime:2023-04-21 21:38:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 21 21:38:38.364: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9750  63765a24-0213-4103-b856-0f7cccfb1a1a 23686 2 2023-04-21 21:38:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 0276755a-c922-449d-85fe-eb23954c2171 0xc004386067 0xc004386068}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0276755a-c922-449d-85fe-eb23954c2171\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004386118 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 21:38:38.364: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Apr 21 21:38:38.364: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9750  140449a9-6e0f-469b-86a8-1d9e51f8cddc 23695 2 2023-04-21 21:38:17 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 0276755a-c922-449d-85fe-eb23954c2171 0xc00300e347 0xc00300e348}] [] [{e2e.test Update apps/v1 2023-04-21 21:38:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0276755a-c922-449d-85fe-eb23954c2171\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:37 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00300e428 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 21:38:38.364: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9750  9e8ceb81-2467-4403-b29f-d3229ae0a2bd 23660 2 2023-04-21 21:38:24 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 0276755a-c922-449d-85fe-eb23954c2171 0xc004386187 0xc004386188}] [] [{kube-controller-manager Update apps/v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0276755a-c922-449d-85fe-eb23954c2171\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004386238 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 21 21:38:38.366: INFO: Pod "test-rollover-deployment-6c6df9974f-2df4l" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-2df4l test-rollover-deployment-6c6df9974f- deployment-9750  5220f44d-56ab-4700-8bd8-7b43468f8fbb 23674 0 2023-04-21 21:38:26 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 63765a24-0213-4103-b856-0f7cccfb1a1a 0xc004386787 0xc004386788}] [] [{kube-controller-manager Update v1 2023-04-21 21:38:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"63765a24-0213-4103-b856-0f7cccfb1a1a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-21 21:38:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r245g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r245g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:38:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:38:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:38:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-21 21:38:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.221,StartTime:2023-04-21 21:38:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-21 21:38:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:docker://e8740aa5098f184cd176bbeec2a11702d5a1b378a9ba7cd60d1accc1cd6da52b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:38:38.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9750" for this suite. 04/21/23 21:38:38.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:38:38.374
Apr 21 21:38:38.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename resourcequota 04/21/23 21:38:38.374
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:38.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:38.383
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 04/21/23 21:38:38.385
STEP: Counting existing ResourceQuota 04/21/23 21:38:43.387
STEP: Creating a ResourceQuota 04/21/23 21:38:48.389
STEP: Ensuring resource quota status is calculated 04/21/23 21:38:48.393
STEP: Creating a Secret 04/21/23 21:38:50.395
STEP: Ensuring resource quota status captures secret creation 04/21/23 21:38:50.405
STEP: Deleting a secret 04/21/23 21:38:52.408
STEP: Ensuring resource quota status released usage 04/21/23 21:38:52.412
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 21 21:38:54.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9279" for this suite. 04/21/23 21:38:54.417
------------------------------
â€¢ [SLOW TEST] [16.049 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:38:38.374
    Apr 21 21:38:38.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename resourcequota 04/21/23 21:38:38.374
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:38.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:38.383
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 04/21/23 21:38:38.385
    STEP: Counting existing ResourceQuota 04/21/23 21:38:43.387
    STEP: Creating a ResourceQuota 04/21/23 21:38:48.389
    STEP: Ensuring resource quota status is calculated 04/21/23 21:38:48.393
    STEP: Creating a Secret 04/21/23 21:38:50.395
    STEP: Ensuring resource quota status captures secret creation 04/21/23 21:38:50.405
    STEP: Deleting a secret 04/21/23 21:38:52.408
    STEP: Ensuring resource quota status released usage 04/21/23 21:38:52.412
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:38:54.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9279" for this suite. 04/21/23 21:38:54.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:38:54.424
Apr 21 21:38:54.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:38:54.424
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:54.431
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:54.433
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 04/21/23 21:38:54.435
Apr 21 21:38:54.435: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3268 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 04/21/23 21:38:54.476
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:38:54.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3268" for this suite. 04/21/23 21:38:54.483
------------------------------
â€¢ [0.063 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:38:54.424
    Apr 21 21:38:54.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:38:54.424
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:54.431
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:54.433
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 04/21/23 21:38:54.435
    Apr 21 21:38:54.435: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-3268 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 04/21/23 21:38:54.476
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:38:54.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3268" for this suite. 04/21/23 21:38:54.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:38:54.487
Apr 21 21:38:54.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:38:54.488
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:54.496
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:54.497
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-dbe1f318-9238-4e56-96c1-1f6ae5a519d7 04/21/23 21:38:54.499
STEP: Creating a pod to test consume secrets 04/21/23 21:38:54.502
Apr 21 21:38:54.507: INFO: Waiting up to 5m0s for pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9" in namespace "secrets-6962" to be "Succeeded or Failed"
Apr 21 21:38:54.509: INFO: Pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.462322ms
Apr 21 21:38:56.511: INFO: Pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004247364s
Apr 21 21:38:58.513: INFO: Pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005400263s
STEP: Saw pod success 04/21/23 21:38:58.513
Apr 21 21:38:58.513: INFO: Pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9" satisfied condition "Succeeded or Failed"
Apr 21 21:38:58.514: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9 container secret-volume-test: <nil>
STEP: delete the pod 04/21/23 21:38:58.519
Apr 21 21:38:58.526: INFO: Waiting for pod pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9 to disappear
Apr 21 21:38:58.527: INFO: Pod pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:38:58.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6962" for this suite. 04/21/23 21:38:58.529
------------------------------
â€¢ [4.044 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:38:54.487
    Apr 21 21:38:54.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:38:54.488
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:54.496
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:54.497
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-dbe1f318-9238-4e56-96c1-1f6ae5a519d7 04/21/23 21:38:54.499
    STEP: Creating a pod to test consume secrets 04/21/23 21:38:54.502
    Apr 21 21:38:54.507: INFO: Waiting up to 5m0s for pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9" in namespace "secrets-6962" to be "Succeeded or Failed"
    Apr 21 21:38:54.509: INFO: Pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 1.462322ms
    Apr 21 21:38:56.511: INFO: Pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004247364s
    Apr 21 21:38:58.513: INFO: Pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005400263s
    STEP: Saw pod success 04/21/23 21:38:58.513
    Apr 21 21:38:58.513: INFO: Pod "pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9" satisfied condition "Succeeded or Failed"
    Apr 21 21:38:58.514: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9 container secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 21:38:58.519
    Apr 21 21:38:58.526: INFO: Waiting for pod pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9 to disappear
    Apr 21 21:38:58.527: INFO: Pod pod-secrets-f79629bd-48c8-42c4-8d27-530d3b78d5c9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:38:58.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6962" for this suite. 04/21/23 21:38:58.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:38:58.533
Apr 21 21:38:58.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename runtimeclass 04/21/23 21:38:58.534
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:58.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:58.541
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-1067-delete-me 04/21/23 21:38:58.545
STEP: Waiting for the RuntimeClass to disappear 04/21/23 21:38:58.548
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 21 21:38:58.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1067" for this suite. 04/21/23 21:38:58.554
------------------------------
â€¢ [0.024 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:38:58.533
    Apr 21 21:38:58.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename runtimeclass 04/21/23 21:38:58.534
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:58.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:58.541
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-1067-delete-me 04/21/23 21:38:58.545
    STEP: Waiting for the RuntimeClass to disappear 04/21/23 21:38:58.548
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:38:58.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1067" for this suite. 04/21/23 21:38:58.554
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:38:58.557
Apr 21 21:38:58.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename endpointslicemirroring 04/21/23 21:38:58.558
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:58.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:58.566
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 04/21/23 21:38:58.574
Apr 21 21:38:58.579: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 04/21/23 21:39:00.582
Apr 21 21:39:00.586: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 04/21/23 21:39:02.59
Apr 21 21:39:02.595: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Apr 21 21:39:04.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-8363" for this suite. 04/21/23 21:39:04.6
------------------------------
â€¢ [SLOW TEST] [6.046 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:38:58.557
    Apr 21 21:38:58.557: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename endpointslicemirroring 04/21/23 21:38:58.558
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:38:58.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:38:58.566
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 04/21/23 21:38:58.574
    Apr 21 21:38:58.579: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 04/21/23 21:39:00.582
    Apr 21 21:39:00.586: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 04/21/23 21:39:02.59
    Apr 21 21:39:02.595: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:39:04.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-8363" for this suite. 04/21/23 21:39:04.6
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:39:04.603
Apr 21 21:39:04.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:39:04.604
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:39:04.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:39:04.615
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-9396/secret-test-1c5404bf-2200-4da0-bacb-789a14709cc8 04/21/23 21:39:04.616
STEP: Creating a pod to test consume secrets 04/21/23 21:39:04.621
Apr 21 21:39:04.625: INFO: Waiting up to 5m0s for pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6" in namespace "secrets-9396" to be "Succeeded or Failed"
Apr 21 21:39:04.626: INFO: Pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505007ms
Apr 21 21:39:06.629: INFO: Pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003806417s
Apr 21 21:39:08.630: INFO: Pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004934826s
STEP: Saw pod success 04/21/23 21:39:08.63
Apr 21 21:39:08.630: INFO: Pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6" satisfied condition "Succeeded or Failed"
Apr 21 21:39:08.632: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6 container env-test: <nil>
STEP: delete the pod 04/21/23 21:39:08.637
Apr 21 21:39:08.645: INFO: Waiting for pod pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6 to disappear
Apr 21 21:39:08.646: INFO: Pod pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:39:08.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-9396" for this suite. 04/21/23 21:39:08.648
------------------------------
â€¢ [4.048 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:39:04.603
    Apr 21 21:39:04.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:39:04.604
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:39:04.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:39:04.615
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-9396/secret-test-1c5404bf-2200-4da0-bacb-789a14709cc8 04/21/23 21:39:04.616
    STEP: Creating a pod to test consume secrets 04/21/23 21:39:04.621
    Apr 21 21:39:04.625: INFO: Waiting up to 5m0s for pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6" in namespace "secrets-9396" to be "Succeeded or Failed"
    Apr 21 21:39:04.626: INFO: Pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.505007ms
    Apr 21 21:39:06.629: INFO: Pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003806417s
    Apr 21 21:39:08.630: INFO: Pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004934826s
    STEP: Saw pod success 04/21/23 21:39:08.63
    Apr 21 21:39:08.630: INFO: Pod "pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6" satisfied condition "Succeeded or Failed"
    Apr 21 21:39:08.632: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6 container env-test: <nil>
    STEP: delete the pod 04/21/23 21:39:08.637
    Apr 21 21:39:08.645: INFO: Waiting for pod pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6 to disappear
    Apr 21 21:39:08.646: INFO: Pod pod-configmaps-571c8004-7107-4bf4-b82c-83d74abfd4c6 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:39:08.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-9396" for this suite. 04/21/23 21:39:08.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:39:08.652
Apr 21 21:39:08.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename webhook 04/21/23 21:39:08.653
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:39:08.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:39:08.661
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/21/23 21:39:08.668
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:39:08.906
STEP: Deploying the webhook pod 04/21/23 21:39:08.911
STEP: Wait for the deployment to be ready 04/21/23 21:39:08.918
Apr 21 21:39:08.923: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/21/23 21:39:10.929
STEP: Verifying the service has paired with the endpoint 04/21/23 21:39:10.938
Apr 21 21:39:11.938: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 04/21/23 21:39:11.94
STEP: Registering slow webhook via the AdmissionRegistration API 04/21/23 21:39:11.941
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 04/21/23 21:39:11.952
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 04/21/23 21:39:12.958
STEP: Registering slow webhook via the AdmissionRegistration API 04/21/23 21:39:12.959
STEP: Having no error when timeout is longer than webhook latency 04/21/23 21:39:13.976
STEP: Registering slow webhook via the AdmissionRegistration API 04/21/23 21:39:13.976
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 04/21/23 21:39:18.997
STEP: Registering slow webhook via the AdmissionRegistration API 04/21/23 21:39:18.998
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:39:24.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6882" for this suite. 04/21/23 21:39:24.04
STEP: Destroying namespace "webhook-6882-markers" for this suite. 04/21/23 21:39:24.043
------------------------------
â€¢ [SLOW TEST] [15.395 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:39:08.652
    Apr 21 21:39:08.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename webhook 04/21/23 21:39:08.653
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:39:08.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:39:08.661
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/21/23 21:39:08.668
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/21/23 21:39:08.906
    STEP: Deploying the webhook pod 04/21/23 21:39:08.911
    STEP: Wait for the deployment to be ready 04/21/23 21:39:08.918
    Apr 21 21:39:08.923: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/21/23 21:39:10.929
    STEP: Verifying the service has paired with the endpoint 04/21/23 21:39:10.938
    Apr 21 21:39:11.938: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 04/21/23 21:39:11.94
    STEP: Registering slow webhook via the AdmissionRegistration API 04/21/23 21:39:11.941
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 04/21/23 21:39:11.952
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 04/21/23 21:39:12.958
    STEP: Registering slow webhook via the AdmissionRegistration API 04/21/23 21:39:12.959
    STEP: Having no error when timeout is longer than webhook latency 04/21/23 21:39:13.976
    STEP: Registering slow webhook via the AdmissionRegistration API 04/21/23 21:39:13.976
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 04/21/23 21:39:18.997
    STEP: Registering slow webhook via the AdmissionRegistration API 04/21/23 21:39:18.998
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:39:24.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6882" for this suite. 04/21/23 21:39:24.04
    STEP: Destroying namespace "webhook-6882-markers" for this suite. 04/21/23 21:39:24.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:39:24.048
Apr 21 21:39:24.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename watch 04/21/23 21:39:24.049
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:39:24.056
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:39:24.062
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 04/21/23 21:39:24.065
STEP: creating a watch on configmaps with label B 04/21/23 21:39:24.066
STEP: creating a watch on configmaps with label A or B 04/21/23 21:39:24.066
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 04/21/23 21:39:24.067
Apr 21 21:39:24.071: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23941 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:39:24.071: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23941 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 04/21/23 21:39:24.071
Apr 21 21:39:24.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23942 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:39:24.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23942 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 04/21/23 21:39:24.076
Apr 21 21:39:24.080: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23943 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:39:24.081: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23943 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 04/21/23 21:39:24.081
Apr 21 21:39:24.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23944 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:39:24.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23944 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 04/21/23 21:39:24.084
Apr 21 21:39:24.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  4aa7fa2a-d6ae-4b21-a94d-7b5111ee2bc3 23945 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:39:24.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  4aa7fa2a-d6ae-4b21-a94d-7b5111ee2bc3 23945 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 04/21/23 21:39:34.09
Apr 21 21:39:34.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  4aa7fa2a-d6ae-4b21-a94d-7b5111ee2bc3 23970 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:39:34.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  4aa7fa2a-d6ae-4b21-a94d-7b5111ee2bc3 23970 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 21 21:39:44.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9373" for this suite. 04/21/23 21:39:44.1
------------------------------
â€¢ [SLOW TEST] [20.056 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:39:24.048
    Apr 21 21:39:24.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename watch 04/21/23 21:39:24.049
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:39:24.056
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:39:24.062
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 04/21/23 21:39:24.065
    STEP: creating a watch on configmaps with label B 04/21/23 21:39:24.066
    STEP: creating a watch on configmaps with label A or B 04/21/23 21:39:24.066
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 04/21/23 21:39:24.067
    Apr 21 21:39:24.071: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23941 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:39:24.071: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23941 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 04/21/23 21:39:24.071
    Apr 21 21:39:24.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23942 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:39:24.076: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23942 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 04/21/23 21:39:24.076
    Apr 21 21:39:24.080: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23943 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:39:24.081: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23943 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 04/21/23 21:39:24.081
    Apr 21 21:39:24.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23944 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:39:24.084: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9373  1361855c-ab98-4cf2-97d5-c4ae33de06f0 23944 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 04/21/23 21:39:24.084
    Apr 21 21:39:24.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  4aa7fa2a-d6ae-4b21-a94d-7b5111ee2bc3 23945 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:39:24.089: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  4aa7fa2a-d6ae-4b21-a94d-7b5111ee2bc3 23945 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 04/21/23 21:39:34.09
    Apr 21 21:39:34.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  4aa7fa2a-d6ae-4b21-a94d-7b5111ee2bc3 23970 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:39:34.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9373  4aa7fa2a-d6ae-4b21-a94d-7b5111ee2bc3 23970 0 2023-04-21 21:39:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-21 21:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:39:44.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9373" for this suite. 04/21/23 21:39:44.1
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:39:44.105
Apr 21 21:39:44.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename var-expansion 04/21/23 21:39:44.106
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:39:44.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:39:44.116
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 04/21/23 21:39:44.117
STEP: waiting for pod running 04/21/23 21:39:44.122
Apr 21 21:39:44.122: INFO: Waiting up to 2m0s for pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" in namespace "var-expansion-953" to be "running"
Apr 21 21:39:44.124: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.591856ms
Apr 21 21:39:46.126: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004009966s
Apr 21 21:39:46.127: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" satisfied condition "running"
STEP: creating a file in subpath 04/21/23 21:39:46.127
Apr 21 21:39:46.128: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-953 PodName:var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:39:46.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:39:46.129: INFO: ExecWithOptions: Clientset creation
Apr 21 21:39:46.129: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-953/pods/var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 04/21/23 21:39:46.201
Apr 21 21:39:46.203: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-953 PodName:var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:39:46.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:39:46.204: INFO: ExecWithOptions: Clientset creation
Apr 21 21:39:46.204: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-953/pods/var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 04/21/23 21:39:46.246
Apr 21 21:39:46.753: INFO: Successfully updated pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f"
STEP: waiting for annotated pod running 04/21/23 21:39:46.753
Apr 21 21:39:46.753: INFO: Waiting up to 2m0s for pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" in namespace "var-expansion-953" to be "running"
Apr 21 21:39:46.755: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f": Phase="Running", Reason="", readiness=true. Elapsed: 1.556464ms
Apr 21 21:39:46.755: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" satisfied condition "running"
STEP: deleting the pod gracefully 04/21/23 21:39:46.755
Apr 21 21:39:46.755: INFO: Deleting pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" in namespace "var-expansion-953"
Apr 21 21:39:46.759: INFO: Wait up to 5m0s for pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 21 21:40:18.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-953" for this suite. 04/21/23 21:40:18.765
------------------------------
â€¢ [SLOW TEST] [34.663 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:39:44.105
    Apr 21 21:39:44.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename var-expansion 04/21/23 21:39:44.106
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:39:44.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:39:44.116
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 04/21/23 21:39:44.117
    STEP: waiting for pod running 04/21/23 21:39:44.122
    Apr 21 21:39:44.122: INFO: Waiting up to 2m0s for pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" in namespace "var-expansion-953" to be "running"
    Apr 21 21:39:44.124: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.591856ms
    Apr 21 21:39:46.126: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f": Phase="Running", Reason="", readiness=true. Elapsed: 2.004009966s
    Apr 21 21:39:46.127: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" satisfied condition "running"
    STEP: creating a file in subpath 04/21/23 21:39:46.127
    Apr 21 21:39:46.128: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-953 PodName:var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:39:46.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:39:46.129: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:39:46.129: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-953/pods/var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 04/21/23 21:39:46.201
    Apr 21 21:39:46.203: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-953 PodName:var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:39:46.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:39:46.204: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:39:46.204: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-953/pods/var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 04/21/23 21:39:46.246
    Apr 21 21:39:46.753: INFO: Successfully updated pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f"
    STEP: waiting for annotated pod running 04/21/23 21:39:46.753
    Apr 21 21:39:46.753: INFO: Waiting up to 2m0s for pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" in namespace "var-expansion-953" to be "running"
    Apr 21 21:39:46.755: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f": Phase="Running", Reason="", readiness=true. Elapsed: 1.556464ms
    Apr 21 21:39:46.755: INFO: Pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" satisfied condition "running"
    STEP: deleting the pod gracefully 04/21/23 21:39:46.755
    Apr 21 21:39:46.755: INFO: Deleting pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" in namespace "var-expansion-953"
    Apr 21 21:39:46.759: INFO: Wait up to 5m0s for pod "var-expansion-c16bb428-79f3-4dda-88fc-c19574c1358f" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:40:18.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-953" for this suite. 04/21/23 21:40:18.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:40:18.77
Apr 21 21:40:18.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename watch 04/21/23 21:40:18.77
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:18.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:18.781
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 04/21/23 21:40:18.782
STEP: creating a new configmap 04/21/23 21:40:18.783
STEP: modifying the configmap once 04/21/23 21:40:18.785
STEP: changing the label value of the configmap 04/21/23 21:40:18.789
STEP: Expecting to observe a delete notification for the watched object 04/21/23 21:40:18.792
Apr 21 21:40:18.793: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24039 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:40:18.793: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24040 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:40:18.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24041 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 04/21/23 21:40:18.793
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 04/21/23 21:40:18.797
STEP: changing the label value of the configmap back 04/21/23 21:40:28.798
STEP: modifying the configmap a third time 04/21/23 21:40:28.805
STEP: deleting the configmap 04/21/23 21:40:28.809
STEP: Expecting to observe an add notification for the watched object when the label value was restored 04/21/23 21:40:28.812
Apr 21 21:40:28.812: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24062 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:40:28.812: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24063 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 21 21:40:28.812: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24064 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 21 21:40:28.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9224" for this suite. 04/21/23 21:40:28.814
------------------------------
â€¢ [SLOW TEST] [10.048 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:40:18.77
    Apr 21 21:40:18.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename watch 04/21/23 21:40:18.77
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:18.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:18.781
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 04/21/23 21:40:18.782
    STEP: creating a new configmap 04/21/23 21:40:18.783
    STEP: modifying the configmap once 04/21/23 21:40:18.785
    STEP: changing the label value of the configmap 04/21/23 21:40:18.789
    STEP: Expecting to observe a delete notification for the watched object 04/21/23 21:40:18.792
    Apr 21 21:40:18.793: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24039 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:40:18.793: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24040 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:40:18.793: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24041 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:18 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 04/21/23 21:40:18.793
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 04/21/23 21:40:18.797
    STEP: changing the label value of the configmap back 04/21/23 21:40:28.798
    STEP: modifying the configmap a third time 04/21/23 21:40:28.805
    STEP: deleting the configmap 04/21/23 21:40:28.809
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 04/21/23 21:40:28.812
    Apr 21 21:40:28.812: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24062 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:40:28.812: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24063 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 21 21:40:28.812: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9224  99c241ca-9228-4957-9243-efbd85381807 24064 0 2023-04-21 21:40:18 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-21 21:40:28 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:40:28.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9224" for this suite. 04/21/23 21:40:28.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:40:28.818
Apr 21 21:40:28.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename events 04/21/23 21:40:28.818
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:28.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:28.827
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 04/21/23 21:40:28.829
STEP: get a list of Events with a label in the current namespace 04/21/23 21:40:28.838
STEP: delete a list of events 04/21/23 21:40:28.84
Apr 21 21:40:28.840: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 04/21/23 21:40:28.85
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Apr 21 21:40:28.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8398" for this suite. 04/21/23 21:40:28.853
------------------------------
â€¢ [0.038 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:40:28.818
    Apr 21 21:40:28.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename events 04/21/23 21:40:28.818
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:28.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:28.827
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 04/21/23 21:40:28.829
    STEP: get a list of Events with a label in the current namespace 04/21/23 21:40:28.838
    STEP: delete a list of events 04/21/23 21:40:28.84
    Apr 21 21:40:28.840: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 04/21/23 21:40:28.85
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:40:28.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8398" for this suite. 04/21/23 21:40:28.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:40:28.856
Apr 21 21:40:28.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:40:28.857
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:28.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:28.866
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-baf097c9-fa67-467d-90ee-f4ac688a4e25 04/21/23 21:40:28.868
STEP: Creating a pod to test consume secrets 04/21/23 21:40:28.871
Apr 21 21:40:28.875: INFO: Waiting up to 5m0s for pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5" in namespace "secrets-7087" to be "Succeeded or Failed"
Apr 21 21:40:28.877: INFO: Pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495769ms
Apr 21 21:40:30.879: INFO: Pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004169901s
Apr 21 21:40:32.880: INFO: Pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004329769s
STEP: Saw pod success 04/21/23 21:40:32.88
Apr 21 21:40:32.880: INFO: Pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5" satisfied condition "Succeeded or Failed"
Apr 21 21:40:32.882: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5 container secret-volume-test: <nil>
STEP: delete the pod 04/21/23 21:40:32.887
Apr 21 21:40:32.893: INFO: Waiting for pod pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5 to disappear
Apr 21 21:40:32.895: INFO: Pod pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:40:32.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7087" for this suite. 04/21/23 21:40:32.897
------------------------------
â€¢ [4.043 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:40:28.856
    Apr 21 21:40:28.856: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:40:28.857
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:28.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:28.866
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-baf097c9-fa67-467d-90ee-f4ac688a4e25 04/21/23 21:40:28.868
    STEP: Creating a pod to test consume secrets 04/21/23 21:40:28.871
    Apr 21 21:40:28.875: INFO: Waiting up to 5m0s for pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5" in namespace "secrets-7087" to be "Succeeded or Failed"
    Apr 21 21:40:28.877: INFO: Pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.495769ms
    Apr 21 21:40:30.879: INFO: Pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004169901s
    Apr 21 21:40:32.880: INFO: Pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004329769s
    STEP: Saw pod success 04/21/23 21:40:32.88
    Apr 21 21:40:32.880: INFO: Pod "pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5" satisfied condition "Succeeded or Failed"
    Apr 21 21:40:32.882: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5 container secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 21:40:32.887
    Apr 21 21:40:32.893: INFO: Waiting for pod pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5 to disappear
    Apr 21 21:40:32.895: INFO: Pod pod-secrets-3a844360-bd44-4f28-a582-7e043676f3d5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:40:32.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7087" for this suite. 04/21/23 21:40:32.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:40:32.901
Apr 21 21:40:32.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename namespaces 04/21/23 21:40:32.902
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:32.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:32.91
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 04/21/23 21:40:32.911
STEP: patching the Namespace 04/21/23 21:40:32.917
STEP: get the Namespace and ensuring it has the label 04/21/23 21:40:32.92
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:40:32.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-7319" for this suite. 04/21/23 21:40:32.923
STEP: Destroying namespace "nspatchtest-13aff235-acc8-4ef8-b8a2-18a62bfdab67-1437" for this suite. 04/21/23 21:40:32.926
------------------------------
â€¢ [0.028 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:40:32.901
    Apr 21 21:40:32.901: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename namespaces 04/21/23 21:40:32.902
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:32.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:32.91
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 04/21/23 21:40:32.911
    STEP: patching the Namespace 04/21/23 21:40:32.917
    STEP: get the Namespace and ensuring it has the label 04/21/23 21:40:32.92
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:40:32.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-7319" for this suite. 04/21/23 21:40:32.923
    STEP: Destroying namespace "nspatchtest-13aff235-acc8-4ef8-b8a2-18a62bfdab67-1437" for this suite. 04/21/23 21:40:32.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:40:32.929
Apr 21 21:40:32.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename events 04/21/23 21:40:32.93
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:32.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:32.938
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 04/21/23 21:40:32.94
STEP: listing all events in all namespaces 04/21/23 21:40:32.942
STEP: patching the test event 04/21/23 21:40:32.944
STEP: fetching the test event 04/21/23 21:40:32.947
STEP: updating the test event 04/21/23 21:40:32.948
STEP: getting the test event 04/21/23 21:40:32.954
STEP: deleting the test event 04/21/23 21:40:32.955
STEP: listing all events in all namespaces 04/21/23 21:40:32.958
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Apr 21 21:40:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-901" for this suite. 04/21/23 21:40:32.961
------------------------------
â€¢ [0.035 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:40:32.929
    Apr 21 21:40:32.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename events 04/21/23 21:40:32.93
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:32.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:32.938
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 04/21/23 21:40:32.94
    STEP: listing all events in all namespaces 04/21/23 21:40:32.942
    STEP: patching the test event 04/21/23 21:40:32.944
    STEP: fetching the test event 04/21/23 21:40:32.947
    STEP: updating the test event 04/21/23 21:40:32.948
    STEP: getting the test event 04/21/23 21:40:32.954
    STEP: deleting the test event 04/21/23 21:40:32.955
    STEP: listing all events in all namespaces 04/21/23 21:40:32.958
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:40:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-901" for this suite. 04/21/23 21:40:32.961
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:40:32.965
Apr 21 21:40:32.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename svc-latency 04/21/23 21:40:32.965
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:32.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:32.973
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Apr 21 21:40:32.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9320 04/21/23 21:40:32.975
I0421 21:40:32.980004      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9320, replica count: 1
I0421 21:40:34.030617      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 21 21:40:34.140: INFO: Created: latency-svc-49m7c
Apr 21 21:40:34.143: INFO: Got endpoints: latency-svc-49m7c [12.20693ms]
Apr 21 21:40:34.152: INFO: Created: latency-svc-vvjk9
Apr 21 21:40:34.159: INFO: Got endpoints: latency-svc-vvjk9 [15.51287ms]
Apr 21 21:40:34.161: INFO: Created: latency-svc-46mpd
Apr 21 21:40:34.165: INFO: Got endpoints: latency-svc-46mpd [20.751325ms]
Apr 21 21:40:34.169: INFO: Created: latency-svc-mphbg
Apr 21 21:40:34.173: INFO: Got endpoints: latency-svc-mphbg [29.015513ms]
Apr 21 21:40:34.175: INFO: Created: latency-svc-p2c4p
Apr 21 21:40:34.181: INFO: Got endpoints: latency-svc-p2c4p [37.168915ms]
Apr 21 21:40:34.182: INFO: Created: latency-svc-zjljg
Apr 21 21:40:34.188: INFO: Got endpoints: latency-svc-zjljg [43.598539ms]
Apr 21 21:40:34.233: INFO: Created: latency-svc-4jsc8
Apr 21 21:40:34.236: INFO: Got endpoints: latency-svc-4jsc8 [91.283496ms]
Apr 21 21:40:34.244: INFO: Created: latency-svc-ls26z
Apr 21 21:40:34.250: INFO: Got endpoints: latency-svc-ls26z [105.120769ms]
Apr 21 21:40:34.250: INFO: Created: latency-svc-kt4bl
Apr 21 21:40:34.255: INFO: Got endpoints: latency-svc-kt4bl [110.114034ms]
Apr 21 21:40:34.258: INFO: Created: latency-svc-244nx
Apr 21 21:40:34.264: INFO: Got endpoints: latency-svc-244nx [119.026187ms]
Apr 21 21:40:34.266: INFO: Created: latency-svc-w5d8m
Apr 21 21:40:34.267: INFO: Got endpoints: latency-svc-w5d8m [122.332869ms]
Apr 21 21:40:34.273: INFO: Created: latency-svc-h4bbd
Apr 21 21:40:34.276: INFO: Got endpoints: latency-svc-h4bbd [131.300472ms]
Apr 21 21:40:34.277: INFO: Created: latency-svc-9qknb
Apr 21 21:40:34.281: INFO: Got endpoints: latency-svc-9qknb [135.519493ms]
Apr 21 21:40:34.285: INFO: Created: latency-svc-rx8w6
Apr 21 21:40:34.287: INFO: Got endpoints: latency-svc-rx8w6 [141.643158ms]
Apr 21 21:40:34.291: INFO: Created: latency-svc-24lk7
Apr 21 21:40:34.332: INFO: Got endpoints: latency-svc-24lk7 [186.60452ms]
Apr 21 21:40:34.356: INFO: Created: latency-svc-9v7qr
Apr 21 21:40:34.360: INFO: Got endpoints: latency-svc-9v7qr [214.087925ms]
Apr 21 21:40:34.438: INFO: Created: latency-svc-x6v7p
Apr 21 21:40:34.444: INFO: Got endpoints: latency-svc-x6v7p [284.486408ms]
Apr 21 21:40:34.445: INFO: Created: latency-svc-2wfzf
Apr 21 21:40:34.450: INFO: Got endpoints: latency-svc-2wfzf [285.834641ms]
Apr 21 21:40:34.452: INFO: Created: latency-svc-86r8t
Apr 21 21:40:34.458: INFO: Got endpoints: latency-svc-86r8t [285.20617ms]
Apr 21 21:40:34.460: INFO: Created: latency-svc-cp5tj
Apr 21 21:40:34.463: INFO: Got endpoints: latency-svc-cp5tj [281.798084ms]
Apr 21 21:40:34.467: INFO: Created: latency-svc-ktb24
Apr 21 21:40:34.472: INFO: Got endpoints: latency-svc-ktb24 [283.49057ms]
Apr 21 21:40:34.472: INFO: Created: latency-svc-rmw2x
Apr 21 21:40:34.477: INFO: Got endpoints: latency-svc-rmw2x [241.482431ms]
Apr 21 21:40:34.479: INFO: Created: latency-svc-l4mpt
Apr 21 21:40:34.483: INFO: Got endpoints: latency-svc-l4mpt [233.769329ms]
Apr 21 21:40:34.486: INFO: Created: latency-svc-8m8jb
Apr 21 21:40:34.489: INFO: Got endpoints: latency-svc-8m8jb [234.003267ms]
Apr 21 21:40:34.490: INFO: Created: latency-svc-7rch5
Apr 21 21:40:34.495: INFO: Got endpoints: latency-svc-7rch5 [231.55716ms]
Apr 21 21:40:34.497: INFO: Created: latency-svc-sc8bl
Apr 21 21:40:34.501: INFO: Got endpoints: latency-svc-sc8bl [233.584341ms]
Apr 21 21:40:34.503: INFO: Created: latency-svc-qkhgn
Apr 21 21:40:34.533: INFO: Got endpoints: latency-svc-qkhgn [257.176677ms]
Apr 21 21:40:34.538: INFO: Created: latency-svc-7kvgj
Apr 21 21:40:34.543: INFO: Got endpoints: latency-svc-7kvgj [262.385549ms]
Apr 21 21:40:34.544: INFO: Created: latency-svc-t6gds
Apr 21 21:40:34.548: INFO: Got endpoints: latency-svc-t6gds [260.879391ms]
Apr 21 21:40:34.549: INFO: Created: latency-svc-cnbsc
Apr 21 21:40:34.554: INFO: Got endpoints: latency-svc-cnbsc [221.954757ms]
Apr 21 21:40:34.556: INFO: Created: latency-svc-285ls
Apr 21 21:40:34.565: INFO: Got endpoints: latency-svc-285ls [205.6127ms]
Apr 21 21:40:34.566: INFO: Created: latency-svc-g2x62
Apr 21 21:40:34.570: INFO: Got endpoints: latency-svc-g2x62 [126.569147ms]
Apr 21 21:40:34.572: INFO: Created: latency-svc-4vxbs
Apr 21 21:40:34.578: INFO: Got endpoints: latency-svc-4vxbs [127.088416ms]
Apr 21 21:40:34.580: INFO: Created: latency-svc-tfnwj
Apr 21 21:40:34.585: INFO: Got endpoints: latency-svc-tfnwj [127.041337ms]
Apr 21 21:40:34.587: INFO: Created: latency-svc-7df79
Apr 21 21:40:34.592: INFO: Got endpoints: latency-svc-7df79 [128.652544ms]
Apr 21 21:40:34.594: INFO: Created: latency-svc-qx4zr
Apr 21 21:40:34.600: INFO: Got endpoints: latency-svc-qx4zr [128.563926ms]
Apr 21 21:40:34.633: INFO: Created: latency-svc-6fvv4
Apr 21 21:40:34.638: INFO: Got endpoints: latency-svc-6fvv4 [161.148374ms]
Apr 21 21:40:34.640: INFO: Created: latency-svc-xcdl4
Apr 21 21:40:34.642: INFO: Got endpoints: latency-svc-xcdl4 [158.373796ms]
Apr 21 21:40:34.647: INFO: Created: latency-svc-nb2mc
Apr 21 21:40:34.652: INFO: Got endpoints: latency-svc-nb2mc [162.837669ms]
Apr 21 21:40:34.654: INFO: Created: latency-svc-2whzz
Apr 21 21:40:34.660: INFO: Got endpoints: latency-svc-2whzz [164.072969ms]
Apr 21 21:40:34.662: INFO: Created: latency-svc-tpc5m
Apr 21 21:40:34.671: INFO: Created: latency-svc-t2stp
Apr 21 21:40:34.676: INFO: Created: latency-svc-r9sjf
Apr 21 21:40:34.681: INFO: Created: latency-svc-ph5ql
Apr 21 21:40:34.687: INFO: Created: latency-svc-b2xtn
Apr 21 21:40:34.693: INFO: Got endpoints: latency-svc-tpc5m [191.8279ms]
Apr 21 21:40:34.695: INFO: Created: latency-svc-njfp6
Apr 21 21:40:34.700: INFO: Created: latency-svc-6nz9m
Apr 21 21:40:34.707: INFO: Created: latency-svc-xhz4g
Apr 21 21:40:34.710: INFO: Created: latency-svc-hq7gb
Apr 21 21:40:34.717: INFO: Created: latency-svc-xj5n4
Apr 21 21:40:34.732: INFO: Created: latency-svc-mbvbs
Apr 21 21:40:34.734: INFO: Created: latency-svc-clg6g
Apr 21 21:40:34.739: INFO: Created: latency-svc-jn2qj
Apr 21 21:40:34.744: INFO: Got endpoints: latency-svc-t2stp [210.83462ms]
Apr 21 21:40:34.745: INFO: Created: latency-svc-7f2gq
Apr 21 21:40:34.752: INFO: Created: latency-svc-dbhxr
Apr 21 21:40:34.759: INFO: Created: latency-svc-d9xd9
Apr 21 21:40:34.765: INFO: Created: latency-svc-qggwg
Apr 21 21:40:34.794: INFO: Got endpoints: latency-svc-r9sjf [250.388151ms]
Apr 21 21:40:34.802: INFO: Created: latency-svc-rn9g5
Apr 21 21:40:34.845: INFO: Got endpoints: latency-svc-ph5ql [296.611315ms]
Apr 21 21:40:34.853: INFO: Created: latency-svc-lhwwh
Apr 21 21:40:34.894: INFO: Got endpoints: latency-svc-b2xtn [339.31897ms]
Apr 21 21:40:34.902: INFO: Created: latency-svc-c67rm
Apr 21 21:40:34.944: INFO: Got endpoints: latency-svc-njfp6 [378.203166ms]
Apr 21 21:40:34.956: INFO: Created: latency-svc-zzsrk
Apr 21 21:40:34.995: INFO: Got endpoints: latency-svc-6nz9m [424.653948ms]
Apr 21 21:40:35.003: INFO: Created: latency-svc-vp9cp
Apr 21 21:40:35.044: INFO: Got endpoints: latency-svc-xhz4g [465.854493ms]
Apr 21 21:40:35.052: INFO: Created: latency-svc-26vnj
Apr 21 21:40:35.094: INFO: Got endpoints: latency-svc-hq7gb [508.357466ms]
Apr 21 21:40:35.103: INFO: Created: latency-svc-8l26n
Apr 21 21:40:35.146: INFO: Got endpoints: latency-svc-xj5n4 [553.678347ms]
Apr 21 21:40:35.155: INFO: Created: latency-svc-fvwrp
Apr 21 21:40:35.197: INFO: Got endpoints: latency-svc-mbvbs [596.758326ms]
Apr 21 21:40:35.206: INFO: Created: latency-svc-kmh4h
Apr 21 21:40:35.243: INFO: Got endpoints: latency-svc-clg6g [604.359378ms]
Apr 21 21:40:35.253: INFO: Created: latency-svc-vbzkr
Apr 21 21:40:35.293: INFO: Got endpoints: latency-svc-jn2qj [651.106916ms]
Apr 21 21:40:35.304: INFO: Created: latency-svc-qjpl6
Apr 21 21:40:35.344: INFO: Got endpoints: latency-svc-7f2gq [691.787548ms]
Apr 21 21:40:35.353: INFO: Created: latency-svc-l2b2r
Apr 21 21:40:35.394: INFO: Got endpoints: latency-svc-dbhxr [734.642321ms]
Apr 21 21:40:35.406: INFO: Created: latency-svc-b6hsf
Apr 21 21:40:35.445: INFO: Got endpoints: latency-svc-d9xd9 [751.549474ms]
Apr 21 21:40:35.453: INFO: Created: latency-svc-p7bv2
Apr 21 21:40:35.494: INFO: Got endpoints: latency-svc-qggwg [749.732238ms]
Apr 21 21:40:35.503: INFO: Created: latency-svc-xrgcc
Apr 21 21:40:35.546: INFO: Got endpoints: latency-svc-rn9g5 [752.341308ms]
Apr 21 21:40:35.555: INFO: Created: latency-svc-qj6dt
Apr 21 21:40:35.594: INFO: Got endpoints: latency-svc-lhwwh [749.097554ms]
Apr 21 21:40:35.603: INFO: Created: latency-svc-c5j57
Apr 21 21:40:35.644: INFO: Got endpoints: latency-svc-c67rm [750.538037ms]
Apr 21 21:40:35.654: INFO: Created: latency-svc-525d8
Apr 21 21:40:35.695: INFO: Got endpoints: latency-svc-zzsrk [751.474086ms]
Apr 21 21:40:35.705: INFO: Created: latency-svc-rrn2n
Apr 21 21:40:35.744: INFO: Got endpoints: latency-svc-vp9cp [749.139181ms]
Apr 21 21:40:35.752: INFO: Created: latency-svc-dvhcn
Apr 21 21:40:35.794: INFO: Got endpoints: latency-svc-26vnj [750.289155ms]
Apr 21 21:40:35.804: INFO: Created: latency-svc-jdnng
Apr 21 21:40:35.844: INFO: Got endpoints: latency-svc-8l26n [750.62257ms]
Apr 21 21:40:35.857: INFO: Created: latency-svc-n69kq
Apr 21 21:40:35.895: INFO: Got endpoints: latency-svc-fvwrp [749.00828ms]
Apr 21 21:40:35.903: INFO: Created: latency-svc-brz52
Apr 21 21:40:35.944: INFO: Got endpoints: latency-svc-kmh4h [746.674804ms]
Apr 21 21:40:35.953: INFO: Created: latency-svc-gkvmz
Apr 21 21:40:35.994: INFO: Got endpoints: latency-svc-vbzkr [751.194251ms]
Apr 21 21:40:36.002: INFO: Created: latency-svc-dwd4w
Apr 21 21:40:36.046: INFO: Got endpoints: latency-svc-qjpl6 [752.591897ms]
Apr 21 21:40:36.054: INFO: Created: latency-svc-775cj
Apr 21 21:40:36.094: INFO: Got endpoints: latency-svc-l2b2r [750.568122ms]
Apr 21 21:40:36.104: INFO: Created: latency-svc-gclwm
Apr 21 21:40:36.143: INFO: Got endpoints: latency-svc-b6hsf [748.974714ms]
Apr 21 21:40:36.153: INFO: Created: latency-svc-ft5mh
Apr 21 21:40:36.194: INFO: Got endpoints: latency-svc-p7bv2 [749.690948ms]
Apr 21 21:40:36.203: INFO: Created: latency-svc-wn4xd
Apr 21 21:40:36.248: INFO: Got endpoints: latency-svc-xrgcc [753.312591ms]
Apr 21 21:40:36.258: INFO: Created: latency-svc-xh58x
Apr 21 21:40:36.293: INFO: Got endpoints: latency-svc-qj6dt [747.266715ms]
Apr 21 21:40:36.304: INFO: Created: latency-svc-9s242
Apr 21 21:40:36.346: INFO: Got endpoints: latency-svc-c5j57 [751.745914ms]
Apr 21 21:40:36.374: INFO: Created: latency-svc-cjqgw
Apr 21 21:40:36.432: INFO: Got endpoints: latency-svc-525d8 [788.222905ms]
Apr 21 21:40:36.441: INFO: Created: latency-svc-fdkwk
Apr 21 21:40:36.443: INFO: Got endpoints: latency-svc-rrn2n [747.558695ms]
Apr 21 21:40:36.451: INFO: Created: latency-svc-lq4xz
Apr 21 21:40:36.493: INFO: Got endpoints: latency-svc-dvhcn [749.247992ms]
Apr 21 21:40:36.501: INFO: Created: latency-svc-d6p6r
Apr 21 21:40:36.544: INFO: Got endpoints: latency-svc-jdnng [750.577262ms]
Apr 21 21:40:36.552: INFO: Created: latency-svc-v4tdn
Apr 21 21:40:36.593: INFO: Got endpoints: latency-svc-n69kq [748.451859ms]
Apr 21 21:40:36.601: INFO: Created: latency-svc-5p84j
Apr 21 21:40:36.644: INFO: Got endpoints: latency-svc-brz52 [749.033567ms]
Apr 21 21:40:36.652: INFO: Created: latency-svc-m7c9l
Apr 21 21:40:36.695: INFO: Got endpoints: latency-svc-gkvmz [751.501698ms]
Apr 21 21:40:36.703: INFO: Created: latency-svc-zl7ql
Apr 21 21:40:36.744: INFO: Got endpoints: latency-svc-dwd4w [749.827459ms]
Apr 21 21:40:36.752: INFO: Created: latency-svc-8zl4q
Apr 21 21:40:36.794: INFO: Got endpoints: latency-svc-775cj [748.532449ms]
Apr 21 21:40:36.803: INFO: Created: latency-svc-9jld8
Apr 21 21:40:36.845: INFO: Got endpoints: latency-svc-gclwm [751.206959ms]
Apr 21 21:40:36.854: INFO: Created: latency-svc-6jcmx
Apr 21 21:40:36.895: INFO: Got endpoints: latency-svc-ft5mh [752.0514ms]
Apr 21 21:40:36.909: INFO: Created: latency-svc-qmfks
Apr 21 21:40:36.944: INFO: Got endpoints: latency-svc-wn4xd [749.550387ms]
Apr 21 21:40:36.953: INFO: Created: latency-svc-k4qgh
Apr 21 21:40:36.993: INFO: Got endpoints: latency-svc-xh58x [745.906215ms]
Apr 21 21:40:37.002: INFO: Created: latency-svc-22ts5
Apr 21 21:40:37.045: INFO: Got endpoints: latency-svc-9s242 [751.959209ms]
Apr 21 21:40:37.054: INFO: Created: latency-svc-65bgr
Apr 21 21:40:37.094: INFO: Got endpoints: latency-svc-cjqgw [748.817311ms]
Apr 21 21:40:37.104: INFO: Created: latency-svc-cgljf
Apr 21 21:40:37.144: INFO: Got endpoints: latency-svc-fdkwk [711.556509ms]
Apr 21 21:40:37.153: INFO: Created: latency-svc-9bkhb
Apr 21 21:40:37.195: INFO: Got endpoints: latency-svc-lq4xz [752.28254ms]
Apr 21 21:40:37.203: INFO: Created: latency-svc-wzx7c
Apr 21 21:40:37.244: INFO: Got endpoints: latency-svc-d6p6r [750.135895ms]
Apr 21 21:40:37.252: INFO: Created: latency-svc-p6fxk
Apr 21 21:40:37.293: INFO: Got endpoints: latency-svc-v4tdn [748.773809ms]
Apr 21 21:40:37.301: INFO: Created: latency-svc-744f7
Apr 21 21:40:37.347: INFO: Got endpoints: latency-svc-5p84j [753.618494ms]
Apr 21 21:40:37.356: INFO: Created: latency-svc-csthv
Apr 21 21:40:37.397: INFO: Got endpoints: latency-svc-m7c9l [752.765397ms]
Apr 21 21:40:37.405: INFO: Created: latency-svc-flxw4
Apr 21 21:40:37.444: INFO: Got endpoints: latency-svc-zl7ql [748.581385ms]
Apr 21 21:40:37.453: INFO: Created: latency-svc-xp642
Apr 21 21:40:37.494: INFO: Got endpoints: latency-svc-8zl4q [749.844213ms]
Apr 21 21:40:37.502: INFO: Created: latency-svc-bv2z9
Apr 21 21:40:37.545: INFO: Got endpoints: latency-svc-9jld8 [750.55269ms]
Apr 21 21:40:37.553: INFO: Created: latency-svc-rn8hn
Apr 21 21:40:37.594: INFO: Got endpoints: latency-svc-6jcmx [748.343151ms]
Apr 21 21:40:37.605: INFO: Created: latency-svc-kvhmr
Apr 21 21:40:37.644: INFO: Got endpoints: latency-svc-qmfks [748.414989ms]
Apr 21 21:40:37.653: INFO: Created: latency-svc-g2qd7
Apr 21 21:40:37.694: INFO: Got endpoints: latency-svc-k4qgh [749.659255ms]
Apr 21 21:40:37.701: INFO: Created: latency-svc-ll6f2
Apr 21 21:40:37.744: INFO: Got endpoints: latency-svc-22ts5 [750.174489ms]
Apr 21 21:40:37.752: INFO: Created: latency-svc-zn7mx
Apr 21 21:40:37.794: INFO: Got endpoints: latency-svc-65bgr [749.006396ms]
Apr 21 21:40:37.803: INFO: Created: latency-svc-cxmqh
Apr 21 21:40:37.844: INFO: Got endpoints: latency-svc-cgljf [749.593103ms]
Apr 21 21:40:37.853: INFO: Created: latency-svc-264d5
Apr 21 21:40:37.894: INFO: Got endpoints: latency-svc-9bkhb [749.949441ms]
Apr 21 21:40:37.903: INFO: Created: latency-svc-td8fv
Apr 21 21:40:37.943: INFO: Got endpoints: latency-svc-wzx7c [748.243727ms]
Apr 21 21:40:37.951: INFO: Created: latency-svc-z6glv
Apr 21 21:40:38.037: INFO: Got endpoints: latency-svc-p6fxk [793.523014ms]
Apr 21 21:40:38.046: INFO: Got endpoints: latency-svc-744f7 [752.217605ms]
Apr 21 21:40:38.046: INFO: Created: latency-svc-9cmbh
Apr 21 21:40:38.055: INFO: Created: latency-svc-8sjph
Apr 21 21:40:38.137: INFO: Got endpoints: latency-svc-csthv [790.091232ms]
Apr 21 21:40:38.144: INFO: Got endpoints: latency-svc-flxw4 [747.593708ms]
Apr 21 21:40:38.150: INFO: Created: latency-svc-kxp8j
Apr 21 21:40:38.158: INFO: Created: latency-svc-vtwvx
Apr 21 21:40:38.235: INFO: Got endpoints: latency-svc-xp642 [791.277256ms]
Apr 21 21:40:38.246: INFO: Got endpoints: latency-svc-bv2z9 [752.08629ms]
Apr 21 21:40:38.250: INFO: Created: latency-svc-hm7m8
Apr 21 21:40:38.257: INFO: Created: latency-svc-tq4qf
Apr 21 21:40:38.295: INFO: Got endpoints: latency-svc-rn8hn [749.81765ms]
Apr 21 21:40:38.303: INFO: Created: latency-svc-n7dks
Apr 21 21:40:38.345: INFO: Got endpoints: latency-svc-kvhmr [751.511819ms]
Apr 21 21:40:38.355: INFO: Created: latency-svc-xbwtm
Apr 21 21:40:38.393: INFO: Got endpoints: latency-svc-g2qd7 [749.399959ms]
Apr 21 21:40:38.402: INFO: Created: latency-svc-b44fw
Apr 21 21:40:38.444: INFO: Got endpoints: latency-svc-ll6f2 [750.189689ms]
Apr 21 21:40:38.454: INFO: Created: latency-svc-57tvr
Apr 21 21:40:38.495: INFO: Got endpoints: latency-svc-zn7mx [751.639877ms]
Apr 21 21:40:38.503: INFO: Created: latency-svc-vs4m7
Apr 21 21:40:38.545: INFO: Got endpoints: latency-svc-cxmqh [750.416221ms]
Apr 21 21:40:38.557: INFO: Created: latency-svc-vpzn8
Apr 21 21:40:38.593: INFO: Got endpoints: latency-svc-264d5 [749.262028ms]
Apr 21 21:40:38.601: INFO: Created: latency-svc-96x7v
Apr 21 21:40:38.645: INFO: Got endpoints: latency-svc-td8fv [750.974438ms]
Apr 21 21:40:38.653: INFO: Created: latency-svc-dpxrk
Apr 21 21:40:38.693: INFO: Got endpoints: latency-svc-z6glv [749.917433ms]
Apr 21 21:40:38.701: INFO: Created: latency-svc-n9n4m
Apr 21 21:40:38.743: INFO: Got endpoints: latency-svc-9cmbh [706.144072ms]
Apr 21 21:40:38.751: INFO: Created: latency-svc-tdprr
Apr 21 21:40:38.795: INFO: Got endpoints: latency-svc-8sjph [749.844685ms]
Apr 21 21:40:38.803: INFO: Created: latency-svc-8c2x2
Apr 21 21:40:38.844: INFO: Got endpoints: latency-svc-kxp8j [706.727199ms]
Apr 21 21:40:38.852: INFO: Created: latency-svc-s6nzx
Apr 21 21:40:38.893: INFO: Got endpoints: latency-svc-vtwvx [748.95684ms]
Apr 21 21:40:38.902: INFO: Created: latency-svc-kxmbm
Apr 21 21:40:38.944: INFO: Got endpoints: latency-svc-hm7m8 [708.313566ms]
Apr 21 21:40:38.952: INFO: Created: latency-svc-79kjt
Apr 21 21:40:38.993: INFO: Got endpoints: latency-svc-tq4qf [747.389101ms]
Apr 21 21:40:39.003: INFO: Created: latency-svc-5g7ll
Apr 21 21:40:39.044: INFO: Got endpoints: latency-svc-n7dks [749.430784ms]
Apr 21 21:40:39.053: INFO: Created: latency-svc-tkp47
Apr 21 21:40:39.093: INFO: Got endpoints: latency-svc-xbwtm [747.7173ms]
Apr 21 21:40:39.101: INFO: Created: latency-svc-mrblg
Apr 21 21:40:39.145: INFO: Got endpoints: latency-svc-b44fw [752.050937ms]
Apr 21 21:40:39.155: INFO: Created: latency-svc-jdwc2
Apr 21 21:40:39.195: INFO: Got endpoints: latency-svc-57tvr [751.368287ms]
Apr 21 21:40:39.204: INFO: Created: latency-svc-jhg24
Apr 21 21:40:39.244: INFO: Got endpoints: latency-svc-vs4m7 [748.627007ms]
Apr 21 21:40:39.252: INFO: Created: latency-svc-n2ssx
Apr 21 21:40:39.294: INFO: Got endpoints: latency-svc-vpzn8 [749.116928ms]
Apr 21 21:40:39.301: INFO: Created: latency-svc-zwmxm
Apr 21 21:40:39.346: INFO: Got endpoints: latency-svc-96x7v [752.228322ms]
Apr 21 21:40:39.353: INFO: Created: latency-svc-fn7dn
Apr 21 21:40:39.393: INFO: Got endpoints: latency-svc-dpxrk [748.360468ms]
Apr 21 21:40:39.402: INFO: Created: latency-svc-bb55f
Apr 21 21:40:39.444: INFO: Got endpoints: latency-svc-n9n4m [750.282649ms]
Apr 21 21:40:39.452: INFO: Created: latency-svc-bfhs4
Apr 21 21:40:39.495: INFO: Got endpoints: latency-svc-tdprr [751.690734ms]
Apr 21 21:40:39.503: INFO: Created: latency-svc-l46d4
Apr 21 21:40:39.544: INFO: Got endpoints: latency-svc-8c2x2 [748.182659ms]
Apr 21 21:40:39.552: INFO: Created: latency-svc-txt8x
Apr 21 21:40:39.594: INFO: Got endpoints: latency-svc-s6nzx [749.962185ms]
Apr 21 21:40:39.605: INFO: Created: latency-svc-frjhw
Apr 21 21:40:39.644: INFO: Got endpoints: latency-svc-kxmbm [750.579854ms]
Apr 21 21:40:39.657: INFO: Created: latency-svc-zbrrn
Apr 21 21:40:39.694: INFO: Got endpoints: latency-svc-79kjt [750.48363ms]
Apr 21 21:40:39.704: INFO: Created: latency-svc-xl5jm
Apr 21 21:40:39.744: INFO: Got endpoints: latency-svc-5g7ll [750.837105ms]
Apr 21 21:40:39.752: INFO: Created: latency-svc-zkdqp
Apr 21 21:40:39.794: INFO: Got endpoints: latency-svc-tkp47 [749.48076ms]
Apr 21 21:40:39.802: INFO: Created: latency-svc-tg64f
Apr 21 21:40:39.844: INFO: Got endpoints: latency-svc-mrblg [750.640178ms]
Apr 21 21:40:39.852: INFO: Created: latency-svc-jg5zq
Apr 21 21:40:39.896: INFO: Got endpoints: latency-svc-jdwc2 [750.796957ms]
Apr 21 21:40:39.904: INFO: Created: latency-svc-mjhgl
Apr 21 21:40:39.943: INFO: Got endpoints: latency-svc-jhg24 [748.080601ms]
Apr 21 21:40:39.952: INFO: Created: latency-svc-kh7tr
Apr 21 21:40:39.993: INFO: Got endpoints: latency-svc-n2ssx [749.16897ms]
Apr 21 21:40:40.003: INFO: Created: latency-svc-8f4gx
Apr 21 21:40:40.044: INFO: Got endpoints: latency-svc-zwmxm [749.974513ms]
Apr 21 21:40:40.052: INFO: Created: latency-svc-7qqk5
Apr 21 21:40:40.096: INFO: Got endpoints: latency-svc-fn7dn [749.953172ms]
Apr 21 21:40:40.105: INFO: Created: latency-svc-5h99t
Apr 21 21:40:40.143: INFO: Got endpoints: latency-svc-bb55f [750.077375ms]
Apr 21 21:40:40.153: INFO: Created: latency-svc-9sszq
Apr 21 21:40:40.194: INFO: Got endpoints: latency-svc-bfhs4 [750.4227ms]
Apr 21 21:40:40.207: INFO: Created: latency-svc-cd5wv
Apr 21 21:40:40.245: INFO: Got endpoints: latency-svc-l46d4 [749.430581ms]
Apr 21 21:40:40.253: INFO: Created: latency-svc-wn8gw
Apr 21 21:40:40.294: INFO: Got endpoints: latency-svc-txt8x [750.333242ms]
Apr 21 21:40:40.302: INFO: Created: latency-svc-qwzz2
Apr 21 21:40:40.344: INFO: Got endpoints: latency-svc-frjhw [750.167003ms]
Apr 21 21:40:40.352: INFO: Created: latency-svc-jdldg
Apr 21 21:40:40.395: INFO: Got endpoints: latency-svc-zbrrn [751.336447ms]
Apr 21 21:40:40.403: INFO: Created: latency-svc-ld9b2
Apr 21 21:40:40.445: INFO: Got endpoints: latency-svc-xl5jm [751.240057ms]
Apr 21 21:40:40.453: INFO: Created: latency-svc-gbgnq
Apr 21 21:40:40.493: INFO: Got endpoints: latency-svc-zkdqp [749.016734ms]
Apr 21 21:40:40.502: INFO: Created: latency-svc-rtzpk
Apr 21 21:40:40.544: INFO: Got endpoints: latency-svc-tg64f [750.206566ms]
Apr 21 21:40:40.552: INFO: Created: latency-svc-czskp
Apr 21 21:40:40.596: INFO: Got endpoints: latency-svc-jg5zq [751.899122ms]
Apr 21 21:40:40.604: INFO: Created: latency-svc-9hxk2
Apr 21 21:40:40.644: INFO: Got endpoints: latency-svc-mjhgl [747.807622ms]
Apr 21 21:40:40.653: INFO: Created: latency-svc-2zm98
Apr 21 21:40:40.694: INFO: Got endpoints: latency-svc-kh7tr [750.244672ms]
Apr 21 21:40:40.702: INFO: Created: latency-svc-mfr8w
Apr 21 21:40:40.746: INFO: Got endpoints: latency-svc-8f4gx [752.851348ms]
Apr 21 21:40:40.754: INFO: Created: latency-svc-nkxbg
Apr 21 21:40:40.794: INFO: Got endpoints: latency-svc-7qqk5 [749.462045ms]
Apr 21 21:40:40.803: INFO: Created: latency-svc-26dzd
Apr 21 21:40:40.844: INFO: Got endpoints: latency-svc-5h99t [748.350709ms]
Apr 21 21:40:40.856: INFO: Created: latency-svc-h97vs
Apr 21 21:40:40.895: INFO: Got endpoints: latency-svc-9sszq [751.505967ms]
Apr 21 21:40:40.903: INFO: Created: latency-svc-vlbk7
Apr 21 21:40:40.944: INFO: Got endpoints: latency-svc-cd5wv [750.122917ms]
Apr 21 21:40:40.955: INFO: Created: latency-svc-7wqxw
Apr 21 21:40:40.993: INFO: Got endpoints: latency-svc-wn8gw [748.853743ms]
Apr 21 21:40:41.002: INFO: Created: latency-svc-gv487
Apr 21 21:40:41.046: INFO: Got endpoints: latency-svc-qwzz2 [751.828692ms]
Apr 21 21:40:41.058: INFO: Created: latency-svc-g4jg8
Apr 21 21:40:41.094: INFO: Got endpoints: latency-svc-jdldg [749.767572ms]
Apr 21 21:40:41.103: INFO: Created: latency-svc-pftl8
Apr 21 21:40:41.144: INFO: Got endpoints: latency-svc-ld9b2 [748.452298ms]
Apr 21 21:40:41.154: INFO: Created: latency-svc-r5wx8
Apr 21 21:40:41.193: INFO: Got endpoints: latency-svc-gbgnq [748.080152ms]
Apr 21 21:40:41.203: INFO: Created: latency-svc-dsllv
Apr 21 21:40:41.244: INFO: Got endpoints: latency-svc-rtzpk [750.76711ms]
Apr 21 21:40:41.252: INFO: Created: latency-svc-hvcz4
Apr 21 21:40:41.294: INFO: Got endpoints: latency-svc-czskp [749.847131ms]
Apr 21 21:40:41.304: INFO: Created: latency-svc-rkkdd
Apr 21 21:40:41.343: INFO: Got endpoints: latency-svc-9hxk2 [747.562134ms]
Apr 21 21:40:41.351: INFO: Created: latency-svc-bnn5k
Apr 21 21:40:41.396: INFO: Got endpoints: latency-svc-2zm98 [751.51323ms]
Apr 21 21:40:41.404: INFO: Created: latency-svc-pvpnm
Apr 21 21:40:41.443: INFO: Got endpoints: latency-svc-mfr8w [749.760638ms]
Apr 21 21:40:41.454: INFO: Created: latency-svc-6qwnw
Apr 21 21:40:41.494: INFO: Got endpoints: latency-svc-nkxbg [747.982986ms]
Apr 21 21:40:41.504: INFO: Created: latency-svc-w5j5g
Apr 21 21:40:41.543: INFO: Got endpoints: latency-svc-26dzd [749.911956ms]
Apr 21 21:40:41.552: INFO: Created: latency-svc-wkk8z
Apr 21 21:40:41.595: INFO: Got endpoints: latency-svc-h97vs [750.761574ms]
Apr 21 21:40:41.605: INFO: Created: latency-svc-vdwfz
Apr 21 21:40:41.645: INFO: Got endpoints: latency-svc-vlbk7 [750.016958ms]
Apr 21 21:40:41.653: INFO: Created: latency-svc-zqmgz
Apr 21 21:40:41.694: INFO: Got endpoints: latency-svc-7wqxw [749.945318ms]
Apr 21 21:40:41.702: INFO: Created: latency-svc-djqmp
Apr 21 21:40:41.745: INFO: Got endpoints: latency-svc-gv487 [751.166979ms]
Apr 21 21:40:41.752: INFO: Created: latency-svc-swtb7
Apr 21 21:40:41.793: INFO: Got endpoints: latency-svc-g4jg8 [747.170878ms]
Apr 21 21:40:41.801: INFO: Created: latency-svc-6dsnw
Apr 21 21:40:41.843: INFO: Got endpoints: latency-svc-pftl8 [749.536486ms]
Apr 21 21:40:41.853: INFO: Created: latency-svc-ncldv
Apr 21 21:40:41.894: INFO: Got endpoints: latency-svc-r5wx8 [749.970872ms]
Apr 21 21:40:41.902: INFO: Created: latency-svc-hn26l
Apr 21 21:40:41.946: INFO: Got endpoints: latency-svc-dsllv [752.1528ms]
Apr 21 21:40:41.955: INFO: Created: latency-svc-ht4p9
Apr 21 21:40:41.996: INFO: Got endpoints: latency-svc-hvcz4 [752.254398ms]
Apr 21 21:40:42.045: INFO: Got endpoints: latency-svc-rkkdd [750.808262ms]
Apr 21 21:40:42.094: INFO: Got endpoints: latency-svc-bnn5k [750.759299ms]
Apr 21 21:40:42.145: INFO: Got endpoints: latency-svc-pvpnm [749.809097ms]
Apr 21 21:40:42.194: INFO: Got endpoints: latency-svc-6qwnw [750.733285ms]
Apr 21 21:40:42.244: INFO: Got endpoints: latency-svc-w5j5g [749.665834ms]
Apr 21 21:40:42.294: INFO: Got endpoints: latency-svc-wkk8z [750.280691ms]
Apr 21 21:40:42.344: INFO: Got endpoints: latency-svc-vdwfz [749.60914ms]
Apr 21 21:40:42.394: INFO: Got endpoints: latency-svc-zqmgz [748.617376ms]
Apr 21 21:40:42.444: INFO: Got endpoints: latency-svc-djqmp [749.47014ms]
Apr 21 21:40:42.496: INFO: Got endpoints: latency-svc-swtb7 [751.122288ms]
Apr 21 21:40:42.543: INFO: Got endpoints: latency-svc-6dsnw [750.286408ms]
Apr 21 21:40:42.594: INFO: Got endpoints: latency-svc-ncldv [750.875386ms]
Apr 21 21:40:42.643: INFO: Got endpoints: latency-svc-hn26l [749.623223ms]
Apr 21 21:40:42.694: INFO: Got endpoints: latency-svc-ht4p9 [748.361874ms]
Apr 21 21:40:42.694: INFO: Latencies: [15.51287ms 20.751325ms 29.015513ms 37.168915ms 43.598539ms 91.283496ms 105.120769ms 110.114034ms 119.026187ms 122.332869ms 126.569147ms 127.041337ms 127.088416ms 128.563926ms 128.652544ms 131.300472ms 135.519493ms 141.643158ms 158.373796ms 161.148374ms 162.837669ms 164.072969ms 186.60452ms 191.8279ms 205.6127ms 210.83462ms 214.087925ms 221.954757ms 231.55716ms 233.584341ms 233.769329ms 234.003267ms 241.482431ms 250.388151ms 257.176677ms 260.879391ms 262.385549ms 281.798084ms 283.49057ms 284.486408ms 285.20617ms 285.834641ms 296.611315ms 339.31897ms 378.203166ms 424.653948ms 465.854493ms 508.357466ms 553.678347ms 596.758326ms 604.359378ms 651.106916ms 691.787548ms 706.144072ms 706.727199ms 708.313566ms 711.556509ms 734.642321ms 745.906215ms 746.674804ms 747.170878ms 747.266715ms 747.389101ms 747.558695ms 747.562134ms 747.593708ms 747.7173ms 747.807622ms 747.982986ms 748.080152ms 748.080601ms 748.182659ms 748.243727ms 748.343151ms 748.350709ms 748.360468ms 748.361874ms 748.414989ms 748.451859ms 748.452298ms 748.532449ms 748.581385ms 748.617376ms 748.627007ms 748.773809ms 748.817311ms 748.853743ms 748.95684ms 748.974714ms 749.006396ms 749.00828ms 749.016734ms 749.033567ms 749.097554ms 749.116928ms 749.139181ms 749.16897ms 749.247992ms 749.262028ms 749.399959ms 749.430581ms 749.430784ms 749.462045ms 749.47014ms 749.48076ms 749.536486ms 749.550387ms 749.593103ms 749.60914ms 749.623223ms 749.659255ms 749.665834ms 749.690948ms 749.732238ms 749.760638ms 749.767572ms 749.809097ms 749.81765ms 749.827459ms 749.844213ms 749.844685ms 749.847131ms 749.911956ms 749.917433ms 749.945318ms 749.949441ms 749.953172ms 749.962185ms 749.970872ms 749.974513ms 750.016958ms 750.077375ms 750.122917ms 750.135895ms 750.167003ms 750.174489ms 750.189689ms 750.206566ms 750.244672ms 750.280691ms 750.282649ms 750.286408ms 750.289155ms 750.333242ms 750.416221ms 750.4227ms 750.48363ms 750.538037ms 750.55269ms 750.568122ms 750.577262ms 750.579854ms 750.62257ms 750.640178ms 750.733285ms 750.759299ms 750.761574ms 750.76711ms 750.796957ms 750.808262ms 750.837105ms 750.875386ms 750.974438ms 751.122288ms 751.166979ms 751.194251ms 751.206959ms 751.240057ms 751.336447ms 751.368287ms 751.474086ms 751.501698ms 751.505967ms 751.511819ms 751.51323ms 751.549474ms 751.639877ms 751.690734ms 751.745914ms 751.828692ms 751.899122ms 751.959209ms 752.050937ms 752.0514ms 752.08629ms 752.1528ms 752.217605ms 752.228322ms 752.254398ms 752.28254ms 752.341308ms 752.591897ms 752.765397ms 752.851348ms 753.312591ms 753.618494ms 788.222905ms 790.091232ms 791.277256ms 793.523014ms]
Apr 21 21:40:42.694: INFO: 50 %ile: 749.430581ms
Apr 21 21:40:42.694: INFO: 90 %ile: 751.899122ms
Apr 21 21:40:42.694: INFO: 99 %ile: 791.277256ms
Apr 21 21:40:42.694: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Apr 21 21:40:42.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-9320" for this suite. 04/21/23 21:40:42.697
------------------------------
â€¢ [SLOW TEST] [9.736 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:40:32.965
    Apr 21 21:40:32.965: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename svc-latency 04/21/23 21:40:32.965
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:32.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:32.973
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Apr 21 21:40:32.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-9320 04/21/23 21:40:32.975
    I0421 21:40:32.980004      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9320, replica count: 1
    I0421 21:40:34.030617      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 21 21:40:34.140: INFO: Created: latency-svc-49m7c
    Apr 21 21:40:34.143: INFO: Got endpoints: latency-svc-49m7c [12.20693ms]
    Apr 21 21:40:34.152: INFO: Created: latency-svc-vvjk9
    Apr 21 21:40:34.159: INFO: Got endpoints: latency-svc-vvjk9 [15.51287ms]
    Apr 21 21:40:34.161: INFO: Created: latency-svc-46mpd
    Apr 21 21:40:34.165: INFO: Got endpoints: latency-svc-46mpd [20.751325ms]
    Apr 21 21:40:34.169: INFO: Created: latency-svc-mphbg
    Apr 21 21:40:34.173: INFO: Got endpoints: latency-svc-mphbg [29.015513ms]
    Apr 21 21:40:34.175: INFO: Created: latency-svc-p2c4p
    Apr 21 21:40:34.181: INFO: Got endpoints: latency-svc-p2c4p [37.168915ms]
    Apr 21 21:40:34.182: INFO: Created: latency-svc-zjljg
    Apr 21 21:40:34.188: INFO: Got endpoints: latency-svc-zjljg [43.598539ms]
    Apr 21 21:40:34.233: INFO: Created: latency-svc-4jsc8
    Apr 21 21:40:34.236: INFO: Got endpoints: latency-svc-4jsc8 [91.283496ms]
    Apr 21 21:40:34.244: INFO: Created: latency-svc-ls26z
    Apr 21 21:40:34.250: INFO: Got endpoints: latency-svc-ls26z [105.120769ms]
    Apr 21 21:40:34.250: INFO: Created: latency-svc-kt4bl
    Apr 21 21:40:34.255: INFO: Got endpoints: latency-svc-kt4bl [110.114034ms]
    Apr 21 21:40:34.258: INFO: Created: latency-svc-244nx
    Apr 21 21:40:34.264: INFO: Got endpoints: latency-svc-244nx [119.026187ms]
    Apr 21 21:40:34.266: INFO: Created: latency-svc-w5d8m
    Apr 21 21:40:34.267: INFO: Got endpoints: latency-svc-w5d8m [122.332869ms]
    Apr 21 21:40:34.273: INFO: Created: latency-svc-h4bbd
    Apr 21 21:40:34.276: INFO: Got endpoints: latency-svc-h4bbd [131.300472ms]
    Apr 21 21:40:34.277: INFO: Created: latency-svc-9qknb
    Apr 21 21:40:34.281: INFO: Got endpoints: latency-svc-9qknb [135.519493ms]
    Apr 21 21:40:34.285: INFO: Created: latency-svc-rx8w6
    Apr 21 21:40:34.287: INFO: Got endpoints: latency-svc-rx8w6 [141.643158ms]
    Apr 21 21:40:34.291: INFO: Created: latency-svc-24lk7
    Apr 21 21:40:34.332: INFO: Got endpoints: latency-svc-24lk7 [186.60452ms]
    Apr 21 21:40:34.356: INFO: Created: latency-svc-9v7qr
    Apr 21 21:40:34.360: INFO: Got endpoints: latency-svc-9v7qr [214.087925ms]
    Apr 21 21:40:34.438: INFO: Created: latency-svc-x6v7p
    Apr 21 21:40:34.444: INFO: Got endpoints: latency-svc-x6v7p [284.486408ms]
    Apr 21 21:40:34.445: INFO: Created: latency-svc-2wfzf
    Apr 21 21:40:34.450: INFO: Got endpoints: latency-svc-2wfzf [285.834641ms]
    Apr 21 21:40:34.452: INFO: Created: latency-svc-86r8t
    Apr 21 21:40:34.458: INFO: Got endpoints: latency-svc-86r8t [285.20617ms]
    Apr 21 21:40:34.460: INFO: Created: latency-svc-cp5tj
    Apr 21 21:40:34.463: INFO: Got endpoints: latency-svc-cp5tj [281.798084ms]
    Apr 21 21:40:34.467: INFO: Created: latency-svc-ktb24
    Apr 21 21:40:34.472: INFO: Got endpoints: latency-svc-ktb24 [283.49057ms]
    Apr 21 21:40:34.472: INFO: Created: latency-svc-rmw2x
    Apr 21 21:40:34.477: INFO: Got endpoints: latency-svc-rmw2x [241.482431ms]
    Apr 21 21:40:34.479: INFO: Created: latency-svc-l4mpt
    Apr 21 21:40:34.483: INFO: Got endpoints: latency-svc-l4mpt [233.769329ms]
    Apr 21 21:40:34.486: INFO: Created: latency-svc-8m8jb
    Apr 21 21:40:34.489: INFO: Got endpoints: latency-svc-8m8jb [234.003267ms]
    Apr 21 21:40:34.490: INFO: Created: latency-svc-7rch5
    Apr 21 21:40:34.495: INFO: Got endpoints: latency-svc-7rch5 [231.55716ms]
    Apr 21 21:40:34.497: INFO: Created: latency-svc-sc8bl
    Apr 21 21:40:34.501: INFO: Got endpoints: latency-svc-sc8bl [233.584341ms]
    Apr 21 21:40:34.503: INFO: Created: latency-svc-qkhgn
    Apr 21 21:40:34.533: INFO: Got endpoints: latency-svc-qkhgn [257.176677ms]
    Apr 21 21:40:34.538: INFO: Created: latency-svc-7kvgj
    Apr 21 21:40:34.543: INFO: Got endpoints: latency-svc-7kvgj [262.385549ms]
    Apr 21 21:40:34.544: INFO: Created: latency-svc-t6gds
    Apr 21 21:40:34.548: INFO: Got endpoints: latency-svc-t6gds [260.879391ms]
    Apr 21 21:40:34.549: INFO: Created: latency-svc-cnbsc
    Apr 21 21:40:34.554: INFO: Got endpoints: latency-svc-cnbsc [221.954757ms]
    Apr 21 21:40:34.556: INFO: Created: latency-svc-285ls
    Apr 21 21:40:34.565: INFO: Got endpoints: latency-svc-285ls [205.6127ms]
    Apr 21 21:40:34.566: INFO: Created: latency-svc-g2x62
    Apr 21 21:40:34.570: INFO: Got endpoints: latency-svc-g2x62 [126.569147ms]
    Apr 21 21:40:34.572: INFO: Created: latency-svc-4vxbs
    Apr 21 21:40:34.578: INFO: Got endpoints: latency-svc-4vxbs [127.088416ms]
    Apr 21 21:40:34.580: INFO: Created: latency-svc-tfnwj
    Apr 21 21:40:34.585: INFO: Got endpoints: latency-svc-tfnwj [127.041337ms]
    Apr 21 21:40:34.587: INFO: Created: latency-svc-7df79
    Apr 21 21:40:34.592: INFO: Got endpoints: latency-svc-7df79 [128.652544ms]
    Apr 21 21:40:34.594: INFO: Created: latency-svc-qx4zr
    Apr 21 21:40:34.600: INFO: Got endpoints: latency-svc-qx4zr [128.563926ms]
    Apr 21 21:40:34.633: INFO: Created: latency-svc-6fvv4
    Apr 21 21:40:34.638: INFO: Got endpoints: latency-svc-6fvv4 [161.148374ms]
    Apr 21 21:40:34.640: INFO: Created: latency-svc-xcdl4
    Apr 21 21:40:34.642: INFO: Got endpoints: latency-svc-xcdl4 [158.373796ms]
    Apr 21 21:40:34.647: INFO: Created: latency-svc-nb2mc
    Apr 21 21:40:34.652: INFO: Got endpoints: latency-svc-nb2mc [162.837669ms]
    Apr 21 21:40:34.654: INFO: Created: latency-svc-2whzz
    Apr 21 21:40:34.660: INFO: Got endpoints: latency-svc-2whzz [164.072969ms]
    Apr 21 21:40:34.662: INFO: Created: latency-svc-tpc5m
    Apr 21 21:40:34.671: INFO: Created: latency-svc-t2stp
    Apr 21 21:40:34.676: INFO: Created: latency-svc-r9sjf
    Apr 21 21:40:34.681: INFO: Created: latency-svc-ph5ql
    Apr 21 21:40:34.687: INFO: Created: latency-svc-b2xtn
    Apr 21 21:40:34.693: INFO: Got endpoints: latency-svc-tpc5m [191.8279ms]
    Apr 21 21:40:34.695: INFO: Created: latency-svc-njfp6
    Apr 21 21:40:34.700: INFO: Created: latency-svc-6nz9m
    Apr 21 21:40:34.707: INFO: Created: latency-svc-xhz4g
    Apr 21 21:40:34.710: INFO: Created: latency-svc-hq7gb
    Apr 21 21:40:34.717: INFO: Created: latency-svc-xj5n4
    Apr 21 21:40:34.732: INFO: Created: latency-svc-mbvbs
    Apr 21 21:40:34.734: INFO: Created: latency-svc-clg6g
    Apr 21 21:40:34.739: INFO: Created: latency-svc-jn2qj
    Apr 21 21:40:34.744: INFO: Got endpoints: latency-svc-t2stp [210.83462ms]
    Apr 21 21:40:34.745: INFO: Created: latency-svc-7f2gq
    Apr 21 21:40:34.752: INFO: Created: latency-svc-dbhxr
    Apr 21 21:40:34.759: INFO: Created: latency-svc-d9xd9
    Apr 21 21:40:34.765: INFO: Created: latency-svc-qggwg
    Apr 21 21:40:34.794: INFO: Got endpoints: latency-svc-r9sjf [250.388151ms]
    Apr 21 21:40:34.802: INFO: Created: latency-svc-rn9g5
    Apr 21 21:40:34.845: INFO: Got endpoints: latency-svc-ph5ql [296.611315ms]
    Apr 21 21:40:34.853: INFO: Created: latency-svc-lhwwh
    Apr 21 21:40:34.894: INFO: Got endpoints: latency-svc-b2xtn [339.31897ms]
    Apr 21 21:40:34.902: INFO: Created: latency-svc-c67rm
    Apr 21 21:40:34.944: INFO: Got endpoints: latency-svc-njfp6 [378.203166ms]
    Apr 21 21:40:34.956: INFO: Created: latency-svc-zzsrk
    Apr 21 21:40:34.995: INFO: Got endpoints: latency-svc-6nz9m [424.653948ms]
    Apr 21 21:40:35.003: INFO: Created: latency-svc-vp9cp
    Apr 21 21:40:35.044: INFO: Got endpoints: latency-svc-xhz4g [465.854493ms]
    Apr 21 21:40:35.052: INFO: Created: latency-svc-26vnj
    Apr 21 21:40:35.094: INFO: Got endpoints: latency-svc-hq7gb [508.357466ms]
    Apr 21 21:40:35.103: INFO: Created: latency-svc-8l26n
    Apr 21 21:40:35.146: INFO: Got endpoints: latency-svc-xj5n4 [553.678347ms]
    Apr 21 21:40:35.155: INFO: Created: latency-svc-fvwrp
    Apr 21 21:40:35.197: INFO: Got endpoints: latency-svc-mbvbs [596.758326ms]
    Apr 21 21:40:35.206: INFO: Created: latency-svc-kmh4h
    Apr 21 21:40:35.243: INFO: Got endpoints: latency-svc-clg6g [604.359378ms]
    Apr 21 21:40:35.253: INFO: Created: latency-svc-vbzkr
    Apr 21 21:40:35.293: INFO: Got endpoints: latency-svc-jn2qj [651.106916ms]
    Apr 21 21:40:35.304: INFO: Created: latency-svc-qjpl6
    Apr 21 21:40:35.344: INFO: Got endpoints: latency-svc-7f2gq [691.787548ms]
    Apr 21 21:40:35.353: INFO: Created: latency-svc-l2b2r
    Apr 21 21:40:35.394: INFO: Got endpoints: latency-svc-dbhxr [734.642321ms]
    Apr 21 21:40:35.406: INFO: Created: latency-svc-b6hsf
    Apr 21 21:40:35.445: INFO: Got endpoints: latency-svc-d9xd9 [751.549474ms]
    Apr 21 21:40:35.453: INFO: Created: latency-svc-p7bv2
    Apr 21 21:40:35.494: INFO: Got endpoints: latency-svc-qggwg [749.732238ms]
    Apr 21 21:40:35.503: INFO: Created: latency-svc-xrgcc
    Apr 21 21:40:35.546: INFO: Got endpoints: latency-svc-rn9g5 [752.341308ms]
    Apr 21 21:40:35.555: INFO: Created: latency-svc-qj6dt
    Apr 21 21:40:35.594: INFO: Got endpoints: latency-svc-lhwwh [749.097554ms]
    Apr 21 21:40:35.603: INFO: Created: latency-svc-c5j57
    Apr 21 21:40:35.644: INFO: Got endpoints: latency-svc-c67rm [750.538037ms]
    Apr 21 21:40:35.654: INFO: Created: latency-svc-525d8
    Apr 21 21:40:35.695: INFO: Got endpoints: latency-svc-zzsrk [751.474086ms]
    Apr 21 21:40:35.705: INFO: Created: latency-svc-rrn2n
    Apr 21 21:40:35.744: INFO: Got endpoints: latency-svc-vp9cp [749.139181ms]
    Apr 21 21:40:35.752: INFO: Created: latency-svc-dvhcn
    Apr 21 21:40:35.794: INFO: Got endpoints: latency-svc-26vnj [750.289155ms]
    Apr 21 21:40:35.804: INFO: Created: latency-svc-jdnng
    Apr 21 21:40:35.844: INFO: Got endpoints: latency-svc-8l26n [750.62257ms]
    Apr 21 21:40:35.857: INFO: Created: latency-svc-n69kq
    Apr 21 21:40:35.895: INFO: Got endpoints: latency-svc-fvwrp [749.00828ms]
    Apr 21 21:40:35.903: INFO: Created: latency-svc-brz52
    Apr 21 21:40:35.944: INFO: Got endpoints: latency-svc-kmh4h [746.674804ms]
    Apr 21 21:40:35.953: INFO: Created: latency-svc-gkvmz
    Apr 21 21:40:35.994: INFO: Got endpoints: latency-svc-vbzkr [751.194251ms]
    Apr 21 21:40:36.002: INFO: Created: latency-svc-dwd4w
    Apr 21 21:40:36.046: INFO: Got endpoints: latency-svc-qjpl6 [752.591897ms]
    Apr 21 21:40:36.054: INFO: Created: latency-svc-775cj
    Apr 21 21:40:36.094: INFO: Got endpoints: latency-svc-l2b2r [750.568122ms]
    Apr 21 21:40:36.104: INFO: Created: latency-svc-gclwm
    Apr 21 21:40:36.143: INFO: Got endpoints: latency-svc-b6hsf [748.974714ms]
    Apr 21 21:40:36.153: INFO: Created: latency-svc-ft5mh
    Apr 21 21:40:36.194: INFO: Got endpoints: latency-svc-p7bv2 [749.690948ms]
    Apr 21 21:40:36.203: INFO: Created: latency-svc-wn4xd
    Apr 21 21:40:36.248: INFO: Got endpoints: latency-svc-xrgcc [753.312591ms]
    Apr 21 21:40:36.258: INFO: Created: latency-svc-xh58x
    Apr 21 21:40:36.293: INFO: Got endpoints: latency-svc-qj6dt [747.266715ms]
    Apr 21 21:40:36.304: INFO: Created: latency-svc-9s242
    Apr 21 21:40:36.346: INFO: Got endpoints: latency-svc-c5j57 [751.745914ms]
    Apr 21 21:40:36.374: INFO: Created: latency-svc-cjqgw
    Apr 21 21:40:36.432: INFO: Got endpoints: latency-svc-525d8 [788.222905ms]
    Apr 21 21:40:36.441: INFO: Created: latency-svc-fdkwk
    Apr 21 21:40:36.443: INFO: Got endpoints: latency-svc-rrn2n [747.558695ms]
    Apr 21 21:40:36.451: INFO: Created: latency-svc-lq4xz
    Apr 21 21:40:36.493: INFO: Got endpoints: latency-svc-dvhcn [749.247992ms]
    Apr 21 21:40:36.501: INFO: Created: latency-svc-d6p6r
    Apr 21 21:40:36.544: INFO: Got endpoints: latency-svc-jdnng [750.577262ms]
    Apr 21 21:40:36.552: INFO: Created: latency-svc-v4tdn
    Apr 21 21:40:36.593: INFO: Got endpoints: latency-svc-n69kq [748.451859ms]
    Apr 21 21:40:36.601: INFO: Created: latency-svc-5p84j
    Apr 21 21:40:36.644: INFO: Got endpoints: latency-svc-brz52 [749.033567ms]
    Apr 21 21:40:36.652: INFO: Created: latency-svc-m7c9l
    Apr 21 21:40:36.695: INFO: Got endpoints: latency-svc-gkvmz [751.501698ms]
    Apr 21 21:40:36.703: INFO: Created: latency-svc-zl7ql
    Apr 21 21:40:36.744: INFO: Got endpoints: latency-svc-dwd4w [749.827459ms]
    Apr 21 21:40:36.752: INFO: Created: latency-svc-8zl4q
    Apr 21 21:40:36.794: INFO: Got endpoints: latency-svc-775cj [748.532449ms]
    Apr 21 21:40:36.803: INFO: Created: latency-svc-9jld8
    Apr 21 21:40:36.845: INFO: Got endpoints: latency-svc-gclwm [751.206959ms]
    Apr 21 21:40:36.854: INFO: Created: latency-svc-6jcmx
    Apr 21 21:40:36.895: INFO: Got endpoints: latency-svc-ft5mh [752.0514ms]
    Apr 21 21:40:36.909: INFO: Created: latency-svc-qmfks
    Apr 21 21:40:36.944: INFO: Got endpoints: latency-svc-wn4xd [749.550387ms]
    Apr 21 21:40:36.953: INFO: Created: latency-svc-k4qgh
    Apr 21 21:40:36.993: INFO: Got endpoints: latency-svc-xh58x [745.906215ms]
    Apr 21 21:40:37.002: INFO: Created: latency-svc-22ts5
    Apr 21 21:40:37.045: INFO: Got endpoints: latency-svc-9s242 [751.959209ms]
    Apr 21 21:40:37.054: INFO: Created: latency-svc-65bgr
    Apr 21 21:40:37.094: INFO: Got endpoints: latency-svc-cjqgw [748.817311ms]
    Apr 21 21:40:37.104: INFO: Created: latency-svc-cgljf
    Apr 21 21:40:37.144: INFO: Got endpoints: latency-svc-fdkwk [711.556509ms]
    Apr 21 21:40:37.153: INFO: Created: latency-svc-9bkhb
    Apr 21 21:40:37.195: INFO: Got endpoints: latency-svc-lq4xz [752.28254ms]
    Apr 21 21:40:37.203: INFO: Created: latency-svc-wzx7c
    Apr 21 21:40:37.244: INFO: Got endpoints: latency-svc-d6p6r [750.135895ms]
    Apr 21 21:40:37.252: INFO: Created: latency-svc-p6fxk
    Apr 21 21:40:37.293: INFO: Got endpoints: latency-svc-v4tdn [748.773809ms]
    Apr 21 21:40:37.301: INFO: Created: latency-svc-744f7
    Apr 21 21:40:37.347: INFO: Got endpoints: latency-svc-5p84j [753.618494ms]
    Apr 21 21:40:37.356: INFO: Created: latency-svc-csthv
    Apr 21 21:40:37.397: INFO: Got endpoints: latency-svc-m7c9l [752.765397ms]
    Apr 21 21:40:37.405: INFO: Created: latency-svc-flxw4
    Apr 21 21:40:37.444: INFO: Got endpoints: latency-svc-zl7ql [748.581385ms]
    Apr 21 21:40:37.453: INFO: Created: latency-svc-xp642
    Apr 21 21:40:37.494: INFO: Got endpoints: latency-svc-8zl4q [749.844213ms]
    Apr 21 21:40:37.502: INFO: Created: latency-svc-bv2z9
    Apr 21 21:40:37.545: INFO: Got endpoints: latency-svc-9jld8 [750.55269ms]
    Apr 21 21:40:37.553: INFO: Created: latency-svc-rn8hn
    Apr 21 21:40:37.594: INFO: Got endpoints: latency-svc-6jcmx [748.343151ms]
    Apr 21 21:40:37.605: INFO: Created: latency-svc-kvhmr
    Apr 21 21:40:37.644: INFO: Got endpoints: latency-svc-qmfks [748.414989ms]
    Apr 21 21:40:37.653: INFO: Created: latency-svc-g2qd7
    Apr 21 21:40:37.694: INFO: Got endpoints: latency-svc-k4qgh [749.659255ms]
    Apr 21 21:40:37.701: INFO: Created: latency-svc-ll6f2
    Apr 21 21:40:37.744: INFO: Got endpoints: latency-svc-22ts5 [750.174489ms]
    Apr 21 21:40:37.752: INFO: Created: latency-svc-zn7mx
    Apr 21 21:40:37.794: INFO: Got endpoints: latency-svc-65bgr [749.006396ms]
    Apr 21 21:40:37.803: INFO: Created: latency-svc-cxmqh
    Apr 21 21:40:37.844: INFO: Got endpoints: latency-svc-cgljf [749.593103ms]
    Apr 21 21:40:37.853: INFO: Created: latency-svc-264d5
    Apr 21 21:40:37.894: INFO: Got endpoints: latency-svc-9bkhb [749.949441ms]
    Apr 21 21:40:37.903: INFO: Created: latency-svc-td8fv
    Apr 21 21:40:37.943: INFO: Got endpoints: latency-svc-wzx7c [748.243727ms]
    Apr 21 21:40:37.951: INFO: Created: latency-svc-z6glv
    Apr 21 21:40:38.037: INFO: Got endpoints: latency-svc-p6fxk [793.523014ms]
    Apr 21 21:40:38.046: INFO: Got endpoints: latency-svc-744f7 [752.217605ms]
    Apr 21 21:40:38.046: INFO: Created: latency-svc-9cmbh
    Apr 21 21:40:38.055: INFO: Created: latency-svc-8sjph
    Apr 21 21:40:38.137: INFO: Got endpoints: latency-svc-csthv [790.091232ms]
    Apr 21 21:40:38.144: INFO: Got endpoints: latency-svc-flxw4 [747.593708ms]
    Apr 21 21:40:38.150: INFO: Created: latency-svc-kxp8j
    Apr 21 21:40:38.158: INFO: Created: latency-svc-vtwvx
    Apr 21 21:40:38.235: INFO: Got endpoints: latency-svc-xp642 [791.277256ms]
    Apr 21 21:40:38.246: INFO: Got endpoints: latency-svc-bv2z9 [752.08629ms]
    Apr 21 21:40:38.250: INFO: Created: latency-svc-hm7m8
    Apr 21 21:40:38.257: INFO: Created: latency-svc-tq4qf
    Apr 21 21:40:38.295: INFO: Got endpoints: latency-svc-rn8hn [749.81765ms]
    Apr 21 21:40:38.303: INFO: Created: latency-svc-n7dks
    Apr 21 21:40:38.345: INFO: Got endpoints: latency-svc-kvhmr [751.511819ms]
    Apr 21 21:40:38.355: INFO: Created: latency-svc-xbwtm
    Apr 21 21:40:38.393: INFO: Got endpoints: latency-svc-g2qd7 [749.399959ms]
    Apr 21 21:40:38.402: INFO: Created: latency-svc-b44fw
    Apr 21 21:40:38.444: INFO: Got endpoints: latency-svc-ll6f2 [750.189689ms]
    Apr 21 21:40:38.454: INFO: Created: latency-svc-57tvr
    Apr 21 21:40:38.495: INFO: Got endpoints: latency-svc-zn7mx [751.639877ms]
    Apr 21 21:40:38.503: INFO: Created: latency-svc-vs4m7
    Apr 21 21:40:38.545: INFO: Got endpoints: latency-svc-cxmqh [750.416221ms]
    Apr 21 21:40:38.557: INFO: Created: latency-svc-vpzn8
    Apr 21 21:40:38.593: INFO: Got endpoints: latency-svc-264d5 [749.262028ms]
    Apr 21 21:40:38.601: INFO: Created: latency-svc-96x7v
    Apr 21 21:40:38.645: INFO: Got endpoints: latency-svc-td8fv [750.974438ms]
    Apr 21 21:40:38.653: INFO: Created: latency-svc-dpxrk
    Apr 21 21:40:38.693: INFO: Got endpoints: latency-svc-z6glv [749.917433ms]
    Apr 21 21:40:38.701: INFO: Created: latency-svc-n9n4m
    Apr 21 21:40:38.743: INFO: Got endpoints: latency-svc-9cmbh [706.144072ms]
    Apr 21 21:40:38.751: INFO: Created: latency-svc-tdprr
    Apr 21 21:40:38.795: INFO: Got endpoints: latency-svc-8sjph [749.844685ms]
    Apr 21 21:40:38.803: INFO: Created: latency-svc-8c2x2
    Apr 21 21:40:38.844: INFO: Got endpoints: latency-svc-kxp8j [706.727199ms]
    Apr 21 21:40:38.852: INFO: Created: latency-svc-s6nzx
    Apr 21 21:40:38.893: INFO: Got endpoints: latency-svc-vtwvx [748.95684ms]
    Apr 21 21:40:38.902: INFO: Created: latency-svc-kxmbm
    Apr 21 21:40:38.944: INFO: Got endpoints: latency-svc-hm7m8 [708.313566ms]
    Apr 21 21:40:38.952: INFO: Created: latency-svc-79kjt
    Apr 21 21:40:38.993: INFO: Got endpoints: latency-svc-tq4qf [747.389101ms]
    Apr 21 21:40:39.003: INFO: Created: latency-svc-5g7ll
    Apr 21 21:40:39.044: INFO: Got endpoints: latency-svc-n7dks [749.430784ms]
    Apr 21 21:40:39.053: INFO: Created: latency-svc-tkp47
    Apr 21 21:40:39.093: INFO: Got endpoints: latency-svc-xbwtm [747.7173ms]
    Apr 21 21:40:39.101: INFO: Created: latency-svc-mrblg
    Apr 21 21:40:39.145: INFO: Got endpoints: latency-svc-b44fw [752.050937ms]
    Apr 21 21:40:39.155: INFO: Created: latency-svc-jdwc2
    Apr 21 21:40:39.195: INFO: Got endpoints: latency-svc-57tvr [751.368287ms]
    Apr 21 21:40:39.204: INFO: Created: latency-svc-jhg24
    Apr 21 21:40:39.244: INFO: Got endpoints: latency-svc-vs4m7 [748.627007ms]
    Apr 21 21:40:39.252: INFO: Created: latency-svc-n2ssx
    Apr 21 21:40:39.294: INFO: Got endpoints: latency-svc-vpzn8 [749.116928ms]
    Apr 21 21:40:39.301: INFO: Created: latency-svc-zwmxm
    Apr 21 21:40:39.346: INFO: Got endpoints: latency-svc-96x7v [752.228322ms]
    Apr 21 21:40:39.353: INFO: Created: latency-svc-fn7dn
    Apr 21 21:40:39.393: INFO: Got endpoints: latency-svc-dpxrk [748.360468ms]
    Apr 21 21:40:39.402: INFO: Created: latency-svc-bb55f
    Apr 21 21:40:39.444: INFO: Got endpoints: latency-svc-n9n4m [750.282649ms]
    Apr 21 21:40:39.452: INFO: Created: latency-svc-bfhs4
    Apr 21 21:40:39.495: INFO: Got endpoints: latency-svc-tdprr [751.690734ms]
    Apr 21 21:40:39.503: INFO: Created: latency-svc-l46d4
    Apr 21 21:40:39.544: INFO: Got endpoints: latency-svc-8c2x2 [748.182659ms]
    Apr 21 21:40:39.552: INFO: Created: latency-svc-txt8x
    Apr 21 21:40:39.594: INFO: Got endpoints: latency-svc-s6nzx [749.962185ms]
    Apr 21 21:40:39.605: INFO: Created: latency-svc-frjhw
    Apr 21 21:40:39.644: INFO: Got endpoints: latency-svc-kxmbm [750.579854ms]
    Apr 21 21:40:39.657: INFO: Created: latency-svc-zbrrn
    Apr 21 21:40:39.694: INFO: Got endpoints: latency-svc-79kjt [750.48363ms]
    Apr 21 21:40:39.704: INFO: Created: latency-svc-xl5jm
    Apr 21 21:40:39.744: INFO: Got endpoints: latency-svc-5g7ll [750.837105ms]
    Apr 21 21:40:39.752: INFO: Created: latency-svc-zkdqp
    Apr 21 21:40:39.794: INFO: Got endpoints: latency-svc-tkp47 [749.48076ms]
    Apr 21 21:40:39.802: INFO: Created: latency-svc-tg64f
    Apr 21 21:40:39.844: INFO: Got endpoints: latency-svc-mrblg [750.640178ms]
    Apr 21 21:40:39.852: INFO: Created: latency-svc-jg5zq
    Apr 21 21:40:39.896: INFO: Got endpoints: latency-svc-jdwc2 [750.796957ms]
    Apr 21 21:40:39.904: INFO: Created: latency-svc-mjhgl
    Apr 21 21:40:39.943: INFO: Got endpoints: latency-svc-jhg24 [748.080601ms]
    Apr 21 21:40:39.952: INFO: Created: latency-svc-kh7tr
    Apr 21 21:40:39.993: INFO: Got endpoints: latency-svc-n2ssx [749.16897ms]
    Apr 21 21:40:40.003: INFO: Created: latency-svc-8f4gx
    Apr 21 21:40:40.044: INFO: Got endpoints: latency-svc-zwmxm [749.974513ms]
    Apr 21 21:40:40.052: INFO: Created: latency-svc-7qqk5
    Apr 21 21:40:40.096: INFO: Got endpoints: latency-svc-fn7dn [749.953172ms]
    Apr 21 21:40:40.105: INFO: Created: latency-svc-5h99t
    Apr 21 21:40:40.143: INFO: Got endpoints: latency-svc-bb55f [750.077375ms]
    Apr 21 21:40:40.153: INFO: Created: latency-svc-9sszq
    Apr 21 21:40:40.194: INFO: Got endpoints: latency-svc-bfhs4 [750.4227ms]
    Apr 21 21:40:40.207: INFO: Created: latency-svc-cd5wv
    Apr 21 21:40:40.245: INFO: Got endpoints: latency-svc-l46d4 [749.430581ms]
    Apr 21 21:40:40.253: INFO: Created: latency-svc-wn8gw
    Apr 21 21:40:40.294: INFO: Got endpoints: latency-svc-txt8x [750.333242ms]
    Apr 21 21:40:40.302: INFO: Created: latency-svc-qwzz2
    Apr 21 21:40:40.344: INFO: Got endpoints: latency-svc-frjhw [750.167003ms]
    Apr 21 21:40:40.352: INFO: Created: latency-svc-jdldg
    Apr 21 21:40:40.395: INFO: Got endpoints: latency-svc-zbrrn [751.336447ms]
    Apr 21 21:40:40.403: INFO: Created: latency-svc-ld9b2
    Apr 21 21:40:40.445: INFO: Got endpoints: latency-svc-xl5jm [751.240057ms]
    Apr 21 21:40:40.453: INFO: Created: latency-svc-gbgnq
    Apr 21 21:40:40.493: INFO: Got endpoints: latency-svc-zkdqp [749.016734ms]
    Apr 21 21:40:40.502: INFO: Created: latency-svc-rtzpk
    Apr 21 21:40:40.544: INFO: Got endpoints: latency-svc-tg64f [750.206566ms]
    Apr 21 21:40:40.552: INFO: Created: latency-svc-czskp
    Apr 21 21:40:40.596: INFO: Got endpoints: latency-svc-jg5zq [751.899122ms]
    Apr 21 21:40:40.604: INFO: Created: latency-svc-9hxk2
    Apr 21 21:40:40.644: INFO: Got endpoints: latency-svc-mjhgl [747.807622ms]
    Apr 21 21:40:40.653: INFO: Created: latency-svc-2zm98
    Apr 21 21:40:40.694: INFO: Got endpoints: latency-svc-kh7tr [750.244672ms]
    Apr 21 21:40:40.702: INFO: Created: latency-svc-mfr8w
    Apr 21 21:40:40.746: INFO: Got endpoints: latency-svc-8f4gx [752.851348ms]
    Apr 21 21:40:40.754: INFO: Created: latency-svc-nkxbg
    Apr 21 21:40:40.794: INFO: Got endpoints: latency-svc-7qqk5 [749.462045ms]
    Apr 21 21:40:40.803: INFO: Created: latency-svc-26dzd
    Apr 21 21:40:40.844: INFO: Got endpoints: latency-svc-5h99t [748.350709ms]
    Apr 21 21:40:40.856: INFO: Created: latency-svc-h97vs
    Apr 21 21:40:40.895: INFO: Got endpoints: latency-svc-9sszq [751.505967ms]
    Apr 21 21:40:40.903: INFO: Created: latency-svc-vlbk7
    Apr 21 21:40:40.944: INFO: Got endpoints: latency-svc-cd5wv [750.122917ms]
    Apr 21 21:40:40.955: INFO: Created: latency-svc-7wqxw
    Apr 21 21:40:40.993: INFO: Got endpoints: latency-svc-wn8gw [748.853743ms]
    Apr 21 21:40:41.002: INFO: Created: latency-svc-gv487
    Apr 21 21:40:41.046: INFO: Got endpoints: latency-svc-qwzz2 [751.828692ms]
    Apr 21 21:40:41.058: INFO: Created: latency-svc-g4jg8
    Apr 21 21:40:41.094: INFO: Got endpoints: latency-svc-jdldg [749.767572ms]
    Apr 21 21:40:41.103: INFO: Created: latency-svc-pftl8
    Apr 21 21:40:41.144: INFO: Got endpoints: latency-svc-ld9b2 [748.452298ms]
    Apr 21 21:40:41.154: INFO: Created: latency-svc-r5wx8
    Apr 21 21:40:41.193: INFO: Got endpoints: latency-svc-gbgnq [748.080152ms]
    Apr 21 21:40:41.203: INFO: Created: latency-svc-dsllv
    Apr 21 21:40:41.244: INFO: Got endpoints: latency-svc-rtzpk [750.76711ms]
    Apr 21 21:40:41.252: INFO: Created: latency-svc-hvcz4
    Apr 21 21:40:41.294: INFO: Got endpoints: latency-svc-czskp [749.847131ms]
    Apr 21 21:40:41.304: INFO: Created: latency-svc-rkkdd
    Apr 21 21:40:41.343: INFO: Got endpoints: latency-svc-9hxk2 [747.562134ms]
    Apr 21 21:40:41.351: INFO: Created: latency-svc-bnn5k
    Apr 21 21:40:41.396: INFO: Got endpoints: latency-svc-2zm98 [751.51323ms]
    Apr 21 21:40:41.404: INFO: Created: latency-svc-pvpnm
    Apr 21 21:40:41.443: INFO: Got endpoints: latency-svc-mfr8w [749.760638ms]
    Apr 21 21:40:41.454: INFO: Created: latency-svc-6qwnw
    Apr 21 21:40:41.494: INFO: Got endpoints: latency-svc-nkxbg [747.982986ms]
    Apr 21 21:40:41.504: INFO: Created: latency-svc-w5j5g
    Apr 21 21:40:41.543: INFO: Got endpoints: latency-svc-26dzd [749.911956ms]
    Apr 21 21:40:41.552: INFO: Created: latency-svc-wkk8z
    Apr 21 21:40:41.595: INFO: Got endpoints: latency-svc-h97vs [750.761574ms]
    Apr 21 21:40:41.605: INFO: Created: latency-svc-vdwfz
    Apr 21 21:40:41.645: INFO: Got endpoints: latency-svc-vlbk7 [750.016958ms]
    Apr 21 21:40:41.653: INFO: Created: latency-svc-zqmgz
    Apr 21 21:40:41.694: INFO: Got endpoints: latency-svc-7wqxw [749.945318ms]
    Apr 21 21:40:41.702: INFO: Created: latency-svc-djqmp
    Apr 21 21:40:41.745: INFO: Got endpoints: latency-svc-gv487 [751.166979ms]
    Apr 21 21:40:41.752: INFO: Created: latency-svc-swtb7
    Apr 21 21:40:41.793: INFO: Got endpoints: latency-svc-g4jg8 [747.170878ms]
    Apr 21 21:40:41.801: INFO: Created: latency-svc-6dsnw
    Apr 21 21:40:41.843: INFO: Got endpoints: latency-svc-pftl8 [749.536486ms]
    Apr 21 21:40:41.853: INFO: Created: latency-svc-ncldv
    Apr 21 21:40:41.894: INFO: Got endpoints: latency-svc-r5wx8 [749.970872ms]
    Apr 21 21:40:41.902: INFO: Created: latency-svc-hn26l
    Apr 21 21:40:41.946: INFO: Got endpoints: latency-svc-dsllv [752.1528ms]
    Apr 21 21:40:41.955: INFO: Created: latency-svc-ht4p9
    Apr 21 21:40:41.996: INFO: Got endpoints: latency-svc-hvcz4 [752.254398ms]
    Apr 21 21:40:42.045: INFO: Got endpoints: latency-svc-rkkdd [750.808262ms]
    Apr 21 21:40:42.094: INFO: Got endpoints: latency-svc-bnn5k [750.759299ms]
    Apr 21 21:40:42.145: INFO: Got endpoints: latency-svc-pvpnm [749.809097ms]
    Apr 21 21:40:42.194: INFO: Got endpoints: latency-svc-6qwnw [750.733285ms]
    Apr 21 21:40:42.244: INFO: Got endpoints: latency-svc-w5j5g [749.665834ms]
    Apr 21 21:40:42.294: INFO: Got endpoints: latency-svc-wkk8z [750.280691ms]
    Apr 21 21:40:42.344: INFO: Got endpoints: latency-svc-vdwfz [749.60914ms]
    Apr 21 21:40:42.394: INFO: Got endpoints: latency-svc-zqmgz [748.617376ms]
    Apr 21 21:40:42.444: INFO: Got endpoints: latency-svc-djqmp [749.47014ms]
    Apr 21 21:40:42.496: INFO: Got endpoints: latency-svc-swtb7 [751.122288ms]
    Apr 21 21:40:42.543: INFO: Got endpoints: latency-svc-6dsnw [750.286408ms]
    Apr 21 21:40:42.594: INFO: Got endpoints: latency-svc-ncldv [750.875386ms]
    Apr 21 21:40:42.643: INFO: Got endpoints: latency-svc-hn26l [749.623223ms]
    Apr 21 21:40:42.694: INFO: Got endpoints: latency-svc-ht4p9 [748.361874ms]
    Apr 21 21:40:42.694: INFO: Latencies: [15.51287ms 20.751325ms 29.015513ms 37.168915ms 43.598539ms 91.283496ms 105.120769ms 110.114034ms 119.026187ms 122.332869ms 126.569147ms 127.041337ms 127.088416ms 128.563926ms 128.652544ms 131.300472ms 135.519493ms 141.643158ms 158.373796ms 161.148374ms 162.837669ms 164.072969ms 186.60452ms 191.8279ms 205.6127ms 210.83462ms 214.087925ms 221.954757ms 231.55716ms 233.584341ms 233.769329ms 234.003267ms 241.482431ms 250.388151ms 257.176677ms 260.879391ms 262.385549ms 281.798084ms 283.49057ms 284.486408ms 285.20617ms 285.834641ms 296.611315ms 339.31897ms 378.203166ms 424.653948ms 465.854493ms 508.357466ms 553.678347ms 596.758326ms 604.359378ms 651.106916ms 691.787548ms 706.144072ms 706.727199ms 708.313566ms 711.556509ms 734.642321ms 745.906215ms 746.674804ms 747.170878ms 747.266715ms 747.389101ms 747.558695ms 747.562134ms 747.593708ms 747.7173ms 747.807622ms 747.982986ms 748.080152ms 748.080601ms 748.182659ms 748.243727ms 748.343151ms 748.350709ms 748.360468ms 748.361874ms 748.414989ms 748.451859ms 748.452298ms 748.532449ms 748.581385ms 748.617376ms 748.627007ms 748.773809ms 748.817311ms 748.853743ms 748.95684ms 748.974714ms 749.006396ms 749.00828ms 749.016734ms 749.033567ms 749.097554ms 749.116928ms 749.139181ms 749.16897ms 749.247992ms 749.262028ms 749.399959ms 749.430581ms 749.430784ms 749.462045ms 749.47014ms 749.48076ms 749.536486ms 749.550387ms 749.593103ms 749.60914ms 749.623223ms 749.659255ms 749.665834ms 749.690948ms 749.732238ms 749.760638ms 749.767572ms 749.809097ms 749.81765ms 749.827459ms 749.844213ms 749.844685ms 749.847131ms 749.911956ms 749.917433ms 749.945318ms 749.949441ms 749.953172ms 749.962185ms 749.970872ms 749.974513ms 750.016958ms 750.077375ms 750.122917ms 750.135895ms 750.167003ms 750.174489ms 750.189689ms 750.206566ms 750.244672ms 750.280691ms 750.282649ms 750.286408ms 750.289155ms 750.333242ms 750.416221ms 750.4227ms 750.48363ms 750.538037ms 750.55269ms 750.568122ms 750.577262ms 750.579854ms 750.62257ms 750.640178ms 750.733285ms 750.759299ms 750.761574ms 750.76711ms 750.796957ms 750.808262ms 750.837105ms 750.875386ms 750.974438ms 751.122288ms 751.166979ms 751.194251ms 751.206959ms 751.240057ms 751.336447ms 751.368287ms 751.474086ms 751.501698ms 751.505967ms 751.511819ms 751.51323ms 751.549474ms 751.639877ms 751.690734ms 751.745914ms 751.828692ms 751.899122ms 751.959209ms 752.050937ms 752.0514ms 752.08629ms 752.1528ms 752.217605ms 752.228322ms 752.254398ms 752.28254ms 752.341308ms 752.591897ms 752.765397ms 752.851348ms 753.312591ms 753.618494ms 788.222905ms 790.091232ms 791.277256ms 793.523014ms]
    Apr 21 21:40:42.694: INFO: 50 %ile: 749.430581ms
    Apr 21 21:40:42.694: INFO: 90 %ile: 751.899122ms
    Apr 21 21:40:42.694: INFO: 99 %ile: 791.277256ms
    Apr 21 21:40:42.694: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:40:42.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-9320" for this suite. 04/21/23 21:40:42.697
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:40:42.701
Apr 21 21:40:42.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replicaset 04/21/23 21:40:42.702
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:42.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:42.71
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 04/21/23 21:40:42.712
Apr 21 21:40:42.716: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-774" to be "running and ready"
Apr 21 21:40:42.718: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.526827ms
Apr 21 21:40:42.718: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:40:44.721: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.004291553s
Apr 21 21:40:44.721: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Apr 21 21:40:44.721: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 04/21/23 21:40:44.723
STEP: Then the orphan pod is adopted 04/21/23 21:40:44.726
STEP: When the matched label of one of its pods change 04/21/23 21:40:45.73
Apr 21 21:40:45.732: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 04/21/23 21:40:45.738
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 21 21:40:46.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-774" for this suite. 04/21/23 21:40:46.745
------------------------------
â€¢ [4.047 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:40:42.701
    Apr 21 21:40:42.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replicaset 04/21/23 21:40:42.702
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:42.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:42.71
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 04/21/23 21:40:42.712
    Apr 21 21:40:42.716: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-774" to be "running and ready"
    Apr 21 21:40:42.718: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 1.526827ms
    Apr 21 21:40:42.718: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:40:44.721: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.004291553s
    Apr 21 21:40:44.721: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Apr 21 21:40:44.721: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 04/21/23 21:40:44.723
    STEP: Then the orphan pod is adopted 04/21/23 21:40:44.726
    STEP: When the matched label of one of its pods change 04/21/23 21:40:45.73
    Apr 21 21:40:45.732: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 04/21/23 21:40:45.738
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:40:46.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-774" for this suite. 04/21/23 21:40:46.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:40:46.75
Apr 21 21:40:46.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 21:40:46.751
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:46.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:46.761
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5337 04/21/23 21:40:46.763
STEP: changing the ExternalName service to type=ClusterIP 04/21/23 21:40:46.765
STEP: creating replication controller externalname-service in namespace services-5337 04/21/23 21:40:46.776
I0421 21:40:46.780432      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5337, replica count: 2
I0421 21:40:49.831645      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 21 21:40:49.831: INFO: Creating new exec pod
Apr 21 21:40:49.835: INFO: Waiting up to 5m0s for pod "execpodgsl54" in namespace "services-5337" to be "running"
Apr 21 21:40:49.837: INFO: Pod "execpodgsl54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497523ms
Apr 21 21:40:51.840: INFO: Pod "execpodgsl54": Phase="Running", Reason="", readiness=true. Elapsed: 2.005472669s
Apr 21 21:40:51.840: INFO: Pod "execpodgsl54" satisfied condition "running"
Apr 21 21:40:56.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-5337 exec execpodgsl54 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Apr 21 21:40:56.978: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 21 21:40:56.978: INFO: stdout: ""
Apr 21 21:40:56.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-5337 exec execpodgsl54 -- /bin/sh -x -c nc -v -z -w 2 10.103.246.238 80'
Apr 21 21:40:57.099: INFO: stderr: "+ nc -v -z -w 2 10.103.246.238 80\nConnection to 10.103.246.238 80 port [tcp/http] succeeded!\n"
Apr 21 21:40:57.099: INFO: stdout: ""
Apr 21 21:40:57.099: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 21:40:57.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5337" for this suite. 04/21/23 21:40:57.117
------------------------------
â€¢ [SLOW TEST] [10.370 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:40:46.75
    Apr 21 21:40:46.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 21:40:46.751
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:46.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:46.761
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5337 04/21/23 21:40:46.763
    STEP: changing the ExternalName service to type=ClusterIP 04/21/23 21:40:46.765
    STEP: creating replication controller externalname-service in namespace services-5337 04/21/23 21:40:46.776
    I0421 21:40:46.780432      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5337, replica count: 2
    I0421 21:40:49.831645      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 21 21:40:49.831: INFO: Creating new exec pod
    Apr 21 21:40:49.835: INFO: Waiting up to 5m0s for pod "execpodgsl54" in namespace "services-5337" to be "running"
    Apr 21 21:40:49.837: INFO: Pod "execpodgsl54": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497523ms
    Apr 21 21:40:51.840: INFO: Pod "execpodgsl54": Phase="Running", Reason="", readiness=true. Elapsed: 2.005472669s
    Apr 21 21:40:51.840: INFO: Pod "execpodgsl54" satisfied condition "running"
    Apr 21 21:40:56.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-5337 exec execpodgsl54 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Apr 21 21:40:56.978: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Apr 21 21:40:56.978: INFO: stdout: ""
    Apr 21 21:40:56.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-5337 exec execpodgsl54 -- /bin/sh -x -c nc -v -z -w 2 10.103.246.238 80'
    Apr 21 21:40:57.099: INFO: stderr: "+ nc -v -z -w 2 10.103.246.238 80\nConnection to 10.103.246.238 80 port [tcp/http] succeeded!\n"
    Apr 21 21:40:57.099: INFO: stdout: ""
    Apr 21 21:40:57.099: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:40:57.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5337" for this suite. 04/21/23 21:40:57.117
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:40:57.12
Apr 21 21:40:57.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 04/21/23 21:40:57.121
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:57.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:57.132
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 04/21/23 21:40:57.134
STEP: Creating hostNetwork=false pod 04/21/23 21:40:57.135
Apr 21 21:40:57.140: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-669" to be "running and ready"
Apr 21 21:40:57.142: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109182ms
Apr 21 21:40:57.142: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:40:59.146: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005282508s
Apr 21 21:40:59.146: INFO: The phase of Pod test-pod is Running (Ready = true)
Apr 21 21:40:59.146: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 04/21/23 21:40:59.147
Apr 21 21:40:59.151: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-669" to be "running and ready"
Apr 21 21:40:59.154: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.932677ms
Apr 21 21:40:59.154: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:41:01.157: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00622441s
Apr 21 21:41:01.157: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Apr 21 21:41:01.157: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 04/21/23 21:41:01.159
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 04/21/23 21:41:01.159
Apr 21 21:41:01.159: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.159: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.159: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 21 21:41:01.224: INFO: Exec stderr: ""
Apr 21 21:41:01.224: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.225: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.225: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 21 21:41:01.288: INFO: Exec stderr: ""
Apr 21 21:41:01.288: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.288: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.289: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 21 21:41:01.356: INFO: Exec stderr: ""
Apr 21 21:41:01.357: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.357: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.357: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 21 21:41:01.399: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 04/21/23 21:41:01.399
Apr 21 21:41:01.399: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.400: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.400: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Apr 21 21:41:01.464: INFO: Exec stderr: ""
Apr 21 21:41:01.464: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.464: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.464: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Apr 21 21:41:01.506: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 04/21/23 21:41:01.506
Apr 21 21:41:01.506: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.506: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.506: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 21 21:41:01.578: INFO: Exec stderr: ""
Apr 21 21:41:01.578: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.578: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.579: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 21 21:41:01.648: INFO: Exec stderr: ""
Apr 21 21:41:01.648: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.648: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.648: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 21 21:41:01.708: INFO: Exec stderr: ""
Apr 21 21:41:01.708: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:01.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:01.708: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:01.709: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 21 21:41:01.776: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Apr 21 21:41:01.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-669" for this suite. 04/21/23 21:41:01.779
------------------------------
â€¢ [4.662 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:40:57.12
    Apr 21 21:40:57.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 04/21/23 21:40:57.121
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:40:57.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:40:57.132
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 04/21/23 21:40:57.134
    STEP: Creating hostNetwork=false pod 04/21/23 21:40:57.135
    Apr 21 21:40:57.140: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-669" to be "running and ready"
    Apr 21 21:40:57.142: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.109182ms
    Apr 21 21:40:57.142: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:40:59.146: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005282508s
    Apr 21 21:40:59.146: INFO: The phase of Pod test-pod is Running (Ready = true)
    Apr 21 21:40:59.146: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 04/21/23 21:40:59.147
    Apr 21 21:40:59.151: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-669" to be "running and ready"
    Apr 21 21:40:59.154: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.932677ms
    Apr 21 21:40:59.154: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:41:01.157: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00622441s
    Apr 21 21:41:01.157: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Apr 21 21:41:01.157: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 04/21/23 21:41:01.159
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 04/21/23 21:41:01.159
    Apr 21 21:41:01.159: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.159: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.159: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 21 21:41:01.224: INFO: Exec stderr: ""
    Apr 21 21:41:01.224: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.225: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.225: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 21 21:41:01.288: INFO: Exec stderr: ""
    Apr 21 21:41:01.288: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.288: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.288: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.289: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 21 21:41:01.356: INFO: Exec stderr: ""
    Apr 21 21:41:01.357: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.357: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.357: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 21 21:41:01.399: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 04/21/23 21:41:01.399
    Apr 21 21:41:01.399: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.400: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.400: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Apr 21 21:41:01.464: INFO: Exec stderr: ""
    Apr 21 21:41:01.464: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.464: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.464: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Apr 21 21:41:01.506: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 04/21/23 21:41:01.506
    Apr 21 21:41:01.506: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.506: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.506: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 21 21:41:01.578: INFO: Exec stderr: ""
    Apr 21 21:41:01.578: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.578: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.579: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 21 21:41:01.648: INFO: Exec stderr: ""
    Apr 21 21:41:01.648: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.648: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.648: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 21 21:41:01.708: INFO: Exec stderr: ""
    Apr 21 21:41:01.708: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-669 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:01.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:01.708: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:01.709: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-669/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 21 21:41:01.776: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:41:01.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-669" for this suite. 04/21/23 21:41:01.779
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:41:01.783
Apr 21 21:41:01.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replication-controller 04/21/23 21:41:01.784
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:01.792
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:01.794
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 04/21/23 21:41:01.796
Apr 21 21:41:01.800: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6320" to be "running and ready"
Apr 21 21:41:01.802: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.513126ms
Apr 21 21:41:01.802: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:41:03.806: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.005562198s
Apr 21 21:41:03.806: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Apr 21 21:41:03.806: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 04/21/23 21:41:03.807
STEP: Then the orphan pod is adopted 04/21/23 21:41:03.811
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 21 21:41:04.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6320" for this suite. 04/21/23 21:41:04.819
------------------------------
â€¢ [3.041 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:41:01.783
    Apr 21 21:41:01.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replication-controller 04/21/23 21:41:01.784
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:01.792
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:01.794
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 04/21/23 21:41:01.796
    Apr 21 21:41:01.800: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-6320" to be "running and ready"
    Apr 21 21:41:01.802: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 1.513126ms
    Apr 21 21:41:01.802: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:41:03.806: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.005562198s
    Apr 21 21:41:03.806: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Apr 21 21:41:03.806: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 04/21/23 21:41:03.807
    STEP: Then the orphan pod is adopted 04/21/23 21:41:03.811
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:41:04.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6320" for this suite. 04/21/23 21:41:04.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:41:04.825
Apr 21 21:41:04.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 21:41:04.825
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:04.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:04.836
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-3edb0d9d-f002-49b9-a87c-28dc8d36d5d1 04/21/23 21:41:04.838
STEP: Creating a pod to test consume configMaps 04/21/23 21:41:04.841
Apr 21 21:41:04.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22" in namespace "configmap-6167" to be "Succeeded or Failed"
Apr 21 21:41:04.848: INFO: Pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22": Phase="Pending", Reason="", readiness=false. Elapsed: 1.676184ms
Apr 21 21:41:06.851: INFO: Pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004687387s
Apr 21 21:41:08.851: INFO: Pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004855469s
STEP: Saw pod success 04/21/23 21:41:08.851
Apr 21 21:41:08.851: INFO: Pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22" satisfied condition "Succeeded or Failed"
Apr 21 21:41:08.853: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 21:41:08.859
Apr 21 21:41:08.865: INFO: Waiting for pod pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22 to disappear
Apr 21 21:41:08.866: INFO: Pod pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:41:08.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6167" for this suite. 04/21/23 21:41:08.868
------------------------------
â€¢ [4.047 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:41:04.825
    Apr 21 21:41:04.825: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 21:41:04.825
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:04.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:04.836
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-3edb0d9d-f002-49b9-a87c-28dc8d36d5d1 04/21/23 21:41:04.838
    STEP: Creating a pod to test consume configMaps 04/21/23 21:41:04.841
    Apr 21 21:41:04.846: INFO: Waiting up to 5m0s for pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22" in namespace "configmap-6167" to be "Succeeded or Failed"
    Apr 21 21:41:04.848: INFO: Pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22": Phase="Pending", Reason="", readiness=false. Elapsed: 1.676184ms
    Apr 21 21:41:06.851: INFO: Pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004687387s
    Apr 21 21:41:08.851: INFO: Pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004855469s
    STEP: Saw pod success 04/21/23 21:41:08.851
    Apr 21 21:41:08.851: INFO: Pod "pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22" satisfied condition "Succeeded or Failed"
    Apr 21 21:41:08.853: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 21:41:08.859
    Apr 21 21:41:08.865: INFO: Waiting for pod pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22 to disappear
    Apr 21 21:41:08.866: INFO: Pod pod-configmaps-fb47e9e1-8ae9-4b09-b138-f77d47cd6c22 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:41:08.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6167" for this suite. 04/21/23 21:41:08.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:41:08.872
Apr 21 21:41:08.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename emptydir 04/21/23 21:41:08.873
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:08.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:08.881
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 04/21/23 21:41:08.883
Apr 21 21:41:08.887: INFO: Waiting up to 5m0s for pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c" in namespace "emptydir-4863" to be "Succeeded or Failed"
Apr 21 21:41:08.888: INFO: Pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.437236ms
Apr 21 21:41:10.891: INFO: Pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004419552s
Apr 21 21:41:12.892: INFO: Pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004519416s
STEP: Saw pod success 04/21/23 21:41:12.892
Apr 21 21:41:12.892: INFO: Pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c" satisfied condition "Succeeded or Failed"
Apr 21 21:41:12.893: INFO: Trying to get logs from node k8sconformance-m02 pod pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c container test-container: <nil>
STEP: delete the pod 04/21/23 21:41:12.898
Apr 21 21:41:12.906: INFO: Waiting for pod pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c to disappear
Apr 21 21:41:12.908: INFO: Pod pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:41:12.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4863" for this suite. 04/21/23 21:41:12.91
------------------------------
â€¢ [4.041 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:41:08.872
    Apr 21 21:41:08.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename emptydir 04/21/23 21:41:08.873
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:08.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:08.881
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 04/21/23 21:41:08.883
    Apr 21 21:41:08.887: INFO: Waiting up to 5m0s for pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c" in namespace "emptydir-4863" to be "Succeeded or Failed"
    Apr 21 21:41:08.888: INFO: Pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.437236ms
    Apr 21 21:41:10.891: INFO: Pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004419552s
    Apr 21 21:41:12.892: INFO: Pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004519416s
    STEP: Saw pod success 04/21/23 21:41:12.892
    Apr 21 21:41:12.892: INFO: Pod "pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c" satisfied condition "Succeeded or Failed"
    Apr 21 21:41:12.893: INFO: Trying to get logs from node k8sconformance-m02 pod pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c container test-container: <nil>
    STEP: delete the pod 04/21/23 21:41:12.898
    Apr 21 21:41:12.906: INFO: Waiting for pod pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c to disappear
    Apr 21 21:41:12.908: INFO: Pod pod-d5135f95-2b8f-412e-92b2-3c78fcd0790c no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:41:12.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4863" for this suite. 04/21/23 21:41:12.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:41:12.913
Apr 21 21:41:12.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 21:41:12.914
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:12.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:12.923
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-7097/configmap-test-c249a2a9-0e9c-46c4-b427-aa1e9ade12ae 04/21/23 21:41:12.924
STEP: Creating a pod to test consume configMaps 04/21/23 21:41:12.927
Apr 21 21:41:12.931: INFO: Waiting up to 5m0s for pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5" in namespace "configmap-7097" to be "Succeeded or Failed"
Apr 21 21:41:12.933: INFO: Pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.430006ms
Apr 21 21:41:14.935: INFO: Pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003807353s
Apr 21 21:41:16.936: INFO: Pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004872192s
STEP: Saw pod success 04/21/23 21:41:16.936
Apr 21 21:41:16.936: INFO: Pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5" satisfied condition "Succeeded or Failed"
Apr 21 21:41:16.938: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5 container env-test: <nil>
STEP: delete the pod 04/21/23 21:41:16.943
Apr 21 21:41:16.951: INFO: Waiting for pod pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5 to disappear
Apr 21 21:41:16.953: INFO: Pod pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:41:16.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7097" for this suite. 04/21/23 21:41:16.955
------------------------------
â€¢ [4.044 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:41:12.913
    Apr 21 21:41:12.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 21:41:12.914
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:12.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:12.923
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-7097/configmap-test-c249a2a9-0e9c-46c4-b427-aa1e9ade12ae 04/21/23 21:41:12.924
    STEP: Creating a pod to test consume configMaps 04/21/23 21:41:12.927
    Apr 21 21:41:12.931: INFO: Waiting up to 5m0s for pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5" in namespace "configmap-7097" to be "Succeeded or Failed"
    Apr 21 21:41:12.933: INFO: Pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 1.430006ms
    Apr 21 21:41:14.935: INFO: Pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003807353s
    Apr 21 21:41:16.936: INFO: Pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004872192s
    STEP: Saw pod success 04/21/23 21:41:16.936
    Apr 21 21:41:16.936: INFO: Pod "pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5" satisfied condition "Succeeded or Failed"
    Apr 21 21:41:16.938: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5 container env-test: <nil>
    STEP: delete the pod 04/21/23 21:41:16.943
    Apr 21 21:41:16.951: INFO: Waiting for pod pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5 to disappear
    Apr 21 21:41:16.953: INFO: Pod pod-configmaps-0db41c21-1310-444f-9fe1-a1b4a2164ec5 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:41:16.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7097" for this suite. 04/21/23 21:41:16.955
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:41:16.96
Apr 21 21:41:16.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 21:41:16.96
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:16.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:16.97
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-3367 04/21/23 21:41:16.972
STEP: creating service affinity-nodeport-transition in namespace services-3367 04/21/23 21:41:16.972
STEP: creating replication controller affinity-nodeport-transition in namespace services-3367 04/21/23 21:41:16.981
I0421 21:41:16.986107      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3367, replica count: 3
I0421 21:41:20.036393      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 21 21:41:20.041: INFO: Creating new exec pod
Apr 21 21:41:20.046: INFO: Waiting up to 5m0s for pod "execpod-affinityj5q99" in namespace "services-3367" to be "running"
Apr 21 21:41:20.047: INFO: Pod "execpod-affinityj5q99": Phase="Pending", Reason="", readiness=false. Elapsed: 1.622544ms
Apr 21 21:41:22.049: INFO: Pod "execpod-affinityj5q99": Phase="Running", Reason="", readiness=true. Elapsed: 2.003540314s
Apr 21 21:41:22.049: INFO: Pod "execpod-affinityj5q99" satisfied condition "running"
Apr 21 21:41:23.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Apr 21 21:41:23.169: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr 21 21:41:23.169: INFO: stdout: ""
Apr 21 21:41:23.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c nc -v -z -w 2 10.102.225.22 80'
Apr 21 21:41:23.299: INFO: stderr: "+ nc -v -z -w 2 10.102.225.22 80\nConnection to 10.102.225.22 80 port [tcp/http] succeeded!\n"
Apr 21 21:41:23.299: INFO: stdout: ""
Apr 21 21:41:23.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c nc -v -z -w 2 192.168.49.2 31861'
Apr 21 21:41:23.419: INFO: stderr: "+ nc -v -z -w 2 192.168.49.2 31861\nConnection to 192.168.49.2 31861 port [tcp/*] succeeded!\n"
Apr 21 21:41:23.419: INFO: stdout: ""
Apr 21 21:41:23.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c nc -v -z -w 2 192.168.49.3 31861'
Apr 21 21:41:23.535: INFO: stderr: "+ nc -v -z -w 2 192.168.49.3 31861\nConnection to 192.168.49.3 31861 port [tcp/*] succeeded!\n"
Apr 21 21:41:23.535: INFO: stdout: ""
Apr 21 21:41:23.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:31861/ ; done'
Apr 21 21:41:23.715: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n"
Apr 21 21:41:23.715: INFO: stdout: "\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-gmpcm\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-gmpcm\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-gmpcm"
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-gmpcm
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-gmpcm
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-gmpcm
Apr 21 21:41:23.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:31861/ ; done'
Apr 21 21:41:23.894: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n"
Apr 21 21:41:23.894: INFO: stdout: "\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554"
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
Apr 21 21:41:23.894: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3367, will wait for the garbage collector to delete the pods 04/21/23 21:41:23.901
Apr 21 21:41:23.957: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.745353ms
Apr 21 21:41:24.058: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.915363ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 21:41:25.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3367" for this suite. 04/21/23 21:41:25.874
------------------------------
â€¢ [SLOW TEST] [8.917 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:41:16.96
    Apr 21 21:41:16.960: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 21:41:16.96
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:16.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:16.97
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-3367 04/21/23 21:41:16.972
    STEP: creating service affinity-nodeport-transition in namespace services-3367 04/21/23 21:41:16.972
    STEP: creating replication controller affinity-nodeport-transition in namespace services-3367 04/21/23 21:41:16.981
    I0421 21:41:16.986107      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-3367, replica count: 3
    I0421 21:41:20.036393      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 21 21:41:20.041: INFO: Creating new exec pod
    Apr 21 21:41:20.046: INFO: Waiting up to 5m0s for pod "execpod-affinityj5q99" in namespace "services-3367" to be "running"
    Apr 21 21:41:20.047: INFO: Pod "execpod-affinityj5q99": Phase="Pending", Reason="", readiness=false. Elapsed: 1.622544ms
    Apr 21 21:41:22.049: INFO: Pod "execpod-affinityj5q99": Phase="Running", Reason="", readiness=true. Elapsed: 2.003540314s
    Apr 21 21:41:22.049: INFO: Pod "execpod-affinityj5q99" satisfied condition "running"
    Apr 21 21:41:23.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Apr 21 21:41:23.169: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Apr 21 21:41:23.169: INFO: stdout: ""
    Apr 21 21:41:23.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c nc -v -z -w 2 10.102.225.22 80'
    Apr 21 21:41:23.299: INFO: stderr: "+ nc -v -z -w 2 10.102.225.22 80\nConnection to 10.102.225.22 80 port [tcp/http] succeeded!\n"
    Apr 21 21:41:23.299: INFO: stdout: ""
    Apr 21 21:41:23.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c nc -v -z -w 2 192.168.49.2 31861'
    Apr 21 21:41:23.419: INFO: stderr: "+ nc -v -z -w 2 192.168.49.2 31861\nConnection to 192.168.49.2 31861 port [tcp/*] succeeded!\n"
    Apr 21 21:41:23.419: INFO: stdout: ""
    Apr 21 21:41:23.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c nc -v -z -w 2 192.168.49.3 31861'
    Apr 21 21:41:23.535: INFO: stderr: "+ nc -v -z -w 2 192.168.49.3 31861\nConnection to 192.168.49.3 31861 port [tcp/*] succeeded!\n"
    Apr 21 21:41:23.535: INFO: stdout: ""
    Apr 21 21:41:23.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:31861/ ; done'
    Apr 21 21:41:23.715: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n"
    Apr 21 21:41:23.715: INFO: stdout: "\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-gmpcm\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-gmpcm\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-vj7x2\naffinity-nodeport-transition-gmpcm"
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-gmpcm
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-gmpcm
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-vj7x2
    Apr 21 21:41:23.715: INFO: Received response from host: affinity-nodeport-transition-gmpcm
    Apr 21 21:41:23.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-3367 exec execpod-affinityj5q99 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:31861/ ; done'
    Apr 21 21:41:23.894: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31861/\n"
    Apr 21 21:41:23.894: INFO: stdout: "\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554\naffinity-nodeport-transition-h7554"
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Received response from host: affinity-nodeport-transition-h7554
    Apr 21 21:41:23.894: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-3367, will wait for the garbage collector to delete the pods 04/21/23 21:41:23.901
    Apr 21 21:41:23.957: INFO: Deleting ReplicationController affinity-nodeport-transition took: 3.745353ms
    Apr 21 21:41:24.058: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.915363ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:41:25.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3367" for this suite. 04/21/23 21:41:25.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:41:25.877
Apr 21 21:41:25.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename daemonsets 04/21/23 21:41:25.878
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:25.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:25.888
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Apr 21 21:41:25.898: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 04/21/23 21:41:25.902
Apr 21 21:41:25.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:41:25.904: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 04/21/23 21:41:25.904
Apr 21 21:41:25.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:41:25.913: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:41:26.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:41:26.917: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:41:27.916: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:41:27.916: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 04/21/23 21:41:27.918
Apr 21 21:41:27.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:41:27.929: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Apr 21 21:41:28.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:41:28.932: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 04/21/23 21:41:28.932
Apr 21 21:41:28.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:41:28.938: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:41:29.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:41:29.941: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:41:30.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:41:30.940: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
Apr 21 21:41:31.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 21 21:41:31.941: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:41:31.944
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-82, will wait for the garbage collector to delete the pods 04/21/23 21:41:31.944
Apr 21 21:41:32.001: INFO: Deleting DaemonSet.extensions daemon-set took: 4.101745ms
Apr 21 21:41:32.101: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.123304ms
Apr 21 21:41:34.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 21 21:41:34.703: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 21 21:41:34.705: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26314"},"items":null}

Apr 21 21:41:34.706: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26314"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:41:34.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-82" for this suite. 04/21/23 21:41:34.719
------------------------------
â€¢ [SLOW TEST] [8.846 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:41:25.877
    Apr 21 21:41:25.877: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename daemonsets 04/21/23 21:41:25.878
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:25.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:25.888
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Apr 21 21:41:25.898: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 04/21/23 21:41:25.902
    Apr 21 21:41:25.904: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:41:25.904: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 04/21/23 21:41:25.904
    Apr 21 21:41:25.913: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:41:25.913: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:41:26.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:41:26.917: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:41:27.916: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:41:27.916: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 04/21/23 21:41:27.918
    Apr 21 21:41:27.929: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:41:27.929: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Apr 21 21:41:28.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:41:28.932: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 04/21/23 21:41:28.932
    Apr 21 21:41:28.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:41:28.938: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:41:29.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:41:29.941: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:41:30.940: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:41:30.940: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
    Apr 21 21:41:31.941: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 21 21:41:31.941: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/21/23 21:41:31.944
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-82, will wait for the garbage collector to delete the pods 04/21/23 21:41:31.944
    Apr 21 21:41:32.001: INFO: Deleting DaemonSet.extensions daemon-set took: 4.101745ms
    Apr 21 21:41:32.101: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.123304ms
    Apr 21 21:41:34.703: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 21 21:41:34.703: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 21 21:41:34.705: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"26314"},"items":null}

    Apr 21 21:41:34.706: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"26314"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:41:34.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-82" for this suite. 04/21/23 21:41:34.719
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:41:34.724
Apr 21 21:41:34.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pod-network-test 04/21/23 21:41:34.725
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:34.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:34.734
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-2305 04/21/23 21:41:34.736
STEP: creating a selector 04/21/23 21:41:34.736
STEP: Creating the service pods in kubernetes 04/21/23 21:41:34.736
Apr 21 21:41:34.736: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 21 21:41:34.748: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2305" to be "running and ready"
Apr 21 21:41:34.750: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.504037ms
Apr 21 21:41:34.750: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:41:36.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.003882943s
Apr 21 21:41:36.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 21:41:38.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005049956s
Apr 21 21:41:38.754: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 21:41:40.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004477107s
Apr 21 21:41:40.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 21:41:42.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004097156s
Apr 21 21:41:42.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 21:41:44.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004524323s
Apr 21 21:41:44.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 21 21:41:46.755: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006091103s
Apr 21 21:41:46.755: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 21 21:41:46.755: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 21 21:41:46.757: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2305" to be "running and ready"
Apr 21 21:41:46.758: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.779041ms
Apr 21 21:41:46.758: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 21 21:41:46.758: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 04/21/23 21:41:46.76
Apr 21 21:41:46.768: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2305" to be "running"
Apr 21 21:41:46.769: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.804612ms
Apr 21 21:41:48.773: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004975354s
Apr 21 21:41:48.773: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 21 21:41:48.774: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2305" to be "running"
Apr 21 21:41:48.776: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.455216ms
Apr 21 21:41:48.776: INFO: Pod "host-test-container-pod" satisfied condition "running"
Apr 21 21:41:48.777: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Apr 21 21:41:48.777: INFO: Going to poll 10.244.0.221 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Apr 21 21:41:48.779: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.221:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2305 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:48.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:48.779: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:48.779: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.221%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 21 21:41:48.850: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 21 21:41:48.850: INFO: Going to poll 10.244.1.242 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Apr 21 21:41:48.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.242:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2305 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 21 21:41:48.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
Apr 21 21:41:48.853: INFO: ExecWithOptions: Clientset creation
Apr 21 21:41:48.853: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.242%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 21 21:41:48.897: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 21 21:41:48.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-2305" for this suite. 04/21/23 21:41:48.9
------------------------------
â€¢ [SLOW TEST] [14.180 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:41:34.724
    Apr 21 21:41:34.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pod-network-test 04/21/23 21:41:34.725
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:34.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:34.734
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-2305 04/21/23 21:41:34.736
    STEP: creating a selector 04/21/23 21:41:34.736
    STEP: Creating the service pods in kubernetes 04/21/23 21:41:34.736
    Apr 21 21:41:34.736: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 21 21:41:34.748: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-2305" to be "running and ready"
    Apr 21 21:41:34.750: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.504037ms
    Apr 21 21:41:34.750: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:41:36.752: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.003882943s
    Apr 21 21:41:36.752: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 21:41:38.754: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.005049956s
    Apr 21 21:41:38.754: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 21:41:40.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.004477107s
    Apr 21 21:41:40.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 21:41:42.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.004097156s
    Apr 21 21:41:42.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 21:41:44.753: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.004524323s
    Apr 21 21:41:44.753: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 21 21:41:46.755: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.006091103s
    Apr 21 21:41:46.755: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 21 21:41:46.755: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 21 21:41:46.757: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-2305" to be "running and ready"
    Apr 21 21:41:46.758: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 1.779041ms
    Apr 21 21:41:46.758: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 21 21:41:46.758: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 04/21/23 21:41:46.76
    Apr 21 21:41:46.768: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-2305" to be "running"
    Apr 21 21:41:46.769: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.804612ms
    Apr 21 21:41:48.773: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.004975354s
    Apr 21 21:41:48.773: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 21 21:41:48.774: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-2305" to be "running"
    Apr 21 21:41:48.776: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 1.455216ms
    Apr 21 21:41:48.776: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Apr 21 21:41:48.777: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Apr 21 21:41:48.777: INFO: Going to poll 10.244.0.221 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Apr 21 21:41:48.779: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.221:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2305 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:48.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:48.779: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:48.779: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.221%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 21 21:41:48.850: INFO: Found all 1 expected endpoints: [netserver-0]
    Apr 21 21:41:48.850: INFO: Going to poll 10.244.1.242 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Apr 21 21:41:48.853: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.242:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2305 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 21 21:41:48.853: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    Apr 21 21:41:48.853: INFO: ExecWithOptions: Clientset creation
    Apr 21 21:41:48.853: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-2305/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.242%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 21 21:41:48.897: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:41:48.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-2305" for this suite. 04/21/23 21:41:48.9
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:41:48.904
Apr 21 21:41:48.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename namespaces 04/21/23 21:41:48.905
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:48.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:48.914
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 04/21/23 21:41:48.916
Apr 21 21:41:48.918: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 04/21/23 21:41:48.918
Apr 21 21:41:48.921: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 04/21/23 21:41:48.921
Apr 21 21:41:48.925: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:41:48.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9613" for this suite. 04/21/23 21:41:48.927
------------------------------
â€¢ [0.027 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:41:48.904
    Apr 21 21:41:48.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename namespaces 04/21/23 21:41:48.905
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:48.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:48.914
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 04/21/23 21:41:48.916
    Apr 21 21:41:48.918: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 04/21/23 21:41:48.918
    Apr 21 21:41:48.921: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 04/21/23 21:41:48.921
    Apr 21 21:41:48.925: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:41:48.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9613" for this suite. 04/21/23 21:41:48.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:41:48.932
Apr 21 21:41:48.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename sched-preemption 04/21/23 21:41:48.933
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:48.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:48.941
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 21 21:41:48.952: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 21 21:42:48.963: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 04/21/23 21:42:48.965
Apr 21 21:42:48.978: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr 21 21:42:48.983: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr 21 21:42:48.994: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr 21 21:42:49.001: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 04/21/23 21:42:49.001
Apr 21 21:42:49.001: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4969" to be "running"
Apr 21 21:42:49.003: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913161ms
Apr 21 21:42:51.006: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005057753s
Apr 21 21:42:51.006: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Apr 21 21:42:51.006: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4969" to be "running"
Apr 21 21:42:51.008: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.74652ms
Apr 21 21:42:51.008: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 21 21:42:51.008: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4969" to be "running"
Apr 21 21:42:51.010: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.554075ms
Apr 21 21:42:51.010: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 21 21:42:51.010: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4969" to be "running"
Apr 21 21:42:51.011: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.488247ms
Apr 21 21:42:51.011: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 04/21/23 21:42:51.011
Apr 21 21:42:51.018: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Apr 21 21:42:51.020: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512236ms
Apr 21 21:42:53.023: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004307024s
Apr 21 21:42:55.022: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004066805s
Apr 21 21:42:57.024: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005269419s
Apr 21 21:42:57.024: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:42:57.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-4969" for this suite. 04/21/23 21:42:57.058
------------------------------
â€¢ [SLOW TEST] [68.129 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:41:48.932
    Apr 21 21:41:48.932: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename sched-preemption 04/21/23 21:41:48.933
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:41:48.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:41:48.941
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 21 21:41:48.952: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 21 21:42:48.963: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 04/21/23 21:42:48.965
    Apr 21 21:42:48.978: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Apr 21 21:42:48.983: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Apr 21 21:42:48.994: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Apr 21 21:42:49.001: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 04/21/23 21:42:49.001
    Apr 21 21:42:49.001: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4969" to be "running"
    Apr 21 21:42:49.003: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 1.913161ms
    Apr 21 21:42:51.006: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.005057753s
    Apr 21 21:42:51.006: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Apr 21 21:42:51.006: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4969" to be "running"
    Apr 21 21:42:51.008: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.74652ms
    Apr 21 21:42:51.008: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 21 21:42:51.008: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4969" to be "running"
    Apr 21 21:42:51.010: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.554075ms
    Apr 21 21:42:51.010: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 21 21:42:51.010: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4969" to be "running"
    Apr 21 21:42:51.011: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.488247ms
    Apr 21 21:42:51.011: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 04/21/23 21:42:51.011
    Apr 21 21:42:51.018: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Apr 21 21:42:51.020: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512236ms
    Apr 21 21:42:53.023: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004307024s
    Apr 21 21:42:55.022: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.004066805s
    Apr 21 21:42:57.024: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.005269419s
    Apr 21 21:42:57.024: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:42:57.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-4969" for this suite. 04/21/23 21:42:57.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:42:57.061
Apr 21 21:42:57.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename dns 04/21/23 21:42:57.062
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:42:57.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:42:57.072
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 04/21/23 21:42:57.074
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2922.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2922.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 04/21/23 21:42:57.077
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2922.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2922.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 04/21/23 21:42:57.077
STEP: creating a pod to probe DNS 04/21/23 21:42:57.077
STEP: submitting the pod to kubernetes 04/21/23 21:42:57.077
Apr 21 21:42:57.083: INFO: Waiting up to 15m0s for pod "dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866" in namespace "dns-2922" to be "running"
Apr 21 21:42:57.084: INFO: Pod "dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866": Phase="Pending", Reason="", readiness=false. Elapsed: 1.437685ms
Apr 21 21:42:59.088: INFO: Pod "dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866": Phase="Running", Reason="", readiness=true. Elapsed: 2.00465743s
Apr 21 21:42:59.088: INFO: Pod "dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866" satisfied condition "running"
STEP: retrieving the pod 04/21/23 21:42:59.088
STEP: looking for the results for each expected name from probers 04/21/23 21:42:59.089
Apr 21 21:42:59.100: INFO: DNS probes using dns-2922/dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866 succeeded

STEP: deleting the pod 04/21/23 21:42:59.1
STEP: deleting the test headless service 04/21/23 21:42:59.109
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 21 21:42:59.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2922" for this suite. 04/21/23 21:42:59.118
------------------------------
â€¢ [2.061 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:42:57.061
    Apr 21 21:42:57.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename dns 04/21/23 21:42:57.062
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:42:57.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:42:57.072
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 04/21/23 21:42:57.074
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2922.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2922.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     04/21/23 21:42:57.077
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2922.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2922.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     04/21/23 21:42:57.077
    STEP: creating a pod to probe DNS 04/21/23 21:42:57.077
    STEP: submitting the pod to kubernetes 04/21/23 21:42:57.077
    Apr 21 21:42:57.083: INFO: Waiting up to 15m0s for pod "dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866" in namespace "dns-2922" to be "running"
    Apr 21 21:42:57.084: INFO: Pod "dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866": Phase="Pending", Reason="", readiness=false. Elapsed: 1.437685ms
    Apr 21 21:42:59.088: INFO: Pod "dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866": Phase="Running", Reason="", readiness=true. Elapsed: 2.00465743s
    Apr 21 21:42:59.088: INFO: Pod "dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866" satisfied condition "running"
    STEP: retrieving the pod 04/21/23 21:42:59.088
    STEP: looking for the results for each expected name from probers 04/21/23 21:42:59.089
    Apr 21 21:42:59.100: INFO: DNS probes using dns-2922/dns-test-aeecd4c2-bfe8-455c-b62a-24816b640866 succeeded

    STEP: deleting the pod 04/21/23 21:42:59.1
    STEP: deleting the test headless service 04/21/23 21:42:59.109
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:42:59.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2922" for this suite. 04/21/23 21:42:59.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:42:59.123
Apr 21 21:42:59.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename init-container 04/21/23 21:42:59.124
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:42:59.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:42:59.132
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 04/21/23 21:42:59.133
Apr 21 21:42:59.133: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 21 21:43:02.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2897" for this suite. 04/21/23 21:43:02.992
------------------------------
â€¢ [3.873 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:42:59.123
    Apr 21 21:42:59.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename init-container 04/21/23 21:42:59.124
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:42:59.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:42:59.132
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 04/21/23 21:42:59.133
    Apr 21 21:42:59.133: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:43:02.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2897" for this suite. 04/21/23 21:43:02.992
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:43:02.996
Apr 21 21:43:02.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename kubectl 04/21/23 21:43:02.997
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:03.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:03.009
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 04/21/23 21:43:03.011
Apr 21 21:43:03.011: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr 21 21:43:03.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
Apr 21 21:43:03.515: INFO: stderr: ""
Apr 21 21:43:03.515: INFO: stdout: "service/agnhost-replica created\n"
Apr 21 21:43:03.515: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr 21 21:43:03.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
Apr 21 21:43:03.694: INFO: stderr: ""
Apr 21 21:43:03.694: INFO: stdout: "service/agnhost-primary created\n"
Apr 21 21:43:03.695: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 21 21:43:03.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
Apr 21 21:43:03.846: INFO: stderr: ""
Apr 21 21:43:03.847: INFO: stdout: "service/frontend created\n"
Apr 21 21:43:03.847: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr 21 21:43:03.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
Apr 21 21:43:03.990: INFO: stderr: ""
Apr 21 21:43:03.990: INFO: stdout: "deployment.apps/frontend created\n"
Apr 21 21:43:03.990: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 21 21:43:03.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
Apr 21 21:43:04.165: INFO: stderr: ""
Apr 21 21:43:04.165: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr 21 21:43:04.166: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 21 21:43:04.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
Apr 21 21:43:04.331: INFO: stderr: ""
Apr 21 21:43:04.331: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 04/21/23 21:43:04.331
Apr 21 21:43:04.331: INFO: Waiting for all frontend pods to be Running.
Apr 21 21:43:09.383: INFO: Waiting for frontend to serve content.
Apr 21 21:43:09.389: INFO: Trying to add a new entry to the guestbook.
Apr 21 21:43:09.394: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 04/21/23 21:43:09.397
Apr 21 21:43:09.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
Apr 21 21:43:09.462: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 21 21:43:09.462: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 04/21/23 21:43:09.462
Apr 21 21:43:09.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
Apr 21 21:43:09.528: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 21 21:43:09.528: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 04/21/23 21:43:09.528
Apr 21 21:43:09.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
Apr 21 21:43:09.595: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 21 21:43:09.595: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 04/21/23 21:43:09.595
Apr 21 21:43:09.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
Apr 21 21:43:09.654: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 21 21:43:09.654: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 04/21/23 21:43:09.654
Apr 21 21:43:09.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
Apr 21 21:43:09.750: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 21 21:43:09.750: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 04/21/23 21:43:09.75
Apr 21 21:43:09.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
Apr 21 21:43:09.856: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 21 21:43:09.856: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 21 21:43:09.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1800" for this suite. 04/21/23 21:43:09.859
------------------------------
â€¢ [SLOW TEST] [6.872 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:43:02.996
    Apr 21 21:43:02.996: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename kubectl 04/21/23 21:43:02.997
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:03.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:03.009
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 04/21/23 21:43:03.011
    Apr 21 21:43:03.011: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Apr 21 21:43:03.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
    Apr 21 21:43:03.515: INFO: stderr: ""
    Apr 21 21:43:03.515: INFO: stdout: "service/agnhost-replica created\n"
    Apr 21 21:43:03.515: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Apr 21 21:43:03.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
    Apr 21 21:43:03.694: INFO: stderr: ""
    Apr 21 21:43:03.694: INFO: stdout: "service/agnhost-primary created\n"
    Apr 21 21:43:03.695: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Apr 21 21:43:03.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
    Apr 21 21:43:03.846: INFO: stderr: ""
    Apr 21 21:43:03.847: INFO: stdout: "service/frontend created\n"
    Apr 21 21:43:03.847: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Apr 21 21:43:03.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
    Apr 21 21:43:03.990: INFO: stderr: ""
    Apr 21 21:43:03.990: INFO: stdout: "deployment.apps/frontend created\n"
    Apr 21 21:43:03.990: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Apr 21 21:43:03.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
    Apr 21 21:43:04.165: INFO: stderr: ""
    Apr 21 21:43:04.165: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Apr 21 21:43:04.166: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Apr 21 21:43:04.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 create -f -'
    Apr 21 21:43:04.331: INFO: stderr: ""
    Apr 21 21:43:04.331: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 04/21/23 21:43:04.331
    Apr 21 21:43:04.331: INFO: Waiting for all frontend pods to be Running.
    Apr 21 21:43:09.383: INFO: Waiting for frontend to serve content.
    Apr 21 21:43:09.389: INFO: Trying to add a new entry to the guestbook.
    Apr 21 21:43:09.394: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 04/21/23 21:43:09.397
    Apr 21 21:43:09.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
    Apr 21 21:43:09.462: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 21 21:43:09.462: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 04/21/23 21:43:09.462
    Apr 21 21:43:09.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
    Apr 21 21:43:09.528: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 21 21:43:09.528: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 04/21/23 21:43:09.528
    Apr 21 21:43:09.528: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
    Apr 21 21:43:09.595: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 21 21:43:09.595: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 04/21/23 21:43:09.595
    Apr 21 21:43:09.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
    Apr 21 21:43:09.654: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 21 21:43:09.654: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 04/21/23 21:43:09.654
    Apr 21 21:43:09.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
    Apr 21 21:43:09.750: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 21 21:43:09.750: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 04/21/23 21:43:09.75
    Apr 21 21:43:09.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=kubectl-1800 delete --grace-period=0 --force -f -'
    Apr 21 21:43:09.856: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 21 21:43:09.856: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:43:09.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1800" for this suite. 04/21/23 21:43:09.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:43:09.869
Apr 21 21:43:09.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 21:43:09.87
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:09.881
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:09.883
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-11ae7960-21ec-4de2-93a6-d3f299eff72d 04/21/23 21:43:09.935
STEP: Creating a pod to test consume configMaps 04/21/23 21:43:09.939
Apr 21 21:43:09.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be" in namespace "configmap-6293" to be "Succeeded or Failed"
Apr 21 21:43:09.950: INFO: Pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447171ms
Apr 21 21:43:11.953: INFO: Pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00577551s
Apr 21 21:43:13.953: INFO: Pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005802509s
STEP: Saw pod success 04/21/23 21:43:13.953
Apr 21 21:43:13.953: INFO: Pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be" satisfied condition "Succeeded or Failed"
Apr 21 21:43:13.955: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be container agnhost-container: <nil>
STEP: delete the pod 04/21/23 21:43:13.965
Apr 21 21:43:13.974: INFO: Waiting for pod pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be to disappear
Apr 21 21:43:13.976: INFO: Pod pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:43:13.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6293" for this suite. 04/21/23 21:43:13.978
------------------------------
â€¢ [4.112 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:43:09.869
    Apr 21 21:43:09.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 21:43:09.87
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:09.881
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:09.883
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-11ae7960-21ec-4de2-93a6-d3f299eff72d 04/21/23 21:43:09.935
    STEP: Creating a pod to test consume configMaps 04/21/23 21:43:09.939
    Apr 21 21:43:09.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be" in namespace "configmap-6293" to be "Succeeded or Failed"
    Apr 21 21:43:09.950: INFO: Pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be": Phase="Pending", Reason="", readiness=false. Elapsed: 3.447171ms
    Apr 21 21:43:11.953: INFO: Pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00577551s
    Apr 21 21:43:13.953: INFO: Pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005802509s
    STEP: Saw pod success 04/21/23 21:43:13.953
    Apr 21 21:43:13.953: INFO: Pod "pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be" satisfied condition "Succeeded or Failed"
    Apr 21 21:43:13.955: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 21:43:13.965
    Apr 21 21:43:13.974: INFO: Waiting for pod pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be to disappear
    Apr 21 21:43:13.976: INFO: Pod pod-configmaps-2543f130-1f9c-4faa-829c-9128098261be no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:43:13.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6293" for this suite. 04/21/23 21:43:13.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:43:13.981
Apr 21 21:43:13.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename gc 04/21/23 21:43:13.982
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:13.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:13.99
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Apr 21 21:43:14.008: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"67c3478d-ae3f-4fab-9005-a5f9ac106121", Controller:(*bool)(0xc00503c3d6), BlockOwnerDeletion:(*bool)(0xc00503c3d7)}}
Apr 21 21:43:14.012: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8089ea3f-3cde-44e1-8d5c-84a2cd0fd6dd", Controller:(*bool)(0xc00422cdde), BlockOwnerDeletion:(*bool)(0xc00422cddf)}}
Apr 21 21:43:14.016: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"034a3237-47d9-44fd-8f97-a22d2f6e9093", Controller:(*bool)(0xc00503c81e), BlockOwnerDeletion:(*bool)(0xc00503c81f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 21 21:43:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-8810" for this suite. 04/21/23 21:43:19.026
------------------------------
â€¢ [SLOW TEST] [5.048 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:43:13.981
    Apr 21 21:43:13.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename gc 04/21/23 21:43:13.982
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:13.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:13.99
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Apr 21 21:43:14.008: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"67c3478d-ae3f-4fab-9005-a5f9ac106121", Controller:(*bool)(0xc00503c3d6), BlockOwnerDeletion:(*bool)(0xc00503c3d7)}}
    Apr 21 21:43:14.012: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"8089ea3f-3cde-44e1-8d5c-84a2cd0fd6dd", Controller:(*bool)(0xc00422cdde), BlockOwnerDeletion:(*bool)(0xc00422cddf)}}
    Apr 21 21:43:14.016: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"034a3237-47d9-44fd-8f97-a22d2f6e9093", Controller:(*bool)(0xc00503c81e), BlockOwnerDeletion:(*bool)(0xc00503c81f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:43:19.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-8810" for this suite. 04/21/23 21:43:19.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:43:19.03
Apr 21 21:43:19.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename aggregator 04/21/23 21:43:19.031
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:19.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:19.04
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Apr 21 21:43:19.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 04/21/23 21:43:19.043
Apr 21 21:43:19.391: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr 21 21:43:21.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:23.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:25.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:27.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:29.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:31.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:33.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:35.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:37.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:39.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:41.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 21 21:43:43.547: INFO: Waited 117.96078ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 04/21/23 21:43:43.579
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 04/21/23 21:43:43.581
STEP: List APIServices 04/21/23 21:43:43.586
Apr 21 21:43:43.589: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Apr 21 21:43:43.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-4186" for this suite. 04/21/23 21:43:43.689
------------------------------
â€¢ [SLOW TEST] [24.714 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:43:19.03
    Apr 21 21:43:19.030: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename aggregator 04/21/23 21:43:19.031
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:19.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:19.04
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Apr 21 21:43:19.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 04/21/23 21:43:19.043
    Apr 21 21:43:19.391: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Apr 21 21:43:21.422: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:23.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:25.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:27.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:29.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:31.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:33.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:35.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:37.425: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:39.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:41.426: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 21, 21, 43, 19, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 21 21:43:43.547: INFO: Waited 117.96078ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 04/21/23 21:43:43.579
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 04/21/23 21:43:43.581
    STEP: List APIServices 04/21/23 21:43:43.586
    Apr 21 21:43:43.589: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:43:43.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-4186" for this suite. 04/21/23 21:43:43.689
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:43:43.745
Apr 21 21:43:43.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:43:43.746
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:43.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:43.756
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  04/21/23 21:43:43.757
Apr 21 21:43:43.763: INFO: Waiting up to 5m0s for pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf" in namespace "svcaccounts-9813" to be "Succeeded or Failed"
Apr 21 21:43:43.764: INFO: Pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548396ms
Apr 21 21:43:45.767: INFO: Pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004223245s
Apr 21 21:43:47.768: INFO: Pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004716916s
STEP: Saw pod success 04/21/23 21:43:47.768
Apr 21 21:43:47.768: INFO: Pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf" satisfied condition "Succeeded or Failed"
Apr 21 21:43:47.770: INFO: Trying to get logs from node k8sconformance-m02 pod test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf container agnhost-container: <nil>
STEP: delete the pod 04/21/23 21:43:47.775
Apr 21 21:43:47.782: INFO: Waiting for pod test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf to disappear
Apr 21 21:43:47.784: INFO: Pod test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 21 21:43:47.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-9813" for this suite. 04/21/23 21:43:47.786
------------------------------
â€¢ [4.044 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:43:43.745
    Apr 21 21:43:43.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename svcaccounts 04/21/23 21:43:43.746
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:43.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:43.756
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  04/21/23 21:43:43.757
    Apr 21 21:43:43.763: INFO: Waiting up to 5m0s for pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf" in namespace "svcaccounts-9813" to be "Succeeded or Failed"
    Apr 21 21:43:43.764: INFO: Pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.548396ms
    Apr 21 21:43:45.767: INFO: Pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004223245s
    Apr 21 21:43:47.768: INFO: Pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004716916s
    STEP: Saw pod success 04/21/23 21:43:47.768
    Apr 21 21:43:47.768: INFO: Pod "test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf" satisfied condition "Succeeded or Failed"
    Apr 21 21:43:47.770: INFO: Trying to get logs from node k8sconformance-m02 pod test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 21:43:47.775
    Apr 21 21:43:47.782: INFO: Waiting for pod test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf to disappear
    Apr 21 21:43:47.784: INFO: Pod test-pod-4b57fe9b-d5e8-42b1-a24b-77a3af672faf no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:43:47.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-9813" for this suite. 04/21/23 21:43:47.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:43:47.789
Apr 21 21:43:47.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:43:47.79
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:47.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:47.799
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-e11957ab-69d3-4bd0-bb51-bcc9a2c8a51b 04/21/23 21:43:47.802
STEP: Creating secret with name s-test-opt-upd-b1f37092-56c0-49b9-b0b2-71b57cc68e64 04/21/23 21:43:47.806
STEP: Creating the pod 04/21/23 21:43:47.808
Apr 21 21:43:47.814: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa" in namespace "projected-6604" to be "running and ready"
Apr 21 21:43:47.815: INFO: Pod "pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55795ms
Apr 21 21:43:47.815: INFO: The phase of Pod pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:43:49.818: INFO: Pod "pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa": Phase="Running", Reason="", readiness=true. Elapsed: 2.004328262s
Apr 21 21:43:49.818: INFO: The phase of Pod pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa is Running (Ready = true)
Apr 21 21:43:49.818: INFO: Pod "pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-e11957ab-69d3-4bd0-bb51-bcc9a2c8a51b 04/21/23 21:43:49.834
STEP: Updating secret s-test-opt-upd-b1f37092-56c0-49b9-b0b2-71b57cc68e64 04/21/23 21:43:49.837
STEP: Creating secret with name s-test-opt-create-89862a9f-8f4a-4935-bebf-e159023285fe 04/21/23 21:43:49.84
STEP: waiting to observe update in volume 04/21/23 21:43:49.842
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 21 21:43:51.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6604" for this suite. 04/21/23 21:43:51.865
------------------------------
â€¢ [4.080 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:43:47.789
    Apr 21 21:43:47.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:43:47.79
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:47.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:47.799
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-e11957ab-69d3-4bd0-bb51-bcc9a2c8a51b 04/21/23 21:43:47.802
    STEP: Creating secret with name s-test-opt-upd-b1f37092-56c0-49b9-b0b2-71b57cc68e64 04/21/23 21:43:47.806
    STEP: Creating the pod 04/21/23 21:43:47.808
    Apr 21 21:43:47.814: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa" in namespace "projected-6604" to be "running and ready"
    Apr 21 21:43:47.815: INFO: Pod "pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa": Phase="Pending", Reason="", readiness=false. Elapsed: 1.55795ms
    Apr 21 21:43:47.815: INFO: The phase of Pod pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:43:49.818: INFO: Pod "pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa": Phase="Running", Reason="", readiness=true. Elapsed: 2.004328262s
    Apr 21 21:43:49.818: INFO: The phase of Pod pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa is Running (Ready = true)
    Apr 21 21:43:49.818: INFO: Pod "pod-projected-secrets-227dc848-1ac4-45d6-915a-4094de37cbfa" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-e11957ab-69d3-4bd0-bb51-bcc9a2c8a51b 04/21/23 21:43:49.834
    STEP: Updating secret s-test-opt-upd-b1f37092-56c0-49b9-b0b2-71b57cc68e64 04/21/23 21:43:49.837
    STEP: Creating secret with name s-test-opt-create-89862a9f-8f4a-4935-bebf-e159023285fe 04/21/23 21:43:49.84
    STEP: waiting to observe update in volume 04/21/23 21:43:49.842
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:43:51.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6604" for this suite. 04/21/23 21:43:51.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:43:51.87
Apr 21 21:43:51.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename projected 04/21/23 21:43:51.87
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:51.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:51.88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-cbeb9c3f-9106-4b9f-87f3-d8eca8aad3bf 04/21/23 21:43:51.883
STEP: Creating the pod 04/21/23 21:43:51.887
Apr 21 21:43:51.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118" in namespace "projected-1760" to be "running and ready"
Apr 21 21:43:51.893: INFO: Pod "pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584446ms
Apr 21 21:43:51.893: INFO: The phase of Pod pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:43:53.897: INFO: Pod "pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118": Phase="Running", Reason="", readiness=true. Elapsed: 2.004827165s
Apr 21 21:43:53.897: INFO: The phase of Pod pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118 is Running (Ready = true)
Apr 21 21:43:53.897: INFO: Pod "pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-cbeb9c3f-9106-4b9f-87f3-d8eca8aad3bf 04/21/23 21:43:53.903
STEP: waiting to observe update in volume 04/21/23 21:43:53.907
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:43:55.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1760" for this suite. 04/21/23 21:43:55.921
------------------------------
â€¢ [4.055 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:43:51.87
    Apr 21 21:43:51.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename projected 04/21/23 21:43:51.87
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:51.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:51.88
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-cbeb9c3f-9106-4b9f-87f3-d8eca8aad3bf 04/21/23 21:43:51.883
    STEP: Creating the pod 04/21/23 21:43:51.887
    Apr 21 21:43:51.892: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118" in namespace "projected-1760" to be "running and ready"
    Apr 21 21:43:51.893: INFO: Pod "pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584446ms
    Apr 21 21:43:51.893: INFO: The phase of Pod pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:43:53.897: INFO: Pod "pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118": Phase="Running", Reason="", readiness=true. Elapsed: 2.004827165s
    Apr 21 21:43:53.897: INFO: The phase of Pod pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118 is Running (Ready = true)
    Apr 21 21:43:53.897: INFO: Pod "pod-projected-configmaps-ccb6eb4c-da74-4352-8fe5-46ae3a050118" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-cbeb9c3f-9106-4b9f-87f3-d8eca8aad3bf 04/21/23 21:43:53.903
    STEP: waiting to observe update in volume 04/21/23 21:43:53.907
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:43:55.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1760" for this suite. 04/21/23 21:43:55.921
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:43:55.925
Apr 21 21:43:55.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename disruption 04/21/23 21:43:55.925
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:55.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:55.938
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 04/21/23 21:43:55.942
STEP: Updating PodDisruptionBudget status 04/21/23 21:43:57.947
STEP: Waiting for all pods to be running 04/21/23 21:43:57.953
Apr 21 21:43:57.955: INFO: running pods: 0 < 1
STEP: locating a running pod 04/21/23 21:43:59.958
STEP: Waiting for the pdb to be processed 04/21/23 21:43:59.965
STEP: Patching PodDisruptionBudget status 04/21/23 21:43:59.97
STEP: Waiting for the pdb to be processed 04/21/23 21:43:59.975
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 21 21:43:59.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-9643" for this suite. 04/21/23 21:43:59.978
------------------------------
â€¢ [4.057 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:43:55.925
    Apr 21 21:43:55.925: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename disruption 04/21/23 21:43:55.925
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:55.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:55.938
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 04/21/23 21:43:55.942
    STEP: Updating PodDisruptionBudget status 04/21/23 21:43:57.947
    STEP: Waiting for all pods to be running 04/21/23 21:43:57.953
    Apr 21 21:43:57.955: INFO: running pods: 0 < 1
    STEP: locating a running pod 04/21/23 21:43:59.958
    STEP: Waiting for the pdb to be processed 04/21/23 21:43:59.965
    STEP: Patching PodDisruptionBudget status 04/21/23 21:43:59.97
    STEP: Waiting for the pdb to be processed 04/21/23 21:43:59.975
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:43:59.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-9643" for this suite. 04/21/23 21:43:59.978
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:43:59.982
Apr 21 21:43:59.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename replication-controller 04/21/23 21:43:59.982
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:59.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:59.992
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Apr 21 21:43:59.994: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 04/21/23 21:44:00.999
STEP: Checking rc "condition-test" has the desired failure condition set 04/21/23 21:44:01.003
STEP: Scaling down rc "condition-test" to satisfy pod quota 04/21/23 21:44:02.007
Apr 21 21:44:02.015: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 04/21/23 21:44:02.015
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 21 21:44:03.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2445" for this suite. 04/21/23 21:44:03.021
------------------------------
â€¢ [3.043 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:43:59.982
    Apr 21 21:43:59.982: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename replication-controller 04/21/23 21:43:59.982
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:43:59.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:43:59.992
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Apr 21 21:43:59.994: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 04/21/23 21:44:00.999
    STEP: Checking rc "condition-test" has the desired failure condition set 04/21/23 21:44:01.003
    STEP: Scaling down rc "condition-test" to satisfy pod quota 04/21/23 21:44:02.007
    Apr 21 21:44:02.015: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 04/21/23 21:44:02.015
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:44:03.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2445" for this suite. 04/21/23 21:44:03.021
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:44:03.025
Apr 21 21:44:03.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename csiinlinevolumes 04/21/23 21:44:03.026
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:03.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:03.036
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 04/21/23 21:44:03.037
STEP: getting 04/21/23 21:44:03.048
STEP: listing 04/21/23 21:44:03.051
STEP: deleting 04/21/23 21:44:03.052
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Apr 21 21:44:03.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-6154" for this suite. 04/21/23 21:44:03.061
------------------------------
â€¢ [0.039 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:44:03.025
    Apr 21 21:44:03.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename csiinlinevolumes 04/21/23 21:44:03.026
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:03.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:03.036
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 04/21/23 21:44:03.037
    STEP: getting 04/21/23 21:44:03.048
    STEP: listing 04/21/23 21:44:03.051
    STEP: deleting 04/21/23 21:44:03.052
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:44:03.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-6154" for this suite. 04/21/23 21:44:03.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:44:03.065
Apr 21 21:44:03.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename configmap 04/21/23 21:44:03.066
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:03.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:03.073
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-efa6db49-0cf6-483e-b084-f9004c29a7e2 04/21/23 21:44:03.075
STEP: Creating a pod to test consume configMaps 04/21/23 21:44:03.077
Apr 21 21:44:03.081: INFO: Waiting up to 5m0s for pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61" in namespace "configmap-1231" to be "Succeeded or Failed"
Apr 21 21:44:03.083: INFO: Pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457439ms
Apr 21 21:44:05.085: INFO: Pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003922147s
Apr 21 21:44:07.085: INFO: Pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004167921s
STEP: Saw pod success 04/21/23 21:44:07.086
Apr 21 21:44:07.086: INFO: Pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61" satisfied condition "Succeeded or Failed"
Apr 21 21:44:07.087: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61 container agnhost-container: <nil>
STEP: delete the pod 04/21/23 21:44:07.092
Apr 21 21:44:07.101: INFO: Waiting for pod pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61 to disappear
Apr 21 21:44:07.102: INFO: Pod pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 21 21:44:07.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1231" for this suite. 04/21/23 21:44:07.104
------------------------------
â€¢ [4.043 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:44:03.065
    Apr 21 21:44:03.065: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename configmap 04/21/23 21:44:03.066
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:03.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:03.073
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-efa6db49-0cf6-483e-b084-f9004c29a7e2 04/21/23 21:44:03.075
    STEP: Creating a pod to test consume configMaps 04/21/23 21:44:03.077
    Apr 21 21:44:03.081: INFO: Waiting up to 5m0s for pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61" in namespace "configmap-1231" to be "Succeeded or Failed"
    Apr 21 21:44:03.083: INFO: Pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61": Phase="Pending", Reason="", readiness=false. Elapsed: 1.457439ms
    Apr 21 21:44:05.085: INFO: Pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003922147s
    Apr 21 21:44:07.085: INFO: Pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004167921s
    STEP: Saw pod success 04/21/23 21:44:07.086
    Apr 21 21:44:07.086: INFO: Pod "pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61" satisfied condition "Succeeded or Failed"
    Apr 21 21:44:07.087: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61 container agnhost-container: <nil>
    STEP: delete the pod 04/21/23 21:44:07.092
    Apr 21 21:44:07.101: INFO: Waiting for pod pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61 to disappear
    Apr 21 21:44:07.102: INFO: Pod pod-configmaps-e678ecd3-b23f-45b4-b648-dbfcfeb86e61 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:44:07.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1231" for this suite. 04/21/23 21:44:07.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:44:07.109
Apr 21 21:44:07.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename secrets 04/21/23 21:44:07.11
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:07.116
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:07.118
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-4cc6fcd5-a9cc-43d6-854e-41f921c671e5 04/21/23 21:44:07.129
STEP: Creating a pod to test consume secrets 04/21/23 21:44:07.131
Apr 21 21:44:07.136: INFO: Waiting up to 5m0s for pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0" in namespace "secrets-1246" to be "Succeeded or Failed"
Apr 21 21:44:07.138: INFO: Pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.509785ms
Apr 21 21:44:09.141: INFO: Pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004760857s
Apr 21 21:44:11.141: INFO: Pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004579195s
STEP: Saw pod success 04/21/23 21:44:11.141
Apr 21 21:44:11.141: INFO: Pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0" satisfied condition "Succeeded or Failed"
Apr 21 21:44:11.143: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0 container secret-volume-test: <nil>
STEP: delete the pod 04/21/23 21:44:11.148
Apr 21 21:44:11.155: INFO: Waiting for pod pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0 to disappear
Apr 21 21:44:11.157: INFO: Pod pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 21 21:44:11.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1246" for this suite. 04/21/23 21:44:11.159
STEP: Destroying namespace "secret-namespace-7337" for this suite. 04/21/23 21:44:11.164
------------------------------
â€¢ [4.058 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:44:07.109
    Apr 21 21:44:07.109: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename secrets 04/21/23 21:44:07.11
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:07.116
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:07.118
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-4cc6fcd5-a9cc-43d6-854e-41f921c671e5 04/21/23 21:44:07.129
    STEP: Creating a pod to test consume secrets 04/21/23 21:44:07.131
    Apr 21 21:44:07.136: INFO: Waiting up to 5m0s for pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0" in namespace "secrets-1246" to be "Succeeded or Failed"
    Apr 21 21:44:07.138: INFO: Pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.509785ms
    Apr 21 21:44:09.141: INFO: Pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004760857s
    Apr 21 21:44:11.141: INFO: Pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.004579195s
    STEP: Saw pod success 04/21/23 21:44:11.141
    Apr 21 21:44:11.141: INFO: Pod "pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0" satisfied condition "Succeeded or Failed"
    Apr 21 21:44:11.143: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0 container secret-volume-test: <nil>
    STEP: delete the pod 04/21/23 21:44:11.148
    Apr 21 21:44:11.155: INFO: Waiting for pod pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0 to disappear
    Apr 21 21:44:11.157: INFO: Pod pod-secrets-c2925783-371a-4ddd-9005-d284fce90ed0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:44:11.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1246" for this suite. 04/21/23 21:44:11.159
    STEP: Destroying namespace "secret-namespace-7337" for this suite. 04/21/23 21:44:11.164
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:44:11.167
Apr 21 21:44:11.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 21:44:11.168
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:11.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:11.176
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Apr 21 21:44:11.182: INFO: Waiting up to 5m0s for pod "server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4" in namespace "pods-8830" to be "running and ready"
Apr 21 21:44:11.183: INFO: Pod "server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.540583ms
Apr 21 21:44:11.183: INFO: The phase of Pod server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:44:13.186: INFO: Pod "server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.003920097s
Apr 21 21:44:13.186: INFO: The phase of Pod server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4 is Running (Ready = true)
Apr 21 21:44:13.186: INFO: Pod "server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4" satisfied condition "running and ready"
Apr 21 21:44:13.201: INFO: Waiting up to 5m0s for pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef" in namespace "pods-8830" to be "Succeeded or Failed"
Apr 21 21:44:13.204: INFO: Pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868454ms
Apr 21 21:44:15.206: INFO: Pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005446326s
Apr 21 21:44:17.207: INFO: Pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005988189s
STEP: Saw pod success 04/21/23 21:44:17.207
Apr 21 21:44:17.207: INFO: Pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef" satisfied condition "Succeeded or Failed"
Apr 21 21:44:17.209: INFO: Trying to get logs from node k8sconformance-m02 pod client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef container env3cont: <nil>
STEP: delete the pod 04/21/23 21:44:17.214
Apr 21 21:44:17.221: INFO: Waiting for pod client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef to disappear
Apr 21 21:44:17.223: INFO: Pod client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 21:44:17.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8830" for this suite. 04/21/23 21:44:17.225
------------------------------
â€¢ [SLOW TEST] [6.060 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:44:11.167
    Apr 21 21:44:11.167: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 21:44:11.168
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:11.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:11.176
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Apr 21 21:44:11.182: INFO: Waiting up to 5m0s for pod "server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4" in namespace "pods-8830" to be "running and ready"
    Apr 21 21:44:11.183: INFO: Pod "server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.540583ms
    Apr 21 21:44:11.183: INFO: The phase of Pod server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:44:13.186: INFO: Pod "server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4": Phase="Running", Reason="", readiness=true. Elapsed: 2.003920097s
    Apr 21 21:44:13.186: INFO: The phase of Pod server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4 is Running (Ready = true)
    Apr 21 21:44:13.186: INFO: Pod "server-envvars-9ec6352d-0d33-4f80-96d2-a909c3a329c4" satisfied condition "running and ready"
    Apr 21 21:44:13.201: INFO: Waiting up to 5m0s for pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef" in namespace "pods-8830" to be "Succeeded or Failed"
    Apr 21 21:44:13.204: INFO: Pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868454ms
    Apr 21 21:44:15.206: INFO: Pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005446326s
    Apr 21 21:44:17.207: INFO: Pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005988189s
    STEP: Saw pod success 04/21/23 21:44:17.207
    Apr 21 21:44:17.207: INFO: Pod "client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef" satisfied condition "Succeeded or Failed"
    Apr 21 21:44:17.209: INFO: Trying to get logs from node k8sconformance-m02 pod client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef container env3cont: <nil>
    STEP: delete the pod 04/21/23 21:44:17.214
    Apr 21 21:44:17.221: INFO: Waiting for pod client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef to disappear
    Apr 21 21:44:17.223: INFO: Pod client-envvars-89d53911-f8cb-4e85-bf24-aa28bb07c2ef no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:44:17.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8830" for this suite. 04/21/23 21:44:17.225
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:44:17.228
Apr 21 21:44:17.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename security-context 04/21/23 21:44:17.229
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:17.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:17.238
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/21/23 21:44:17.24
Apr 21 21:44:17.245: INFO: Waiting up to 5m0s for pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e" in namespace "security-context-1795" to be "Succeeded or Failed"
Apr 21 21:44:17.246: INFO: Pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335114ms
Apr 21 21:44:19.249: INFO: Pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004235717s
Apr 21 21:44:21.249: INFO: Pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003985715s
STEP: Saw pod success 04/21/23 21:44:21.249
Apr 21 21:44:21.249: INFO: Pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e" satisfied condition "Succeeded or Failed"
Apr 21 21:44:21.251: INFO: Trying to get logs from node k8sconformance-m02 pod security-context-d6e19327-fca2-4182-b38c-4b727289e22e container test-container: <nil>
STEP: delete the pod 04/21/23 21:44:21.255
Apr 21 21:44:21.262: INFO: Waiting for pod security-context-d6e19327-fca2-4182-b38c-4b727289e22e to disappear
Apr 21 21:44:21.263: INFO: Pod security-context-d6e19327-fca2-4182-b38c-4b727289e22e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 21 21:44:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1795" for this suite. 04/21/23 21:44:21.265
------------------------------
â€¢ [4.040 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:44:17.228
    Apr 21 21:44:17.228: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename security-context 04/21/23 21:44:17.229
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:17.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:17.238
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/21/23 21:44:17.24
    Apr 21 21:44:17.245: INFO: Waiting up to 5m0s for pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e" in namespace "security-context-1795" to be "Succeeded or Failed"
    Apr 21 21:44:17.246: INFO: Pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.335114ms
    Apr 21 21:44:19.249: INFO: Pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004235717s
    Apr 21 21:44:21.249: INFO: Pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.003985715s
    STEP: Saw pod success 04/21/23 21:44:21.249
    Apr 21 21:44:21.249: INFO: Pod "security-context-d6e19327-fca2-4182-b38c-4b727289e22e" satisfied condition "Succeeded or Failed"
    Apr 21 21:44:21.251: INFO: Trying to get logs from node k8sconformance-m02 pod security-context-d6e19327-fca2-4182-b38c-4b727289e22e container test-container: <nil>
    STEP: delete the pod 04/21/23 21:44:21.255
    Apr 21 21:44:21.262: INFO: Waiting for pod security-context-d6e19327-fca2-4182-b38c-4b727289e22e to disappear
    Apr 21 21:44:21.263: INFO: Pod security-context-d6e19327-fca2-4182-b38c-4b727289e22e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:44:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1795" for this suite. 04/21/23 21:44:21.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:44:21.269
Apr 21 21:44:21.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename pods 04/21/23 21:44:21.27
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:21.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:21.279
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 04/21/23 21:44:21.283
STEP: watching for Pod to be ready 04/21/23 21:44:21.287
Apr 21 21:44:21.288: INFO: observed Pod pod-test in namespace pods-8398 in phase Pending with labels: map[test-pod-static:true] & conditions []
Apr 21 21:44:21.291: INFO: observed Pod pod-test in namespace pods-8398 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  }]
Apr 21 21:44:21.299: INFO: observed Pod pod-test in namespace pods-8398 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  }]
Apr 21 21:44:22.630: INFO: Found Pod pod-test in namespace pods-8398 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 04/21/23 21:44:22.633
STEP: getting the Pod and ensuring that it's patched 04/21/23 21:44:22.64
STEP: replacing the Pod's status Ready condition to False 04/21/23 21:44:22.642
STEP: check the Pod again to ensure its Ready conditions are False 04/21/23 21:44:22.652
STEP: deleting the Pod via a Collection with a LabelSelector 04/21/23 21:44:22.652
STEP: watching for the Pod to be deleted 04/21/23 21:44:22.656
Apr 21 21:44:22.657: INFO: observed event type MODIFIED
Apr 21 21:44:23.582: INFO: observed event type MODIFIED
Apr 21 21:44:25.688: INFO: observed event type MODIFIED
Apr 21 21:44:25.691: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 21 21:44:25.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8398" for this suite. 04/21/23 21:44:25.699
------------------------------
â€¢ [4.433 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:44:21.269
    Apr 21 21:44:21.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename pods 04/21/23 21:44:21.27
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:21.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:21.279
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 04/21/23 21:44:21.283
    STEP: watching for Pod to be ready 04/21/23 21:44:21.287
    Apr 21 21:44:21.288: INFO: observed Pod pod-test in namespace pods-8398 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Apr 21 21:44:21.291: INFO: observed Pod pod-test in namespace pods-8398 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  }]
    Apr 21 21:44:21.299: INFO: observed Pod pod-test in namespace pods-8398 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  }]
    Apr 21 21:44:22.630: INFO: Found Pod pod-test in namespace pods-8398 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-21 21:44:21 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 04/21/23 21:44:22.633
    STEP: getting the Pod and ensuring that it's patched 04/21/23 21:44:22.64
    STEP: replacing the Pod's status Ready condition to False 04/21/23 21:44:22.642
    STEP: check the Pod again to ensure its Ready conditions are False 04/21/23 21:44:22.652
    STEP: deleting the Pod via a Collection with a LabelSelector 04/21/23 21:44:22.652
    STEP: watching for the Pod to be deleted 04/21/23 21:44:22.656
    Apr 21 21:44:22.657: INFO: observed event type MODIFIED
    Apr 21 21:44:23.582: INFO: observed event type MODIFIED
    Apr 21 21:44:25.688: INFO: observed event type MODIFIED
    Apr 21 21:44:25.691: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:44:25.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8398" for this suite. 04/21/23 21:44:25.699
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:44:25.702
Apr 21 21:44:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename events 04/21/23 21:44:25.703
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:25.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:25.711
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 04/21/23 21:44:25.712
STEP: listing events in all namespaces 04/21/23 21:44:25.717
STEP: listing events in test namespace 04/21/23 21:44:25.721
STEP: listing events with field selection filtering on source 04/21/23 21:44:25.722
STEP: listing events with field selection filtering on reportingController 04/21/23 21:44:25.723
STEP: getting the test event 04/21/23 21:44:25.725
STEP: patching the test event 04/21/23 21:44:25.726
STEP: getting the test event 04/21/23 21:44:25.733
STEP: updating the test event 04/21/23 21:44:25.734
STEP: getting the test event 04/21/23 21:44:25.737
STEP: deleting the test event 04/21/23 21:44:25.739
STEP: listing events in all namespaces 04/21/23 21:44:25.742
STEP: listing events in test namespace 04/21/23 21:44:25.744
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Apr 21 21:44:25.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-2239" for this suite. 04/21/23 21:44:25.747
------------------------------
â€¢ [0.047 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:44:25.702
    Apr 21 21:44:25.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename events 04/21/23 21:44:25.703
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:25.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:25.711
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 04/21/23 21:44:25.712
    STEP: listing events in all namespaces 04/21/23 21:44:25.717
    STEP: listing events in test namespace 04/21/23 21:44:25.721
    STEP: listing events with field selection filtering on source 04/21/23 21:44:25.722
    STEP: listing events with field selection filtering on reportingController 04/21/23 21:44:25.723
    STEP: getting the test event 04/21/23 21:44:25.725
    STEP: patching the test event 04/21/23 21:44:25.726
    STEP: getting the test event 04/21/23 21:44:25.733
    STEP: updating the test event 04/21/23 21:44:25.734
    STEP: getting the test event 04/21/23 21:44:25.737
    STEP: deleting the test event 04/21/23 21:44:25.739
    STEP: listing events in all namespaces 04/21/23 21:44:25.742
    STEP: listing events in test namespace 04/21/23 21:44:25.744
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:44:25.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-2239" for this suite. 04/21/23 21:44:25.747
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:44:25.75
Apr 21 21:44:25.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename services 04/21/23 21:44:25.751
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:25.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:25.759
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-8980 04/21/23 21:44:25.761
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[] 04/21/23 21:44:25.768
Apr 21 21:44:25.770: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Apr 21 21:44:26.774: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8980 04/21/23 21:44:26.775
Apr 21 21:44:26.779: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8980" to be "running and ready"
Apr 21 21:44:26.781: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.589573ms
Apr 21 21:44:26.781: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:44:28.784: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.0044063s
Apr 21 21:44:28.784: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr 21 21:44:28.784: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod1:[80]] 04/21/23 21:44:28.785
Apr 21 21:44:28.790: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 04/21/23 21:44:28.79
Apr 21 21:44:28.791: INFO: Creating new exec pod
Apr 21 21:44:28.795: INFO: Waiting up to 5m0s for pod "execpod5tnxc" in namespace "services-8980" to be "running"
Apr 21 21:44:28.796: INFO: Pod "execpod5tnxc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481115ms
Apr 21 21:44:30.799: INFO: Pod "execpod5tnxc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004040807s
Apr 21 21:44:30.799: INFO: Pod "execpod5tnxc" satisfied condition "running"
Apr 21 21:44:31.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Apr 21 21:44:31.916: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 21 21:44:31.916: INFO: stdout: ""
Apr 21 21:44:31.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 10.99.20.78 80'
Apr 21 21:44:32.035: INFO: stderr: "+ nc -v -z -w 2 10.99.20.78 80\nConnection to 10.99.20.78 80 port [tcp/http] succeeded!\n"
Apr 21 21:44:32.035: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-8980 04/21/23 21:44:32.035
Apr 21 21:44:32.039: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8980" to be "running and ready"
Apr 21 21:44:32.041: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.900096ms
Apr 21 21:44:32.041: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 21 21:44:34.044: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004539274s
Apr 21 21:44:34.044: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr 21 21:44:34.044: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod1:[80] pod2:[80]] 04/21/23 21:44:34.045
Apr 21 21:44:34.052: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 04/21/23 21:44:34.052
Apr 21 21:44:35.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Apr 21 21:44:35.172: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 21 21:44:35.172: INFO: stdout: ""
Apr 21 21:44:35.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 10.99.20.78 80'
Apr 21 21:44:35.295: INFO: stderr: "+ nc -v -z -w 2 10.99.20.78 80\nConnection to 10.99.20.78 80 port [tcp/http] succeeded!\n"
Apr 21 21:44:35.295: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-8980 04/21/23 21:44:35.295
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod2:[80]] 04/21/23 21:44:35.302
Apr 21 21:44:35.311: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 04/21/23 21:44:35.311
Apr 21 21:44:36.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Apr 21 21:44:36.427: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 21 21:44:36.427: INFO: stdout: ""
Apr 21 21:44:36.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 10.99.20.78 80'
Apr 21 21:44:36.554: INFO: stderr: "+ nc -v -z -w 2 10.99.20.78 80\nConnection to 10.99.20.78 80 port [tcp/http] succeeded!\n"
Apr 21 21:44:36.554: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-8980 04/21/23 21:44:36.554
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[] 04/21/23 21:44:36.561
Apr 21 21:44:36.566: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 21 21:44:36.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8980" for this suite. 04/21/23 21:44:36.582
------------------------------
â€¢ [SLOW TEST] [10.838 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:44:25.75
    Apr 21 21:44:25.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename services 04/21/23 21:44:25.751
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:25.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:25.759
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-8980 04/21/23 21:44:25.761
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[] 04/21/23 21:44:25.768
    Apr 21 21:44:25.770: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Apr 21 21:44:26.774: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8980 04/21/23 21:44:26.775
    Apr 21 21:44:26.779: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8980" to be "running and ready"
    Apr 21 21:44:26.781: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.589573ms
    Apr 21 21:44:26.781: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:44:28.784: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.0044063s
    Apr 21 21:44:28.784: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr 21 21:44:28.784: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod1:[80]] 04/21/23 21:44:28.785
    Apr 21 21:44:28.790: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 04/21/23 21:44:28.79
    Apr 21 21:44:28.791: INFO: Creating new exec pod
    Apr 21 21:44:28.795: INFO: Waiting up to 5m0s for pod "execpod5tnxc" in namespace "services-8980" to be "running"
    Apr 21 21:44:28.796: INFO: Pod "execpod5tnxc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.481115ms
    Apr 21 21:44:30.799: INFO: Pod "execpod5tnxc": Phase="Running", Reason="", readiness=true. Elapsed: 2.004040807s
    Apr 21 21:44:30.799: INFO: Pod "execpod5tnxc" satisfied condition "running"
    Apr 21 21:44:31.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Apr 21 21:44:31.916: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr 21 21:44:31.916: INFO: stdout: ""
    Apr 21 21:44:31.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 10.99.20.78 80'
    Apr 21 21:44:32.035: INFO: stderr: "+ nc -v -z -w 2 10.99.20.78 80\nConnection to 10.99.20.78 80 port [tcp/http] succeeded!\n"
    Apr 21 21:44:32.035: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-8980 04/21/23 21:44:32.035
    Apr 21 21:44:32.039: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8980" to be "running and ready"
    Apr 21 21:44:32.041: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.900096ms
    Apr 21 21:44:32.041: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 21 21:44:34.044: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.004539274s
    Apr 21 21:44:34.044: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr 21 21:44:34.044: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod1:[80] pod2:[80]] 04/21/23 21:44:34.045
    Apr 21 21:44:34.052: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 04/21/23 21:44:34.052
    Apr 21 21:44:35.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Apr 21 21:44:35.172: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr 21 21:44:35.172: INFO: stdout: ""
    Apr 21 21:44:35.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 10.99.20.78 80'
    Apr 21 21:44:35.295: INFO: stderr: "+ nc -v -z -w 2 10.99.20.78 80\nConnection to 10.99.20.78 80 port [tcp/http] succeeded!\n"
    Apr 21 21:44:35.295: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-8980 04/21/23 21:44:35.295
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod2:[80]] 04/21/23 21:44:35.302
    Apr 21 21:44:35.311: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 04/21/23 21:44:35.311
    Apr 21 21:44:36.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Apr 21 21:44:36.427: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr 21 21:44:36.427: INFO: stdout: ""
    Apr 21 21:44:36.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3604583783 --namespace=services-8980 exec execpod5tnxc -- /bin/sh -x -c nc -v -z -w 2 10.99.20.78 80'
    Apr 21 21:44:36.554: INFO: stderr: "+ nc -v -z -w 2 10.99.20.78 80\nConnection to 10.99.20.78 80 port [tcp/http] succeeded!\n"
    Apr 21 21:44:36.554: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-8980 04/21/23 21:44:36.554
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[] 04/21/23 21:44:36.561
    Apr 21 21:44:36.566: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:44:36.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8980" for this suite. 04/21/23 21:44:36.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/21/23 21:44:36.598
Apr 21 21:44:36.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
STEP: Building a namespace api object, basename cronjob 04/21/23 21:44:36.599
STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:36.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:36.638
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 04/21/23 21:44:36.641
STEP: Ensuring more than one job is running at a time 04/21/23 21:44:36.646
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 04/21/23 21:46:00.649
STEP: Removing cronjob 04/21/23 21:46:00.652
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 21 21:46:00.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1882" for this suite. 04/21/23 21:46:00.658
------------------------------
â€¢ [SLOW TEST] [84.069 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/21/23 21:44:36.598
    Apr 21 21:44:36.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3604583783
    STEP: Building a namespace api object, basename cronjob 04/21/23 21:44:36.599
    STEP: Waiting for a default service account to be provisioned in namespace 04/21/23 21:44:36.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/21/23 21:44:36.638
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 04/21/23 21:44:36.641
    STEP: Ensuring more than one job is running at a time 04/21/23 21:44:36.646
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 04/21/23 21:46:00.649
    STEP: Removing cronjob 04/21/23 21:46:00.652
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 21 21:46:00.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1882" for this suite. 04/21/23 21:46:00.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Apr 21 21:46:00.669: INFO: Running AfterSuite actions on node 1
Apr 21 21:46:00.669: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Apr 21 21:46:00.669: INFO: Running AfterSuite actions on node 1
    Apr 21 21:46:00.669: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.100 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5521.091 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h32m1.411745853s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

