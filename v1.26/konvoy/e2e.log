I0417 21:09:22.966794      23 e2e.go:126] Starting e2e run "a8206136-e860-42f7-86bb-baf1eab83f33" on Ginkgo node 1
Apr 17 21:09:22.978: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1681765762 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Apr 17 21:09:23.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:09:23.088: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
E0417 21:09:23.089071      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Apr 17 21:09:23.107: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr 17 21:09:23.131: INFO: 33 / 33 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr 17 21:09:23.131: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
Apr 17 21:09:23.131: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr 17 21:09:23.135: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
Apr 17 21:09:23.135: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Apr 17 21:09:23.135: INFO: e2e test version: v1.26.3
Apr 17 21:09:23.136: INFO: kube-apiserver version: v1.26.3
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Apr 17 21:09:23.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:09:23.140: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.054 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Apr 17 21:09:23.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:09:23.088: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    E0417 21:09:23.089071      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Apr 17 21:09:23.107: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Apr 17 21:09:23.131: INFO: 33 / 33 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Apr 17 21:09:23.131: INFO: expected 7 pod replicas in namespace 'kube-system', 7 are Running and Ready.
    Apr 17 21:09:23.131: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Apr 17 21:09:23.135: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'ebs-csi-node' (0 seconds elapsed)
    Apr 17 21:09:23.135: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Apr 17 21:09:23.135: INFO: e2e test version: v1.26.3
    Apr 17 21:09:23.136: INFO: kube-apiserver version: v1.26.3
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Apr 17 21:09:23.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:09:23.140: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:09:23.158
Apr 17 21:09:23.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 21:09:23.159
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:09:23.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:09:23.177
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-fc7ab8d7-16c1-46b6-9fe7-7a20cd0c9c80 04/17/23 21:09:23.179
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 21:09:23.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6422" for this suite. 04/17/23 21:09:23.185
------------------------------
• [0.031 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:09:23.158
    Apr 17 21:09:23.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 21:09:23.159
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:09:23.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:09:23.177
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-fc7ab8d7-16c1-46b6-9fe7-7a20cd0c9c80 04/17/23 21:09:23.179
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:09:23.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6422" for this suite. 04/17/23 21:09:23.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:09:23.189
Apr 17 21:09:23.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
E0417 21:09:23.190235      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Building a namespace api object, basename container-probe 04/17/23 21:09:23.19
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:09:23.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:09:23.204
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6 in namespace container-probe-593 04/17/23 21:09:23.207
Apr 17 21:09:23.213: INFO: Waiting up to 5m0s for pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6" in namespace "container-probe-593" to be "not pending"
Apr 17 21:09:23.215: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501763ms
Apr 17 21:09:25.220: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007082675s
Apr 17 21:09:27.219: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006678463s
Apr 17 21:09:29.219: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6": Phase="Running", Reason="", readiness=true. Elapsed: 6.006901699s
Apr 17 21:09:29.219: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6" satisfied condition "not pending"
Apr 17 21:09:29.220: INFO: Started pod test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6 in namespace container-probe-593
STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 21:09:29.22
Apr 17 21:09:29.222: INFO: Initial restart count of pod test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6 is 0
STEP: deleting the pod 04/17/23 21:13:29.694
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 17 21:13:29.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-593" for this suite. 04/17/23 21:13:29.711
------------------------------
• [SLOW TEST] [246.527 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:09:23.189
    Apr 17 21:09:23.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    E0417 21:09:23.190235      23 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    STEP: Building a namespace api object, basename container-probe 04/17/23 21:09:23.19
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:09:23.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:09:23.204
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6 in namespace container-probe-593 04/17/23 21:09:23.207
    Apr 17 21:09:23.213: INFO: Waiting up to 5m0s for pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6" in namespace "container-probe-593" to be "not pending"
    Apr 17 21:09:23.215: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501763ms
    Apr 17 21:09:25.220: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007082675s
    Apr 17 21:09:27.219: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006678463s
    Apr 17 21:09:29.219: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6": Phase="Running", Reason="", readiness=true. Elapsed: 6.006901699s
    Apr 17 21:09:29.219: INFO: Pod "test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6" satisfied condition "not pending"
    Apr 17 21:09:29.220: INFO: Started pod test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6 in namespace container-probe-593
    STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 21:09:29.22
    Apr 17 21:09:29.222: INFO: Initial restart count of pod test-webserver-76192dce-78e8-4fc2-9de4-8314ee3a2ca6 is 0
    STEP: deleting the pod 04/17/23 21:13:29.694
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:13:29.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-593" for this suite. 04/17/23 21:13:29.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:13:29.717
Apr 17 21:13:29.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 21:13:29.718
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:13:29.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:13:29.733
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 04/17/23 21:13:29.735
Apr 17 21:13:29.735: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Apr 17 21:13:29.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
Apr 17 21:13:31.896: INFO: stderr: ""
Apr 17 21:13:31.896: INFO: stdout: "service/agnhost-replica created\n"
Apr 17 21:13:31.897: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Apr 17 21:13:31.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
Apr 17 21:13:32.288: INFO: stderr: ""
Apr 17 21:13:32.288: INFO: stdout: "service/agnhost-primary created\n"
Apr 17 21:13:32.288: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr 17 21:13:32.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
Apr 17 21:13:32.733: INFO: stderr: ""
Apr 17 21:13:32.733: INFO: stdout: "service/frontend created\n"
Apr 17 21:13:32.733: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Apr 17 21:13:32.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
Apr 17 21:13:34.817: INFO: stderr: ""
Apr 17 21:13:34.817: INFO: stdout: "deployment.apps/frontend created\n"
Apr 17 21:13:34.817: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 17 21:13:34.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
Apr 17 21:13:35.187: INFO: stderr: ""
Apr 17 21:13:35.187: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Apr 17 21:13:35.187: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr 17 21:13:35.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
Apr 17 21:13:35.560: INFO: stderr: ""
Apr 17 21:13:35.560: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 04/17/23 21:13:35.56
Apr 17 21:13:35.561: INFO: Waiting for all frontend pods to be Running.
Apr 17 21:13:40.612: INFO: Waiting for frontend to serve content.
Apr 17 21:13:40.619: INFO: Trying to add a new entry to the guestbook.
Apr 17 21:13:40.627: INFO: Verifying that added entry can be retrieved.
Apr 17 21:13:40.634: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources 04/17/23 21:13:45.643
Apr 17 21:13:45.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
Apr 17 21:13:45.755: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 21:13:45.755: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 04/17/23 21:13:45.755
Apr 17 21:13:45.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
Apr 17 21:13:45.828: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 21:13:45.828: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 04/17/23 21:13:45.828
Apr 17 21:13:45.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
Apr 17 21:13:45.896: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 21:13:45.896: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 04/17/23 21:13:45.896
Apr 17 21:13:45.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
Apr 17 21:13:45.955: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 21:13:45.955: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 04/17/23 21:13:45.955
Apr 17 21:13:45.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
Apr 17 21:13:46.012: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 21:13:46.012: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 04/17/23 21:13:46.012
Apr 17 21:13:46.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
Apr 17 21:13:46.068: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 21:13:46.068: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 21:13:46.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1595" for this suite. 04/17/23 21:13:46.073
------------------------------
• [SLOW TEST] [16.361 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:13:29.717
    Apr 17 21:13:29.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 21:13:29.718
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:13:29.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:13:29.733
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 04/17/23 21:13:29.735
    Apr 17 21:13:29.735: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Apr 17 21:13:29.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
    Apr 17 21:13:31.896: INFO: stderr: ""
    Apr 17 21:13:31.896: INFO: stdout: "service/agnhost-replica created\n"
    Apr 17 21:13:31.897: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Apr 17 21:13:31.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
    Apr 17 21:13:32.288: INFO: stderr: ""
    Apr 17 21:13:32.288: INFO: stdout: "service/agnhost-primary created\n"
    Apr 17 21:13:32.288: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Apr 17 21:13:32.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
    Apr 17 21:13:32.733: INFO: stderr: ""
    Apr 17 21:13:32.733: INFO: stdout: "service/frontend created\n"
    Apr 17 21:13:32.733: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Apr 17 21:13:32.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
    Apr 17 21:13:34.817: INFO: stderr: ""
    Apr 17 21:13:34.817: INFO: stdout: "deployment.apps/frontend created\n"
    Apr 17 21:13:34.817: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Apr 17 21:13:34.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
    Apr 17 21:13:35.187: INFO: stderr: ""
    Apr 17 21:13:35.187: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Apr 17 21:13:35.187: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Apr 17 21:13:35.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 create -f -'
    Apr 17 21:13:35.560: INFO: stderr: ""
    Apr 17 21:13:35.560: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 04/17/23 21:13:35.56
    Apr 17 21:13:35.561: INFO: Waiting for all frontend pods to be Running.
    Apr 17 21:13:40.612: INFO: Waiting for frontend to serve content.
    Apr 17 21:13:40.619: INFO: Trying to add a new entry to the guestbook.
    Apr 17 21:13:40.627: INFO: Verifying that added entry can be retrieved.
    Apr 17 21:13:40.634: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
    STEP: using delete to clean up resources 04/17/23 21:13:45.643
    Apr 17 21:13:45.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
    Apr 17 21:13:45.755: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 17 21:13:45.755: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 04/17/23 21:13:45.755
    Apr 17 21:13:45.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
    Apr 17 21:13:45.828: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 17 21:13:45.828: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 04/17/23 21:13:45.828
    Apr 17 21:13:45.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
    Apr 17 21:13:45.896: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 17 21:13:45.896: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 04/17/23 21:13:45.896
    Apr 17 21:13:45.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
    Apr 17 21:13:45.955: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 17 21:13:45.955: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 04/17/23 21:13:45.955
    Apr 17 21:13:45.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
    Apr 17 21:13:46.012: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 17 21:13:46.012: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 04/17/23 21:13:46.012
    Apr 17 21:13:46.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1595 delete --grace-period=0 --force -f -'
    Apr 17 21:13:46.068: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 17 21:13:46.068: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:13:46.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1595" for this suite. 04/17/23 21:13:46.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:13:46.079
Apr 17 21:13:46.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 21:13:46.08
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:13:46.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:13:46.095
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 04/17/23 21:13:46.097
Apr 17 21:13:46.103: INFO: Waiting up to 5m0s for pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5" in namespace "downward-api-6471" to be "running and ready"
Apr 17 21:13:46.106: INFO: Pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175294ms
Apr 17 21:13:46.106: INFO: The phase of Pod annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:13:48.110: INFO: Pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006031918s
Apr 17 21:13:48.110: INFO: The phase of Pod annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5 is Running (Ready = true)
Apr 17 21:13:48.110: INFO: Pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5" satisfied condition "running and ready"
Apr 17 21:13:48.634: INFO: Successfully updated pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 21:13:50.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6471" for this suite. 04/17/23 21:13:50.65
------------------------------
• [4.576 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:13:46.079
    Apr 17 21:13:46.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 21:13:46.08
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:13:46.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:13:46.095
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 04/17/23 21:13:46.097
    Apr 17 21:13:46.103: INFO: Waiting up to 5m0s for pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5" in namespace "downward-api-6471" to be "running and ready"
    Apr 17 21:13:46.106: INFO: Pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.175294ms
    Apr 17 21:13:46.106: INFO: The phase of Pod annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:13:48.110: INFO: Pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.006031918s
    Apr 17 21:13:48.110: INFO: The phase of Pod annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5 is Running (Ready = true)
    Apr 17 21:13:48.110: INFO: Pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5" satisfied condition "running and ready"
    Apr 17 21:13:48.634: INFO: Successfully updated pod "annotationupdatee82bbb54-510d-482f-91a7-fff841b048c5"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:13:50.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6471" for this suite. 04/17/23 21:13:50.65
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:13:50.656
Apr 17 21:13:50.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename dns 04/17/23 21:13:50.657
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:13:50.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:13:50.669
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 04/17/23 21:13:50.671
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1478 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1478;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1478 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1478;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1478.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1478.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1478.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1478.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1478.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1478.svc;check="$$(dig +notcp +noall +answer +search 237.199.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.199.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.199.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.199.237_tcp@PTR;sleep 1; done
 04/17/23 21:13:50.733
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1478 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1478;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1478 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1478;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1478.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1478.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1478.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1478.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1478.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1478.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1478.svc;check="$$(dig +notcp +noall +answer +search 237.199.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.199.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.199.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.199.237_tcp@PTR;sleep 1; done
 04/17/23 21:13:50.733
STEP: creating a pod to probe DNS 04/17/23 21:13:50.733
STEP: submitting the pod to kubernetes 04/17/23 21:13:50.733
Apr 17 21:13:50.743: INFO: Waiting up to 15m0s for pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61" in namespace "dns-1478" to be "running"
Apr 17 21:13:50.749: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078887ms
Apr 17 21:13:52.753: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009859529s
Apr 17 21:13:54.752: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009781021s
Apr 17 21:13:56.752: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009817343s
Apr 17 21:13:58.752: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009397189s
Apr 17 21:14:00.753: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Running", Reason="", readiness=true. Elapsed: 10.010068805s
Apr 17 21:14:00.753: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61" satisfied condition "running"
STEP: retrieving the pod 04/17/23 21:14:00.753
STEP: looking for the results for each expected name from probers 04/17/23 21:14:00.755
Apr 17 21:14:00.759: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.761: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.764: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.766: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.769: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.771: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.773: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.776: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.788: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.791: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.793: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.796: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.799: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.801: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.803: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.806: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:00.818: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

Apr 17 21:14:05.824: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.827: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.829: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.832: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.834: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.837: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.839: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.855: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.857: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.860: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.862: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.864: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.867: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.869: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.872: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:05.881: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

Apr 17 21:14:10.824: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.827: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.829: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.832: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.834: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.837: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.839: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.854: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.856: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.858: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.863: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.865: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.870: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:10.880: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

Apr 17 21:14:15.823: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.826: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.829: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.832: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.834: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.836: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.839: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.841: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.853: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.856: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.858: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.863: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.865: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.870: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:15.880: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

Apr 17 21:14:20.824: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.827: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.830: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.832: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.835: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.837: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.840: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.854: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.857: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.859: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.864: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.866: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.871: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
Apr 17 21:14:20.880: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

Apr 17 21:14:25.880: INFO: DNS probes using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 succeeded

STEP: deleting the pod 04/17/23 21:14:25.88
STEP: deleting the test service 04/17/23 21:14:25.892
STEP: deleting the test headless service 04/17/23 21:14:26.003
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 17 21:14:26.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1478" for this suite. 04/17/23 21:14:26.02
------------------------------
• [SLOW TEST] [35.368 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:13:50.656
    Apr 17 21:13:50.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename dns 04/17/23 21:13:50.657
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:13:50.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:13:50.669
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 04/17/23 21:13:50.671
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1478 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1478;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1478 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1478;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1478.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1478.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1478.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1478.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1478.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1478.svc;check="$$(dig +notcp +noall +answer +search 237.199.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.199.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.199.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.199.237_tcp@PTR;sleep 1; done
     04/17/23 21:13:50.733
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1478 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1478;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1478 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1478;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1478.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1478.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1478.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1478.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1478.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1478.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1478.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1478.svc;check="$$(dig +notcp +noall +answer +search 237.199.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.199.237_udp@PTR;check="$$(dig +tcp +noall +answer +search 237.199.99.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.99.199.237_tcp@PTR;sleep 1; done
     04/17/23 21:13:50.733
    STEP: creating a pod to probe DNS 04/17/23 21:13:50.733
    STEP: submitting the pod to kubernetes 04/17/23 21:13:50.733
    Apr 17 21:13:50.743: INFO: Waiting up to 15m0s for pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61" in namespace "dns-1478" to be "running"
    Apr 17 21:13:50.749: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.078887ms
    Apr 17 21:13:52.753: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009859529s
    Apr 17 21:13:54.752: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009781021s
    Apr 17 21:13:56.752: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009817343s
    Apr 17 21:13:58.752: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009397189s
    Apr 17 21:14:00.753: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61": Phase="Running", Reason="", readiness=true. Elapsed: 10.010068805s
    Apr 17 21:14:00.753: INFO: Pod "dns-test-910b0c89-ae24-4668-b624-1b6621236a61" satisfied condition "running"
    STEP: retrieving the pod 04/17/23 21:14:00.753
    STEP: looking for the results for each expected name from probers 04/17/23 21:14:00.755
    Apr 17 21:14:00.759: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.761: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.764: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.766: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.769: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.771: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.773: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.776: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.788: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.791: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.793: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.796: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.799: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.801: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.803: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.806: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:00.818: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

    Apr 17 21:14:05.824: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.827: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.829: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.832: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.834: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.837: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.839: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.855: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.857: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.860: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.862: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.864: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.867: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.869: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.872: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:05.881: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

    Apr 17 21:14:10.824: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.827: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.829: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.832: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.834: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.837: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.839: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.854: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.856: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.858: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.863: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.865: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.870: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:10.880: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

    Apr 17 21:14:15.823: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.826: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.829: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.832: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.834: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.836: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.839: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.841: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.853: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.856: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.858: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.863: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.865: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.870: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:15.880: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

    Apr 17 21:14:20.824: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.827: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.830: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.832: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.835: INFO: Unable to read wheezy_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.837: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.840: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.842: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.854: INFO: Unable to read jessie_udp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.857: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.859: INFO: Unable to read jessie_udp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.861: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478 from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.864: INFO: Unable to read jessie_udp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.866: INFO: Unable to read jessie_tcp@dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.868: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.871: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc from pod dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61: the server could not find the requested resource (get pods dns-test-910b0c89-ae24-4668-b624-1b6621236a61)
    Apr 17 21:14:20.880: INFO: Lookups using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-1478 wheezy_tcp@dns-test-service.dns-1478 wheezy_udp@dns-test-service.dns-1478.svc wheezy_tcp@dns-test-service.dns-1478.svc wheezy_udp@_http._tcp.dns-test-service.dns-1478.svc wheezy_tcp@_http._tcp.dns-test-service.dns-1478.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-1478 jessie_tcp@dns-test-service.dns-1478 jessie_udp@dns-test-service.dns-1478.svc jessie_tcp@dns-test-service.dns-1478.svc jessie_udp@_http._tcp.dns-test-service.dns-1478.svc jessie_tcp@_http._tcp.dns-test-service.dns-1478.svc]

    Apr 17 21:14:25.880: INFO: DNS probes using dns-1478/dns-test-910b0c89-ae24-4668-b624-1b6621236a61 succeeded

    STEP: deleting the pod 04/17/23 21:14:25.88
    STEP: deleting the test service 04/17/23 21:14:25.892
    STEP: deleting the test headless service 04/17/23 21:14:26.003
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:14:26.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1478" for this suite. 04/17/23 21:14:26.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:14:26.025
Apr 17 21:14:26.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename deployment 04/17/23 21:14:26.025
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:26.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:26.041
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Apr 17 21:14:26.043: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr 17 21:14:26.049: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 17 21:14:31.055: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/17/23 21:14:31.055
Apr 17 21:14:31.055: INFO: Creating deployment "test-rolling-update-deployment"
Apr 17 21:14:31.059: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr 17 21:14:31.064: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr 17 21:14:33.070: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr 17 21:14:33.073: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 17 21:14:33.081: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1544  decf62c9-85dc-433d-bf1f-1618e90a2d06 8212 1 2023-04-17 21:14:31 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00215b198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-17 21:14:31 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-04-17 21:14:31 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 17 21:14:33.083: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1544  61845795-10e7-42c3-a031-cf4beefc7216 8203 1 2023-04-17 21:14:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment decf62c9-85dc-433d-bf1f-1618e90a2d06 0xc003a0e377 0xc003a0e378}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"decf62c9-85dc-433d-bf1f-1618e90a2d06\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a0e428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 17 21:14:33.083: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr 17 21:14:33.083: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1544  a7db6cfe-a5e4-4423-ae8d-f1367d88c7d9 8211 2 2023-04-17 21:14:26 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment decf62c9-85dc-433d-bf1f-1618e90a2d06 0xc003a0e24f 0xc003a0e260}] [] [{e2e.test Update apps/v1 2023-04-17 21:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"decf62c9-85dc-433d-bf1f-1618e90a2d06\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a0e318 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 21:14:33.086: INFO: Pod "test-rolling-update-deployment-7549d9f46d-vh84m" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-vh84m test-rolling-update-deployment-7549d9f46d- deployment-1544  659a9741-8e02-4a5e-a5b1-38a830fef484 8202 0 2023-04-17 21:14:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:2d12e60ba5ae987cc5bb0ecbe766d8d4ff826fb98808d0c0853c87c9faf15e94 cni.projectcalico.org/podIP:192.168.208.134/32 cni.projectcalico.org/podIPs:192.168.208.134/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 61845795-10e7-42c3-a031-cf4beefc7216 0xc0035eec57 0xc0035eec58}] [] [{calico Update v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61845795-10e7-42c3-a031-cf4beefc7216\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmtmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmtmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.134,StartTime:2023-04-17 21:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:14:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e580eabf7d6bab93fbc8b171ead0897f272e4cbc61846fa0b33a2d0a37d818c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 17 21:14:33.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1544" for this suite. 04/17/23 21:14:33.09
------------------------------
• [SLOW TEST] [7.071 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:14:26.025
    Apr 17 21:14:26.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename deployment 04/17/23 21:14:26.025
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:26.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:26.041
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Apr 17 21:14:26.043: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Apr 17 21:14:26.049: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 17 21:14:31.055: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/17/23 21:14:31.055
    Apr 17 21:14:31.055: INFO: Creating deployment "test-rolling-update-deployment"
    Apr 17 21:14:31.059: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Apr 17 21:14:31.064: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Apr 17 21:14:33.070: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Apr 17 21:14:33.073: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 17 21:14:33.081: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-1544  decf62c9-85dc-433d-bf1f-1618e90a2d06 8212 1 2023-04-17 21:14:31 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00215b198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-17 21:14:31 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-04-17 21:14:31 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 17 21:14:33.083: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-1544  61845795-10e7-42c3-a031-cf4beefc7216 8203 1 2023-04-17 21:14:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment decf62c9-85dc-433d-bf1f-1618e90a2d06 0xc003a0e377 0xc003a0e378}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"decf62c9-85dc-433d-bf1f-1618e90a2d06\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a0e428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 21:14:33.083: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Apr 17 21:14:33.083: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-1544  a7db6cfe-a5e4-4423-ae8d-f1367d88c7d9 8211 2 2023-04-17 21:14:26 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment decf62c9-85dc-433d-bf1f-1618e90a2d06 0xc003a0e24f 0xc003a0e260}] [] [{e2e.test Update apps/v1 2023-04-17 21:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"decf62c9-85dc-433d-bf1f-1618e90a2d06\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003a0e318 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 21:14:33.086: INFO: Pod "test-rolling-update-deployment-7549d9f46d-vh84m" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-vh84m test-rolling-update-deployment-7549d9f46d- deployment-1544  659a9741-8e02-4a5e-a5b1-38a830fef484 8202 0 2023-04-17 21:14:31 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:2d12e60ba5ae987cc5bb0ecbe766d8d4ff826fb98808d0c0853c87c9faf15e94 cni.projectcalico.org/podIP:192.168.208.134/32 cni.projectcalico.org/podIPs:192.168.208.134/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 61845795-10e7-42c3-a031-cf4beefc7216 0xc0035eec57 0xc0035eec58}] [] [{calico Update v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"61845795-10e7-42c3-a031-cf4beefc7216\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:14:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hmtmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hmtmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:14:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.134,StartTime:2023-04-17 21:14:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:14:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://e580eabf7d6bab93fbc8b171ead0897f272e4cbc61846fa0b33a2d0a37d818c4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.134,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:14:33.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1544" for this suite. 04/17/23 21:14:33.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:14:33.096
Apr 17 21:14:33.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename dns 04/17/23 21:14:33.097
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:33.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:33.112
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9980.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9980.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 04/17/23 21:14:33.114
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9980.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9980.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 04/17/23 21:14:33.114
STEP: creating a pod to probe /etc/hosts 04/17/23 21:14:33.114
STEP: submitting the pod to kubernetes 04/17/23 21:14:33.114
Apr 17 21:14:33.121: INFO: Waiting up to 15m0s for pod "dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74" in namespace "dns-9980" to be "running"
Apr 17 21:14:33.124: INFO: Pod "dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.425038ms
Apr 17 21:14:35.127: INFO: Pod "dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74": Phase="Running", Reason="", readiness=true. Elapsed: 2.00578399s
Apr 17 21:14:35.127: INFO: Pod "dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74" satisfied condition "running"
STEP: retrieving the pod 04/17/23 21:14:35.127
STEP: looking for the results for each expected name from probers 04/17/23 21:14:35.13
Apr 17 21:14:35.141: INFO: DNS probes using dns-9980/dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74 succeeded

STEP: deleting the pod 04/17/23 21:14:35.141
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 17 21:14:35.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9980" for this suite. 04/17/23 21:14:35.158
------------------------------
• [2.067 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:14:33.096
    Apr 17 21:14:33.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename dns 04/17/23 21:14:33.097
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:33.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:33.112
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9980.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9980.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     04/17/23 21:14:33.114
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9980.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9980.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     04/17/23 21:14:33.114
    STEP: creating a pod to probe /etc/hosts 04/17/23 21:14:33.114
    STEP: submitting the pod to kubernetes 04/17/23 21:14:33.114
    Apr 17 21:14:33.121: INFO: Waiting up to 15m0s for pod "dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74" in namespace "dns-9980" to be "running"
    Apr 17 21:14:33.124: INFO: Pod "dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.425038ms
    Apr 17 21:14:35.127: INFO: Pod "dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74": Phase="Running", Reason="", readiness=true. Elapsed: 2.00578399s
    Apr 17 21:14:35.127: INFO: Pod "dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74" satisfied condition "running"
    STEP: retrieving the pod 04/17/23 21:14:35.127
    STEP: looking for the results for each expected name from probers 04/17/23 21:14:35.13
    Apr 17 21:14:35.141: INFO: DNS probes using dns-9980/dns-test-4d9a8ba4-83c4-4e6c-a7aa-f0ac23e35b74 succeeded

    STEP: deleting the pod 04/17/23 21:14:35.141
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:14:35.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9980" for this suite. 04/17/23 21:14:35.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:14:35.163
Apr 17 21:14:35.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 21:14:35.164
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:35.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:35.178
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 04/17/23 21:14:35.181
Apr 17 21:14:35.187: INFO: Waiting up to 5m0s for pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe" in namespace "emptydir-839" to be "Succeeded or Failed"
Apr 17 21:14:35.191: INFO: Pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928405ms
Apr 17 21:14:37.195: INFO: Pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007730035s
Apr 17 21:14:39.195: INFO: Pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007538482s
STEP: Saw pod success 04/17/23 21:14:39.195
Apr 17 21:14:39.195: INFO: Pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe" satisfied condition "Succeeded or Failed"
Apr 17 21:14:39.197: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe container test-container: <nil>
STEP: delete the pod 04/17/23 21:14:39.209
Apr 17 21:14:39.221: INFO: Waiting for pod pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe to disappear
Apr 17 21:14:39.224: INFO: Pod pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 21:14:39.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-839" for this suite. 04/17/23 21:14:39.229
------------------------------
• [4.071 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:14:35.163
    Apr 17 21:14:35.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 21:14:35.164
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:35.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:35.178
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 04/17/23 21:14:35.181
    Apr 17 21:14:35.187: INFO: Waiting up to 5m0s for pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe" in namespace "emptydir-839" to be "Succeeded or Failed"
    Apr 17 21:14:35.191: INFO: Pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe": Phase="Pending", Reason="", readiness=false. Elapsed: 3.928405ms
    Apr 17 21:14:37.195: INFO: Pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007730035s
    Apr 17 21:14:39.195: INFO: Pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007538482s
    STEP: Saw pod success 04/17/23 21:14:39.195
    Apr 17 21:14:39.195: INFO: Pod "pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe" satisfied condition "Succeeded or Failed"
    Apr 17 21:14:39.197: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe container test-container: <nil>
    STEP: delete the pod 04/17/23 21:14:39.209
    Apr 17 21:14:39.221: INFO: Waiting for pod pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe to disappear
    Apr 17 21:14:39.224: INFO: Pod pod-dcbc0232-8e8b-449a-8faf-9d8698f0d8fe no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:14:39.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-839" for this suite. 04/17/23 21:14:39.229
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:14:39.235
Apr 17 21:14:39.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename gc 04/17/23 21:14:39.236
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:39.247
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:39.249
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 04/17/23 21:14:39.255
STEP: delete the rc 04/17/23 21:14:44.264
STEP: wait for the rc to be deleted 04/17/23 21:14:44.269
Apr 17 21:14:45.294: INFO: 80 pods remaining
Apr 17 21:14:45.294: INFO: 80 pods has nil DeletionTimestamp
Apr 17 21:14:45.294: INFO: 
Apr 17 21:14:46.281: INFO: 71 pods remaining
Apr 17 21:14:46.281: INFO: 70 pods has nil DeletionTimestamp
Apr 17 21:14:46.281: INFO: 
Apr 17 21:14:47.278: INFO: 60 pods remaining
Apr 17 21:14:47.278: INFO: 60 pods has nil DeletionTimestamp
Apr 17 21:14:47.278: INFO: 
Apr 17 21:14:48.277: INFO: 40 pods remaining
Apr 17 21:14:48.277: INFO: 40 pods has nil DeletionTimestamp
Apr 17 21:14:48.277: INFO: 
Apr 17 21:14:49.279: INFO: 31 pods remaining
Apr 17 21:14:49.279: INFO: 31 pods has nil DeletionTimestamp
Apr 17 21:14:49.279: INFO: 
Apr 17 21:14:50.276: INFO: 20 pods remaining
Apr 17 21:14:50.276: INFO: 20 pods has nil DeletionTimestamp
Apr 17 21:14:50.276: INFO: 
STEP: Gathering metrics 04/17/23 21:14:51.274
Apr 17 21:14:51.296: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
Apr 17 21:14:51.299: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.323522ms
Apr 17 21:14:51.299: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
Apr 17 21:14:51.299: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
Apr 17 21:14:51.367: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 17 21:14:51.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-5353" for this suite. 04/17/23 21:14:51.38
------------------------------
• [SLOW TEST] [12.151 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:14:39.235
    Apr 17 21:14:39.235: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename gc 04/17/23 21:14:39.236
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:39.247
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:39.249
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 04/17/23 21:14:39.255
    STEP: delete the rc 04/17/23 21:14:44.264
    STEP: wait for the rc to be deleted 04/17/23 21:14:44.269
    Apr 17 21:14:45.294: INFO: 80 pods remaining
    Apr 17 21:14:45.294: INFO: 80 pods has nil DeletionTimestamp
    Apr 17 21:14:45.294: INFO: 
    Apr 17 21:14:46.281: INFO: 71 pods remaining
    Apr 17 21:14:46.281: INFO: 70 pods has nil DeletionTimestamp
    Apr 17 21:14:46.281: INFO: 
    Apr 17 21:14:47.278: INFO: 60 pods remaining
    Apr 17 21:14:47.278: INFO: 60 pods has nil DeletionTimestamp
    Apr 17 21:14:47.278: INFO: 
    Apr 17 21:14:48.277: INFO: 40 pods remaining
    Apr 17 21:14:48.277: INFO: 40 pods has nil DeletionTimestamp
    Apr 17 21:14:48.277: INFO: 
    Apr 17 21:14:49.279: INFO: 31 pods remaining
    Apr 17 21:14:49.279: INFO: 31 pods has nil DeletionTimestamp
    Apr 17 21:14:49.279: INFO: 
    Apr 17 21:14:50.276: INFO: 20 pods remaining
    Apr 17 21:14:50.276: INFO: 20 pods has nil DeletionTimestamp
    Apr 17 21:14:50.276: INFO: 
    STEP: Gathering metrics 04/17/23 21:14:51.274
    Apr 17 21:14:51.296: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Apr 17 21:14:51.299: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.323522ms
    Apr 17 21:14:51.299: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
    Apr 17 21:14:51.299: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
    Apr 17 21:14:51.367: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:14:51.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-5353" for this suite. 04/17/23 21:14:51.38
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:14:51.388
Apr 17 21:14:51.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-runtime 04/17/23 21:14:51.388
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:51.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:51.408
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 04/17/23 21:14:51.415
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 04/17/23 21:15:08.483
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 04/17/23 21:15:08.485
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 04/17/23 21:15:08.489
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 04/17/23 21:15:08.489
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 04/17/23 21:15:08.505
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 04/17/23 21:15:11.516
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 04/17/23 21:15:13.527
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 04/17/23 21:15:13.531
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 04/17/23 21:15:13.532
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 04/17/23 21:15:13.549
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 04/17/23 21:15:14.557
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 04/17/23 21:15:17.57
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 04/17/23 21:15:17.575
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 04/17/23 21:15:17.575
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 17 21:15:17.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-8942" for this suite. 04/17/23 21:15:17.598
------------------------------
• [SLOW TEST] [26.216 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:14:51.388
    Apr 17 21:14:51.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-runtime 04/17/23 21:14:51.388
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:14:51.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:14:51.408
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 04/17/23 21:14:51.415
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 04/17/23 21:15:08.483
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 04/17/23 21:15:08.485
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 04/17/23 21:15:08.489
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 04/17/23 21:15:08.489
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 04/17/23 21:15:08.505
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 04/17/23 21:15:11.516
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 04/17/23 21:15:13.527
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 04/17/23 21:15:13.531
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 04/17/23 21:15:13.532
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 04/17/23 21:15:13.549
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 04/17/23 21:15:14.557
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 04/17/23 21:15:17.57
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 04/17/23 21:15:17.575
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 04/17/23 21:15:17.575
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:15:17.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-8942" for this suite. 04/17/23 21:15:17.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:15:17.605
Apr 17 21:15:17.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 21:15:17.606
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:17.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:17.623
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-99492746-c5df-4086-95b5-ae1af239d8fe 04/17/23 21:15:17.625
STEP: Creating a pod to test consume secrets 04/17/23 21:15:17.629
Apr 17 21:15:17.638: INFO: Waiting up to 5m0s for pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427" in namespace "secrets-5312" to be "Succeeded or Failed"
Apr 17 21:15:17.641: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427": Phase="Pending", Reason="", readiness=false. Elapsed: 2.377975ms
Apr 17 21:15:19.654: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427": Phase="Running", Reason="", readiness=true. Elapsed: 2.015945264s
Apr 17 21:15:21.644: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427": Phase="Running", Reason="", readiness=false. Elapsed: 4.005871506s
Apr 17 21:15:23.646: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007197064s
STEP: Saw pod success 04/17/23 21:15:23.646
Apr 17 21:15:23.646: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427" satisfied condition "Succeeded or Failed"
Apr 17 21:15:23.648: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427 container secret-env-test: <nil>
STEP: delete the pod 04/17/23 21:15:23.654
Apr 17 21:15:23.668: INFO: Waiting for pod pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427 to disappear
Apr 17 21:15:23.670: INFO: Pod pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 21:15:23.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5312" for this suite. 04/17/23 21:15:23.676
------------------------------
• [SLOW TEST] [6.075 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:15:17.605
    Apr 17 21:15:17.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 21:15:17.606
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:17.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:17.623
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-99492746-c5df-4086-95b5-ae1af239d8fe 04/17/23 21:15:17.625
    STEP: Creating a pod to test consume secrets 04/17/23 21:15:17.629
    Apr 17 21:15:17.638: INFO: Waiting up to 5m0s for pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427" in namespace "secrets-5312" to be "Succeeded or Failed"
    Apr 17 21:15:17.641: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427": Phase="Pending", Reason="", readiness=false. Elapsed: 2.377975ms
    Apr 17 21:15:19.654: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427": Phase="Running", Reason="", readiness=true. Elapsed: 2.015945264s
    Apr 17 21:15:21.644: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427": Phase="Running", Reason="", readiness=false. Elapsed: 4.005871506s
    Apr 17 21:15:23.646: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007197064s
    STEP: Saw pod success 04/17/23 21:15:23.646
    Apr 17 21:15:23.646: INFO: Pod "pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427" satisfied condition "Succeeded or Failed"
    Apr 17 21:15:23.648: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427 container secret-env-test: <nil>
    STEP: delete the pod 04/17/23 21:15:23.654
    Apr 17 21:15:23.668: INFO: Waiting for pod pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427 to disappear
    Apr 17 21:15:23.670: INFO: Pod pod-secrets-7530d959-6b08-4639-9558-bf61a1b60427 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:15:23.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5312" for this suite. 04/17/23 21:15:23.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:15:23.682
Apr 17 21:15:23.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename security-context 04/17/23 21:15:23.682
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:23.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:23.696
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/17/23 21:15:23.698
Apr 17 21:15:23.705: INFO: Waiting up to 5m0s for pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432" in namespace "security-context-3437" to be "Succeeded or Failed"
Apr 17 21:15:23.707: INFO: Pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.432862ms
Apr 17 21:15:25.711: INFO: Pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006475726s
Apr 17 21:15:27.710: INFO: Pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005837767s
STEP: Saw pod success 04/17/23 21:15:27.71
Apr 17 21:15:27.711: INFO: Pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432" satisfied condition "Succeeded or Failed"
Apr 17 21:15:27.713: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432 container test-container: <nil>
STEP: delete the pod 04/17/23 21:15:27.718
Apr 17 21:15:27.726: INFO: Waiting for pod security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432 to disappear
Apr 17 21:15:27.728: INFO: Pod security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 17 21:15:27.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-3437" for this suite. 04/17/23 21:15:27.733
------------------------------
• [4.056 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:15:23.682
    Apr 17 21:15:23.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename security-context 04/17/23 21:15:23.682
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:23.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:23.696
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/17/23 21:15:23.698
    Apr 17 21:15:23.705: INFO: Waiting up to 5m0s for pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432" in namespace "security-context-3437" to be "Succeeded or Failed"
    Apr 17 21:15:23.707: INFO: Pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.432862ms
    Apr 17 21:15:25.711: INFO: Pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006475726s
    Apr 17 21:15:27.710: INFO: Pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005837767s
    STEP: Saw pod success 04/17/23 21:15:27.71
    Apr 17 21:15:27.711: INFO: Pod "security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432" satisfied condition "Succeeded or Failed"
    Apr 17 21:15:27.713: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432 container test-container: <nil>
    STEP: delete the pod 04/17/23 21:15:27.718
    Apr 17 21:15:27.726: INFO: Waiting for pod security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432 to disappear
    Apr 17 21:15:27.728: INFO: Pod security-context-1582e3fa-f5d6-4512-96b3-00eb666a1432 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:15:27.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-3437" for this suite. 04/17/23 21:15:27.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:15:27.738
Apr 17 21:15:27.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename events 04/17/23 21:15:27.739
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:27.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:27.754
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 04/17/23 21:15:27.756
STEP: get a list of Events with a label in the current namespace 04/17/23 21:15:27.766
STEP: delete a list of events 04/17/23 21:15:27.768
Apr 17 21:15:27.769: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 04/17/23 21:15:27.782
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Apr 17 21:15:27.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1901" for this suite. 04/17/23 21:15:27.788
------------------------------
• [0.054 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:15:27.738
    Apr 17 21:15:27.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename events 04/17/23 21:15:27.739
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:27.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:27.754
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 04/17/23 21:15:27.756
    STEP: get a list of Events with a label in the current namespace 04/17/23 21:15:27.766
    STEP: delete a list of events 04/17/23 21:15:27.768
    Apr 17 21:15:27.769: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 04/17/23 21:15:27.782
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:15:27.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1901" for this suite. 04/17/23 21:15:27.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:15:27.792
Apr 17 21:15:27.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 21:15:27.793
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:27.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:27.807
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-6d8cec10-4af8-4edb-92b1-1df31bddb7f8 04/17/23 21:15:27.809
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 21:15:27.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8252" for this suite. 04/17/23 21:15:27.814
------------------------------
• [0.026 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:15:27.792
    Apr 17 21:15:27.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 21:15:27.793
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:27.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:27.807
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-6d8cec10-4af8-4edb-92b1-1df31bddb7f8 04/17/23 21:15:27.809
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:15:27.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8252" for this suite. 04/17/23 21:15:27.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:15:27.819
Apr 17 21:15:27.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 21:15:27.82
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:27.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:27.834
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 04/17/23 21:15:27.836
Apr 17 21:15:27.842: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720" in namespace "projected-1539" to be "Succeeded or Failed"
Apr 17 21:15:27.844: INFO: Pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169455ms
Apr 17 21:15:29.848: INFO: Pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005969159s
Apr 17 21:15:31.847: INFO: Pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005251075s
STEP: Saw pod success 04/17/23 21:15:31.847
Apr 17 21:15:31.847: INFO: Pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720" satisfied condition "Succeeded or Failed"
Apr 17 21:15:31.850: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720 container client-container: <nil>
STEP: delete the pod 04/17/23 21:15:31.859
Apr 17 21:15:31.871: INFO: Waiting for pod downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720 to disappear
Apr 17 21:15:31.873: INFO: Pod downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 21:15:31.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1539" for this suite. 04/17/23 21:15:31.877
------------------------------
• [4.062 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:15:27.819
    Apr 17 21:15:27.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 21:15:27.82
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:27.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:27.834
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 04/17/23 21:15:27.836
    Apr 17 21:15:27.842: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720" in namespace "projected-1539" to be "Succeeded or Failed"
    Apr 17 21:15:27.844: INFO: Pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.169455ms
    Apr 17 21:15:29.848: INFO: Pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005969159s
    Apr 17 21:15:31.847: INFO: Pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005251075s
    STEP: Saw pod success 04/17/23 21:15:31.847
    Apr 17 21:15:31.847: INFO: Pod "downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720" satisfied condition "Succeeded or Failed"
    Apr 17 21:15:31.850: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720 container client-container: <nil>
    STEP: delete the pod 04/17/23 21:15:31.859
    Apr 17 21:15:31.871: INFO: Waiting for pod downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720 to disappear
    Apr 17 21:15:31.873: INFO: Pod downwardapi-volume-7db810f6-6007-4d9e-ab7f-333e568b7720 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:15:31.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1539" for this suite. 04/17/23 21:15:31.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:15:31.882
Apr 17 21:15:31.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replication-controller 04/17/23 21:15:31.883
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:31.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:31.898
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 04/17/23 21:15:31.903
STEP: waiting for RC to be added 04/17/23 21:15:31.906
STEP: waiting for available Replicas 04/17/23 21:15:31.906
STEP: patching ReplicationController 04/17/23 21:15:33.048
STEP: waiting for RC to be modified 04/17/23 21:15:33.056
STEP: patching ReplicationController status 04/17/23 21:15:33.056
STEP: waiting for RC to be modified 04/17/23 21:15:33.062
STEP: waiting for available Replicas 04/17/23 21:15:33.062
STEP: fetching ReplicationController status 04/17/23 21:15:33.066
STEP: patching ReplicationController scale 04/17/23 21:15:33.075
STEP: waiting for RC to be modified 04/17/23 21:15:33.082
STEP: waiting for ReplicationController's scale to be the max amount 04/17/23 21:15:33.082
STEP: fetching ReplicationController; ensuring that it's patched 04/17/23 21:15:34.149
STEP: updating ReplicationController status 04/17/23 21:15:34.152
STEP: waiting for RC to be modified 04/17/23 21:15:34.156
STEP: listing all ReplicationControllers 04/17/23 21:15:34.157
STEP: checking that ReplicationController has expected values 04/17/23 21:15:34.159
STEP: deleting ReplicationControllers by collection 04/17/23 21:15:34.159
STEP: waiting for ReplicationController to have a DELETED watchEvent 04/17/23 21:15:34.166
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 17 21:15:34.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2410" for this suite. 04/17/23 21:15:34.224
------------------------------
• [2.346 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:15:31.882
    Apr 17 21:15:31.882: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replication-controller 04/17/23 21:15:31.883
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:31.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:31.898
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 04/17/23 21:15:31.903
    STEP: waiting for RC to be added 04/17/23 21:15:31.906
    STEP: waiting for available Replicas 04/17/23 21:15:31.906
    STEP: patching ReplicationController 04/17/23 21:15:33.048
    STEP: waiting for RC to be modified 04/17/23 21:15:33.056
    STEP: patching ReplicationController status 04/17/23 21:15:33.056
    STEP: waiting for RC to be modified 04/17/23 21:15:33.062
    STEP: waiting for available Replicas 04/17/23 21:15:33.062
    STEP: fetching ReplicationController status 04/17/23 21:15:33.066
    STEP: patching ReplicationController scale 04/17/23 21:15:33.075
    STEP: waiting for RC to be modified 04/17/23 21:15:33.082
    STEP: waiting for ReplicationController's scale to be the max amount 04/17/23 21:15:33.082
    STEP: fetching ReplicationController; ensuring that it's patched 04/17/23 21:15:34.149
    STEP: updating ReplicationController status 04/17/23 21:15:34.152
    STEP: waiting for RC to be modified 04/17/23 21:15:34.156
    STEP: listing all ReplicationControllers 04/17/23 21:15:34.157
    STEP: checking that ReplicationController has expected values 04/17/23 21:15:34.159
    STEP: deleting ReplicationControllers by collection 04/17/23 21:15:34.159
    STEP: waiting for ReplicationController to have a DELETED watchEvent 04/17/23 21:15:34.166
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:15:34.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2410" for this suite. 04/17/23 21:15:34.224
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:15:34.229
Apr 17 21:15:34.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename statefulset 04/17/23 21:15:34.23
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:34.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:34.244
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8354 04/17/23 21:15:34.246
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-8354 04/17/23 21:15:34.252
Apr 17 21:15:34.260: INFO: Found 0 stateful pods, waiting for 1
Apr 17 21:15:44.264: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 04/17/23 21:15:44.269
STEP: Getting /status 04/17/23 21:15:44.277
Apr 17 21:15:44.280: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 04/17/23 21:15:44.28
Apr 17 21:15:44.288: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 04/17/23 21:15:44.288
Apr 17 21:15:44.289: INFO: Observed &StatefulSet event: ADDED
Apr 17 21:15:44.289: INFO: Found Statefulset ss in namespace statefulset-8354 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 17 21:15:44.290: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 04/17/23 21:15:44.29
Apr 17 21:15:44.290: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 17 21:15:44.296: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 04/17/23 21:15:44.296
Apr 17 21:15:44.297: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 17 21:15:44.297: INFO: Deleting all statefulset in ns statefulset-8354
Apr 17 21:15:44.299: INFO: Scaling statefulset ss to 0
Apr 17 21:15:54.314: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 21:15:54.316: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 17 21:15:54.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8354" for this suite. 04/17/23 21:15:54.331
------------------------------
• [SLOW TEST] [20.106 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:15:34.229
    Apr 17 21:15:34.229: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename statefulset 04/17/23 21:15:34.23
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:34.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:34.244
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8354 04/17/23 21:15:34.246
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-8354 04/17/23 21:15:34.252
    Apr 17 21:15:34.260: INFO: Found 0 stateful pods, waiting for 1
    Apr 17 21:15:44.264: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 04/17/23 21:15:44.269
    STEP: Getting /status 04/17/23 21:15:44.277
    Apr 17 21:15:44.280: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 04/17/23 21:15:44.28
    Apr 17 21:15:44.288: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 04/17/23 21:15:44.288
    Apr 17 21:15:44.289: INFO: Observed &StatefulSet event: ADDED
    Apr 17 21:15:44.289: INFO: Found Statefulset ss in namespace statefulset-8354 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 17 21:15:44.290: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 04/17/23 21:15:44.29
    Apr 17 21:15:44.290: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr 17 21:15:44.296: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 04/17/23 21:15:44.296
    Apr 17 21:15:44.297: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 17 21:15:44.297: INFO: Deleting all statefulset in ns statefulset-8354
    Apr 17 21:15:44.299: INFO: Scaling statefulset ss to 0
    Apr 17 21:15:54.314: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 21:15:54.316: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:15:54.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8354" for this suite. 04/17/23 21:15:54.331
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:15:54.336
Apr 17 21:15:54.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename endpointslicemirroring 04/17/23 21:15:54.336
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:54.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:54.353
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 04/17/23 21:15:54.368
Apr 17 21:15:54.374: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 04/17/23 21:15:56.378
Apr 17 21:15:56.384: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 04/17/23 21:15:58.386
Apr 17 21:15:58.395: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Apr 17 21:16:00.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-7829" for this suite. 04/17/23 21:16:00.402
------------------------------
• [SLOW TEST] [6.072 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:15:54.336
    Apr 17 21:15:54.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename endpointslicemirroring 04/17/23 21:15:54.336
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:15:54.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:15:54.353
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 04/17/23 21:15:54.368
    Apr 17 21:15:54.374: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 04/17/23 21:15:56.378
    Apr 17 21:15:56.384: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 04/17/23 21:15:58.386
    Apr 17 21:15:58.395: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:16:00.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-7829" for this suite. 04/17/23 21:16:00.402
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:16:00.408
Apr 17 21:16:00.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 21:16:00.409
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:00.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:00.423
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Apr 17 21:16:00.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: creating the pod 04/17/23 21:16:00.426
STEP: submitting the pod to kubernetes 04/17/23 21:16:00.426
Apr 17 21:16:00.434: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07" in namespace "pods-6188" to be "running and ready"
Apr 17 21:16:00.436: INFO: Pod "pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.239763ms
Apr 17 21:16:00.436: INFO: The phase of Pod pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:16:02.440: INFO: Pod "pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07": Phase="Running", Reason="", readiness=true. Elapsed: 2.005758418s
Apr 17 21:16:02.440: INFO: The phase of Pod pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07 is Running (Ready = true)
Apr 17 21:16:02.440: INFO: Pod "pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 21:16:02.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6188" for this suite. 04/17/23 21:16:02.531
------------------------------
• [2.128 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:16:00.408
    Apr 17 21:16:00.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 21:16:00.409
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:00.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:00.423
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Apr 17 21:16:00.425: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: creating the pod 04/17/23 21:16:00.426
    STEP: submitting the pod to kubernetes 04/17/23 21:16:00.426
    Apr 17 21:16:00.434: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07" in namespace "pods-6188" to be "running and ready"
    Apr 17 21:16:00.436: INFO: Pod "pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07": Phase="Pending", Reason="", readiness=false. Elapsed: 2.239763ms
    Apr 17 21:16:00.436: INFO: The phase of Pod pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:16:02.440: INFO: Pod "pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07": Phase="Running", Reason="", readiness=true. Elapsed: 2.005758418s
    Apr 17 21:16:02.440: INFO: The phase of Pod pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07 is Running (Ready = true)
    Apr 17 21:16:02.440: INFO: Pod "pod-exec-websocket-e7aee758-cc14-43dd-8746-0e21079a0f07" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:16:02.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6188" for this suite. 04/17/23 21:16:02.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:16:02.536
Apr 17 21:16:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 21:16:02.537
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:02.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:02.552
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 04/17/23 21:16:02.554
STEP: listing secrets in all namespaces to ensure that there are more than zero 04/17/23 21:16:02.557
STEP: patching the secret 04/17/23 21:16:02.561
STEP: deleting the secret using a LabelSelector 04/17/23 21:16:02.569
STEP: listing secrets in all namespaces, searching for label name and value in patch 04/17/23 21:16:02.581
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 21:16:02.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8696" for this suite. 04/17/23 21:16:02.589
------------------------------
• [0.059 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:16:02.536
    Apr 17 21:16:02.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 21:16:02.537
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:02.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:02.552
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 04/17/23 21:16:02.554
    STEP: listing secrets in all namespaces to ensure that there are more than zero 04/17/23 21:16:02.557
    STEP: patching the secret 04/17/23 21:16:02.561
    STEP: deleting the secret using a LabelSelector 04/17/23 21:16:02.569
    STEP: listing secrets in all namespaces, searching for label name and value in patch 04/17/23 21:16:02.581
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:16:02.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8696" for this suite. 04/17/23 21:16:02.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:16:02.596
Apr 17 21:16:02.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 21:16:02.597
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:02.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:02.616
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-2bc32756-920e-4459-9d18-d2cae3bf72b6 04/17/23 21:16:02.618
STEP: Creating a pod to test consume configMaps 04/17/23 21:16:02.622
Apr 17 21:16:02.628: INFO: Waiting up to 5m0s for pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4" in namespace "configmap-9446" to be "Succeeded or Failed"
Apr 17 21:16:02.631: INFO: Pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.261067ms
Apr 17 21:16:04.634: INFO: Pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006074529s
Apr 17 21:16:06.634: INFO: Pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006079893s
STEP: Saw pod success 04/17/23 21:16:06.634
Apr 17 21:16:06.634: INFO: Pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4" satisfied condition "Succeeded or Failed"
Apr 17 21:16:06.637: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 21:16:06.641
Apr 17 21:16:06.652: INFO: Waiting for pod pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4 to disappear
Apr 17 21:16:06.654: INFO: Pod pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 21:16:06.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9446" for this suite. 04/17/23 21:16:06.658
------------------------------
• [4.066 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:16:02.596
    Apr 17 21:16:02.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 21:16:02.597
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:02.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:02.616
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-2bc32756-920e-4459-9d18-d2cae3bf72b6 04/17/23 21:16:02.618
    STEP: Creating a pod to test consume configMaps 04/17/23 21:16:02.622
    Apr 17 21:16:02.628: INFO: Waiting up to 5m0s for pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4" in namespace "configmap-9446" to be "Succeeded or Failed"
    Apr 17 21:16:02.631: INFO: Pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.261067ms
    Apr 17 21:16:04.634: INFO: Pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006074529s
    Apr 17 21:16:06.634: INFO: Pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006079893s
    STEP: Saw pod success 04/17/23 21:16:06.634
    Apr 17 21:16:06.634: INFO: Pod "pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4" satisfied condition "Succeeded or Failed"
    Apr 17 21:16:06.637: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 21:16:06.641
    Apr 17 21:16:06.652: INFO: Waiting for pod pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4 to disappear
    Apr 17 21:16:06.654: INFO: Pod pod-configmaps-61f2481a-0dae-406a-81a7-4247d6d2c5a4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:16:06.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9446" for this suite. 04/17/23 21:16:06.658
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:16:06.663
Apr 17 21:16:06.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 21:16:06.663
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:06.677
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:06.679
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 04/17/23 21:16:06.681
Apr 17 21:16:06.688: INFO: Waiting up to 5m0s for pod "pod-b780c4c4-f736-4096-bca4-508da6011b75" in namespace "emptydir-1867" to be "Succeeded or Failed"
Apr 17 21:16:06.690: INFO: Pod "pod-b780c4c4-f736-4096-bca4-508da6011b75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44067ms
Apr 17 21:16:08.694: INFO: Pod "pod-b780c4c4-f736-4096-bca4-508da6011b75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006155111s
Apr 17 21:16:10.694: INFO: Pod "pod-b780c4c4-f736-4096-bca4-508da6011b75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00589446s
STEP: Saw pod success 04/17/23 21:16:10.694
Apr 17 21:16:10.694: INFO: Pod "pod-b780c4c4-f736-4096-bca4-508da6011b75" satisfied condition "Succeeded or Failed"
Apr 17 21:16:10.696: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-b780c4c4-f736-4096-bca4-508da6011b75 container test-container: <nil>
STEP: delete the pod 04/17/23 21:16:10.7
Apr 17 21:16:10.710: INFO: Waiting for pod pod-b780c4c4-f736-4096-bca4-508da6011b75 to disappear
Apr 17 21:16:10.712: INFO: Pod pod-b780c4c4-f736-4096-bca4-508da6011b75 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 21:16:10.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1867" for this suite. 04/17/23 21:16:10.716
------------------------------
• [4.057 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:16:06.663
    Apr 17 21:16:06.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 21:16:06.663
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:06.677
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:06.679
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 04/17/23 21:16:06.681
    Apr 17 21:16:06.688: INFO: Waiting up to 5m0s for pod "pod-b780c4c4-f736-4096-bca4-508da6011b75" in namespace "emptydir-1867" to be "Succeeded or Failed"
    Apr 17 21:16:06.690: INFO: Pod "pod-b780c4c4-f736-4096-bca4-508da6011b75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.44067ms
    Apr 17 21:16:08.694: INFO: Pod "pod-b780c4c4-f736-4096-bca4-508da6011b75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006155111s
    Apr 17 21:16:10.694: INFO: Pod "pod-b780c4c4-f736-4096-bca4-508da6011b75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00589446s
    STEP: Saw pod success 04/17/23 21:16:10.694
    Apr 17 21:16:10.694: INFO: Pod "pod-b780c4c4-f736-4096-bca4-508da6011b75" satisfied condition "Succeeded or Failed"
    Apr 17 21:16:10.696: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-b780c4c4-f736-4096-bca4-508da6011b75 container test-container: <nil>
    STEP: delete the pod 04/17/23 21:16:10.7
    Apr 17 21:16:10.710: INFO: Waiting for pod pod-b780c4c4-f736-4096-bca4-508da6011b75 to disappear
    Apr 17 21:16:10.712: INFO: Pod pod-b780c4c4-f736-4096-bca4-508da6011b75 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:16:10.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1867" for this suite. 04/17/23 21:16:10.716
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:16:10.72
Apr 17 21:16:10.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 21:16:10.721
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:10.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:10.736
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 04/17/23 21:16:10.738
STEP: Creating a ResourceQuota 04/17/23 21:16:15.74
STEP: Ensuring resource quota status is calculated 04/17/23 21:16:15.746
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 21:16:17.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8601" for this suite. 04/17/23 21:16:17.754
------------------------------
• [SLOW TEST] [7.039 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:16:10.72
    Apr 17 21:16:10.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 21:16:10.721
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:10.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:10.736
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 04/17/23 21:16:10.738
    STEP: Creating a ResourceQuota 04/17/23 21:16:15.74
    STEP: Ensuring resource quota status is calculated 04/17/23 21:16:15.746
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:16:17.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8601" for this suite. 04/17/23 21:16:17.754
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:16:17.76
Apr 17 21:16:17.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 21:16:17.761
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:17.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:17.779
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-9038/configmap-test-a1fd51bd-367a-4f44-b8c0-42065794b4d7 04/17/23 21:16:17.781
STEP: Creating a pod to test consume configMaps 04/17/23 21:16:17.784
Apr 17 21:16:17.791: INFO: Waiting up to 5m0s for pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6" in namespace "configmap-9038" to be "Succeeded or Failed"
Apr 17 21:16:17.793: INFO: Pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465884ms
Apr 17 21:16:19.796: INFO: Pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005026701s
Apr 17 21:16:21.797: INFO: Pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006401503s
STEP: Saw pod success 04/17/23 21:16:21.797
Apr 17 21:16:21.797: INFO: Pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6" satisfied condition "Succeeded or Failed"
Apr 17 21:16:21.800: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6 container env-test: <nil>
STEP: delete the pod 04/17/23 21:16:21.805
Apr 17 21:16:21.815: INFO: Waiting for pod pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6 to disappear
Apr 17 21:16:21.817: INFO: Pod pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 21:16:21.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9038" for this suite. 04/17/23 21:16:21.821
------------------------------
• [4.066 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:16:17.76
    Apr 17 21:16:17.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 21:16:17.761
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:17.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:17.779
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-9038/configmap-test-a1fd51bd-367a-4f44-b8c0-42065794b4d7 04/17/23 21:16:17.781
    STEP: Creating a pod to test consume configMaps 04/17/23 21:16:17.784
    Apr 17 21:16:17.791: INFO: Waiting up to 5m0s for pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6" in namespace "configmap-9038" to be "Succeeded or Failed"
    Apr 17 21:16:17.793: INFO: Pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465884ms
    Apr 17 21:16:19.796: INFO: Pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005026701s
    Apr 17 21:16:21.797: INFO: Pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006401503s
    STEP: Saw pod success 04/17/23 21:16:21.797
    Apr 17 21:16:21.797: INFO: Pod "pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6" satisfied condition "Succeeded or Failed"
    Apr 17 21:16:21.800: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6 container env-test: <nil>
    STEP: delete the pod 04/17/23 21:16:21.805
    Apr 17 21:16:21.815: INFO: Waiting for pod pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6 to disappear
    Apr 17 21:16:21.817: INFO: Pod pod-configmaps-82d2a7e8-705c-4ed7-9a48-54ddfcea36f6 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:16:21.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9038" for this suite. 04/17/23 21:16:21.821
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:16:21.826
Apr 17 21:16:21.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 21:16:21.826
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:21.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:21.843
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 04/17/23 21:16:21.845
Apr 17 21:16:21.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89" in namespace "projected-5742" to be "Succeeded or Failed"
Apr 17 21:16:21.853: INFO: Pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497362ms
Apr 17 21:16:23.856: INFO: Pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005509261s
Apr 17 21:16:25.858: INFO: Pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00741748s
STEP: Saw pod success 04/17/23 21:16:25.858
Apr 17 21:16:25.858: INFO: Pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89" satisfied condition "Succeeded or Failed"
Apr 17 21:16:25.860: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89 container client-container: <nil>
STEP: delete the pod 04/17/23 21:16:25.865
Apr 17 21:16:25.873: INFO: Waiting for pod downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89 to disappear
Apr 17 21:16:25.875: INFO: Pod downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 21:16:25.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5742" for this suite. 04/17/23 21:16:25.879
------------------------------
• [4.058 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:16:21.826
    Apr 17 21:16:21.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 21:16:21.826
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:21.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:21.843
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 04/17/23 21:16:21.845
    Apr 17 21:16:21.851: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89" in namespace "projected-5742" to be "Succeeded or Failed"
    Apr 17 21:16:21.853: INFO: Pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497362ms
    Apr 17 21:16:23.856: INFO: Pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005509261s
    Apr 17 21:16:25.858: INFO: Pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00741748s
    STEP: Saw pod success 04/17/23 21:16:25.858
    Apr 17 21:16:25.858: INFO: Pod "downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89" satisfied condition "Succeeded or Failed"
    Apr 17 21:16:25.860: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89 container client-container: <nil>
    STEP: delete the pod 04/17/23 21:16:25.865
    Apr 17 21:16:25.873: INFO: Waiting for pod downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89 to disappear
    Apr 17 21:16:25.875: INFO: Pod downwardapi-volume-e072a7ea-bacb-4343-a276-d35e1b6ebf89 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:16:25.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5742" for this suite. 04/17/23 21:16:25.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:16:25.885
Apr 17 21:16:25.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-probe 04/17/23 21:16:25.886
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:25.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:25.9
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-01400a3e-98fe-437c-b585-83cfa0cea981 in namespace container-probe-2283 04/17/23 21:16:25.902
Apr 17 21:16:25.908: INFO: Waiting up to 5m0s for pod "liveness-01400a3e-98fe-437c-b585-83cfa0cea981" in namespace "container-probe-2283" to be "not pending"
Apr 17 21:16:25.910: INFO: Pod "liveness-01400a3e-98fe-437c-b585-83cfa0cea981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085509ms
Apr 17 21:16:27.913: INFO: Pod "liveness-01400a3e-98fe-437c-b585-83cfa0cea981": Phase="Running", Reason="", readiness=true. Elapsed: 2.004967279s
Apr 17 21:16:27.913: INFO: Pod "liveness-01400a3e-98fe-437c-b585-83cfa0cea981" satisfied condition "not pending"
Apr 17 21:16:27.913: INFO: Started pod liveness-01400a3e-98fe-437c-b585-83cfa0cea981 in namespace container-probe-2283
STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 21:16:27.913
Apr 17 21:16:27.916: INFO: Initial restart count of pod liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is 0
Apr 17 21:16:47.955: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 1 (20.039629946s elapsed)
Apr 17 21:17:07.991: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 2 (40.075645264s elapsed)
Apr 17 21:17:28.029: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 3 (1m0.113566449s elapsed)
Apr 17 21:17:48.066: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 4 (1m20.150672944s elapsed)
Apr 17 21:18:50.183: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 5 (2m22.267581777s elapsed)
STEP: deleting the pod 04/17/23 21:18:50.183
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 17 21:18:50.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2283" for this suite. 04/17/23 21:18:50.199
------------------------------
• [SLOW TEST] [144.318 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:16:25.885
    Apr 17 21:16:25.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-probe 04/17/23 21:16:25.886
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:16:25.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:16:25.9
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-01400a3e-98fe-437c-b585-83cfa0cea981 in namespace container-probe-2283 04/17/23 21:16:25.902
    Apr 17 21:16:25.908: INFO: Waiting up to 5m0s for pod "liveness-01400a3e-98fe-437c-b585-83cfa0cea981" in namespace "container-probe-2283" to be "not pending"
    Apr 17 21:16:25.910: INFO: Pod "liveness-01400a3e-98fe-437c-b585-83cfa0cea981": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085509ms
    Apr 17 21:16:27.913: INFO: Pod "liveness-01400a3e-98fe-437c-b585-83cfa0cea981": Phase="Running", Reason="", readiness=true. Elapsed: 2.004967279s
    Apr 17 21:16:27.913: INFO: Pod "liveness-01400a3e-98fe-437c-b585-83cfa0cea981" satisfied condition "not pending"
    Apr 17 21:16:27.913: INFO: Started pod liveness-01400a3e-98fe-437c-b585-83cfa0cea981 in namespace container-probe-2283
    STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 21:16:27.913
    Apr 17 21:16:27.916: INFO: Initial restart count of pod liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is 0
    Apr 17 21:16:47.955: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 1 (20.039629946s elapsed)
    Apr 17 21:17:07.991: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 2 (40.075645264s elapsed)
    Apr 17 21:17:28.029: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 3 (1m0.113566449s elapsed)
    Apr 17 21:17:48.066: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 4 (1m20.150672944s elapsed)
    Apr 17 21:18:50.183: INFO: Restart count of pod container-probe-2283/liveness-01400a3e-98fe-437c-b585-83cfa0cea981 is now 5 (2m22.267581777s elapsed)
    STEP: deleting the pod 04/17/23 21:18:50.183
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:18:50.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2283" for this suite. 04/17/23 21:18:50.199
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:18:50.203
Apr 17 21:18:50.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-preemption 04/17/23 21:18:50.204
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:18:50.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:18:50.267
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 17 21:18:50.279: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 17 21:19:50.329: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:19:50.332
Apr 17 21:19:50.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-preemption-path 04/17/23 21:19:50.333
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:50.346
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:50.348
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Apr 17 21:19:50.361: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Apr 17 21:19:50.363: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Apr 17 21:19:50.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:19:50.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-6925" for this suite. 04/17/23 21:19:50.434
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-1484" for this suite. 04/17/23 21:19:50.438
------------------------------
• [SLOW TEST] [60.241 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:18:50.203
    Apr 17 21:18:50.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-preemption 04/17/23 21:18:50.204
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:18:50.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:18:50.267
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 17 21:18:50.279: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 17 21:19:50.329: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:19:50.332
    Apr 17 21:19:50.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-preemption-path 04/17/23 21:19:50.333
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:50.346
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:50.348
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Apr 17 21:19:50.361: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Apr 17 21:19:50.363: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:19:50.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:19:50.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-6925" for this suite. 04/17/23 21:19:50.434
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-1484" for this suite. 04/17/23 21:19:50.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:19:50.445
Apr 17 21:19:50.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename endpointslice 04/17/23 21:19:50.446
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:50.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:50.46
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 17 21:19:54.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2266" for this suite. 04/17/23 21:19:54.617
------------------------------
• [4.177 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:19:50.445
    Apr 17 21:19:50.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename endpointslice 04/17/23 21:19:50.446
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:50.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:50.46
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:19:54.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2266" for this suite. 04/17/23 21:19:54.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:19:54.623
Apr 17 21:19:54.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename svcaccounts 04/17/23 21:19:54.623
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:54.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:54.639
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-qnxt5"  04/17/23 21:19:54.641
Apr 17 21:19:54.645: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-qnxt5"  04/17/23 21:19:54.645
Apr 17 21:19:54.650: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 17 21:19:54.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7728" for this suite. 04/17/23 21:19:54.654
------------------------------
• [0.036 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:19:54.623
    Apr 17 21:19:54.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename svcaccounts 04/17/23 21:19:54.623
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:54.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:54.639
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-qnxt5"  04/17/23 21:19:54.641
    Apr 17 21:19:54.645: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-qnxt5"  04/17/23 21:19:54.645
    Apr 17 21:19:54.650: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:19:54.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7728" for this suite. 04/17/23 21:19:54.654
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:19:54.659
Apr 17 21:19:54.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 21:19:54.659
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:54.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:54.673
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 04/17/23 21:19:54.675
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 21:19:54.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2771" for this suite. 04/17/23 21:19:54.681
------------------------------
• [0.026 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:19:54.659
    Apr 17 21:19:54.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 21:19:54.659
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:54.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:54.673
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 04/17/23 21:19:54.675
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:19:54.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2771" for this suite. 04/17/23 21:19:54.681
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:19:54.685
Apr 17 21:19:54.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename init-container 04/17/23 21:19:54.686
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:54.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:54.701
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 04/17/23 21:19:54.703
Apr 17 21:19:54.703: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:19:59.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6027" for this suite. 04/17/23 21:19:59.472
------------------------------
• [4.792 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:19:54.685
    Apr 17 21:19:54.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename init-container 04/17/23 21:19:54.686
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:54.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:54.701
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 04/17/23 21:19:54.703
    Apr 17 21:19:54.703: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:19:59.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6027" for this suite. 04/17/23 21:19:59.472
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:19:59.478
Apr 17 21:19:59.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubelet-test 04/17/23 21:19:59.478
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:59.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:59.493
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 04/17/23 21:19:59.502
Apr 17 21:19:59.502: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c" in namespace "kubelet-test-6644" to be "completed"
Apr 17 21:19:59.506: INFO: Pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.294118ms
Apr 17 21:20:01.510: INFO: Pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007291539s
Apr 17 21:20:03.509: INFO: Pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00630186s
Apr 17 21:20:03.509: INFO: Pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:03.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6644" for this suite. 04/17/23 21:20:03.525
------------------------------
• [4.053 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:19:59.478
    Apr 17 21:19:59.478: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubelet-test 04/17/23 21:19:59.478
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:19:59.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:19:59.493
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 04/17/23 21:19:59.502
    Apr 17 21:19:59.502: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c" in namespace "kubelet-test-6644" to be "completed"
    Apr 17 21:19:59.506: INFO: Pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.294118ms
    Apr 17 21:20:01.510: INFO: Pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007291539s
    Apr 17 21:20:03.509: INFO: Pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00630186s
    Apr 17 21:20:03.509: INFO: Pod "agnhost-host-aliasesc8f129ee-8121-4130-a59a-a932bdf9076c" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:03.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6644" for this suite. 04/17/23 21:20:03.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:03.531
Apr 17 21:20:03.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sysctl 04/17/23 21:20:03.531
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:03.545
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:03.547
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 04/17/23 21:20:03.549
STEP: Watching for error events or started pod 04/17/23 21:20:03.557
STEP: Waiting for pod completion 04/17/23 21:20:05.561
Apr 17 21:20:05.561: INFO: Waiting up to 3m0s for pod "sysctl-29f3ee04-1d55-4afe-a67d-7334c297c2c6" in namespace "sysctl-8634" to be "completed"
Apr 17 21:20:05.563: INFO: Pod "sysctl-29f3ee04-1d55-4afe-a67d-7334c297c2c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.335309ms
Apr 17 21:20:07.567: INFO: Pod "sysctl-29f3ee04-1d55-4afe-a67d-7334c297c2c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00613867s
Apr 17 21:20:07.567: INFO: Pod "sysctl-29f3ee04-1d55-4afe-a67d-7334c297c2c6" satisfied condition "completed"
STEP: Checking that the pod succeeded 04/17/23 21:20:07.569
STEP: Getting logs from the pod 04/17/23 21:20:07.569
STEP: Checking that the sysctl is actually updated 04/17/23 21:20:07.574
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:07.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-8634" for this suite. 04/17/23 21:20:07.578
------------------------------
• [4.051 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:03.531
    Apr 17 21:20:03.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sysctl 04/17/23 21:20:03.531
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:03.545
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:03.547
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 04/17/23 21:20:03.549
    STEP: Watching for error events or started pod 04/17/23 21:20:03.557
    STEP: Waiting for pod completion 04/17/23 21:20:05.561
    Apr 17 21:20:05.561: INFO: Waiting up to 3m0s for pod "sysctl-29f3ee04-1d55-4afe-a67d-7334c297c2c6" in namespace "sysctl-8634" to be "completed"
    Apr 17 21:20:05.563: INFO: Pod "sysctl-29f3ee04-1d55-4afe-a67d-7334c297c2c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.335309ms
    Apr 17 21:20:07.567: INFO: Pod "sysctl-29f3ee04-1d55-4afe-a67d-7334c297c2c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00613867s
    Apr 17 21:20:07.567: INFO: Pod "sysctl-29f3ee04-1d55-4afe-a67d-7334c297c2c6" satisfied condition "completed"
    STEP: Checking that the pod succeeded 04/17/23 21:20:07.569
    STEP: Getting logs from the pod 04/17/23 21:20:07.569
    STEP: Checking that the sysctl is actually updated 04/17/23 21:20:07.574
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:07.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-8634" for this suite. 04/17/23 21:20:07.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:07.583
Apr 17 21:20:07.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename containers 04/17/23 21:20:07.583
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:07.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:07.599
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 04/17/23 21:20:07.601
Apr 17 21:20:07.607: INFO: Waiting up to 5m0s for pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a" in namespace "containers-5985" to be "Succeeded or Failed"
Apr 17 21:20:07.609: INFO: Pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.310632ms
Apr 17 21:20:09.613: INFO: Pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005702554s
Apr 17 21:20:11.613: INFO: Pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005737288s
STEP: Saw pod success 04/17/23 21:20:11.613
Apr 17 21:20:11.613: INFO: Pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a" satisfied condition "Succeeded or Failed"
Apr 17 21:20:11.615: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a container agnhost-container: <nil>
STEP: delete the pod 04/17/23 21:20:11.62
Apr 17 21:20:11.633: INFO: Waiting for pod client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a to disappear
Apr 17 21:20:11.636: INFO: Pod client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:11.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-5985" for this suite. 04/17/23 21:20:11.64
------------------------------
• [4.064 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:07.583
    Apr 17 21:20:07.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename containers 04/17/23 21:20:07.583
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:07.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:07.599
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 04/17/23 21:20:07.601
    Apr 17 21:20:07.607: INFO: Waiting up to 5m0s for pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a" in namespace "containers-5985" to be "Succeeded or Failed"
    Apr 17 21:20:07.609: INFO: Pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.310632ms
    Apr 17 21:20:09.613: INFO: Pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005702554s
    Apr 17 21:20:11.613: INFO: Pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005737288s
    STEP: Saw pod success 04/17/23 21:20:11.613
    Apr 17 21:20:11.613: INFO: Pod "client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a" satisfied condition "Succeeded or Failed"
    Apr 17 21:20:11.615: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 21:20:11.62
    Apr 17 21:20:11.633: INFO: Waiting for pod client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a to disappear
    Apr 17 21:20:11.636: INFO: Pod client-containers-07d7a55d-ca3e-4d68-8280-fc744840e33a no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:11.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-5985" for this suite. 04/17/23 21:20:11.64
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:11.646
Apr 17 21:20:11.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 21:20:11.647
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:11.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:11.661
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 21:20:11.674
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:20:11.876
STEP: Deploying the webhook pod 04/17/23 21:20:11.884
STEP: Wait for the deployment to be ready 04/17/23 21:20:11.894
Apr 17 21:20:11.900: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 21:20:13.907
STEP: Verifying the service has paired with the endpoint 04/17/23 21:20:13.921
Apr 17 21:20:14.922: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/17/23 21:20:14.925
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/17/23 21:20:14.94
STEP: Creating a dummy validating-webhook-configuration object 04/17/23 21:20:14.952
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 04/17/23 21:20:14.96
STEP: Creating a dummy mutating-webhook-configuration object 04/17/23 21:20:14.964
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 04/17/23 21:20:14.971
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:14.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5840" for this suite. 04/17/23 21:20:15.024
STEP: Destroying namespace "webhook-5840-markers" for this suite. 04/17/23 21:20:15.029
------------------------------
• [3.391 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:11.646
    Apr 17 21:20:11.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 21:20:11.647
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:11.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:11.661
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 21:20:11.674
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:20:11.876
    STEP: Deploying the webhook pod 04/17/23 21:20:11.884
    STEP: Wait for the deployment to be ready 04/17/23 21:20:11.894
    Apr 17 21:20:11.900: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 21:20:13.907
    STEP: Verifying the service has paired with the endpoint 04/17/23 21:20:13.921
    Apr 17 21:20:14.922: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/17/23 21:20:14.925
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 04/17/23 21:20:14.94
    STEP: Creating a dummy validating-webhook-configuration object 04/17/23 21:20:14.952
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 04/17/23 21:20:14.96
    STEP: Creating a dummy mutating-webhook-configuration object 04/17/23 21:20:14.964
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 04/17/23 21:20:14.971
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:14.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5840" for this suite. 04/17/23 21:20:15.024
    STEP: Destroying namespace "webhook-5840-markers" for this suite. 04/17/23 21:20:15.029
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:15.037
Apr 17 21:20:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 21:20:15.038
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:15.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:15.055
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Apr 17 21:20:15.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/17/23 21:20:18.54
Apr 17 21:20:18.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 --namespace=crd-publish-openapi-6655 create -f -'
Apr 17 21:20:20.092: INFO: stderr: ""
Apr 17 21:20:20.092: INFO: stdout: "e2e-test-crd-publish-openapi-5408-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 17 21:20:20.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 --namespace=crd-publish-openapi-6655 delete e2e-test-crd-publish-openapi-5408-crds test-cr'
Apr 17 21:20:20.153: INFO: stderr: ""
Apr 17 21:20:20.153: INFO: stdout: "e2e-test-crd-publish-openapi-5408-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Apr 17 21:20:20.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 --namespace=crd-publish-openapi-6655 apply -f -'
Apr 17 21:20:21.529: INFO: stderr: ""
Apr 17 21:20:21.529: INFO: stdout: "e2e-test-crd-publish-openapi-5408-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Apr 17 21:20:21.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 --namespace=crd-publish-openapi-6655 delete e2e-test-crd-publish-openapi-5408-crds test-cr'
Apr 17 21:20:21.590: INFO: stderr: ""
Apr 17 21:20:21.590: INFO: stdout: "e2e-test-crd-publish-openapi-5408-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 04/17/23 21:20:21.59
Apr 17 21:20:21.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 explain e2e-test-crd-publish-openapi-5408-crds'
Apr 17 21:20:22.934: INFO: stderr: ""
Apr 17 21:20:22.934: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5408-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:26.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6655" for this suite. 04/17/23 21:20:26.802
------------------------------
• [SLOW TEST] [11.769 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:15.037
    Apr 17 21:20:15.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 21:20:15.038
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:15.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:15.055
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Apr 17 21:20:15.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/17/23 21:20:18.54
    Apr 17 21:20:18.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 --namespace=crd-publish-openapi-6655 create -f -'
    Apr 17 21:20:20.092: INFO: stderr: ""
    Apr 17 21:20:20.092: INFO: stdout: "e2e-test-crd-publish-openapi-5408-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Apr 17 21:20:20.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 --namespace=crd-publish-openapi-6655 delete e2e-test-crd-publish-openapi-5408-crds test-cr'
    Apr 17 21:20:20.153: INFO: stderr: ""
    Apr 17 21:20:20.153: INFO: stdout: "e2e-test-crd-publish-openapi-5408-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Apr 17 21:20:20.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 --namespace=crd-publish-openapi-6655 apply -f -'
    Apr 17 21:20:21.529: INFO: stderr: ""
    Apr 17 21:20:21.529: INFO: stdout: "e2e-test-crd-publish-openapi-5408-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Apr 17 21:20:21.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 --namespace=crd-publish-openapi-6655 delete e2e-test-crd-publish-openapi-5408-crds test-cr'
    Apr 17 21:20:21.590: INFO: stderr: ""
    Apr 17 21:20:21.590: INFO: stdout: "e2e-test-crd-publish-openapi-5408-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 04/17/23 21:20:21.59
    Apr 17 21:20:21.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-6655 explain e2e-test-crd-publish-openapi-5408-crds'
    Apr 17 21:20:22.934: INFO: stderr: ""
    Apr 17 21:20:22.934: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5408-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:26.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6655" for this suite. 04/17/23 21:20:26.802
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:26.807
Apr 17 21:20:26.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 21:20:26.807
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:26.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:26.824
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 04/17/23 21:20:26.826
Apr 17 21:20:26.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b" in namespace "projected-437" to be "Succeeded or Failed"
Apr 17 21:20:26.835: INFO: Pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695173ms
Apr 17 21:20:28.838: INFO: Pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00554826s
Apr 17 21:20:30.839: INFO: Pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006754485s
STEP: Saw pod success 04/17/23 21:20:30.839
Apr 17 21:20:30.839: INFO: Pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b" satisfied condition "Succeeded or Failed"
Apr 17 21:20:30.841: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b container client-container: <nil>
STEP: delete the pod 04/17/23 21:20:30.854
Apr 17 21:20:30.862: INFO: Waiting for pod downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b to disappear
Apr 17 21:20:30.864: INFO: Pod downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:30.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-437" for this suite. 04/17/23 21:20:30.868
------------------------------
• [4.066 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:26.807
    Apr 17 21:20:26.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 21:20:26.807
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:26.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:26.824
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 04/17/23 21:20:26.826
    Apr 17 21:20:26.832: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b" in namespace "projected-437" to be "Succeeded or Failed"
    Apr 17 21:20:26.835: INFO: Pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.695173ms
    Apr 17 21:20:28.838: INFO: Pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00554826s
    Apr 17 21:20:30.839: INFO: Pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006754485s
    STEP: Saw pod success 04/17/23 21:20:30.839
    Apr 17 21:20:30.839: INFO: Pod "downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b" satisfied condition "Succeeded or Failed"
    Apr 17 21:20:30.841: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b container client-container: <nil>
    STEP: delete the pod 04/17/23 21:20:30.854
    Apr 17 21:20:30.862: INFO: Waiting for pod downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b to disappear
    Apr 17 21:20:30.864: INFO: Pod downwardapi-volume-e0e3830f-8cf1-4c25-9900-a50b8f6d815b no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:30.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-437" for this suite. 04/17/23 21:20:30.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:30.873
Apr 17 21:20:30.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename lease-test 04/17/23 21:20:30.874
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:30.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:30.889
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:30.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-7101" for this suite. 04/17/23 21:20:30.947
------------------------------
• [0.079 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:30.873
    Apr 17 21:20:30.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename lease-test 04/17/23 21:20:30.874
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:30.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:30.889
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:30.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-7101" for this suite. 04/17/23 21:20:30.947
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:30.953
Apr 17 21:20:30.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 21:20:30.953
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:30.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:30.967
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1339 04/17/23 21:20:30.969
STEP: creating replication controller nodeport-test in namespace services-1339 04/17/23 21:20:30.984
I0417 21:20:30.990179      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1339, replica count: 2
I0417 21:20:34.041363      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 21:20:34.041: INFO: Creating new exec pod
Apr 17 21:20:34.048: INFO: Waiting up to 5m0s for pod "execpods465d" in namespace "services-1339" to be "running"
Apr 17 21:20:34.050: INFO: Pod "execpods465d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260442ms
Apr 17 21:20:36.053: INFO: Pod "execpods465d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004892373s
Apr 17 21:20:36.053: INFO: Pod "execpods465d" satisfied condition "running"
Apr 17 21:20:37.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-1339 exec execpods465d -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Apr 17 21:20:37.190: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Apr 17 21:20:37.190: INFO: stdout: ""
Apr 17 21:20:37.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-1339 exec execpods465d -- /bin/sh -x -c nc -v -z -w 2 10.102.47.111 80'
Apr 17 21:20:37.329: INFO: stderr: "+ nc -v -z -w 2 10.102.47.111 80\nConnection to 10.102.47.111 80 port [tcp/http] succeeded!\n"
Apr 17 21:20:37.329: INFO: stdout: ""
Apr 17 21:20:37.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-1339 exec execpods465d -- /bin/sh -x -c nc -v -z -w 2 10.0.106.231 32135'
Apr 17 21:20:37.469: INFO: stderr: "+ nc -v -z -w 2 10.0.106.231 32135\nConnection to 10.0.106.231 32135 port [tcp/*] succeeded!\n"
Apr 17 21:20:37.469: INFO: stdout: ""
Apr 17 21:20:37.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-1339 exec execpods465d -- /bin/sh -x -c nc -v -z -w 2 10.0.74.52 32135'
Apr 17 21:20:37.597: INFO: stderr: "+ nc -v -z -w 2 10.0.74.52 32135\nConnection to 10.0.74.52 32135 port [tcp/*] succeeded!\n"
Apr 17 21:20:37.597: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:37.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1339" for this suite. 04/17/23 21:20:37.602
------------------------------
• [SLOW TEST] [6.653 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:30.953
    Apr 17 21:20:30.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 21:20:30.953
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:30.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:30.967
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1339 04/17/23 21:20:30.969
    STEP: creating replication controller nodeport-test in namespace services-1339 04/17/23 21:20:30.984
    I0417 21:20:30.990179      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1339, replica count: 2
    I0417 21:20:34.041363      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 17 21:20:34.041: INFO: Creating new exec pod
    Apr 17 21:20:34.048: INFO: Waiting up to 5m0s for pod "execpods465d" in namespace "services-1339" to be "running"
    Apr 17 21:20:34.050: INFO: Pod "execpods465d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.260442ms
    Apr 17 21:20:36.053: INFO: Pod "execpods465d": Phase="Running", Reason="", readiness=true. Elapsed: 2.004892373s
    Apr 17 21:20:36.053: INFO: Pod "execpods465d" satisfied condition "running"
    Apr 17 21:20:37.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-1339 exec execpods465d -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Apr 17 21:20:37.190: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Apr 17 21:20:37.190: INFO: stdout: ""
    Apr 17 21:20:37.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-1339 exec execpods465d -- /bin/sh -x -c nc -v -z -w 2 10.102.47.111 80'
    Apr 17 21:20:37.329: INFO: stderr: "+ nc -v -z -w 2 10.102.47.111 80\nConnection to 10.102.47.111 80 port [tcp/http] succeeded!\n"
    Apr 17 21:20:37.329: INFO: stdout: ""
    Apr 17 21:20:37.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-1339 exec execpods465d -- /bin/sh -x -c nc -v -z -w 2 10.0.106.231 32135'
    Apr 17 21:20:37.469: INFO: stderr: "+ nc -v -z -w 2 10.0.106.231 32135\nConnection to 10.0.106.231 32135 port [tcp/*] succeeded!\n"
    Apr 17 21:20:37.469: INFO: stdout: ""
    Apr 17 21:20:37.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-1339 exec execpods465d -- /bin/sh -x -c nc -v -z -w 2 10.0.74.52 32135'
    Apr 17 21:20:37.597: INFO: stderr: "+ nc -v -z -w 2 10.0.74.52 32135\nConnection to 10.0.74.52 32135 port [tcp/*] succeeded!\n"
    Apr 17 21:20:37.597: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:37.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1339" for this suite. 04/17/23 21:20:37.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:37.607
Apr 17 21:20:37.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 21:20:37.608
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:37.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:37.623
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:37.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4217" for this suite. 04/17/23 21:20:37.631
------------------------------
• [0.031 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:37.607
    Apr 17 21:20:37.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 21:20:37.608
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:37.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:37.623
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:37.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4217" for this suite. 04/17/23 21:20:37.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:37.638
Apr 17 21:20:37.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replication-controller 04/17/23 21:20:37.639
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:37.65
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:37.652
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896 04/17/23 21:20:37.654
Apr 17 21:20:37.661: INFO: Pod name my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896: Found 0 pods out of 1
Apr 17 21:20:42.666: INFO: Pod name my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896: Found 1 pods out of 1
Apr 17 21:20:42.666: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896" are running
Apr 17 21:20:42.666: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd" in namespace "replication-controller-929" to be "running"
Apr 17 21:20:42.669: INFO: Pod "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd": Phase="Running", Reason="", readiness=true. Elapsed: 2.493592ms
Apr 17 21:20:42.669: INFO: Pod "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd" satisfied condition "running"
Apr 17 21:20:42.669: INFO: Pod "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:20:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:20:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:20:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:20:37 +0000 UTC Reason: Message:}])
Apr 17 21:20:42.669: INFO: Trying to dial the pod
Apr 17 21:20:47.678: INFO: Controller my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896: Got expected result from replica 1 [my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd]: "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:47.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-929" for this suite. 04/17/23 21:20:47.682
------------------------------
• [SLOW TEST] [10.050 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:37.638
    Apr 17 21:20:37.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replication-controller 04/17/23 21:20:37.639
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:37.65
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:37.652
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896 04/17/23 21:20:37.654
    Apr 17 21:20:37.661: INFO: Pod name my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896: Found 0 pods out of 1
    Apr 17 21:20:42.666: INFO: Pod name my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896: Found 1 pods out of 1
    Apr 17 21:20:42.666: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896" are running
    Apr 17 21:20:42.666: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd" in namespace "replication-controller-929" to be "running"
    Apr 17 21:20:42.669: INFO: Pod "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd": Phase="Running", Reason="", readiness=true. Elapsed: 2.493592ms
    Apr 17 21:20:42.669: INFO: Pod "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd" satisfied condition "running"
    Apr 17 21:20:42.669: INFO: Pod "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:20:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:20:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:20:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:20:37 +0000 UTC Reason: Message:}])
    Apr 17 21:20:42.669: INFO: Trying to dial the pod
    Apr 17 21:20:47.678: INFO: Controller my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896: Got expected result from replica 1 [my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd]: "my-hostname-basic-9cf79944-3a3e-456b-ba43-9a2fa456d896-7kwcd", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:47.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-929" for this suite. 04/17/23 21:20:47.682
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:47.688
Apr 17 21:20:47.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename security-context-test 04/17/23 21:20:47.689
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:47.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:47.704
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Apr 17 21:20:47.723: INFO: Waiting up to 5m0s for pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e" in namespace "security-context-test-7980" to be "Succeeded or Failed"
Apr 17 21:20:47.727: INFO: Pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101769ms
Apr 17 21:20:49.730: INFO: Pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e": Phase="Running", Reason="", readiness=false. Elapsed: 2.007263711s
Apr 17 21:20:51.730: INFO: Pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006996429s
Apr 17 21:20:51.730: INFO: Pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 17 21:20:51.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7980" for this suite. 04/17/23 21:20:51.734
------------------------------
• [4.051 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:47.688
    Apr 17 21:20:47.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename security-context-test 04/17/23 21:20:47.689
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:47.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:47.704
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Apr 17 21:20:47.723: INFO: Waiting up to 5m0s for pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e" in namespace "security-context-test-7980" to be "Succeeded or Failed"
    Apr 17 21:20:47.727: INFO: Pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.101769ms
    Apr 17 21:20:49.730: INFO: Pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e": Phase="Running", Reason="", readiness=false. Elapsed: 2.007263711s
    Apr 17 21:20:51.730: INFO: Pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006996429s
    Apr 17 21:20:51.730: INFO: Pod "busybox-user-65534-02489a11-b0f2-41ba-9622-16edb43edf3e" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:20:51.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7980" for this suite. 04/17/23 21:20:51.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:20:51.74
Apr 17 21:20:51.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename dns 04/17/23 21:20:51.741
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:51.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:51.756
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 04/17/23 21:20:51.758
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 04/17/23 21:20:51.759
STEP: creating a pod to probe DNS 04/17/23 21:20:51.759
STEP: submitting the pod to kubernetes 04/17/23 21:20:51.759
Apr 17 21:20:51.766: INFO: Waiting up to 15m0s for pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0" in namespace "dns-9759" to be "running"
Apr 17 21:20:51.769: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868396ms
Apr 17 21:20:53.773: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006592239s
Apr 17 21:20:55.772: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006144996s
Apr 17 21:20:57.772: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006181921s
Apr 17 21:20:59.773: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006979961s
Apr 17 21:21:01.772: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Running", Reason="", readiness=true. Elapsed: 10.005987877s
Apr 17 21:21:01.772: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0" satisfied condition "running"
STEP: retrieving the pod 04/17/23 21:21:01.772
STEP: looking for the results for each expected name from probers 04/17/23 21:21:01.775
Apr 17 21:21:01.786: INFO: DNS probes using dns-9759/dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0 succeeded

STEP: deleting the pod 04/17/23 21:21:01.786
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 17 21:21:01.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9759" for this suite. 04/17/23 21:21:01.801
------------------------------
• [SLOW TEST] [10.067 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:20:51.74
    Apr 17 21:20:51.740: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename dns 04/17/23 21:20:51.741
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:20:51.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:20:51.756
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     04/17/23 21:20:51.758
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     04/17/23 21:20:51.759
    STEP: creating a pod to probe DNS 04/17/23 21:20:51.759
    STEP: submitting the pod to kubernetes 04/17/23 21:20:51.759
    Apr 17 21:20:51.766: INFO: Waiting up to 15m0s for pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0" in namespace "dns-9759" to be "running"
    Apr 17 21:20:51.769: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.868396ms
    Apr 17 21:20:53.773: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006592239s
    Apr 17 21:20:55.772: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006144996s
    Apr 17 21:20:57.772: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006181921s
    Apr 17 21:20:59.773: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006979961s
    Apr 17 21:21:01.772: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0": Phase="Running", Reason="", readiness=true. Elapsed: 10.005987877s
    Apr 17 21:21:01.772: INFO: Pod "dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0" satisfied condition "running"
    STEP: retrieving the pod 04/17/23 21:21:01.772
    STEP: looking for the results for each expected name from probers 04/17/23 21:21:01.775
    Apr 17 21:21:01.786: INFO: DNS probes using dns-9759/dns-test-765d31ee-b2cc-40ab-a40f-43efb9d083b0 succeeded

    STEP: deleting the pod 04/17/23 21:21:01.786
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:21:01.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9759" for this suite. 04/17/23 21:21:01.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:21:01.807
Apr 17 21:21:01.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-preemption 04/17/23 21:21:01.808
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:21:01.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:21:01.823
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 17 21:21:01.835: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 17 21:22:01.882: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:01.886
Apr 17 21:22:01.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-preemption-path 04/17/23 21:22:01.887
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:01.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:01.905
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 04/17/23 21:22:01.907
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/17/23 21:22:01.907
Apr 17 21:22:01.913: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7630" to be "running"
Apr 17 21:22:01.915: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204054ms
Apr 17 21:22:03.919: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005973183s
Apr 17 21:22:03.919: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/17/23 21:22:03.921
Apr 17 21:22:03.933: INFO: found a healthy node: ip-10-0-93-18.us-west-2.compute.internal
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Apr 17 21:22:09.994: INFO: pods created so far: [1 1 1]
Apr 17 21:22:09.994: INFO: length of pods created so far: 3
Apr 17 21:22:14.001: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:21.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:21.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-7630" for this suite. 04/17/23 21:22:21.084
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3434" for this suite. 04/17/23 21:22:21.089
------------------------------
• [SLOW TEST] [79.286 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:21:01.807
    Apr 17 21:21:01.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-preemption 04/17/23 21:21:01.808
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:21:01.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:21:01.823
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 17 21:21:01.835: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 17 21:22:01.882: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:01.886
    Apr 17 21:22:01.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-preemption-path 04/17/23 21:22:01.887
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:01.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:01.905
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 04/17/23 21:22:01.907
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/17/23 21:22:01.907
    Apr 17 21:22:01.913: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-7630" to be "running"
    Apr 17 21:22:01.915: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204054ms
    Apr 17 21:22:03.919: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005973183s
    Apr 17 21:22:03.919: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/17/23 21:22:03.921
    Apr 17 21:22:03.933: INFO: found a healthy node: ip-10-0-93-18.us-west-2.compute.internal
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Apr 17 21:22:09.994: INFO: pods created so far: [1 1 1]
    Apr 17 21:22:09.994: INFO: length of pods created so far: 3
    Apr 17 21:22:14.001: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:21.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:21.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-7630" for this suite. 04/17/23 21:22:21.084
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3434" for this suite. 04/17/23 21:22:21.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:21.093
Apr 17 21:22:21.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 21:22:21.094
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:21.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:21.109
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 04/17/23 21:22:21.112
Apr 17 21:22:21.118: INFO: Waiting up to 5m0s for pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3" in namespace "projected-8439" to be "Succeeded or Failed"
Apr 17 21:22:21.120: INFO: Pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.219894ms
Apr 17 21:22:23.123: INFO: Pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005417237s
Apr 17 21:22:25.123: INFO: Pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005494305s
STEP: Saw pod success 04/17/23 21:22:25.123
Apr 17 21:22:25.124: INFO: Pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3" satisfied condition "Succeeded or Failed"
Apr 17 21:22:25.126: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3 container client-container: <nil>
STEP: delete the pod 04/17/23 21:22:25.138
Apr 17 21:22:25.149: INFO: Waiting for pod downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3 to disappear
Apr 17 21:22:25.151: INFO: Pod downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:25.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8439" for this suite. 04/17/23 21:22:25.155
------------------------------
• [4.066 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:21.093
    Apr 17 21:22:21.093: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 21:22:21.094
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:21.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:21.109
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 04/17/23 21:22:21.112
    Apr 17 21:22:21.118: INFO: Waiting up to 5m0s for pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3" in namespace "projected-8439" to be "Succeeded or Failed"
    Apr 17 21:22:21.120: INFO: Pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.219894ms
    Apr 17 21:22:23.123: INFO: Pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005417237s
    Apr 17 21:22:25.123: INFO: Pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005494305s
    STEP: Saw pod success 04/17/23 21:22:25.123
    Apr 17 21:22:25.124: INFO: Pod "downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3" satisfied condition "Succeeded or Failed"
    Apr 17 21:22:25.126: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3 container client-container: <nil>
    STEP: delete the pod 04/17/23 21:22:25.138
    Apr 17 21:22:25.149: INFO: Waiting for pod downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3 to disappear
    Apr 17 21:22:25.151: INFO: Pod downwardapi-volume-752b299a-0ace-4f0a-ba7c-5caae28d3ed3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:25.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8439" for this suite. 04/17/23 21:22:25.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:25.16
Apr 17 21:22:25.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename limitrange 04/17/23 21:22:25.161
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:25.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:25.176
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-lzh52" in namespace "limitrange-7894" 04/17/23 21:22:25.178
STEP: Creating another limitRange in another namespace 04/17/23 21:22:25.182
Apr 17 21:22:25.196: INFO: Namespace "e2e-limitrange-lzh52-8989" created
Apr 17 21:22:25.196: INFO: Creating LimitRange "e2e-limitrange-lzh52" in namespace "e2e-limitrange-lzh52-8989"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-lzh52" 04/17/23 21:22:25.201
Apr 17 21:22:25.203: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-lzh52" in "limitrange-7894" namespace 04/17/23 21:22:25.203
Apr 17 21:22:25.208: INFO: LimitRange "e2e-limitrange-lzh52" has been patched
STEP: Delete LimitRange "e2e-limitrange-lzh52" by Collection with labelSelector: "e2e-limitrange-lzh52=patched" 04/17/23 21:22:25.208
STEP: Confirm that the limitRange "e2e-limitrange-lzh52" has been deleted 04/17/23 21:22:25.214
Apr 17 21:22:25.214: INFO: Requesting list of LimitRange to confirm quantity
Apr 17 21:22:25.217: INFO: Found 0 LimitRange with label "e2e-limitrange-lzh52=patched"
Apr 17 21:22:25.217: INFO: LimitRange "e2e-limitrange-lzh52" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-lzh52" 04/17/23 21:22:25.217
Apr 17 21:22:25.219: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:25.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-7894" for this suite. 04/17/23 21:22:25.223
STEP: Destroying namespace "e2e-limitrange-lzh52-8989" for this suite. 04/17/23 21:22:25.228
------------------------------
• [0.072 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:25.16
    Apr 17 21:22:25.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename limitrange 04/17/23 21:22:25.161
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:25.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:25.176
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-lzh52" in namespace "limitrange-7894" 04/17/23 21:22:25.178
    STEP: Creating another limitRange in another namespace 04/17/23 21:22:25.182
    Apr 17 21:22:25.196: INFO: Namespace "e2e-limitrange-lzh52-8989" created
    Apr 17 21:22:25.196: INFO: Creating LimitRange "e2e-limitrange-lzh52" in namespace "e2e-limitrange-lzh52-8989"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-lzh52" 04/17/23 21:22:25.201
    Apr 17 21:22:25.203: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-lzh52" in "limitrange-7894" namespace 04/17/23 21:22:25.203
    Apr 17 21:22:25.208: INFO: LimitRange "e2e-limitrange-lzh52" has been patched
    STEP: Delete LimitRange "e2e-limitrange-lzh52" by Collection with labelSelector: "e2e-limitrange-lzh52=patched" 04/17/23 21:22:25.208
    STEP: Confirm that the limitRange "e2e-limitrange-lzh52" has been deleted 04/17/23 21:22:25.214
    Apr 17 21:22:25.214: INFO: Requesting list of LimitRange to confirm quantity
    Apr 17 21:22:25.217: INFO: Found 0 LimitRange with label "e2e-limitrange-lzh52=patched"
    Apr 17 21:22:25.217: INFO: LimitRange "e2e-limitrange-lzh52" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-lzh52" 04/17/23 21:22:25.217
    Apr 17 21:22:25.219: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:25.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-7894" for this suite. 04/17/23 21:22:25.223
    STEP: Destroying namespace "e2e-limitrange-lzh52-8989" for this suite. 04/17/23 21:22:25.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:25.233
Apr 17 21:22:25.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 21:22:25.234
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:25.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:25.248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 04/17/23 21:22:25.25
Apr 17 21:22:25.256: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91" in namespace "downward-api-8736" to be "Succeeded or Failed"
Apr 17 21:22:25.259: INFO: Pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773267ms
Apr 17 21:22:27.262: INFO: Pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00584315s
Apr 17 21:22:29.263: INFO: Pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006771578s
STEP: Saw pod success 04/17/23 21:22:29.263
Apr 17 21:22:29.263: INFO: Pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91" satisfied condition "Succeeded or Failed"
Apr 17 21:22:29.265: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91 container client-container: <nil>
STEP: delete the pod 04/17/23 21:22:29.281
Apr 17 21:22:29.311: INFO: Waiting for pod downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91 to disappear
Apr 17 21:22:29.321: INFO: Pod downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:29.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8736" for this suite. 04/17/23 21:22:29.335
------------------------------
• [4.127 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:25.233
    Apr 17 21:22:25.233: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 21:22:25.234
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:25.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:25.248
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 04/17/23 21:22:25.25
    Apr 17 21:22:25.256: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91" in namespace "downward-api-8736" to be "Succeeded or Failed"
    Apr 17 21:22:25.259: INFO: Pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773267ms
    Apr 17 21:22:27.262: INFO: Pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00584315s
    Apr 17 21:22:29.263: INFO: Pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006771578s
    STEP: Saw pod success 04/17/23 21:22:29.263
    Apr 17 21:22:29.263: INFO: Pod "downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91" satisfied condition "Succeeded or Failed"
    Apr 17 21:22:29.265: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91 container client-container: <nil>
    STEP: delete the pod 04/17/23 21:22:29.281
    Apr 17 21:22:29.311: INFO: Waiting for pod downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91 to disappear
    Apr 17 21:22:29.321: INFO: Pod downwardapi-volume-cc4f98e2-16ff-417d-a89c-efaa7d817d91 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:29.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8736" for this suite. 04/17/23 21:22:29.335
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:29.36
Apr 17 21:22:29.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename namespaces 04/17/23 21:22:29.361
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:29.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:29.414
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-1484" 04/17/23 21:22:29.418
Apr 17 21:22:29.439: INFO: Namespace "namespaces-1484" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"a8206136-e860-42f7-86bb-baf1eab83f33", "kubernetes.io/metadata.name":"namespaces-1484", "namespaces-1484":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:29.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1484" for this suite. 04/17/23 21:22:29.449
------------------------------
• [0.098 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:29.36
    Apr 17 21:22:29.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename namespaces 04/17/23 21:22:29.361
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:29.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:29.414
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-1484" 04/17/23 21:22:29.418
    Apr 17 21:22:29.439: INFO: Namespace "namespaces-1484" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"a8206136-e860-42f7-86bb-baf1eab83f33", "kubernetes.io/metadata.name":"namespaces-1484", "namespaces-1484":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:29.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1484" for this suite. 04/17/23 21:22:29.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:29.459
Apr 17 21:22:29.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename podtemplate 04/17/23 21:22:29.46
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:29.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:29.49
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 04/17/23 21:22:29.492
STEP: Replace a pod template 04/17/23 21:22:29.498
Apr 17 21:22:29.505: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:29.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-3540" for this suite. 04/17/23 21:22:29.511
------------------------------
• [0.063 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:29.459
    Apr 17 21:22:29.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename podtemplate 04/17/23 21:22:29.46
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:29.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:29.49
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 04/17/23 21:22:29.492
    STEP: Replace a pod template 04/17/23 21:22:29.498
    Apr 17 21:22:29.505: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:29.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-3540" for this suite. 04/17/23 21:22:29.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:29.522
Apr 17 21:22:29.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename security-context-test 04/17/23 21:22:29.523
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:29.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:29.54
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Apr 17 21:22:29.549: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7" in namespace "security-context-test-2585" to be "Succeeded or Failed"
Apr 17 21:22:29.553: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428809ms
Apr 17 21:22:31.556: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006635585s
Apr 17 21:22:33.556: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006812277s
Apr 17 21:22:35.556: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006217479s
Apr 17 21:22:35.556: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:35.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-2585" for this suite. 04/17/23 21:22:35.564
------------------------------
• [SLOW TEST] [6.048 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:29.522
    Apr 17 21:22:29.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename security-context-test 04/17/23 21:22:29.523
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:29.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:29.54
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Apr 17 21:22:29.549: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7" in namespace "security-context-test-2585" to be "Succeeded or Failed"
    Apr 17 21:22:29.553: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428809ms
    Apr 17 21:22:31.556: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006635585s
    Apr 17 21:22:33.556: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006812277s
    Apr 17 21:22:35.556: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.006217479s
    Apr 17 21:22:35.556: INFO: Pod "alpine-nnp-false-fb799756-af37-41ad-b4a4-258903c084a7" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:35.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-2585" for this suite. 04/17/23 21:22:35.564
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:35.57
Apr 17 21:22:35.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 21:22:35.571
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:35.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:35.586
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 04/17/23 21:22:35.588
Apr 17 21:22:35.595: INFO: Waiting up to 5m0s for pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02" in namespace "emptydir-4964" to be "Succeeded or Failed"
Apr 17 21:22:35.599: INFO: Pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.653792ms
Apr 17 21:22:37.603: INFO: Pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007496792s
Apr 17 21:22:39.602: INFO: Pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006601631s
STEP: Saw pod success 04/17/23 21:22:39.602
Apr 17 21:22:39.602: INFO: Pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02" satisfied condition "Succeeded or Failed"
Apr 17 21:22:39.604: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-c2457f6a-5db3-446b-b07e-63f771397f02 container test-container: <nil>
STEP: delete the pod 04/17/23 21:22:39.609
Apr 17 21:22:39.619: INFO: Waiting for pod pod-c2457f6a-5db3-446b-b07e-63f771397f02 to disappear
Apr 17 21:22:39.622: INFO: Pod pod-c2457f6a-5db3-446b-b07e-63f771397f02 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:39.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4964" for this suite. 04/17/23 21:22:39.626
------------------------------
• [4.061 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:35.57
    Apr 17 21:22:35.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 21:22:35.571
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:35.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:35.586
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 04/17/23 21:22:35.588
    Apr 17 21:22:35.595: INFO: Waiting up to 5m0s for pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02" in namespace "emptydir-4964" to be "Succeeded or Failed"
    Apr 17 21:22:35.599: INFO: Pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.653792ms
    Apr 17 21:22:37.603: INFO: Pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007496792s
    Apr 17 21:22:39.602: INFO: Pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006601631s
    STEP: Saw pod success 04/17/23 21:22:39.602
    Apr 17 21:22:39.602: INFO: Pod "pod-c2457f6a-5db3-446b-b07e-63f771397f02" satisfied condition "Succeeded or Failed"
    Apr 17 21:22:39.604: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-c2457f6a-5db3-446b-b07e-63f771397f02 container test-container: <nil>
    STEP: delete the pod 04/17/23 21:22:39.609
    Apr 17 21:22:39.619: INFO: Waiting for pod pod-c2457f6a-5db3-446b-b07e-63f771397f02 to disappear
    Apr 17 21:22:39.622: INFO: Pod pod-c2457f6a-5db3-446b-b07e-63f771397f02 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:39.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4964" for this suite. 04/17/23 21:22:39.626
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:39.632
Apr 17 21:22:39.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 21:22:39.632
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:39.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:39.648
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 04/17/23 21:22:39.653
STEP: watching for the Service to be added 04/17/23 21:22:39.667
Apr 17 21:22:39.668: INFO: Found Service test-service-rdw28 in namespace services-9518 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Apr 17 21:22:39.668: INFO: Service test-service-rdw28 created
STEP: Getting /status 04/17/23 21:22:39.668
Apr 17 21:22:39.671: INFO: Service test-service-rdw28 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 04/17/23 21:22:39.671
STEP: watching for the Service to be patched 04/17/23 21:22:39.677
Apr 17 21:22:39.678: INFO: observed Service test-service-rdw28 in namespace services-9518 with annotations: map[] & LoadBalancer: {[]}
Apr 17 21:22:39.678: INFO: Found Service test-service-rdw28 in namespace services-9518 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Apr 17 21:22:39.678: INFO: Service test-service-rdw28 has service status patched
STEP: updating the ServiceStatus 04/17/23 21:22:39.678
Apr 17 21:22:39.685: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 04/17/23 21:22:39.685
Apr 17 21:22:39.687: INFO: Observed Service test-service-rdw28 in namespace services-9518 with annotations: map[] & Conditions: {[]}
Apr 17 21:22:39.687: INFO: Observed event: &Service{ObjectMeta:{test-service-rdw28  services-9518  89c2cb25-ab07-479e-8127-b683108bf95e 15937 0 2023-04-17 21:22:39 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-17 21:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-17 21:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.105.233.229,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.105.233.229],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Apr 17 21:22:39.687: INFO: Found Service test-service-rdw28 in namespace services-9518 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 17 21:22:39.687: INFO: Service test-service-rdw28 has service status updated
STEP: patching the service 04/17/23 21:22:39.687
STEP: watching for the Service to be patched 04/17/23 21:22:39.699
Apr 17 21:22:39.701: INFO: observed Service test-service-rdw28 in namespace services-9518 with labels: map[test-service-static:true]
Apr 17 21:22:39.701: INFO: observed Service test-service-rdw28 in namespace services-9518 with labels: map[test-service-static:true]
Apr 17 21:22:39.701: INFO: observed Service test-service-rdw28 in namespace services-9518 with labels: map[test-service-static:true]
Apr 17 21:22:39.701: INFO: Found Service test-service-rdw28 in namespace services-9518 with labels: map[test-service:patched test-service-static:true]
Apr 17 21:22:39.701: INFO: Service test-service-rdw28 patched
STEP: deleting the service 04/17/23 21:22:39.701
STEP: watching for the Service to be deleted 04/17/23 21:22:39.716
Apr 17 21:22:39.717: INFO: Observed event: ADDED
Apr 17 21:22:39.717: INFO: Observed event: MODIFIED
Apr 17 21:22:39.717: INFO: Observed event: MODIFIED
Apr 17 21:22:39.717: INFO: Observed event: MODIFIED
Apr 17 21:22:39.717: INFO: Found Service test-service-rdw28 in namespace services-9518 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Apr 17 21:22:39.717: INFO: Service test-service-rdw28 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:39.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9518" for this suite. 04/17/23 21:22:39.722
------------------------------
• [0.096 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:39.632
    Apr 17 21:22:39.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 21:22:39.632
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:39.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:39.648
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 04/17/23 21:22:39.653
    STEP: watching for the Service to be added 04/17/23 21:22:39.667
    Apr 17 21:22:39.668: INFO: Found Service test-service-rdw28 in namespace services-9518 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Apr 17 21:22:39.668: INFO: Service test-service-rdw28 created
    STEP: Getting /status 04/17/23 21:22:39.668
    Apr 17 21:22:39.671: INFO: Service test-service-rdw28 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 04/17/23 21:22:39.671
    STEP: watching for the Service to be patched 04/17/23 21:22:39.677
    Apr 17 21:22:39.678: INFO: observed Service test-service-rdw28 in namespace services-9518 with annotations: map[] & LoadBalancer: {[]}
    Apr 17 21:22:39.678: INFO: Found Service test-service-rdw28 in namespace services-9518 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Apr 17 21:22:39.678: INFO: Service test-service-rdw28 has service status patched
    STEP: updating the ServiceStatus 04/17/23 21:22:39.678
    Apr 17 21:22:39.685: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 04/17/23 21:22:39.685
    Apr 17 21:22:39.687: INFO: Observed Service test-service-rdw28 in namespace services-9518 with annotations: map[] & Conditions: {[]}
    Apr 17 21:22:39.687: INFO: Observed event: &Service{ObjectMeta:{test-service-rdw28  services-9518  89c2cb25-ab07-479e-8127-b683108bf95e 15937 0 2023-04-17 21:22:39 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-04-17 21:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-04-17 21:22:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.105.233.229,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.105.233.229],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Apr 17 21:22:39.687: INFO: Found Service test-service-rdw28 in namespace services-9518 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 17 21:22:39.687: INFO: Service test-service-rdw28 has service status updated
    STEP: patching the service 04/17/23 21:22:39.687
    STEP: watching for the Service to be patched 04/17/23 21:22:39.699
    Apr 17 21:22:39.701: INFO: observed Service test-service-rdw28 in namespace services-9518 with labels: map[test-service-static:true]
    Apr 17 21:22:39.701: INFO: observed Service test-service-rdw28 in namespace services-9518 with labels: map[test-service-static:true]
    Apr 17 21:22:39.701: INFO: observed Service test-service-rdw28 in namespace services-9518 with labels: map[test-service-static:true]
    Apr 17 21:22:39.701: INFO: Found Service test-service-rdw28 in namespace services-9518 with labels: map[test-service:patched test-service-static:true]
    Apr 17 21:22:39.701: INFO: Service test-service-rdw28 patched
    STEP: deleting the service 04/17/23 21:22:39.701
    STEP: watching for the Service to be deleted 04/17/23 21:22:39.716
    Apr 17 21:22:39.717: INFO: Observed event: ADDED
    Apr 17 21:22:39.717: INFO: Observed event: MODIFIED
    Apr 17 21:22:39.717: INFO: Observed event: MODIFIED
    Apr 17 21:22:39.717: INFO: Observed event: MODIFIED
    Apr 17 21:22:39.717: INFO: Found Service test-service-rdw28 in namespace services-9518 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Apr 17 21:22:39.717: INFO: Service test-service-rdw28 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:39.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9518" for this suite. 04/17/23 21:22:39.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:39.728
Apr 17 21:22:39.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename init-container 04/17/23 21:22:39.728
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:39.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:39.746
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 04/17/23 21:22:39.749
Apr 17 21:22:39.749: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:22:44.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6856" for this suite. 04/17/23 21:22:44.833
------------------------------
• [SLOW TEST] [5.110 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:39.728
    Apr 17 21:22:39.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename init-container 04/17/23 21:22:39.728
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:39.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:39.746
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 04/17/23 21:22:39.749
    Apr 17 21:22:39.749: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:22:44.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6856" for this suite. 04/17/23 21:22:44.833
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:22:44.84
Apr 17 21:22:44.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename statefulset 04/17/23 21:22:44.841
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:44.855
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:44.857
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3484 04/17/23 21:22:44.859
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-3484 04/17/23 21:22:44.864
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3484 04/17/23 21:22:44.869
Apr 17 21:22:44.873: INFO: Found 0 stateful pods, waiting for 1
Apr 17 21:22:54.878: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 04/17/23 21:22:54.878
Apr 17 21:22:54.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 21:22:55.039: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 21:22:55.039: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 21:22:55.039: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 21:22:55.041: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 17 21:23:05.046: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 21:23:05.046: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 21:23:05.057: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 17 21:23:05.057: INFO: ss-0  ip-10-0-106-231.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  }]
Apr 17 21:23:05.057: INFO: 
Apr 17 21:23:05.057: INFO: StatefulSet ss has not reached scale 3, at 1
Apr 17 21:23:06.060: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997769251s
Apr 17 21:23:07.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994703872s
Apr 17 21:23:08.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990624355s
Apr 17 21:23:09.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988018938s
Apr 17 21:23:10.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984962595s
Apr 17 21:23:11.076: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981967384s
Apr 17 21:23:12.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978839297s
Apr 17 21:23:13.082: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975785841s
Apr 17 21:23:14.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 972.51055ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3484 04/17/23 21:23:15.086
Apr 17 21:23:15.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 21:23:15.247: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 21:23:15.247: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 21:23:15.247: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 21:23:15.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 21:23:15.398: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 17 21:23:15.398: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 21:23:15.398: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 21:23:15.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 21:23:15.537: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr 17 21:23:15.537: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 21:23:15.537: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 21:23:15.540: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr 17 21:23:25.544: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 21:23:25.544: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 21:23:25.544: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 04/17/23 21:23:25.544
Apr 17 21:23:25.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 21:23:25.675: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 21:23:25.675: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 21:23:25.675: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 21:23:25.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 21:23:25.823: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 21:23:25.823: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 21:23:25.823: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 21:23:25.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 21:23:25.959: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 21:23:25.959: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 21:23:25.959: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 21:23:25.959: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 21:23:25.962: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr 17 21:23:35.970: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 21:23:35.970: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 21:23:35.970: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 21:23:35.980: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 17 21:23:35.980: INFO: ss-0  ip-10-0-106-231.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  }]
Apr 17 21:23:35.980: INFO: ss-1  ip-10-0-74-52.us-west-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  }]
Apr 17 21:23:35.980: INFO: ss-2  ip-10-0-93-18.us-west-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  }]
Apr 17 21:23:35.980: INFO: 
Apr 17 21:23:35.980: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 17 21:23:36.983: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Apr 17 21:23:36.983: INFO: ss-0  ip-10-0-106-231.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  }]
Apr 17 21:23:36.983: INFO: ss-1  ip-10-0-74-52.us-west-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  }]
Apr 17 21:23:36.983: INFO: ss-2  ip-10-0-93-18.us-west-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  }]
Apr 17 21:23:36.983: INFO: 
Apr 17 21:23:36.983: INFO: StatefulSet ss has not reached scale 0, at 3
Apr 17 21:23:37.986: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993850492s
Apr 17 21:23:38.989: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.991236723s
Apr 17 21:23:39.992: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.987915074s
Apr 17 21:23:40.995: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.985154958s
Apr 17 21:23:41.998: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.982272181s
Apr 17 21:23:43.001: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.979402226s
Apr 17 21:23:44.003: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.976509936s
Apr 17 21:23:45.007: INFO: Verifying statefulset ss doesn't scale past 0 for another 973.961632ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3484 04/17/23 21:23:46.007
Apr 17 21:23:46.010: INFO: Scaling statefulset ss to 0
Apr 17 21:23:46.017: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 17 21:23:46.019: INFO: Deleting all statefulset in ns statefulset-3484
Apr 17 21:23:46.021: INFO: Scaling statefulset ss to 0
Apr 17 21:23:46.031: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 21:23:46.033: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 17 21:23:46.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3484" for this suite. 04/17/23 21:23:46.046
------------------------------
• [SLOW TEST] [61.215 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:22:44.84
    Apr 17 21:22:44.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename statefulset 04/17/23 21:22:44.841
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:22:44.855
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:22:44.857
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3484 04/17/23 21:22:44.859
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-3484 04/17/23 21:22:44.864
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3484 04/17/23 21:22:44.869
    Apr 17 21:22:44.873: INFO: Found 0 stateful pods, waiting for 1
    Apr 17 21:22:54.878: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 04/17/23 21:22:54.878
    Apr 17 21:22:54.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 21:22:55.039: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 21:22:55.039: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 21:22:55.039: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 17 21:22:55.041: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Apr 17 21:23:05.046: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 17 21:23:05.046: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 21:23:05.057: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
    Apr 17 21:23:05.057: INFO: ss-0  ip-10-0-106-231.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  }]
    Apr 17 21:23:05.057: INFO: 
    Apr 17 21:23:05.057: INFO: StatefulSet ss has not reached scale 3, at 1
    Apr 17 21:23:06.060: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997769251s
    Apr 17 21:23:07.063: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994703872s
    Apr 17 21:23:08.067: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990624355s
    Apr 17 21:23:09.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988018938s
    Apr 17 21:23:10.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984962595s
    Apr 17 21:23:11.076: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981967384s
    Apr 17 21:23:12.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978839297s
    Apr 17 21:23:13.082: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975785841s
    Apr 17 21:23:14.085: INFO: Verifying statefulset ss doesn't scale past 3 for another 972.51055ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3484 04/17/23 21:23:15.086
    Apr 17 21:23:15.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 17 21:23:15.247: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 17 21:23:15.247: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 17 21:23:15.247: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 17 21:23:15.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 17 21:23:15.398: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Apr 17 21:23:15.398: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 17 21:23:15.398: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 17 21:23:15.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 17 21:23:15.537: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Apr 17 21:23:15.537: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 17 21:23:15.537: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 17 21:23:15.540: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Apr 17 21:23:25.544: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 21:23:25.544: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 21:23:25.544: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 04/17/23 21:23:25.544
    Apr 17 21:23:25.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 21:23:25.675: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 21:23:25.675: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 21:23:25.675: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 17 21:23:25.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 21:23:25.823: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 21:23:25.823: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 21:23:25.823: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 17 21:23:25.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-3484 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 21:23:25.959: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 21:23:25.959: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 21:23:25.959: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 17 21:23:25.959: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 21:23:25.962: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Apr 17 21:23:35.970: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 17 21:23:35.970: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Apr 17 21:23:35.970: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Apr 17 21:23:35.980: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
    Apr 17 21:23:35.980: INFO: ss-0  ip-10-0-106-231.us-west-2.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  }]
    Apr 17 21:23:35.980: INFO: ss-1  ip-10-0-74-52.us-west-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  }]
    Apr 17 21:23:35.980: INFO: ss-2  ip-10-0-93-18.us-west-2.compute.internal    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  }]
    Apr 17 21:23:35.980: INFO: 
    Apr 17 21:23:35.980: INFO: StatefulSet ss has not reached scale 0, at 3
    Apr 17 21:23:36.983: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
    Apr 17 21:23:36.983: INFO: ss-0  ip-10-0-106-231.us-west-2.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:22:44 +0000 UTC  }]
    Apr 17 21:23:36.983: INFO: ss-1  ip-10-0-74-52.us-west-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  }]
    Apr 17 21:23:36.983: INFO: ss-2  ip-10-0-93-18.us-west-2.compute.internal    Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:26 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 21:23:05 +0000 UTC  }]
    Apr 17 21:23:36.983: INFO: 
    Apr 17 21:23:36.983: INFO: StatefulSet ss has not reached scale 0, at 3
    Apr 17 21:23:37.986: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.993850492s
    Apr 17 21:23:38.989: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.991236723s
    Apr 17 21:23:39.992: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.987915074s
    Apr 17 21:23:40.995: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.985154958s
    Apr 17 21:23:41.998: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.982272181s
    Apr 17 21:23:43.001: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.979402226s
    Apr 17 21:23:44.003: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.976509936s
    Apr 17 21:23:45.007: INFO: Verifying statefulset ss doesn't scale past 0 for another 973.961632ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3484 04/17/23 21:23:46.007
    Apr 17 21:23:46.010: INFO: Scaling statefulset ss to 0
    Apr 17 21:23:46.017: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 17 21:23:46.019: INFO: Deleting all statefulset in ns statefulset-3484
    Apr 17 21:23:46.021: INFO: Scaling statefulset ss to 0
    Apr 17 21:23:46.031: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 21:23:46.033: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:23:46.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3484" for this suite. 04/17/23 21:23:46.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:23:46.056
Apr 17 21:23:46.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 21:23:46.056
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:46.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:46.072
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Apr 17 21:23:46.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2860 version'
Apr 17 21:23:46.120: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Apr 17 21:23:46.120: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:12Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 21:23:46.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2860" for this suite. 04/17/23 21:23:46.124
------------------------------
• [0.073 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:23:46.056
    Apr 17 21:23:46.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 21:23:46.056
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:46.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:46.072
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Apr 17 21:23:46.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2860 version'
    Apr 17 21:23:46.120: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Apr 17 21:23:46.120: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:40:17Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.3\", GitCommit:\"9e644106593f3f4aa98f8a84b23db5fa378900bd\", GitTreeState:\"clean\", BuildDate:\"2023-03-15T13:33:12Z\", GoVersion:\"go1.19.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:23:46.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2860" for this suite. 04/17/23 21:23:46.124
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:23:46.13
Apr 17 21:23:46.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-pred 04/17/23 21:23:46.13
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:46.144
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:46.147
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 17 21:23:46.149: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 17 21:23:46.157: INFO: Waiting for terminating namespaces to be deleted...
Apr 17 21:23:46.159: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-106-231.us-west-2.compute.internal before test
Apr 17 21:23:46.168: INFO: calico-node-qwskb from calico-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.168: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:23:46.168: INFO: calico-typha-6dc6fbd9f5-tcn27 from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.168: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:23:46.168: INFO: csi-node-driver-pptcd from calico-system started at 2023-04-17 21:05:10 +0000 UTC (2 container statuses recorded)
Apr 17 21:23:46.168: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:23:46.168: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:23:46.168: INFO: ebs-csi-node-lhxwm from kube-system started at 2023-04-17 21:05:10 +0000 UTC (3 container statuses recorded)
Apr 17 21:23:46.168: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:23:46.168: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:23:46.168: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:23:46.168: INFO: kube-proxy-h249s from kube-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.168: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:23:46.168: INFO: node-feature-discovery-worker-9bc5l from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.168: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:23:46.168: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:23:46.168: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:23:46.168: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:23:46.168: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-64-189.us-west-2.compute.internal before test
Apr 17 21:23:46.177: INFO: calico-node-b7r2c from calico-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.177: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:23:46.177: INFO: csi-node-driver-rzhbn from calico-system started at 2023-04-17 21:05:08 +0000 UTC (2 container statuses recorded)
Apr 17 21:23:46.177: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:23:46.177: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:23:46.177: INFO: ebs-csi-node-7xssn from kube-system started at 2023-04-17 21:05:08 +0000 UTC (3 container statuses recorded)
Apr 17 21:23:46.177: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:23:46.177: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:23:46.177: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:23:46.177: INFO: kube-proxy-j772r from kube-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.177: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:23:46.177: INFO: node-feature-discovery-worker-cbvbq from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.177: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:23:46.177: INFO: sonobuoy-e2e-job-777eb8e1ef7a4686 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:23:46.177: INFO: 	Container e2e ready: true, restart count 0
Apr 17 21:23:46.177: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:23:46.177: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:23:46.177: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:23:46.177: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:23:46.177: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-74-52.us-west-2.compute.internal before test
Apr 17 21:23:46.186: INFO: calico-node-pqj88 from calico-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.186: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:23:46.186: INFO: calico-typha-6dc6fbd9f5-xbmcd from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.186: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:23:46.186: INFO: csi-node-driver-n474j from calico-system started at 2023-04-17 21:05:14 +0000 UTC (2 container statuses recorded)
Apr 17 21:23:46.186: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:23:46.186: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:23:46.186: INFO: ebs-csi-node-xhdpg from kube-system started at 2023-04-17 21:05:14 +0000 UTC (3 container statuses recorded)
Apr 17 21:23:46.186: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:23:46.186: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:23:46.186: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:23:46.186: INFO: kube-proxy-8tbrz from kube-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.186: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:23:46.186: INFO: node-feature-discovery-worker-bl2dc from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.186: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:23:46.186: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:23:46.186: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:23:46.186: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:23:46.186: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-93-18.us-west-2.compute.internal before test
Apr 17 21:23:46.201: INFO: calico-node-zbl7g from calico-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.201: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:23:46.201: INFO: csi-node-driver-58wnm from calico-system started at 2023-04-17 21:05:13 +0000 UTC (2 container statuses recorded)
Apr 17 21:23:46.201: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:23:46.201: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:23:46.201: INFO: ebs-csi-node-bqd6h from kube-system started at 2023-04-17 21:05:13 +0000 UTC (3 container statuses recorded)
Apr 17 21:23:46.201: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:23:46.201: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:23:46.201: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:23:46.201: INFO: kube-proxy-88qrm from kube-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.201: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:23:46.201: INFO: node-feature-discovery-worker-m4mrl from node-feature-discovery started at 2023-04-17 21:05:56 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.201: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:23:46.201: INFO: sonobuoy from sonobuoy started at 2023-04-17 21:09:00 +0000 UTC (1 container statuses recorded)
Apr 17 21:23:46.201: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 17 21:23:46.201: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:23:46.201: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:23:46.201: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node ip-10-0-106-231.us-west-2.compute.internal 04/17/23 21:23:46.229
STEP: verifying the node has the label node ip-10-0-64-189.us-west-2.compute.internal 04/17/23 21:23:46.244
STEP: verifying the node has the label node ip-10-0-74-52.us-west-2.compute.internal 04/17/23 21:23:46.263
STEP: verifying the node has the label node ip-10-0-93-18.us-west-2.compute.internal 04/17/23 21:23:46.28
Apr 17 21:23:46.303: INFO: Pod calico-node-b7r2c requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod calico-node-pqj88 requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod calico-node-qwskb requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod calico-node-zbl7g requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod calico-typha-6dc6fbd9f5-tcn27 requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod calico-typha-6dc6fbd9f5-xbmcd requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod csi-node-driver-58wnm requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod csi-node-driver-n474j requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod csi-node-driver-pptcd requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod csi-node-driver-rzhbn requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod ebs-csi-node-7xssn requesting resource cpu=30m on Node ip-10-0-64-189.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod ebs-csi-node-bqd6h requesting resource cpu=30m on Node ip-10-0-93-18.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod ebs-csi-node-lhxwm requesting resource cpu=30m on Node ip-10-0-106-231.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod ebs-csi-node-xhdpg requesting resource cpu=30m on Node ip-10-0-74-52.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod kube-proxy-88qrm requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod kube-proxy-8tbrz requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod kube-proxy-h249s requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod kube-proxy-j772r requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod node-feature-discovery-worker-9bc5l requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod node-feature-discovery-worker-bl2dc requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod node-feature-discovery-worker-cbvbq requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod node-feature-discovery-worker-m4mrl requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod sonobuoy-e2e-job-777eb8e1ef7a4686 requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
Apr 17 21:23:46.303: INFO: Pod sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
STEP: Starting Pods to consume most of the cluster CPU. 04/17/23 21:23:46.303
Apr 17 21:23:46.303: INFO: Creating a pod which consumes cpu=5579m on Node ip-10-0-106-231.us-west-2.compute.internal
Apr 17 21:23:46.311: INFO: Creating a pod which consumes cpu=5579m on Node ip-10-0-64-189.us-west-2.compute.internal
Apr 17 21:23:46.317: INFO: Creating a pod which consumes cpu=5579m on Node ip-10-0-74-52.us-west-2.compute.internal
Apr 17 21:23:46.326: INFO: Creating a pod which consumes cpu=5579m on Node ip-10-0-93-18.us-west-2.compute.internal
Apr 17 21:23:46.334: INFO: Waiting up to 5m0s for pod "filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548" in namespace "sched-pred-5919" to be "running"
Apr 17 21:23:46.341: INFO: Pod "filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548": Phase="Pending", Reason="", readiness=false. Elapsed: 7.634311ms
Apr 17 21:23:48.345: INFO: Pod "filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548": Phase="Running", Reason="", readiness=true. Elapsed: 2.011070651s
Apr 17 21:23:48.345: INFO: Pod "filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548" satisfied condition "running"
Apr 17 21:23:48.345: INFO: Waiting up to 5m0s for pod "filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1" in namespace "sched-pred-5919" to be "running"
Apr 17 21:23:48.347: INFO: Pod "filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1": Phase="Running", Reason="", readiness=true. Elapsed: 2.291113ms
Apr 17 21:23:48.347: INFO: Pod "filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1" satisfied condition "running"
Apr 17 21:23:48.347: INFO: Waiting up to 5m0s for pod "filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb" in namespace "sched-pred-5919" to be "running"
Apr 17 21:23:48.349: INFO: Pod "filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.137789ms
Apr 17 21:23:48.349: INFO: Pod "filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb" satisfied condition "running"
Apr 17 21:23:48.349: INFO: Waiting up to 5m0s for pod "filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769" in namespace "sched-pred-5919" to be "running"
Apr 17 21:23:48.351: INFO: Pod "filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769": Phase="Running", Reason="", readiness=true. Elapsed: 1.99241ms
Apr 17 21:23:48.351: INFO: Pod "filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 04/17/23 21:23:48.351
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb.1756d5d615be12f9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5919/filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb to ip-10-0-74-52.us-west-2.compute.internal] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb.1756d5d63b439963], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb.1756d5d63c3d30b8], Reason = [Created], Message = [Created container filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb.1756d5d640f12682], Reason = [Started], Message = [Started container filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548.1756d5d614ba1fa3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5919/filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548 to ip-10-0-106-231.us-west-2.compute.internal] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548.1756d5d63d329895], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548.1756d5d63e3d3623], Reason = [Created], Message = [Created container filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548.1756d5d643f52580], Reason = [Started], Message = [Started container filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769.1756d5d6163ab4e7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5919/filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769 to ip-10-0-93-18.us-west-2.compute.internal] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769.1756d5d63ced0daf], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769.1756d5d63df90c2c], Reason = [Created], Message = [Created container filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769.1756d5d64522c9fd], Reason = [Started], Message = [Started container filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1.1756d5d6151778bf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5919/filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1 to ip-10-0-64-189.us-west-2.compute.internal] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1.1756d5d63bd0f8f5], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/17/23 21:23:48.354
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1.1756d5d63cc93552], Reason = [Created], Message = [Created container filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1] 04/17/23 21:23:48.355
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1.1756d5d6441b5198], Reason = [Started], Message = [Started container filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1] 04/17/23 21:23:48.355
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1756d5d68e7ffb85], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 Insufficient cpu. preemption: 0/7 nodes are available: 3 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod..] 04/17/23 21:23:48.363
STEP: removing the label node off the node ip-10-0-106-231.us-west-2.compute.internal 04/17/23 21:23:49.363
STEP: verifying the node doesn't have the label node 04/17/23 21:23:49.376
STEP: removing the label node off the node ip-10-0-64-189.us-west-2.compute.internal 04/17/23 21:23:49.378
STEP: verifying the node doesn't have the label node 04/17/23 21:23:49.391
STEP: removing the label node off the node ip-10-0-74-52.us-west-2.compute.internal 04/17/23 21:23:49.394
STEP: verifying the node doesn't have the label node 04/17/23 21:23:49.409
STEP: removing the label node off the node ip-10-0-93-18.us-west-2.compute.internal 04/17/23 21:23:49.414
STEP: verifying the node doesn't have the label node 04/17/23 21:23:49.427
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:23:49.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-5919" for this suite. 04/17/23 21:23:49.436
------------------------------
• [3.310 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:23:46.13
    Apr 17 21:23:46.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-pred 04/17/23 21:23:46.13
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:46.144
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:46.147
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 17 21:23:46.149: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 17 21:23:46.157: INFO: Waiting for terminating namespaces to be deleted...
    Apr 17 21:23:46.159: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-106-231.us-west-2.compute.internal before test
    Apr 17 21:23:46.168: INFO: calico-node-qwskb from calico-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.168: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: calico-typha-6dc6fbd9f5-tcn27 from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.168: INFO: 	Container calico-typha ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: csi-node-driver-pptcd from calico-system started at 2023-04-17 21:05:10 +0000 UTC (2 container statuses recorded)
    Apr 17 21:23:46.168: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: ebs-csi-node-lhxwm from kube-system started at 2023-04-17 21:05:10 +0000 UTC (3 container statuses recorded)
    Apr 17 21:23:46.168: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: kube-proxy-h249s from kube-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.168: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: node-feature-discovery-worker-9bc5l from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.168: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:23:46.168: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:23:46.168: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-64-189.us-west-2.compute.internal before test
    Apr 17 21:23:46.177: INFO: calico-node-b7r2c from calico-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.177: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: csi-node-driver-rzhbn from calico-system started at 2023-04-17 21:05:08 +0000 UTC (2 container statuses recorded)
    Apr 17 21:23:46.177: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: ebs-csi-node-7xssn from kube-system started at 2023-04-17 21:05:08 +0000 UTC (3 container statuses recorded)
    Apr 17 21:23:46.177: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: kube-proxy-j772r from kube-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.177: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: node-feature-discovery-worker-cbvbq from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.177: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: sonobuoy-e2e-job-777eb8e1ef7a4686 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:23:46.177: INFO: 	Container e2e ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:23:46.177: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:23:46.177: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-74-52.us-west-2.compute.internal before test
    Apr 17 21:23:46.186: INFO: calico-node-pqj88 from calico-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.186: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: calico-typha-6dc6fbd9f5-xbmcd from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.186: INFO: 	Container calico-typha ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: csi-node-driver-n474j from calico-system started at 2023-04-17 21:05:14 +0000 UTC (2 container statuses recorded)
    Apr 17 21:23:46.186: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: ebs-csi-node-xhdpg from kube-system started at 2023-04-17 21:05:14 +0000 UTC (3 container statuses recorded)
    Apr 17 21:23:46.186: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: kube-proxy-8tbrz from kube-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.186: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: node-feature-discovery-worker-bl2dc from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.186: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:23:46.186: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:23:46.186: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-93-18.us-west-2.compute.internal before test
    Apr 17 21:23:46.201: INFO: calico-node-zbl7g from calico-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.201: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: csi-node-driver-58wnm from calico-system started at 2023-04-17 21:05:13 +0000 UTC (2 container statuses recorded)
    Apr 17 21:23:46.201: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: ebs-csi-node-bqd6h from kube-system started at 2023-04-17 21:05:13 +0000 UTC (3 container statuses recorded)
    Apr 17 21:23:46.201: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: kube-proxy-88qrm from kube-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.201: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: node-feature-discovery-worker-m4mrl from node-feature-discovery started at 2023-04-17 21:05:56 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.201: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: sonobuoy from sonobuoy started at 2023-04-17 21:09:00 +0000 UTC (1 container statuses recorded)
    Apr 17 21:23:46.201: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:23:46.201: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:23:46.201: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node ip-10-0-106-231.us-west-2.compute.internal 04/17/23 21:23:46.229
    STEP: verifying the node has the label node ip-10-0-64-189.us-west-2.compute.internal 04/17/23 21:23:46.244
    STEP: verifying the node has the label node ip-10-0-74-52.us-west-2.compute.internal 04/17/23 21:23:46.263
    STEP: verifying the node has the label node ip-10-0-93-18.us-west-2.compute.internal 04/17/23 21:23:46.28
    Apr 17 21:23:46.303: INFO: Pod calico-node-b7r2c requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod calico-node-pqj88 requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod calico-node-qwskb requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod calico-node-zbl7g requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod calico-typha-6dc6fbd9f5-tcn27 requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod calico-typha-6dc6fbd9f5-xbmcd requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod csi-node-driver-58wnm requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod csi-node-driver-n474j requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod csi-node-driver-pptcd requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod csi-node-driver-rzhbn requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod ebs-csi-node-7xssn requesting resource cpu=30m on Node ip-10-0-64-189.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod ebs-csi-node-bqd6h requesting resource cpu=30m on Node ip-10-0-93-18.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod ebs-csi-node-lhxwm requesting resource cpu=30m on Node ip-10-0-106-231.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod ebs-csi-node-xhdpg requesting resource cpu=30m on Node ip-10-0-74-52.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod kube-proxy-88qrm requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod kube-proxy-8tbrz requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod kube-proxy-h249s requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod kube-proxy-j772r requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod node-feature-discovery-worker-9bc5l requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod node-feature-discovery-worker-bl2dc requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod node-feature-discovery-worker-cbvbq requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod node-feature-discovery-worker-m4mrl requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod sonobuoy-e2e-job-777eb8e1ef7a4686 requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 requesting resource cpu=0m on Node ip-10-0-93-18.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv requesting resource cpu=0m on Node ip-10-0-64-189.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr requesting resource cpu=0m on Node ip-10-0-106-231.us-west-2.compute.internal
    Apr 17 21:23:46.303: INFO: Pod sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz requesting resource cpu=0m on Node ip-10-0-74-52.us-west-2.compute.internal
    STEP: Starting Pods to consume most of the cluster CPU. 04/17/23 21:23:46.303
    Apr 17 21:23:46.303: INFO: Creating a pod which consumes cpu=5579m on Node ip-10-0-106-231.us-west-2.compute.internal
    Apr 17 21:23:46.311: INFO: Creating a pod which consumes cpu=5579m on Node ip-10-0-64-189.us-west-2.compute.internal
    Apr 17 21:23:46.317: INFO: Creating a pod which consumes cpu=5579m on Node ip-10-0-74-52.us-west-2.compute.internal
    Apr 17 21:23:46.326: INFO: Creating a pod which consumes cpu=5579m on Node ip-10-0-93-18.us-west-2.compute.internal
    Apr 17 21:23:46.334: INFO: Waiting up to 5m0s for pod "filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548" in namespace "sched-pred-5919" to be "running"
    Apr 17 21:23:46.341: INFO: Pod "filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548": Phase="Pending", Reason="", readiness=false. Elapsed: 7.634311ms
    Apr 17 21:23:48.345: INFO: Pod "filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548": Phase="Running", Reason="", readiness=true. Elapsed: 2.011070651s
    Apr 17 21:23:48.345: INFO: Pod "filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548" satisfied condition "running"
    Apr 17 21:23:48.345: INFO: Waiting up to 5m0s for pod "filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1" in namespace "sched-pred-5919" to be "running"
    Apr 17 21:23:48.347: INFO: Pod "filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1": Phase="Running", Reason="", readiness=true. Elapsed: 2.291113ms
    Apr 17 21:23:48.347: INFO: Pod "filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1" satisfied condition "running"
    Apr 17 21:23:48.347: INFO: Waiting up to 5m0s for pod "filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb" in namespace "sched-pred-5919" to be "running"
    Apr 17 21:23:48.349: INFO: Pod "filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.137789ms
    Apr 17 21:23:48.349: INFO: Pod "filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb" satisfied condition "running"
    Apr 17 21:23:48.349: INFO: Waiting up to 5m0s for pod "filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769" in namespace "sched-pred-5919" to be "running"
    Apr 17 21:23:48.351: INFO: Pod "filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769": Phase="Running", Reason="", readiness=true. Elapsed: 1.99241ms
    Apr 17 21:23:48.351: INFO: Pod "filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 04/17/23 21:23:48.351
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb.1756d5d615be12f9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5919/filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb to ip-10-0-74-52.us-west-2.compute.internal] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb.1756d5d63b439963], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb.1756d5d63c3d30b8], Reason = [Created], Message = [Created container filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb.1756d5d640f12682], Reason = [Started], Message = [Started container filler-pod-8d1eddf4-d9a0-46ff-9134-c00c023d62eb] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548.1756d5d614ba1fa3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5919/filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548 to ip-10-0-106-231.us-west-2.compute.internal] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548.1756d5d63d329895], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548.1756d5d63e3d3623], Reason = [Created], Message = [Created container filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548.1756d5d643f52580], Reason = [Started], Message = [Started container filler-pod-a975b041-e6f2-485d-96f4-c5e365e0b548] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769.1756d5d6163ab4e7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5919/filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769 to ip-10-0-93-18.us-west-2.compute.internal] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769.1756d5d63ced0daf], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769.1756d5d63df90c2c], Reason = [Created], Message = [Created container filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769.1756d5d64522c9fd], Reason = [Started], Message = [Started container filler-pod-aeaa53a1-e7a2-425e-97f5-1dae5aa95769] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1.1756d5d6151778bf], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5919/filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1 to ip-10-0-64-189.us-west-2.compute.internal] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1.1756d5d63bd0f8f5], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 04/17/23 21:23:48.354
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1.1756d5d63cc93552], Reason = [Created], Message = [Created container filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1] 04/17/23 21:23:48.355
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1.1756d5d6441b5198], Reason = [Started], Message = [Started container filler-pod-b5c43830-7cad-4ba8-8c05-481626013ec1] 04/17/23 21:23:48.355
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.1756d5d68e7ffb85], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 Insufficient cpu. preemption: 0/7 nodes are available: 3 Preemption is not helpful for scheduling, 4 No preemption victims found for incoming pod..] 04/17/23 21:23:48.363
    STEP: removing the label node off the node ip-10-0-106-231.us-west-2.compute.internal 04/17/23 21:23:49.363
    STEP: verifying the node doesn't have the label node 04/17/23 21:23:49.376
    STEP: removing the label node off the node ip-10-0-64-189.us-west-2.compute.internal 04/17/23 21:23:49.378
    STEP: verifying the node doesn't have the label node 04/17/23 21:23:49.391
    STEP: removing the label node off the node ip-10-0-74-52.us-west-2.compute.internal 04/17/23 21:23:49.394
    STEP: verifying the node doesn't have the label node 04/17/23 21:23:49.409
    STEP: removing the label node off the node ip-10-0-93-18.us-west-2.compute.internal 04/17/23 21:23:49.414
    STEP: verifying the node doesn't have the label node 04/17/23 21:23:49.427
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:23:49.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-5919" for this suite. 04/17/23 21:23:49.436
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:23:49.441
Apr 17 21:23:49.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename server-version 04/17/23 21:23:49.442
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:49.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:49.457
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 04/17/23 21:23:49.459
STEP: Confirm major version 04/17/23 21:23:49.46
Apr 17 21:23:49.460: INFO: Major version: 1
STEP: Confirm minor version 04/17/23 21:23:49.46
Apr 17 21:23:49.460: INFO: cleanMinorVersion: 26
Apr 17 21:23:49.460: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Apr 17 21:23:49.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-8127" for this suite. 04/17/23 21:23:49.464
------------------------------
• [0.028 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:23:49.441
    Apr 17 21:23:49.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename server-version 04/17/23 21:23:49.442
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:49.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:49.457
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 04/17/23 21:23:49.459
    STEP: Confirm major version 04/17/23 21:23:49.46
    Apr 17 21:23:49.460: INFO: Major version: 1
    STEP: Confirm minor version 04/17/23 21:23:49.46
    Apr 17 21:23:49.460: INFO: cleanMinorVersion: 26
    Apr 17 21:23:49.460: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:23:49.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-8127" for this suite. 04/17/23 21:23:49.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:23:49.47
Apr 17 21:23:49.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 21:23:49.47
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:49.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:49.485
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 04/17/23 21:23:49.487
Apr 17 21:23:49.494: INFO: Waiting up to 5m0s for pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269" in namespace "downward-api-8199" to be "Succeeded or Failed"
Apr 17 21:23:49.496: INFO: Pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.237329ms
Apr 17 21:23:51.500: INFO: Pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006443353s
Apr 17 21:23:53.500: INFO: Pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005845543s
STEP: Saw pod success 04/17/23 21:23:53.5
Apr 17 21:23:53.500: INFO: Pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269" satisfied condition "Succeeded or Failed"
Apr 17 21:23:53.502: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downward-api-0c428cb6-30d0-4613-b330-d9112032b269 container dapi-container: <nil>
STEP: delete the pod 04/17/23 21:23:53.507
Apr 17 21:23:53.516: INFO: Waiting for pod downward-api-0c428cb6-30d0-4613-b330-d9112032b269 to disappear
Apr 17 21:23:53.518: INFO: Pod downward-api-0c428cb6-30d0-4613-b330-d9112032b269 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 17 21:23:53.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8199" for this suite. 04/17/23 21:23:53.522
------------------------------
• [4.057 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:23:49.47
    Apr 17 21:23:49.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 21:23:49.47
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:49.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:49.485
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 04/17/23 21:23:49.487
    Apr 17 21:23:49.494: INFO: Waiting up to 5m0s for pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269" in namespace "downward-api-8199" to be "Succeeded or Failed"
    Apr 17 21:23:49.496: INFO: Pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.237329ms
    Apr 17 21:23:51.500: INFO: Pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006443353s
    Apr 17 21:23:53.500: INFO: Pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005845543s
    STEP: Saw pod success 04/17/23 21:23:53.5
    Apr 17 21:23:53.500: INFO: Pod "downward-api-0c428cb6-30d0-4613-b330-d9112032b269" satisfied condition "Succeeded or Failed"
    Apr 17 21:23:53.502: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downward-api-0c428cb6-30d0-4613-b330-d9112032b269 container dapi-container: <nil>
    STEP: delete the pod 04/17/23 21:23:53.507
    Apr 17 21:23:53.516: INFO: Waiting for pod downward-api-0c428cb6-30d0-4613-b330-d9112032b269 to disappear
    Apr 17 21:23:53.518: INFO: Pod downward-api-0c428cb6-30d0-4613-b330-d9112032b269 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:23:53.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8199" for this suite. 04/17/23 21:23:53.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:23:53.527
Apr 17 21:23:53.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 21:23:53.528
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:53.541
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:53.543
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 04/17/23 21:23:53.545
Apr 17 21:23:53.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:23:57.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:10.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8037" for this suite. 04/17/23 21:24:10.717
------------------------------
• [SLOW TEST] [17.197 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:23:53.527
    Apr 17 21:23:53.527: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 21:23:53.528
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:23:53.541
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:23:53.543
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 04/17/23 21:23:53.545
    Apr 17 21:23:53.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:23:57.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:10.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8037" for this suite. 04/17/23 21:24:10.717
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:10.725
Apr 17 21:24:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename watch 04/17/23 21:24:10.725
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:10.741
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:10.743
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 04/17/23 21:24:10.746
STEP: creating a new configmap 04/17/23 21:24:10.747
STEP: modifying the configmap once 04/17/23 21:24:10.752
STEP: closing the watch once it receives two notifications 04/17/23 21:24:10.76
Apr 17 21:24:10.760: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3263  c74f54c7-6ea8-43e9-aa4b-40b128018061 17191 0 2023-04-17 21:24:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-17 21:24:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 21:24:10.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3263  c74f54c7-6ea8-43e9-aa4b-40b128018061 17192 0 2023-04-17 21:24:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-17 21:24:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 04/17/23 21:24:10.76
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 04/17/23 21:24:10.767
STEP: deleting the configmap 04/17/23 21:24:10.768
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 04/17/23 21:24:10.774
Apr 17 21:24:10.774: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3263  c74f54c7-6ea8-43e9-aa4b-40b128018061 17193 0 2023-04-17 21:24:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-17 21:24:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 21:24:10.775: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3263  c74f54c7-6ea8-43e9-aa4b-40b128018061 17194 0 2023-04-17 21:24:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-17 21:24:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:10.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3263" for this suite. 04/17/23 21:24:10.78
------------------------------
• [0.063 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:10.725
    Apr 17 21:24:10.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename watch 04/17/23 21:24:10.725
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:10.741
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:10.743
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 04/17/23 21:24:10.746
    STEP: creating a new configmap 04/17/23 21:24:10.747
    STEP: modifying the configmap once 04/17/23 21:24:10.752
    STEP: closing the watch once it receives two notifications 04/17/23 21:24:10.76
    Apr 17 21:24:10.760: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3263  c74f54c7-6ea8-43e9-aa4b-40b128018061 17191 0 2023-04-17 21:24:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-17 21:24:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 21:24:10.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3263  c74f54c7-6ea8-43e9-aa4b-40b128018061 17192 0 2023-04-17 21:24:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-17 21:24:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 04/17/23 21:24:10.76
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 04/17/23 21:24:10.767
    STEP: deleting the configmap 04/17/23 21:24:10.768
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 04/17/23 21:24:10.774
    Apr 17 21:24:10.774: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3263  c74f54c7-6ea8-43e9-aa4b-40b128018061 17193 0 2023-04-17 21:24:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-17 21:24:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 21:24:10.775: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3263  c74f54c7-6ea8-43e9-aa4b-40b128018061 17194 0 2023-04-17 21:24:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-04-17 21:24:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:10.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3263" for this suite. 04/17/23 21:24:10.78
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:10.787
Apr 17 21:24:10.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename certificates 04/17/23 21:24:10.788
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:10.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:10.806
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 04/17/23 21:24:11.498
STEP: getting /apis/certificates.k8s.io 04/17/23 21:24:11.502
STEP: getting /apis/certificates.k8s.io/v1 04/17/23 21:24:11.503
STEP: creating 04/17/23 21:24:11.504
STEP: getting 04/17/23 21:24:11.522
STEP: listing 04/17/23 21:24:11.525
STEP: watching 04/17/23 21:24:11.528
Apr 17 21:24:11.528: INFO: starting watch
STEP: patching 04/17/23 21:24:11.529
STEP: updating 04/17/23 21:24:11.534
Apr 17 21:24:11.612: INFO: waiting for watch events with expected annotations
Apr 17 21:24:11.612: INFO: saw patched and updated annotations
STEP: getting /approval 04/17/23 21:24:11.612
STEP: patching /approval 04/17/23 21:24:11.615
STEP: updating /approval 04/17/23 21:24:11.621
STEP: getting /status 04/17/23 21:24:11.628
STEP: patching /status 04/17/23 21:24:11.631
STEP: updating /status 04/17/23 21:24:11.639
STEP: deleting 04/17/23 21:24:11.647
STEP: deleting a collection 04/17/23 21:24:11.661
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:11.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-9565" for this suite. 04/17/23 21:24:11.682
------------------------------
• [0.901 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:10.787
    Apr 17 21:24:10.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename certificates 04/17/23 21:24:10.788
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:10.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:10.806
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 04/17/23 21:24:11.498
    STEP: getting /apis/certificates.k8s.io 04/17/23 21:24:11.502
    STEP: getting /apis/certificates.k8s.io/v1 04/17/23 21:24:11.503
    STEP: creating 04/17/23 21:24:11.504
    STEP: getting 04/17/23 21:24:11.522
    STEP: listing 04/17/23 21:24:11.525
    STEP: watching 04/17/23 21:24:11.528
    Apr 17 21:24:11.528: INFO: starting watch
    STEP: patching 04/17/23 21:24:11.529
    STEP: updating 04/17/23 21:24:11.534
    Apr 17 21:24:11.612: INFO: waiting for watch events with expected annotations
    Apr 17 21:24:11.612: INFO: saw patched and updated annotations
    STEP: getting /approval 04/17/23 21:24:11.612
    STEP: patching /approval 04/17/23 21:24:11.615
    STEP: updating /approval 04/17/23 21:24:11.621
    STEP: getting /status 04/17/23 21:24:11.628
    STEP: patching /status 04/17/23 21:24:11.631
    STEP: updating /status 04/17/23 21:24:11.639
    STEP: deleting 04/17/23 21:24:11.647
    STEP: deleting a collection 04/17/23 21:24:11.661
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:11.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-9565" for this suite. 04/17/23 21:24:11.682
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:11.689
Apr 17 21:24:11.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replicaset 04/17/23 21:24:11.69
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:11.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:11.709
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 04/17/23 21:24:11.715
STEP: Verify that the required pods have come up. 04/17/23 21:24:11.721
Apr 17 21:24:11.724: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 17 21:24:16.732: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/17/23 21:24:16.732
STEP: Getting /status 04/17/23 21:24:16.732
Apr 17 21:24:16.735: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 04/17/23 21:24:16.735
Apr 17 21:24:16.750: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 04/17/23 21:24:16.75
Apr 17 21:24:16.752: INFO: Observed &ReplicaSet event: ADDED
Apr 17 21:24:16.752: INFO: Observed &ReplicaSet event: MODIFIED
Apr 17 21:24:16.752: INFO: Observed &ReplicaSet event: MODIFIED
Apr 17 21:24:16.752: INFO: Observed &ReplicaSet event: MODIFIED
Apr 17 21:24:16.752: INFO: Found replicaset test-rs in namespace replicaset-9292 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 17 21:24:16.752: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 04/17/23 21:24:16.752
Apr 17 21:24:16.752: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 17 21:24:16.759: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 04/17/23 21:24:16.759
Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: ADDED
Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: MODIFIED
Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: MODIFIED
Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: MODIFIED
Apr 17 21:24:16.761: INFO: Observed replicaset test-rs in namespace replicaset-9292 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: MODIFIED
Apr 17 21:24:16.761: INFO: Found replicaset test-rs in namespace replicaset-9292 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Apr 17 21:24:16.761: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:16.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9292" for this suite. 04/17/23 21:24:16.766
------------------------------
• [SLOW TEST] [5.084 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:11.689
    Apr 17 21:24:11.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replicaset 04/17/23 21:24:11.69
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:11.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:11.709
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 04/17/23 21:24:11.715
    STEP: Verify that the required pods have come up. 04/17/23 21:24:11.721
    Apr 17 21:24:11.724: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 17 21:24:16.732: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/17/23 21:24:16.732
    STEP: Getting /status 04/17/23 21:24:16.732
    Apr 17 21:24:16.735: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 04/17/23 21:24:16.735
    Apr 17 21:24:16.750: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 04/17/23 21:24:16.75
    Apr 17 21:24:16.752: INFO: Observed &ReplicaSet event: ADDED
    Apr 17 21:24:16.752: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 17 21:24:16.752: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 17 21:24:16.752: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 17 21:24:16.752: INFO: Found replicaset test-rs in namespace replicaset-9292 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 17 21:24:16.752: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 04/17/23 21:24:16.752
    Apr 17 21:24:16.752: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr 17 21:24:16.759: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 04/17/23 21:24:16.759
    Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: ADDED
    Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 17 21:24:16.761: INFO: Observed replicaset test-rs in namespace replicaset-9292 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 17 21:24:16.761: INFO: Observed &ReplicaSet event: MODIFIED
    Apr 17 21:24:16.761: INFO: Found replicaset test-rs in namespace replicaset-9292 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Apr 17 21:24:16.761: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:16.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9292" for this suite. 04/17/23 21:24:16.766
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:16.774
Apr 17 21:24:16.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename runtimeclass 04/17/23 21:24:16.774
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:16.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:16.792
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:16.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5700" for this suite. 04/17/23 21:24:16.807
------------------------------
• [0.041 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:16.774
    Apr 17 21:24:16.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename runtimeclass 04/17/23 21:24:16.774
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:16.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:16.792
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:16.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5700" for this suite. 04/17/23 21:24:16.807
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:16.815
Apr 17 21:24:16.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename namespaces 04/17/23 21:24:16.815
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:16.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:16.832
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 04/17/23 21:24:16.834
Apr 17 21:24:16.838: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 04/17/23 21:24:16.838
Apr 17 21:24:16.844: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 04/17/23 21:24:16.845
Apr 17 21:24:16.859: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:16.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4343" for this suite. 04/17/23 21:24:16.864
------------------------------
• [0.055 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:16.815
    Apr 17 21:24:16.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename namespaces 04/17/23 21:24:16.815
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:16.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:16.832
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 04/17/23 21:24:16.834
    Apr 17 21:24:16.838: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 04/17/23 21:24:16.838
    Apr 17 21:24:16.844: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 04/17/23 21:24:16.845
    Apr 17 21:24:16.859: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:16.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4343" for this suite. 04/17/23 21:24:16.864
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:16.87
Apr 17 21:24:16.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename daemonsets 04/17/23 21:24:16.871
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:16.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:16.89
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 04/17/23 21:24:16.915
STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 21:24:16.921
Apr 17 21:24:16.929: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:16.929: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:16.929: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:16.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 21:24:16.933: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:24:17.942: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:17.942: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:17.942: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:17.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 17 21:24:17.945: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:24:18.940: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:18.940: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:18.940: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:18.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr 17 21:24:18.944: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:24:19.939: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:19.939: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:19.939: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:19.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr 17 21:24:19.943: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:24:20.940: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:20.940: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:20.940: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:20.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr 17 21:24:20.944: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:24:21.938: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:21.938: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:21.938: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:21.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr 17 21:24:21.942: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:24:22.938: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:22.939: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:22.939: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:22.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr 17 21:24:22.942: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 04/17/23 21:24:22.946
Apr 17 21:24:23.045: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:23.045: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:23.045: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:23.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr 17 21:24:23.049: INFO: Node ip-10-0-93-18.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:24:24.056: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:24.056: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:24.056: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:24.059: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr 17 21:24:24.059: INFO: Node ip-10-0-93-18.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:24:25.060: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:25.061: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:25.061: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:24:25.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr 17 21:24:25.065: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/17/23 21:24:25.068
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1879, will wait for the garbage collector to delete the pods 04/17/23 21:24:25.068
Apr 17 21:24:25.130: INFO: Deleting DaemonSet.extensions daemon-set took: 7.420365ms
Apr 17 21:24:25.230: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.741047ms
Apr 17 21:24:28.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 21:24:28.035: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 17 21:24:28.038: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17551"},"items":null}

Apr 17 21:24:28.042: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17551"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:28.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1879" for this suite. 04/17/23 21:24:28.065
------------------------------
• [SLOW TEST] [11.202 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:16.87
    Apr 17 21:24:16.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename daemonsets 04/17/23 21:24:16.871
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:16.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:16.89
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 04/17/23 21:24:16.915
    STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 21:24:16.921
    Apr 17 21:24:16.929: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:16.929: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:16.929: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:16.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 21:24:16.933: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:24:17.942: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:17.942: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:17.942: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:17.945: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 17 21:24:17.945: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:24:18.940: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:18.940: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:18.940: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:18.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr 17 21:24:18.944: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:24:19.939: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:19.939: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:19.939: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:19.943: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr 17 21:24:19.943: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:24:20.940: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:20.940: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:20.940: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:20.944: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr 17 21:24:20.944: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:24:21.938: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:21.938: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:21.938: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:21.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr 17 21:24:21.942: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:24:22.938: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:22.939: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:22.939: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:22.942: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr 17 21:24:22.942: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 04/17/23 21:24:22.946
    Apr 17 21:24:23.045: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:23.045: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:23.045: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:23.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr 17 21:24:23.049: INFO: Node ip-10-0-93-18.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:24:24.056: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:24.056: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:24.056: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:24.059: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr 17 21:24:24.059: INFO: Node ip-10-0-93-18.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:24:25.060: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:25.061: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:25.061: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:24:25.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr 17 21:24:25.065: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/17/23 21:24:25.068
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1879, will wait for the garbage collector to delete the pods 04/17/23 21:24:25.068
    Apr 17 21:24:25.130: INFO: Deleting DaemonSet.extensions daemon-set took: 7.420365ms
    Apr 17 21:24:25.230: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.741047ms
    Apr 17 21:24:28.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 21:24:28.035: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 17 21:24:28.038: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"17551"},"items":null}

    Apr 17 21:24:28.042: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"17551"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:28.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1879" for this suite. 04/17/23 21:24:28.065
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:28.073
Apr 17 21:24:28.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 21:24:28.073
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:28.087
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:28.09
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 04/17/23 21:24:28.092
STEP: submitting the pod to kubernetes 04/17/23 21:24:28.092
Apr 17 21:24:28.101: INFO: Waiting up to 5m0s for pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de" in namespace "pods-7164" to be "running and ready"
Apr 17 21:24:28.105: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477736ms
Apr 17 21:24:28.105: INFO: The phase of Pod pod-update-efde0b26-df2f-460f-afd9-bd669c3364de is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:24:30.110: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de": Phase="Running", Reason="", readiness=true. Elapsed: 2.008444106s
Apr 17 21:24:30.110: INFO: The phase of Pod pod-update-efde0b26-df2f-460f-afd9-bd669c3364de is Running (Ready = true)
Apr 17 21:24:30.110: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 04/17/23 21:24:30.113
STEP: updating the pod 04/17/23 21:24:30.117
Apr 17 21:24:30.629: INFO: Successfully updated pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de"
Apr 17 21:24:30.629: INFO: Waiting up to 5m0s for pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de" in namespace "pods-7164" to be "running"
Apr 17 21:24:30.632: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de": Phase="Running", Reason="", readiness=true. Elapsed: 3.299664ms
Apr 17 21:24:30.632: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 04/17/23 21:24:30.632
Apr 17 21:24:30.636: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:30.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-7164" for this suite. 04/17/23 21:24:30.641
------------------------------
• [2.574 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:28.073
    Apr 17 21:24:28.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 21:24:28.073
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:28.087
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:28.09
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 04/17/23 21:24:28.092
    STEP: submitting the pod to kubernetes 04/17/23 21:24:28.092
    Apr 17 21:24:28.101: INFO: Waiting up to 5m0s for pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de" in namespace "pods-7164" to be "running and ready"
    Apr 17 21:24:28.105: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.477736ms
    Apr 17 21:24:28.105: INFO: The phase of Pod pod-update-efde0b26-df2f-460f-afd9-bd669c3364de is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:24:30.110: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de": Phase="Running", Reason="", readiness=true. Elapsed: 2.008444106s
    Apr 17 21:24:30.110: INFO: The phase of Pod pod-update-efde0b26-df2f-460f-afd9-bd669c3364de is Running (Ready = true)
    Apr 17 21:24:30.110: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 04/17/23 21:24:30.113
    STEP: updating the pod 04/17/23 21:24:30.117
    Apr 17 21:24:30.629: INFO: Successfully updated pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de"
    Apr 17 21:24:30.629: INFO: Waiting up to 5m0s for pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de" in namespace "pods-7164" to be "running"
    Apr 17 21:24:30.632: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de": Phase="Running", Reason="", readiness=true. Elapsed: 3.299664ms
    Apr 17 21:24:30.632: INFO: Pod "pod-update-efde0b26-df2f-460f-afd9-bd669c3364de" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 04/17/23 21:24:30.632
    Apr 17 21:24:30.636: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:30.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-7164" for this suite. 04/17/23 21:24:30.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:30.648
Apr 17 21:24:30.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename statefulset 04/17/23 21:24:30.649
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:30.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:30.665
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3836 04/17/23 21:24:30.667
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-3836 04/17/23 21:24:30.672
Apr 17 21:24:30.683: INFO: Found 0 stateful pods, waiting for 1
Apr 17 21:24:40.688: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 04/17/23 21:24:40.695
STEP: updating a scale subresource 04/17/23 21:24:40.698
STEP: verifying the statefulset Spec.Replicas was modified 04/17/23 21:24:40.704
STEP: Patch a scale subresource 04/17/23 21:24:40.707
STEP: verifying the statefulset Spec.Replicas was modified 04/17/23 21:24:40.716
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 17 21:24:40.720: INFO: Deleting all statefulset in ns statefulset-3836
Apr 17 21:24:40.723: INFO: Scaling statefulset ss to 0
Apr 17 21:24:50.744: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 21:24:50.747: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:50.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3836" for this suite. 04/17/23 21:24:50.767
------------------------------
• [SLOW TEST] [20.126 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:30.648
    Apr 17 21:24:30.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename statefulset 04/17/23 21:24:30.649
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:30.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:30.665
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3836 04/17/23 21:24:30.667
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-3836 04/17/23 21:24:30.672
    Apr 17 21:24:30.683: INFO: Found 0 stateful pods, waiting for 1
    Apr 17 21:24:40.688: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 04/17/23 21:24:40.695
    STEP: updating a scale subresource 04/17/23 21:24:40.698
    STEP: verifying the statefulset Spec.Replicas was modified 04/17/23 21:24:40.704
    STEP: Patch a scale subresource 04/17/23 21:24:40.707
    STEP: verifying the statefulset Spec.Replicas was modified 04/17/23 21:24:40.716
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 17 21:24:40.720: INFO: Deleting all statefulset in ns statefulset-3836
    Apr 17 21:24:40.723: INFO: Scaling statefulset ss to 0
    Apr 17 21:24:50.744: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 21:24:50.747: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:50.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3836" for this suite. 04/17/23 21:24:50.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:50.774
Apr 17 21:24:50.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename namespaces 04/17/23 21:24:50.775
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:50.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:50.802
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 04/17/23 21:24:50.804
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:50.819
STEP: Creating a service in the namespace 04/17/23 21:24:50.821
STEP: Deleting the namespace 04/17/23 21:24:50.926
STEP: Waiting for the namespace to be removed. 04/17/23 21:24:50.933
STEP: Recreating the namespace 04/17/23 21:24:56.937
STEP: Verifying there is no service in the namespace 04/17/23 21:24:56.953
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:24:56.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9467" for this suite. 04/17/23 21:24:56.961
STEP: Destroying namespace "nsdeletetest-2216" for this suite. 04/17/23 21:24:56.967
Apr 17 21:24:56.971: INFO: Namespace nsdeletetest-2216 was already deleted
STEP: Destroying namespace "nsdeletetest-8785" for this suite. 04/17/23 21:24:56.971
------------------------------
• [SLOW TEST] [6.203 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:50.774
    Apr 17 21:24:50.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename namespaces 04/17/23 21:24:50.775
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:50.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:50.802
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 04/17/23 21:24:50.804
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:50.819
    STEP: Creating a service in the namespace 04/17/23 21:24:50.821
    STEP: Deleting the namespace 04/17/23 21:24:50.926
    STEP: Waiting for the namespace to be removed. 04/17/23 21:24:50.933
    STEP: Recreating the namespace 04/17/23 21:24:56.937
    STEP: Verifying there is no service in the namespace 04/17/23 21:24:56.953
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:24:56.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9467" for this suite. 04/17/23 21:24:56.961
    STEP: Destroying namespace "nsdeletetest-2216" for this suite. 04/17/23 21:24:56.967
    Apr 17 21:24:56.971: INFO: Namespace nsdeletetest-2216 was already deleted
    STEP: Destroying namespace "nsdeletetest-8785" for this suite. 04/17/23 21:24:56.971
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:24:56.977
Apr 17 21:24:56.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename ephemeral-containers-test 04/17/23 21:24:56.978
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:56.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:56.994
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 04/17/23 21:24:56.996
Apr 17 21:24:57.004: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6516" to be "running and ready"
Apr 17 21:24:57.009: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.72152ms
Apr 17 21:24:57.009: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:24:59.013: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009483965s
Apr 17 21:24:59.013: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:25:01.013: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009355398s
Apr 17 21:25:01.013: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Apr 17 21:25:01.013: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 04/17/23 21:25:01.017
Apr 17 21:25:01.027: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6516" to be "container debugger running"
Apr 17 21:25:01.031: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.344636ms
Apr 17 21:25:03.090: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.063356851s
Apr 17 21:25:03.090: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 04/17/23 21:25:03.09
Apr 17 21:25:03.090: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6516 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:25:03.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:25:03.091: INFO: ExecWithOptions: Clientset creation
Apr 17 21:25:03.091: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-6516/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Apr 17 21:25:03.184: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:25:03.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-6516" for this suite. 04/17/23 21:25:03.203
------------------------------
• [SLOW TEST] [6.232 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:24:56.977
    Apr 17 21:24:56.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename ephemeral-containers-test 04/17/23 21:24:56.978
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:24:56.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:24:56.994
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 04/17/23 21:24:56.996
    Apr 17 21:24:57.004: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6516" to be "running and ready"
    Apr 17 21:24:57.009: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.72152ms
    Apr 17 21:24:57.009: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:24:59.013: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009483965s
    Apr 17 21:24:59.013: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:25:01.013: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009355398s
    Apr 17 21:25:01.013: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Apr 17 21:25:01.013: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 04/17/23 21:25:01.017
    Apr 17 21:25:01.027: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-6516" to be "container debugger running"
    Apr 17 21:25:01.031: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.344636ms
    Apr 17 21:25:03.090: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.063356851s
    Apr 17 21:25:03.090: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 04/17/23 21:25:03.09
    Apr 17 21:25:03.090: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-6516 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:25:03.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:25:03.091: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:25:03.091: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-6516/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Apr 17 21:25:03.184: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:25:03.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-6516" for this suite. 04/17/23 21:25:03.203
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:25:03.21
Apr 17 21:25:03.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename cronjob 04/17/23 21:25:03.211
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:25:03.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:25:03.23
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 04/17/23 21:25:03.233
STEP: Ensuring a job is scheduled 04/17/23 21:25:03.238
STEP: Ensuring exactly one is scheduled 04/17/23 21:26:01.242
STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/17/23 21:26:01.246
STEP: Ensuring the job is replaced with a new one 04/17/23 21:26:01.249
STEP: Removing cronjob 04/17/23 21:27:01.256
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 17 21:27:01.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1913" for this suite. 04/17/23 21:27:01.27
------------------------------
• [SLOW TEST] [118.068 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:25:03.21
    Apr 17 21:25:03.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename cronjob 04/17/23 21:25:03.211
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:25:03.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:25:03.23
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 04/17/23 21:25:03.233
    STEP: Ensuring a job is scheduled 04/17/23 21:25:03.238
    STEP: Ensuring exactly one is scheduled 04/17/23 21:26:01.242
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/17/23 21:26:01.246
    STEP: Ensuring the job is replaced with a new one 04/17/23 21:26:01.249
    STEP: Removing cronjob 04/17/23 21:27:01.256
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:27:01.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1913" for this suite. 04/17/23 21:27:01.27
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:27:01.278
Apr 17 21:27:01.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename var-expansion 04/17/23 21:27:01.279
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:27:01.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:27:01.298
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Apr 17 21:27:01.309: INFO: Waiting up to 2m0s for pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135" in namespace "var-expansion-8360" to be "container 0 failed with reason CreateContainerConfigError"
Apr 17 21:27:01.312: INFO: Pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055344ms
Apr 17 21:27:03.317: INFO: Pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008095259s
Apr 17 21:27:03.317: INFO: Pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Apr 17 21:27:03.317: INFO: Deleting pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135" in namespace "var-expansion-8360"
Apr 17 21:27:03.326: INFO: Wait up to 5m0s for pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 17 21:27:05.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8360" for this suite. 04/17/23 21:27:05.339
------------------------------
• [4.068 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:27:01.278
    Apr 17 21:27:01.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename var-expansion 04/17/23 21:27:01.279
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:27:01.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:27:01.298
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Apr 17 21:27:01.309: INFO: Waiting up to 2m0s for pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135" in namespace "var-expansion-8360" to be "container 0 failed with reason CreateContainerConfigError"
    Apr 17 21:27:01.312: INFO: Pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135": Phase="Pending", Reason="", readiness=false. Elapsed: 3.055344ms
    Apr 17 21:27:03.317: INFO: Pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008095259s
    Apr 17 21:27:03.317: INFO: Pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Apr 17 21:27:03.317: INFO: Deleting pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135" in namespace "var-expansion-8360"
    Apr 17 21:27:03.326: INFO: Wait up to 5m0s for pod "var-expansion-d4cb2b62-a12c-4491-a8be-13c4c8221135" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:27:05.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8360" for this suite. 04/17/23 21:27:05.339
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:27:05.346
Apr 17 21:27:05.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sysctl 04/17/23 21:27:05.347
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:27:05.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:27:05.364
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 04/17/23 21:27:05.366
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:27:05.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-394" for this suite. 04/17/23 21:27:05.376
------------------------------
• [0.037 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:27:05.346
    Apr 17 21:27:05.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sysctl 04/17/23 21:27:05.347
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:27:05.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:27:05.364
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 04/17/23 21:27:05.366
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:27:05.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-394" for this suite. 04/17/23 21:27:05.376
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:27:05.384
Apr 17 21:27:05.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename cronjob 04/17/23 21:27:05.384
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:27:05.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:27:05.404
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 04/17/23 21:27:05.407
STEP: Ensuring no jobs are scheduled 04/17/23 21:27:05.417
STEP: Ensuring no job exists by listing jobs explicitly 04/17/23 21:32:05.425
STEP: Removing cronjob 04/17/23 21:32:05.428
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 17 21:32:05.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-7791" for this suite. 04/17/23 21:32:05.439
------------------------------
• [SLOW TEST] [300.062 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:27:05.384
    Apr 17 21:27:05.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename cronjob 04/17/23 21:27:05.384
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:27:05.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:27:05.404
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 04/17/23 21:27:05.407
    STEP: Ensuring no jobs are scheduled 04/17/23 21:27:05.417
    STEP: Ensuring no job exists by listing jobs explicitly 04/17/23 21:32:05.425
    STEP: Removing cronjob 04/17/23 21:32:05.428
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:32:05.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-7791" for this suite. 04/17/23 21:32:05.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:32:05.447
Apr 17 21:32:05.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-probe 04/17/23 21:32:05.447
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:32:05.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:32:05.466
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 17 21:33:05.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9762" for this suite. 04/17/23 21:33:05.486
------------------------------
• [SLOW TEST] [60.045 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:32:05.447
    Apr 17 21:32:05.447: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-probe 04/17/23 21:32:05.447
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:32:05.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:32:05.466
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:33:05.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9762" for this suite. 04/17/23 21:33:05.486
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:33:05.492
Apr 17 21:33:05.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 21:33:05.493
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:05.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:05.512
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/17/23 21:33:05.514
Apr 17 21:33:05.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2779 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr 17 21:33:05.574: INFO: stderr: ""
Apr 17 21:33:05.574: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 04/17/23 21:33:05.574
Apr 17 21:33:05.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2779 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Apr 17 21:33:06.989: INFO: stderr: ""
Apr 17 21:33:06.989: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/17/23 21:33:06.989
Apr 17 21:33:06.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2779 delete pods e2e-test-httpd-pod'
Apr 17 21:33:09.741: INFO: stderr: ""
Apr 17 21:33:09.741: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 21:33:09.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2779" for this suite. 04/17/23 21:33:09.747
------------------------------
• [4.262 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:33:05.492
    Apr 17 21:33:05.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 21:33:05.493
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:05.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:05.512
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/17/23 21:33:05.514
    Apr 17 21:33:05.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2779 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Apr 17 21:33:05.574: INFO: stderr: ""
    Apr 17 21:33:05.574: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 04/17/23 21:33:05.574
    Apr 17 21:33:05.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2779 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Apr 17 21:33:06.989: INFO: stderr: ""
    Apr 17 21:33:06.989: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/17/23 21:33:06.989
    Apr 17 21:33:06.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2779 delete pods e2e-test-httpd-pod'
    Apr 17 21:33:09.741: INFO: stderr: ""
    Apr 17 21:33:09.741: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:33:09.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2779" for this suite. 04/17/23 21:33:09.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:33:09.755
Apr 17 21:33:09.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pod-network-test 04/17/23 21:33:09.755
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:09.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:09.77
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-9795 04/17/23 21:33:09.773
STEP: creating a selector 04/17/23 21:33:09.773
STEP: Creating the service pods in kubernetes 04/17/23 21:33:09.773
Apr 17 21:33:09.773: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 17 21:33:09.813: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9795" to be "running and ready"
Apr 17 21:33:09.819: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166849ms
Apr 17 21:33:09.819: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:33:11.822: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009925443s
Apr 17 21:33:11.822: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:33:13.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010619546s
Apr 17 21:33:13.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:33:15.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010371388s
Apr 17 21:33:15.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:33:17.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01115406s
Apr 17 21:33:17.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:33:19.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010016957s
Apr 17 21:33:19.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:33:21.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011669422s
Apr 17 21:33:21.824: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 17 21:33:21.824: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 17 21:33:21.828: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9795" to be "running and ready"
Apr 17 21:33:21.831: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.022907ms
Apr 17 21:33:21.831: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 17 21:33:21.831: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr 17 21:33:21.833: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9795" to be "running and ready"
Apr 17 21:33:21.836: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.955382ms
Apr 17 21:33:21.836: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr 17 21:33:21.836: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr 17 21:33:21.839: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9795" to be "running and ready"
Apr 17 21:33:21.842: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.910125ms
Apr 17 21:33:21.842: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr 17 21:33:21.842: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 04/17/23 21:33:21.845
Apr 17 21:33:21.850: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9795" to be "running"
Apr 17 21:33:21.853: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.961025ms
Apr 17 21:33:23.858: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007910662s
Apr 17 21:33:23.858: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 17 21:33:23.861: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Apr 17 21:33:23.861: INFO: Breadth first check of 192.168.163.236 on host 10.0.106.231...
Apr 17 21:33:23.864: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.208.179:9080/dial?request=hostname&protocol=http&host=192.168.163.236&port=8083&tries=1'] Namespace:pod-network-test-9795 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:33:23.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:33:23.865: INFO: ExecWithOptions: Clientset creation
Apr 17 21:33:23.865: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.208.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.163.236%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 17 21:33:23.948: INFO: Waiting for responses: map[]
Apr 17 21:33:23.948: INFO: reached 192.168.163.236 after 0/1 tries
Apr 17 21:33:23.948: INFO: Breadth first check of 192.168.200.172 on host 10.0.64.189...
Apr 17 21:33:23.951: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.208.179:9080/dial?request=hostname&protocol=http&host=192.168.200.172&port=8083&tries=1'] Namespace:pod-network-test-9795 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:33:23.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:33:23.951: INFO: ExecWithOptions: Clientset creation
Apr 17 21:33:23.951: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.208.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.200.172%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 17 21:33:24.022: INFO: Waiting for responses: map[]
Apr 17 21:33:24.022: INFO: reached 192.168.200.172 after 0/1 tries
Apr 17 21:33:24.022: INFO: Breadth first check of 192.168.208.178 on host 10.0.74.52...
Apr 17 21:33:24.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.208.179:9080/dial?request=hostname&protocol=http&host=192.168.208.178&port=8083&tries=1'] Namespace:pod-network-test-9795 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:33:24.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:33:24.025: INFO: ExecWithOptions: Clientset creation
Apr 17 21:33:24.026: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.208.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.208.178%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 17 21:33:24.111: INFO: Waiting for responses: map[]
Apr 17 21:33:24.111: INFO: reached 192.168.208.178 after 0/1 tries
Apr 17 21:33:24.111: INFO: Breadth first check of 192.168.213.49 on host 10.0.93.18...
Apr 17 21:33:24.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.208.179:9080/dial?request=hostname&protocol=http&host=192.168.213.49&port=8083&tries=1'] Namespace:pod-network-test-9795 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:33:24.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:33:24.114: INFO: ExecWithOptions: Clientset creation
Apr 17 21:33:24.114: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.208.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.213.49%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 17 21:33:24.168: INFO: Waiting for responses: map[]
Apr 17 21:33:24.169: INFO: reached 192.168.213.49 after 0/1 tries
Apr 17 21:33:24.169: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 17 21:33:24.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-9795" for this suite. 04/17/23 21:33:24.174
------------------------------
• [SLOW TEST] [14.426 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:33:09.755
    Apr 17 21:33:09.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pod-network-test 04/17/23 21:33:09.755
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:09.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:09.77
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-9795 04/17/23 21:33:09.773
    STEP: creating a selector 04/17/23 21:33:09.773
    STEP: Creating the service pods in kubernetes 04/17/23 21:33:09.773
    Apr 17 21:33:09.773: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 17 21:33:09.813: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-9795" to be "running and ready"
    Apr 17 21:33:09.819: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.166849ms
    Apr 17 21:33:09.819: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:33:11.822: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009925443s
    Apr 17 21:33:11.822: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:33:13.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010619546s
    Apr 17 21:33:13.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:33:15.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010371388s
    Apr 17 21:33:15.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:33:17.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.01115406s
    Apr 17 21:33:17.824: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:33:19.823: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010016957s
    Apr 17 21:33:19.823: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:33:21.824: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011669422s
    Apr 17 21:33:21.824: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 17 21:33:21.824: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 17 21:33:21.828: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-9795" to be "running and ready"
    Apr 17 21:33:21.831: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.022907ms
    Apr 17 21:33:21.831: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 17 21:33:21.831: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr 17 21:33:21.833: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-9795" to be "running and ready"
    Apr 17 21:33:21.836: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.955382ms
    Apr 17 21:33:21.836: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr 17 21:33:21.836: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr 17 21:33:21.839: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-9795" to be "running and ready"
    Apr 17 21:33:21.842: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.910125ms
    Apr 17 21:33:21.842: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr 17 21:33:21.842: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 04/17/23 21:33:21.845
    Apr 17 21:33:21.850: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-9795" to be "running"
    Apr 17 21:33:21.853: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.961025ms
    Apr 17 21:33:23.858: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007910662s
    Apr 17 21:33:23.858: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 17 21:33:23.861: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Apr 17 21:33:23.861: INFO: Breadth first check of 192.168.163.236 on host 10.0.106.231...
    Apr 17 21:33:23.864: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.208.179:9080/dial?request=hostname&protocol=http&host=192.168.163.236&port=8083&tries=1'] Namespace:pod-network-test-9795 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:33:23.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:33:23.865: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:33:23.865: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.208.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.163.236%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 17 21:33:23.948: INFO: Waiting for responses: map[]
    Apr 17 21:33:23.948: INFO: reached 192.168.163.236 after 0/1 tries
    Apr 17 21:33:23.948: INFO: Breadth first check of 192.168.200.172 on host 10.0.64.189...
    Apr 17 21:33:23.951: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.208.179:9080/dial?request=hostname&protocol=http&host=192.168.200.172&port=8083&tries=1'] Namespace:pod-network-test-9795 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:33:23.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:33:23.951: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:33:23.951: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.208.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.200.172%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 17 21:33:24.022: INFO: Waiting for responses: map[]
    Apr 17 21:33:24.022: INFO: reached 192.168.200.172 after 0/1 tries
    Apr 17 21:33:24.022: INFO: Breadth first check of 192.168.208.178 on host 10.0.74.52...
    Apr 17 21:33:24.025: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.208.179:9080/dial?request=hostname&protocol=http&host=192.168.208.178&port=8083&tries=1'] Namespace:pod-network-test-9795 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:33:24.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:33:24.025: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:33:24.026: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.208.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.208.178%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 17 21:33:24.111: INFO: Waiting for responses: map[]
    Apr 17 21:33:24.111: INFO: reached 192.168.208.178 after 0/1 tries
    Apr 17 21:33:24.111: INFO: Breadth first check of 192.168.213.49 on host 10.0.93.18...
    Apr 17 21:33:24.114: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.208.179:9080/dial?request=hostname&protocol=http&host=192.168.213.49&port=8083&tries=1'] Namespace:pod-network-test-9795 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:33:24.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:33:24.114: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:33:24.114: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-9795/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.208.179%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.213.49%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 17 21:33:24.168: INFO: Waiting for responses: map[]
    Apr 17 21:33:24.169: INFO: reached 192.168.213.49 after 0/1 tries
    Apr 17 21:33:24.169: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:33:24.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-9795" for this suite. 04/17/23 21:33:24.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:33:24.182
Apr 17 21:33:24.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 21:33:24.183
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:24.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:24.199
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-d376e04c-e8f6-4af8-841c-d15119750a13 04/17/23 21:33:24.202
STEP: Creating a pod to test consume secrets 04/17/23 21:33:24.206
Apr 17 21:33:24.214: INFO: Waiting up to 5m0s for pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d" in namespace "secrets-1372" to be "Succeeded or Failed"
Apr 17 21:33:24.217: INFO: Pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.968697ms
Apr 17 21:33:26.222: INFO: Pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008006415s
Apr 17 21:33:28.222: INFO: Pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007805964s
STEP: Saw pod success 04/17/23 21:33:28.222
Apr 17 21:33:28.222: INFO: Pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d" satisfied condition "Succeeded or Failed"
Apr 17 21:33:28.225: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d container secret-volume-test: <nil>
STEP: delete the pod 04/17/23 21:33:28.241
Apr 17 21:33:28.252: INFO: Waiting for pod pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d to disappear
Apr 17 21:33:28.255: INFO: Pod pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 21:33:28.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1372" for this suite. 04/17/23 21:33:28.26
------------------------------
• [4.086 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:33:24.182
    Apr 17 21:33:24.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 21:33:24.183
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:24.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:24.199
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-d376e04c-e8f6-4af8-841c-d15119750a13 04/17/23 21:33:24.202
    STEP: Creating a pod to test consume secrets 04/17/23 21:33:24.206
    Apr 17 21:33:24.214: INFO: Waiting up to 5m0s for pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d" in namespace "secrets-1372" to be "Succeeded or Failed"
    Apr 17 21:33:24.217: INFO: Pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.968697ms
    Apr 17 21:33:26.222: INFO: Pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008006415s
    Apr 17 21:33:28.222: INFO: Pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007805964s
    STEP: Saw pod success 04/17/23 21:33:28.222
    Apr 17 21:33:28.222: INFO: Pod "pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d" satisfied condition "Succeeded or Failed"
    Apr 17 21:33:28.225: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d container secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 21:33:28.241
    Apr 17 21:33:28.252: INFO: Waiting for pod pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d to disappear
    Apr 17 21:33:28.255: INFO: Pod pod-secrets-854ad74e-5e13-4702-9c55-aa011b1f307d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:33:28.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1372" for this suite. 04/17/23 21:33:28.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:33:28.268
Apr 17 21:33:28.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 21:33:28.269
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:28.283
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:28.285
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 04/17/23 21:33:28.288
Apr 17 21:33:28.297: INFO: Waiting up to 5m0s for pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51" in namespace "emptydir-6852" to be "Succeeded or Failed"
Apr 17 21:33:28.302: INFO: Pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51": Phase="Pending", Reason="", readiness=false. Elapsed: 5.526174ms
Apr 17 21:33:30.307: INFO: Pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010223863s
Apr 17 21:33:32.307: INFO: Pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010642006s
STEP: Saw pod success 04/17/23 21:33:32.307
Apr 17 21:33:32.308: INFO: Pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51" satisfied condition "Succeeded or Failed"
Apr 17 21:33:32.311: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51 container test-container: <nil>
STEP: delete the pod 04/17/23 21:33:32.327
Apr 17 21:33:32.342: INFO: Waiting for pod pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51 to disappear
Apr 17 21:33:32.345: INFO: Pod pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 21:33:32.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6852" for this suite. 04/17/23 21:33:32.351
------------------------------
• [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:33:28.268
    Apr 17 21:33:28.269: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 21:33:28.269
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:28.283
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:28.285
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 04/17/23 21:33:28.288
    Apr 17 21:33:28.297: INFO: Waiting up to 5m0s for pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51" in namespace "emptydir-6852" to be "Succeeded or Failed"
    Apr 17 21:33:28.302: INFO: Pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51": Phase="Pending", Reason="", readiness=false. Elapsed: 5.526174ms
    Apr 17 21:33:30.307: INFO: Pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010223863s
    Apr 17 21:33:32.307: INFO: Pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010642006s
    STEP: Saw pod success 04/17/23 21:33:32.307
    Apr 17 21:33:32.308: INFO: Pod "pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51" satisfied condition "Succeeded or Failed"
    Apr 17 21:33:32.311: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51 container test-container: <nil>
    STEP: delete the pod 04/17/23 21:33:32.327
    Apr 17 21:33:32.342: INFO: Waiting for pod pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51 to disappear
    Apr 17 21:33:32.345: INFO: Pod pod-24b346a9-3d2e-4c0f-b8fb-180825fb7d51 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:33:32.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6852" for this suite. 04/17/23 21:33:32.351
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:33:32.358
Apr 17 21:33:32.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename proxy 04/17/23 21:33:32.358
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:32.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:32.375
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 04/17/23 21:33:32.477
STEP: creating replication controller proxy-service-9xnm9 in namespace proxy-9338 04/17/23 21:33:32.477
I0417 21:33:32.485705      23 runners.go:193] Created replication controller with name: proxy-service-9xnm9, namespace: proxy-9338, replica count: 1
I0417 21:33:33.536430      23 runners.go:193] proxy-service-9xnm9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0417 21:33:34.537505      23 runners.go:193] proxy-service-9xnm9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0417 21:33:35.538399      23 runners.go:193] proxy-service-9xnm9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 21:33:35.542: INFO: setup took 3.164391744s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 04/17/23 21:33:35.542
Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 10.424256ms)
Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 10.420147ms)
Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.555064ms)
Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 10.466822ms)
Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 10.615843ms)
Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 10.702687ms)
Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 10.855863ms)
Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 10.943059ms)
Apr 17 21:33:35.554: INFO: (0) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 12.27674ms)
Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 12.393977ms)
Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 12.398688ms)
Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 12.629076ms)
Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 12.63398ms)
Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 12.72987ms)
Apr 17 21:33:35.556: INFO: (0) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 14.316189ms)
Apr 17 21:33:35.556: INFO: (0) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 14.264987ms)
Apr 17 21:33:35.561: INFO: (1) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 4.441462ms)
Apr 17 21:33:35.562: INFO: (1) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 5.491568ms)
Apr 17 21:33:35.562: INFO: (1) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 5.534671ms)
Apr 17 21:33:35.562: INFO: (1) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 5.736387ms)
Apr 17 21:33:35.563: INFO: (1) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 6.521797ms)
Apr 17 21:33:35.563: INFO: (1) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 6.476025ms)
Apr 17 21:33:35.564: INFO: (1) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.04927ms)
Apr 17 21:33:35.564: INFO: (1) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.21947ms)
Apr 17 21:33:35.564: INFO: (1) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.274606ms)
Apr 17 21:33:35.564: INFO: (1) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.211706ms)
Apr 17 21:33:35.565: INFO: (1) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 8.838273ms)
Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.468241ms)
Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.420843ms)
Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.5929ms)
Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.609976ms)
Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.730013ms)
Apr 17 21:33:35.572: INFO: (2) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 5.136999ms)
Apr 17 21:33:35.573: INFO: (2) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 5.323784ms)
Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 6.710331ms)
Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 6.865555ms)
Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 6.845985ms)
Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.008513ms)
Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.00237ms)
Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 6.979696ms)
Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.057671ms)
Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.100331ms)
Apr 17 21:33:35.576: INFO: (2) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 8.873371ms)
Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.379696ms)
Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.393057ms)
Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 10.488617ms)
Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.549077ms)
Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.504313ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 6.910378ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.143325ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.354327ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.430867ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.452969ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.489749ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.514359ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.541788ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.64717ms)
Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.653616ms)
Apr 17 21:33:35.587: INFO: (3) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 8.936264ms)
Apr 17 21:33:35.587: INFO: (3) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.370579ms)
Apr 17 21:33:35.587: INFO: (3) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 9.44455ms)
Apr 17 21:33:35.589: INFO: (3) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.776221ms)
Apr 17 21:33:35.589: INFO: (3) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.939982ms)
Apr 17 21:33:35.589: INFO: (3) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.918555ms)
Apr 17 21:33:35.596: INFO: (4) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.627005ms)
Apr 17 21:33:35.596: INFO: (4) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.623432ms)
Apr 17 21:33:35.596: INFO: (4) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.520426ms)
Apr 17 21:33:35.596: INFO: (4) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.595729ms)
Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.717217ms)
Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.722756ms)
Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.855681ms)
Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.8121ms)
Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.801311ms)
Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.817247ms)
Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 8.434795ms)
Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.921487ms)
Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 9.964157ms)
Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.011821ms)
Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.034799ms)
Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.031353ms)
Apr 17 21:33:35.606: INFO: (5) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.13137ms)
Apr 17 21:33:35.606: INFO: (5) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.267581ms)
Apr 17 21:33:35.606: INFO: (5) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.320112ms)
Apr 17 21:33:35.606: INFO: (5) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.32902ms)
Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.544092ms)
Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.522598ms)
Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.57356ms)
Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.505803ms)
Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.590464ms)
Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.620826ms)
Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 8.352575ms)
Apr 17 21:33:35.608: INFO: (5) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 9.16359ms)
Apr 17 21:33:35.610: INFO: (5) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.620952ms)
Apr 17 21:33:35.610: INFO: (5) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.729041ms)
Apr 17 21:33:35.610: INFO: (5) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.756501ms)
Apr 17 21:33:35.610: INFO: (5) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.75768ms)
Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.119961ms)
Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.151438ms)
Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.248563ms)
Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.375714ms)
Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.436726ms)
Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.37796ms)
Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.45908ms)
Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.505267ms)
Apr 17 21:33:35.618: INFO: (6) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.708554ms)
Apr 17 21:33:35.618: INFO: (6) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.780079ms)
Apr 17 21:33:35.618: INFO: (6) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 8.407738ms)
Apr 17 21:33:35.619: INFO: (6) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.435279ms)
Apr 17 21:33:35.619: INFO: (6) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 9.516484ms)
Apr 17 21:33:35.621: INFO: (6) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.749232ms)
Apr 17 21:33:35.621: INFO: (6) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 10.838398ms)
Apr 17 21:33:35.621: INFO: (6) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.883756ms)
Apr 17 21:33:35.628: INFO: (7) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.359917ms)
Apr 17 21:33:35.628: INFO: (7) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.317348ms)
Apr 17 21:33:35.628: INFO: (7) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.272746ms)
Apr 17 21:33:35.628: INFO: (7) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.288262ms)
Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.526431ms)
Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.621793ms)
Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.555655ms)
Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.80229ms)
Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.854567ms)
Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.830117ms)
Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 8.470359ms)
Apr 17 21:33:35.630: INFO: (7) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 9.414304ms)
Apr 17 21:33:35.632: INFO: (7) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.827625ms)
Apr 17 21:33:35.632: INFO: (7) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.821281ms)
Apr 17 21:33:35.632: INFO: (7) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 10.837802ms)
Apr 17 21:33:35.632: INFO: (7) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.977212ms)
Apr 17 21:33:35.639: INFO: (8) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.318036ms)
Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.60268ms)
Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.573988ms)
Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.555181ms)
Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.643532ms)
Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.702876ms)
Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.776828ms)
Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.877276ms)
Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.851039ms)
Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.896846ms)
Apr 17 21:33:35.641: INFO: (8) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 8.751098ms)
Apr 17 21:33:35.642: INFO: (8) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 9.658002ms)
Apr 17 21:33:35.643: INFO: (8) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 11.18343ms)
Apr 17 21:33:35.643: INFO: (8) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 11.226985ms)
Apr 17 21:33:35.643: INFO: (8) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 11.405382ms)
Apr 17 21:33:35.644: INFO: (8) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 11.478729ms)
Apr 17 21:33:35.655: INFO: (9) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 11.066174ms)
Apr 17 21:33:35.657: INFO: (9) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 13.101771ms)
Apr 17 21:33:35.657: INFO: (9) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 13.671321ms)
Apr 17 21:33:35.659: INFO: (9) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 15.045789ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 16.130375ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 16.167143ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 16.205994ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 16.213945ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 16.187073ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 16.19463ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 16.202073ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 16.238094ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 16.500778ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 16.783189ms)
Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 16.763583ms)
Apr 17 21:33:35.661: INFO: (9) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 17.163303ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.09455ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.043034ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.016606ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.286724ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.177617ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.261372ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.399185ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.382576ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.563589ms)
Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.59625ms)
Apr 17 21:33:35.669: INFO: (10) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 8.450614ms)
Apr 17 21:33:35.670: INFO: (10) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.626171ms)
Apr 17 21:33:35.670: INFO: (10) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 9.65601ms)
Apr 17 21:33:35.671: INFO: (10) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 9.833036ms)
Apr 17 21:33:35.671: INFO: (10) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 9.757632ms)
Apr 17 21:33:35.671: INFO: (10) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.948309ms)
Apr 17 21:33:35.676: INFO: (11) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 5.19938ms)
Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.220041ms)
Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.258037ms)
Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.225428ms)
Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.314572ms)
Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.364387ms)
Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.251974ms)
Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.323499ms)
Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.272856ms)
Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.31784ms)
Apr 17 21:33:35.680: INFO: (11) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.302746ms)
Apr 17 21:33:35.680: INFO: (11) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 9.331009ms)
Apr 17 21:33:35.682: INFO: (11) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.93687ms)
Apr 17 21:33:35.682: INFO: (11) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 11.003454ms)
Apr 17 21:33:35.682: INFO: (11) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 11.043888ms)
Apr 17 21:33:35.682: INFO: (11) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 11.043633ms)
Apr 17 21:33:35.687: INFO: (12) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 5.136179ms)
Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 8.057396ms)
Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 8.067286ms)
Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 8.177369ms)
Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 8.38464ms)
Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 8.334605ms)
Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 8.460414ms)
Apr 17 21:33:35.691: INFO: (12) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 8.491441ms)
Apr 17 21:33:35.691: INFO: (12) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 8.606358ms)
Apr 17 21:33:35.691: INFO: (12) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 8.529098ms)
Apr 17 21:33:35.692: INFO: (12) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.226511ms)
Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 11.58139ms)
Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 11.835006ms)
Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 11.760419ms)
Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 11.758158ms)
Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 11.882042ms)
Apr 17 21:33:35.698: INFO: (13) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 4.556936ms)
Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 6.776891ms)
Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 6.744665ms)
Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 6.806733ms)
Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 6.945743ms)
Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.100969ms)
Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.10909ms)
Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.154233ms)
Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.148763ms)
Apr 17 21:33:35.702: INFO: (13) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.765225ms)
Apr 17 21:33:35.703: INFO: (13) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.021468ms)
Apr 17 21:33:35.703: INFO: (13) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 9.061821ms)
Apr 17 21:33:35.704: INFO: (13) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.416948ms)
Apr 17 21:33:35.704: INFO: (13) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.441156ms)
Apr 17 21:33:35.705: INFO: (13) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.546027ms)
Apr 17 21:33:35.705: INFO: (13) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.549897ms)
Apr 17 21:33:35.710: INFO: (14) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 5.303625ms)
Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.147663ms)
Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.319191ms)
Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.287468ms)
Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.271584ms)
Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.520255ms)
Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.495321ms)
Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.556399ms)
Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.530217ms)
Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.627864ms)
Apr 17 21:33:35.714: INFO: (14) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 9.318465ms)
Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.961303ms)
Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.976987ms)
Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 11.072491ms)
Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 11.272388ms)
Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 11.356303ms)
Apr 17 21:33:35.720: INFO: (15) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 4.002713ms)
Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 5.618272ms)
Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 6.313183ms)
Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 6.418929ms)
Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 6.480383ms)
Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 6.411971ms)
Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.662309ms)
Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.692417ms)
Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.771893ms)
Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.911361ms)
Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 8.339501ms)
Apr 17 21:33:35.725: INFO: (15) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 8.685112ms)
Apr 17 21:33:35.725: INFO: (15) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 8.767738ms)
Apr 17 21:33:35.725: INFO: (15) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 8.834925ms)
Apr 17 21:33:35.726: INFO: (15) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.564714ms)
Apr 17 21:33:35.726: INFO: (15) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.603552ms)
Apr 17 21:33:35.732: INFO: (16) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 6.135562ms)
Apr 17 21:33:35.732: INFO: (16) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 6.275514ms)
Apr 17 21:33:35.732: INFO: (16) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 6.641434ms)
Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.023434ms)
Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.267009ms)
Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.295963ms)
Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.263623ms)
Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.306638ms)
Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 7.441844ms)
Apr 17 21:33:35.734: INFO: (16) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.789586ms)
Apr 17 21:33:35.734: INFO: (16) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 8.060822ms)
Apr 17 21:33:35.735: INFO: (16) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 8.947931ms)
Apr 17 21:33:35.735: INFO: (16) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.000456ms)
Apr 17 21:33:35.736: INFO: (16) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.614136ms)
Apr 17 21:33:35.737: INFO: (16) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.792865ms)
Apr 17 21:33:35.738: INFO: (16) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 11.731897ms)
Apr 17 21:33:35.743: INFO: (17) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 5.371811ms)
Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.040098ms)
Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.28894ms)
Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.384165ms)
Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.551116ms)
Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.624369ms)
Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.672252ms)
Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.591472ms)
Apr 17 21:33:35.746: INFO: (17) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 8.270144ms)
Apr 17 21:33:35.746: INFO: (17) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 8.591229ms)
Apr 17 21:33:35.747: INFO: (17) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 8.991969ms)
Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.749595ms)
Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.722872ms)
Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.799108ms)
Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.739576ms)
Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 10.79136ms)
Apr 17 21:33:35.753: INFO: (18) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 4.296228ms)
Apr 17 21:33:35.755: INFO: (18) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 6.965254ms)
Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 6.994978ms)
Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.092713ms)
Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.214263ms)
Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.419985ms)
Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.497641ms)
Apr 17 21:33:35.757: INFO: (18) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 8.866535ms)
Apr 17 21:33:35.757: INFO: (18) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 8.893609ms)
Apr 17 21:33:35.757: INFO: (18) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 8.955068ms)
Apr 17 21:33:35.758: INFO: (18) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.815348ms)
Apr 17 21:33:35.759: INFO: (18) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.07076ms)
Apr 17 21:33:35.759: INFO: (18) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.086345ms)
Apr 17 21:33:35.759: INFO: (18) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.127163ms)
Apr 17 21:33:35.760: INFO: (18) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 11.047982ms)
Apr 17 21:33:35.760: INFO: (18) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 11.150288ms)
Apr 17 21:33:35.767: INFO: (19) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.742534ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.726241ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.787313ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.91064ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.937501ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 8.024231ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.966698ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.995427ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.991286ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 8.008335ms)
Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 8.166997ms)
Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.848802ms)
Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.899263ms)
Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 9.892178ms)
Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 9.895608ms)
Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 9.945092ms)
STEP: deleting ReplicationController proxy-service-9xnm9 in namespace proxy-9338, will wait for the garbage collector to delete the pods 04/17/23 21:33:35.77
Apr 17 21:33:35.829: INFO: Deleting ReplicationController proxy-service-9xnm9 took: 6.136346ms
Apr 17 21:33:35.930: INFO: Terminating ReplicationController proxy-service-9xnm9 pods took: 100.32576ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Apr 17 21:33:38.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9338" for this suite. 04/17/23 21:33:38.037
------------------------------
• [SLOW TEST] [5.686 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:33:32.358
    Apr 17 21:33:32.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename proxy 04/17/23 21:33:32.358
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:32.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:32.375
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 04/17/23 21:33:32.477
    STEP: creating replication controller proxy-service-9xnm9 in namespace proxy-9338 04/17/23 21:33:32.477
    I0417 21:33:32.485705      23 runners.go:193] Created replication controller with name: proxy-service-9xnm9, namespace: proxy-9338, replica count: 1
    I0417 21:33:33.536430      23 runners.go:193] proxy-service-9xnm9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0417 21:33:34.537505      23 runners.go:193] proxy-service-9xnm9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0417 21:33:35.538399      23 runners.go:193] proxy-service-9xnm9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 17 21:33:35.542: INFO: setup took 3.164391744s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 04/17/23 21:33:35.542
    Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 10.424256ms)
    Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 10.420147ms)
    Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.555064ms)
    Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 10.466822ms)
    Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 10.615843ms)
    Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 10.702687ms)
    Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 10.855863ms)
    Apr 17 21:33:35.553: INFO: (0) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 10.943059ms)
    Apr 17 21:33:35.554: INFO: (0) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 12.27674ms)
    Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 12.393977ms)
    Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 12.398688ms)
    Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 12.629076ms)
    Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 12.63398ms)
    Apr 17 21:33:35.555: INFO: (0) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 12.72987ms)
    Apr 17 21:33:35.556: INFO: (0) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 14.316189ms)
    Apr 17 21:33:35.556: INFO: (0) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 14.264987ms)
    Apr 17 21:33:35.561: INFO: (1) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 4.441462ms)
    Apr 17 21:33:35.562: INFO: (1) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 5.491568ms)
    Apr 17 21:33:35.562: INFO: (1) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 5.534671ms)
    Apr 17 21:33:35.562: INFO: (1) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 5.736387ms)
    Apr 17 21:33:35.563: INFO: (1) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 6.521797ms)
    Apr 17 21:33:35.563: INFO: (1) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 6.476025ms)
    Apr 17 21:33:35.564: INFO: (1) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.04927ms)
    Apr 17 21:33:35.564: INFO: (1) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.21947ms)
    Apr 17 21:33:35.564: INFO: (1) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.274606ms)
    Apr 17 21:33:35.564: INFO: (1) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.211706ms)
    Apr 17 21:33:35.565: INFO: (1) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 8.838273ms)
    Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.468241ms)
    Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.420843ms)
    Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.5929ms)
    Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.609976ms)
    Apr 17 21:33:35.567: INFO: (1) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.730013ms)
    Apr 17 21:33:35.572: INFO: (2) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 5.136999ms)
    Apr 17 21:33:35.573: INFO: (2) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 5.323784ms)
    Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 6.710331ms)
    Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 6.865555ms)
    Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 6.845985ms)
    Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.008513ms)
    Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.00237ms)
    Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 6.979696ms)
    Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.057671ms)
    Apr 17 21:33:35.574: INFO: (2) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.100331ms)
    Apr 17 21:33:35.576: INFO: (2) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 8.873371ms)
    Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.379696ms)
    Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.393057ms)
    Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 10.488617ms)
    Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.549077ms)
    Apr 17 21:33:35.578: INFO: (2) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.504313ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 6.910378ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.143325ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.354327ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.430867ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.452969ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.489749ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.514359ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.541788ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.64717ms)
    Apr 17 21:33:35.585: INFO: (3) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.653616ms)
    Apr 17 21:33:35.587: INFO: (3) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 8.936264ms)
    Apr 17 21:33:35.587: INFO: (3) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.370579ms)
    Apr 17 21:33:35.587: INFO: (3) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 9.44455ms)
    Apr 17 21:33:35.589: INFO: (3) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.776221ms)
    Apr 17 21:33:35.589: INFO: (3) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.939982ms)
    Apr 17 21:33:35.589: INFO: (3) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.918555ms)
    Apr 17 21:33:35.596: INFO: (4) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.627005ms)
    Apr 17 21:33:35.596: INFO: (4) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.623432ms)
    Apr 17 21:33:35.596: INFO: (4) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.520426ms)
    Apr 17 21:33:35.596: INFO: (4) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.595729ms)
    Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.717217ms)
    Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.722756ms)
    Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.855681ms)
    Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.8121ms)
    Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.801311ms)
    Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.817247ms)
    Apr 17 21:33:35.597: INFO: (4) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 8.434795ms)
    Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.921487ms)
    Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 9.964157ms)
    Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.011821ms)
    Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.034799ms)
    Apr 17 21:33:35.599: INFO: (4) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.031353ms)
    Apr 17 21:33:35.606: INFO: (5) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.13137ms)
    Apr 17 21:33:35.606: INFO: (5) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.267581ms)
    Apr 17 21:33:35.606: INFO: (5) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.320112ms)
    Apr 17 21:33:35.606: INFO: (5) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.32902ms)
    Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.544092ms)
    Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.522598ms)
    Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.57356ms)
    Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.505803ms)
    Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.590464ms)
    Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.620826ms)
    Apr 17 21:33:35.607: INFO: (5) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 8.352575ms)
    Apr 17 21:33:35.608: INFO: (5) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 9.16359ms)
    Apr 17 21:33:35.610: INFO: (5) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.620952ms)
    Apr 17 21:33:35.610: INFO: (5) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.729041ms)
    Apr 17 21:33:35.610: INFO: (5) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.756501ms)
    Apr 17 21:33:35.610: INFO: (5) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.75768ms)
    Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.119961ms)
    Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.151438ms)
    Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.248563ms)
    Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.375714ms)
    Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.436726ms)
    Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.37796ms)
    Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.45908ms)
    Apr 17 21:33:35.617: INFO: (6) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.505267ms)
    Apr 17 21:33:35.618: INFO: (6) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.708554ms)
    Apr 17 21:33:35.618: INFO: (6) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.780079ms)
    Apr 17 21:33:35.618: INFO: (6) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 8.407738ms)
    Apr 17 21:33:35.619: INFO: (6) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.435279ms)
    Apr 17 21:33:35.619: INFO: (6) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 9.516484ms)
    Apr 17 21:33:35.621: INFO: (6) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.749232ms)
    Apr 17 21:33:35.621: INFO: (6) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 10.838398ms)
    Apr 17 21:33:35.621: INFO: (6) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.883756ms)
    Apr 17 21:33:35.628: INFO: (7) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.359917ms)
    Apr 17 21:33:35.628: INFO: (7) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.317348ms)
    Apr 17 21:33:35.628: INFO: (7) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.272746ms)
    Apr 17 21:33:35.628: INFO: (7) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.288262ms)
    Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.526431ms)
    Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.621793ms)
    Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.555655ms)
    Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.80229ms)
    Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.854567ms)
    Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.830117ms)
    Apr 17 21:33:35.629: INFO: (7) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 8.470359ms)
    Apr 17 21:33:35.630: INFO: (7) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 9.414304ms)
    Apr 17 21:33:35.632: INFO: (7) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.827625ms)
    Apr 17 21:33:35.632: INFO: (7) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.821281ms)
    Apr 17 21:33:35.632: INFO: (7) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 10.837802ms)
    Apr 17 21:33:35.632: INFO: (7) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.977212ms)
    Apr 17 21:33:35.639: INFO: (8) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.318036ms)
    Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.60268ms)
    Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.573988ms)
    Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.555181ms)
    Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.643532ms)
    Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.702876ms)
    Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.776828ms)
    Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.877276ms)
    Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.851039ms)
    Apr 17 21:33:35.640: INFO: (8) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.896846ms)
    Apr 17 21:33:35.641: INFO: (8) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 8.751098ms)
    Apr 17 21:33:35.642: INFO: (8) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 9.658002ms)
    Apr 17 21:33:35.643: INFO: (8) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 11.18343ms)
    Apr 17 21:33:35.643: INFO: (8) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 11.226985ms)
    Apr 17 21:33:35.643: INFO: (8) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 11.405382ms)
    Apr 17 21:33:35.644: INFO: (8) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 11.478729ms)
    Apr 17 21:33:35.655: INFO: (9) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 11.066174ms)
    Apr 17 21:33:35.657: INFO: (9) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 13.101771ms)
    Apr 17 21:33:35.657: INFO: (9) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 13.671321ms)
    Apr 17 21:33:35.659: INFO: (9) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 15.045789ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 16.130375ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 16.167143ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 16.205994ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 16.213945ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 16.187073ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 16.19463ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 16.202073ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 16.238094ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 16.500778ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 16.783189ms)
    Apr 17 21:33:35.660: INFO: (9) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 16.763583ms)
    Apr 17 21:33:35.661: INFO: (9) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 17.163303ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.09455ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.043034ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.016606ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.286724ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.177617ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.261372ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.399185ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.382576ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.563589ms)
    Apr 17 21:33:35.668: INFO: (10) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.59625ms)
    Apr 17 21:33:35.669: INFO: (10) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 8.450614ms)
    Apr 17 21:33:35.670: INFO: (10) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.626171ms)
    Apr 17 21:33:35.670: INFO: (10) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 9.65601ms)
    Apr 17 21:33:35.671: INFO: (10) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 9.833036ms)
    Apr 17 21:33:35.671: INFO: (10) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 9.757632ms)
    Apr 17 21:33:35.671: INFO: (10) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.948309ms)
    Apr 17 21:33:35.676: INFO: (11) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 5.19938ms)
    Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.220041ms)
    Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.258037ms)
    Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.225428ms)
    Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.314572ms)
    Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.364387ms)
    Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.251974ms)
    Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.323499ms)
    Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.272856ms)
    Apr 17 21:33:35.678: INFO: (11) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.31784ms)
    Apr 17 21:33:35.680: INFO: (11) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.302746ms)
    Apr 17 21:33:35.680: INFO: (11) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 9.331009ms)
    Apr 17 21:33:35.682: INFO: (11) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.93687ms)
    Apr 17 21:33:35.682: INFO: (11) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 11.003454ms)
    Apr 17 21:33:35.682: INFO: (11) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 11.043888ms)
    Apr 17 21:33:35.682: INFO: (11) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 11.043633ms)
    Apr 17 21:33:35.687: INFO: (12) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 5.136179ms)
    Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 8.057396ms)
    Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 8.067286ms)
    Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 8.177369ms)
    Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 8.38464ms)
    Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 8.334605ms)
    Apr 17 21:33:35.690: INFO: (12) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 8.460414ms)
    Apr 17 21:33:35.691: INFO: (12) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 8.491441ms)
    Apr 17 21:33:35.691: INFO: (12) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 8.606358ms)
    Apr 17 21:33:35.691: INFO: (12) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 8.529098ms)
    Apr 17 21:33:35.692: INFO: (12) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.226511ms)
    Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 11.58139ms)
    Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 11.835006ms)
    Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 11.760419ms)
    Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 11.758158ms)
    Apr 17 21:33:35.694: INFO: (12) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 11.882042ms)
    Apr 17 21:33:35.698: INFO: (13) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 4.556936ms)
    Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 6.776891ms)
    Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 6.744665ms)
    Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 6.806733ms)
    Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 6.945743ms)
    Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.100969ms)
    Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.10909ms)
    Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.154233ms)
    Apr 17 21:33:35.701: INFO: (13) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.148763ms)
    Apr 17 21:33:35.702: INFO: (13) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.765225ms)
    Apr 17 21:33:35.703: INFO: (13) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.021468ms)
    Apr 17 21:33:35.703: INFO: (13) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 9.061821ms)
    Apr 17 21:33:35.704: INFO: (13) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.416948ms)
    Apr 17 21:33:35.704: INFO: (13) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.441156ms)
    Apr 17 21:33:35.705: INFO: (13) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.546027ms)
    Apr 17 21:33:35.705: INFO: (13) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.549897ms)
    Apr 17 21:33:35.710: INFO: (14) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 5.303625ms)
    Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.147663ms)
    Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.319191ms)
    Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.287468ms)
    Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.271584ms)
    Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.520255ms)
    Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.495321ms)
    Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.556399ms)
    Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.530217ms)
    Apr 17 21:33:35.712: INFO: (14) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.627864ms)
    Apr 17 21:33:35.714: INFO: (14) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 9.318465ms)
    Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.961303ms)
    Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.976987ms)
    Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 11.072491ms)
    Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 11.272388ms)
    Apr 17 21:33:35.716: INFO: (14) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 11.356303ms)
    Apr 17 21:33:35.720: INFO: (15) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 4.002713ms)
    Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 5.618272ms)
    Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 6.313183ms)
    Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 6.418929ms)
    Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 6.480383ms)
    Apr 17 21:33:35.722: INFO: (15) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 6.411971ms)
    Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.662309ms)
    Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.692417ms)
    Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.771893ms)
    Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.911361ms)
    Apr 17 21:33:35.724: INFO: (15) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 8.339501ms)
    Apr 17 21:33:35.725: INFO: (15) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 8.685112ms)
    Apr 17 21:33:35.725: INFO: (15) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 8.767738ms)
    Apr 17 21:33:35.725: INFO: (15) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 8.834925ms)
    Apr 17 21:33:35.726: INFO: (15) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.564714ms)
    Apr 17 21:33:35.726: INFO: (15) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.603552ms)
    Apr 17 21:33:35.732: INFO: (16) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 6.135562ms)
    Apr 17 21:33:35.732: INFO: (16) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 6.275514ms)
    Apr 17 21:33:35.732: INFO: (16) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 6.641434ms)
    Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.023434ms)
    Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.267009ms)
    Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.295963ms)
    Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.263623ms)
    Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.306638ms)
    Apr 17 21:33:35.733: INFO: (16) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 7.441844ms)
    Apr 17 21:33:35.734: INFO: (16) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.789586ms)
    Apr 17 21:33:35.734: INFO: (16) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 8.060822ms)
    Apr 17 21:33:35.735: INFO: (16) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 8.947931ms)
    Apr 17 21:33:35.735: INFO: (16) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.000456ms)
    Apr 17 21:33:35.736: INFO: (16) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.614136ms)
    Apr 17 21:33:35.737: INFO: (16) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.792865ms)
    Apr 17 21:33:35.738: INFO: (16) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 11.731897ms)
    Apr 17 21:33:35.743: INFO: (17) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 5.371811ms)
    Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.040098ms)
    Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.28894ms)
    Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.384165ms)
    Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.551116ms)
    Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.624369ms)
    Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.672252ms)
    Apr 17 21:33:35.745: INFO: (17) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.591472ms)
    Apr 17 21:33:35.746: INFO: (17) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 8.270144ms)
    Apr 17 21:33:35.746: INFO: (17) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 8.591229ms)
    Apr 17 21:33:35.747: INFO: (17) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 8.991969ms)
    Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.749595ms)
    Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.722872ms)
    Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 10.799108ms)
    Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 10.739576ms)
    Apr 17 21:33:35.748: INFO: (17) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 10.79136ms)
    Apr 17 21:33:35.753: INFO: (18) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 4.296228ms)
    Apr 17 21:33:35.755: INFO: (18) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 6.965254ms)
    Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 6.994978ms)
    Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.092713ms)
    Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 7.214263ms)
    Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.419985ms)
    Apr 17 21:33:35.756: INFO: (18) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.497641ms)
    Apr 17 21:33:35.757: INFO: (18) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 8.866535ms)
    Apr 17 21:33:35.757: INFO: (18) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 8.893609ms)
    Apr 17 21:33:35.757: INFO: (18) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 8.955068ms)
    Apr 17 21:33:35.758: INFO: (18) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.815348ms)
    Apr 17 21:33:35.759: INFO: (18) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 10.07076ms)
    Apr 17 21:33:35.759: INFO: (18) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 10.086345ms)
    Apr 17 21:33:35.759: INFO: (18) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 10.127163ms)
    Apr 17 21:33:35.760: INFO: (18) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 11.047982ms)
    Apr 17 21:33:35.760: INFO: (18) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 11.150288ms)
    Apr 17 21:33:35.767: INFO: (19) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.742534ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">test<... (200; 7.726241ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:1080/proxy/rewriteme">... (200; 7.787313ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:162/proxy/: bar (200; 7.91064ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 7.937501ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/http:proxy-service-9xnm9-kxqjd:160/proxy/: foo (200; 8.024231ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:443/proxy/tlsrewritem... (200; 7.966698ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/: <a href="/api/v1/namespaces/proxy-9338/pods/proxy-service-9xnm9-kxqjd/proxy/rewriteme">test</a> (200; 7.995427ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:462/proxy/: tls qux (200; 7.991286ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/pods/https:proxy-service-9xnm9-kxqjd:460/proxy/: tls baz (200; 8.008335ms)
    Apr 17 21:33:35.768: INFO: (19) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname2/proxy/: tls qux (200; 8.166997ms)
    Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname1/proxy/: foo (200; 9.848802ms)
    Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/http:proxy-service-9xnm9:portname2/proxy/: bar (200; 9.899263ms)
    Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/https:proxy-service-9xnm9:tlsportname1/proxy/: tls baz (200; 9.892178ms)
    Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname1/proxy/: foo (200; 9.895608ms)
    Apr 17 21:33:35.770: INFO: (19) /api/v1/namespaces/proxy-9338/services/proxy-service-9xnm9:portname2/proxy/: bar (200; 9.945092ms)
    STEP: deleting ReplicationController proxy-service-9xnm9 in namespace proxy-9338, will wait for the garbage collector to delete the pods 04/17/23 21:33:35.77
    Apr 17 21:33:35.829: INFO: Deleting ReplicationController proxy-service-9xnm9 took: 6.136346ms
    Apr 17 21:33:35.930: INFO: Terminating ReplicationController proxy-service-9xnm9 pods took: 100.32576ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:33:38.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9338" for this suite. 04/17/23 21:33:38.037
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:33:38.045
Apr 17 21:33:38.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename dns 04/17/23 21:33:38.045
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:38.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:38.062
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 04/17/23 21:33:38.065
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local;sleep 1; done
 04/17/23 21:33:38.07
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local;sleep 1; done
 04/17/23 21:33:38.071
STEP: creating a pod to probe DNS 04/17/23 21:33:38.071
STEP: submitting the pod to kubernetes 04/17/23 21:33:38.071
Apr 17 21:33:38.080: INFO: Waiting up to 15m0s for pod "dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab" in namespace "dns-4427" to be "running"
Apr 17 21:33:38.083: INFO: Pod "dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.215686ms
Apr 17 21:33:40.087: INFO: Pod "dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab": Phase="Running", Reason="", readiness=true. Elapsed: 2.007311117s
Apr 17 21:33:40.087: INFO: Pod "dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab" satisfied condition "running"
STEP: retrieving the pod 04/17/23 21:33:40.087
STEP: looking for the results for each expected name from probers 04/17/23 21:33:40.091
Apr 17 21:33:40.096: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:40.099: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:40.103: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:40.107: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:40.111: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:40.114: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:40.118: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:40.121: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:40.121: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

Apr 17 21:33:45.128: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:45.132: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:45.135: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:45.139: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:45.143: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:45.146: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:45.150: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:45.154: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:45.154: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

Apr 17 21:33:50.127: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:50.131: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:50.135: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:50.139: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:50.143: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:50.146: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:50.150: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:50.153: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:50.153: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

Apr 17 21:33:55.127: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:55.130: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:55.134: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:55.138: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:55.141: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:55.145: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:55.148: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:55.152: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:33:55.152: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

Apr 17 21:34:00.128: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:00.132: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:00.135: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:00.139: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:00.143: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:00.147: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:00.150: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:00.154: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:00.154: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

Apr 17 21:34:05.126: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:05.130: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:05.134: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:05.138: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:05.142: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:05.145: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:05.149: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:05.153: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
Apr 17 21:34:05.153: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

Apr 17 21:34:10.155: INFO: DNS probes using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab succeeded

STEP: deleting the pod 04/17/23 21:34:10.155
STEP: deleting the test headless service 04/17/23 21:34:10.172
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 17 21:34:10.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4427" for this suite. 04/17/23 21:34:10.191
------------------------------
• [SLOW TEST] [32.153 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:33:38.045
    Apr 17 21:33:38.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename dns 04/17/23 21:33:38.045
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:33:38.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:33:38.062
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 04/17/23 21:33:38.065
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local;sleep 1; done
     04/17/23 21:33:38.07
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4427.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local;sleep 1; done
     04/17/23 21:33:38.071
    STEP: creating a pod to probe DNS 04/17/23 21:33:38.071
    STEP: submitting the pod to kubernetes 04/17/23 21:33:38.071
    Apr 17 21:33:38.080: INFO: Waiting up to 15m0s for pod "dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab" in namespace "dns-4427" to be "running"
    Apr 17 21:33:38.083: INFO: Pod "dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab": Phase="Pending", Reason="", readiness=false. Elapsed: 3.215686ms
    Apr 17 21:33:40.087: INFO: Pod "dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab": Phase="Running", Reason="", readiness=true. Elapsed: 2.007311117s
    Apr 17 21:33:40.087: INFO: Pod "dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab" satisfied condition "running"
    STEP: retrieving the pod 04/17/23 21:33:40.087
    STEP: looking for the results for each expected name from probers 04/17/23 21:33:40.091
    Apr 17 21:33:40.096: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:40.099: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:40.103: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:40.107: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:40.111: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:40.114: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:40.118: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:40.121: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:40.121: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

    Apr 17 21:33:45.128: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:45.132: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:45.135: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:45.139: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:45.143: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:45.146: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:45.150: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:45.154: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:45.154: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

    Apr 17 21:33:50.127: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:50.131: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:50.135: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:50.139: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:50.143: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:50.146: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:50.150: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:50.153: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:50.153: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

    Apr 17 21:33:55.127: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:55.130: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:55.134: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:55.138: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:55.141: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:55.145: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:55.148: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:55.152: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:33:55.152: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

    Apr 17 21:34:00.128: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:00.132: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:00.135: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:00.139: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:00.143: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:00.147: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:00.150: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:00.154: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:00.154: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

    Apr 17 21:34:05.126: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:05.130: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:05.134: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:05.138: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:05.142: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:05.145: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:05.149: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:05.153: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local from pod dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab: the server could not find the requested resource (get pods dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab)
    Apr 17 21:34:05.153: INFO: Lookups using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4427.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4427.svc.cluster.local jessie_udp@dns-test-service-2.dns-4427.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4427.svc.cluster.local]

    Apr 17 21:34:10.155: INFO: DNS probes using dns-4427/dns-test-92631f14-3eeb-49df-80e0-27f0b2ab7bab succeeded

    STEP: deleting the pod 04/17/23 21:34:10.155
    STEP: deleting the test headless service 04/17/23 21:34:10.172
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:34:10.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4427" for this suite. 04/17/23 21:34:10.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:34:10.198
Apr 17 21:34:10.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 21:34:10.199
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:34:10.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:34:10.216
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-3ef5c98a-6b41-44cd-9ad3-1da8efe7f507 04/17/23 21:34:10.224
STEP: Creating secret with name s-test-opt-upd-e7acdda8-9d0a-4974-841d-2f6322b767d5 04/17/23 21:34:10.228
STEP: Creating the pod 04/17/23 21:34:10.234
Apr 17 21:34:10.243: INFO: Waiting up to 5m0s for pod "pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3" in namespace "secrets-239" to be "running and ready"
Apr 17 21:34:10.247: INFO: Pod "pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.866035ms
Apr 17 21:34:10.248: INFO: The phase of Pod pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:34:12.252: INFO: Pod "pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009843861s
Apr 17 21:34:12.252: INFO: The phase of Pod pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3 is Running (Ready = true)
Apr 17 21:34:12.252: INFO: Pod "pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-3ef5c98a-6b41-44cd-9ad3-1da8efe7f507 04/17/23 21:34:12.275
STEP: Updating secret s-test-opt-upd-e7acdda8-9d0a-4974-841d-2f6322b767d5 04/17/23 21:34:12.281
STEP: Creating secret with name s-test-opt-create-181e6b2b-1bff-423a-83d3-1fc64a61414d 04/17/23 21:34:12.286
STEP: waiting to observe update in volume 04/17/23 21:34:12.292
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 21:34:16.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-239" for this suite. 04/17/23 21:34:16.333
------------------------------
• [SLOW TEST] [6.141 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:34:10.198
    Apr 17 21:34:10.198: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 21:34:10.199
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:34:10.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:34:10.216
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-3ef5c98a-6b41-44cd-9ad3-1da8efe7f507 04/17/23 21:34:10.224
    STEP: Creating secret with name s-test-opt-upd-e7acdda8-9d0a-4974-841d-2f6322b767d5 04/17/23 21:34:10.228
    STEP: Creating the pod 04/17/23 21:34:10.234
    Apr 17 21:34:10.243: INFO: Waiting up to 5m0s for pod "pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3" in namespace "secrets-239" to be "running and ready"
    Apr 17 21:34:10.247: INFO: Pod "pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.866035ms
    Apr 17 21:34:10.248: INFO: The phase of Pod pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:34:12.252: INFO: Pod "pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3": Phase="Running", Reason="", readiness=true. Elapsed: 2.009843861s
    Apr 17 21:34:12.252: INFO: The phase of Pod pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3 is Running (Ready = true)
    Apr 17 21:34:12.252: INFO: Pod "pod-secrets-b07aa517-a229-43e2-8f5e-6f3ebec7aca3" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-3ef5c98a-6b41-44cd-9ad3-1da8efe7f507 04/17/23 21:34:12.275
    STEP: Updating secret s-test-opt-upd-e7acdda8-9d0a-4974-841d-2f6322b767d5 04/17/23 21:34:12.281
    STEP: Creating secret with name s-test-opt-create-181e6b2b-1bff-423a-83d3-1fc64a61414d 04/17/23 21:34:12.286
    STEP: waiting to observe update in volume 04/17/23 21:34:12.292
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:34:16.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-239" for this suite. 04/17/23 21:34:16.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:34:16.342
Apr 17 21:34:16.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-runtime 04/17/23 21:34:16.343
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:34:16.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:34:16.36
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 04/17/23 21:34:16.363
STEP: wait for the container to reach Succeeded 04/17/23 21:34:16.371
STEP: get the container status 04/17/23 21:34:20.391
STEP: the container should be terminated 04/17/23 21:34:20.395
STEP: the termination message should be set 04/17/23 21:34:20.395
Apr 17 21:34:20.395: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 04/17/23 21:34:20.395
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 17 21:34:20.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6769" for this suite. 04/17/23 21:34:20.418
------------------------------
• [4.082 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:34:16.342
    Apr 17 21:34:16.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-runtime 04/17/23 21:34:16.343
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:34:16.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:34:16.36
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 04/17/23 21:34:16.363
    STEP: wait for the container to reach Succeeded 04/17/23 21:34:16.371
    STEP: get the container status 04/17/23 21:34:20.391
    STEP: the container should be terminated 04/17/23 21:34:20.395
    STEP: the termination message should be set 04/17/23 21:34:20.395
    Apr 17 21:34:20.395: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 04/17/23 21:34:20.395
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:34:20.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6769" for this suite. 04/17/23 21:34:20.418
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:34:20.426
Apr 17 21:34:20.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 21:34:20.427
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:34:20.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:34:20.443
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 04/17/23 21:34:20.445
Apr 17 21:34:20.454: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43" in namespace "emptydir-1841" to be "running"
Apr 17 21:34:20.459: INFO: Pod "pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43": Phase="Pending", Reason="", readiness=false. Elapsed: 5.266824ms
Apr 17 21:34:22.464: INFO: Pod "pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43": Phase="Running", Reason="", readiness=false. Elapsed: 2.010020515s
Apr 17 21:34:22.464: INFO: Pod "pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43" satisfied condition "running"
STEP: Reading file content from the nginx-container 04/17/23 21:34:22.464
Apr 17 21:34:22.464: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1841 PodName:pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:34:22.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:34:22.465: INFO: ExecWithOptions: Clientset creation
Apr 17 21:34:22.465: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1841/pods/pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Apr 17 21:34:22.551: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 21:34:22.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1841" for this suite. 04/17/23 21:34:22.557
------------------------------
• [2.138 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:34:20.426
    Apr 17 21:34:20.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 21:34:20.427
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:34:20.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:34:20.443
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 04/17/23 21:34:20.445
    Apr 17 21:34:20.454: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43" in namespace "emptydir-1841" to be "running"
    Apr 17 21:34:20.459: INFO: Pod "pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43": Phase="Pending", Reason="", readiness=false. Elapsed: 5.266824ms
    Apr 17 21:34:22.464: INFO: Pod "pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43": Phase="Running", Reason="", readiness=false. Elapsed: 2.010020515s
    Apr 17 21:34:22.464: INFO: Pod "pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43" satisfied condition "running"
    STEP: Reading file content from the nginx-container 04/17/23 21:34:22.464
    Apr 17 21:34:22.464: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-1841 PodName:pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:34:22.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:34:22.465: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:34:22.465: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-1841/pods/pod-sharedvolume-054bd637-aa8a-4bfc-b612-6ff9aebcbe43/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Apr 17 21:34:22.551: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:34:22.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1841" for this suite. 04/17/23 21:34:22.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:34:22.565
Apr 17 21:34:22.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-probe 04/17/23 21:34:22.565
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:34:22.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:34:22.584
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89 in namespace container-probe-6130 04/17/23 21:34:22.587
Apr 17 21:34:22.596: INFO: Waiting up to 5m0s for pod "liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89" in namespace "container-probe-6130" to be "not pending"
Apr 17 21:34:22.601: INFO: Pod "liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914686ms
Apr 17 21:34:24.606: INFO: Pod "liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89": Phase="Running", Reason="", readiness=true. Elapsed: 2.009834649s
Apr 17 21:34:24.606: INFO: Pod "liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89" satisfied condition "not pending"
Apr 17 21:34:24.606: INFO: Started pod liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89 in namespace container-probe-6130
STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 21:34:24.606
Apr 17 21:34:24.610: INFO: Initial restart count of pod liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89 is 0
STEP: deleting the pod 04/17/23 21:38:25.209
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 17 21:38:25.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-6130" for this suite. 04/17/23 21:38:25.231
------------------------------
• [SLOW TEST] [242.673 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:34:22.565
    Apr 17 21:34:22.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-probe 04/17/23 21:34:22.565
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:34:22.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:34:22.584
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89 in namespace container-probe-6130 04/17/23 21:34:22.587
    Apr 17 21:34:22.596: INFO: Waiting up to 5m0s for pod "liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89" in namespace "container-probe-6130" to be "not pending"
    Apr 17 21:34:22.601: INFO: Pod "liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89": Phase="Pending", Reason="", readiness=false. Elapsed: 4.914686ms
    Apr 17 21:34:24.606: INFO: Pod "liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89": Phase="Running", Reason="", readiness=true. Elapsed: 2.009834649s
    Apr 17 21:34:24.606: INFO: Pod "liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89" satisfied condition "not pending"
    Apr 17 21:34:24.606: INFO: Started pod liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89 in namespace container-probe-6130
    STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 21:34:24.606
    Apr 17 21:34:24.610: INFO: Initial restart count of pod liveness-e2eef1d4-89e5-43f1-8ad7-aeb5c36eab89 is 0
    STEP: deleting the pod 04/17/23 21:38:25.209
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:38:25.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-6130" for this suite. 04/17/23 21:38:25.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:38:25.239
Apr 17 21:38:25.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename deployment 04/17/23 21:38:25.239
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:38:25.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:38:25.256
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Apr 17 21:38:25.267: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr 17 21:38:30.273: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/17/23 21:38:30.273
Apr 17 21:38:30.273: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 04/17/23 21:38:30.283
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 17 21:38:30.293: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3761  8581d821-e75a-4f66-8979-126f23e39f68 25653 1 2023-04-17 21:38:30 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-17 21:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d7e858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Apr 17 21:38:30.298: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-3761  78df82b8-431b-493e-b83b-4532c32f9ca6 25655 1 2023-04-17 21:38:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8581d821-e75a-4f66-8979-126f23e39f68 0xc003d7ecb7 0xc003d7ecb8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8581d821-e75a-4f66-8979-126f23e39f68\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d7ed48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 21:38:30.298: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr 17 21:38:30.298: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3761  bd9ff45c-2c0f-4d67-9daf-4bb31fb4b454 25654 1 2023-04-17 21:38:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8581d821-e75a-4f66-8979-126f23e39f68 0xc003d7eb87 0xc003d7eb88}] [] [{e2e.test Update apps/v1 2023-04-17 21:38:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:38:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-17 21:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"8581d821-e75a-4f66-8979-126f23e39f68\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d7ec48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 17 21:38:30.306: INFO: Pod "test-cleanup-controller-lfxcx" is available:
&Pod{ObjectMeta:{test-cleanup-controller-lfxcx test-cleanup-controller- deployment-3761  8698e03d-8f7b-47b0-b5cc-9c4e2f942714 25616 0 2023-04-17 21:38:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:46dc4ca1dae8125583c8593a9f618fc08a34813c681feef849ed3469b4973526 cni.projectcalico.org/podIP:192.168.163.239/32 cni.projectcalico.org/podIPs:192.168.163.239/32] [{apps/v1 ReplicaSet test-cleanup-controller bd9ff45c-2c0f-4d67-9daf-4bb31fb4b454 0xc003d7f2e7 0xc003d7f2e8}] [] [{calico Update v1 2023-04-17 21:38:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:38:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd9ff45c-2c0f-4d67-9daf-4bb31fb4b454\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:38:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cf7r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cf7r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:38:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:38:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.239,StartTime:2023-04-17 21:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:38:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a7a2ee82ba22b3f285fd8e07175876c45683d4a71f1f1f017f24ad09ac535743,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 21:38:30.307: INFO: Pod "test-cleanup-deployment-7698ff6f6b-xct4k" is not available:
&Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-xct4k test-cleanup-deployment-7698ff6f6b- deployment-3761  f8cb5c87-ef19-46cc-87bc-e2ac9835f412 25657 0 2023-04-17 21:38:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 78df82b8-431b-493e-b83b-4532c32f9ca6 0xc003d7f4e7 0xc003d7f4e8}] [] [{kube-controller-manager Update v1 2023-04-17 21:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78df82b8-431b-493e-b83b-4532c32f9ca6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98cxx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98cxx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 17 21:38:30.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3761" for this suite. 04/17/23 21:38:30.315
------------------------------
• [SLOW TEST] [5.086 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:38:25.239
    Apr 17 21:38:25.239: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename deployment 04/17/23 21:38:25.239
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:38:25.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:38:25.256
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Apr 17 21:38:25.267: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Apr 17 21:38:30.273: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/17/23 21:38:30.273
    Apr 17 21:38:30.273: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 04/17/23 21:38:30.283
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 17 21:38:30.293: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-3761  8581d821-e75a-4f66-8979-126f23e39f68 25653 1 2023-04-17 21:38:30 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-04-17 21:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d7e858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Apr 17 21:38:30.298: INFO: New ReplicaSet "test-cleanup-deployment-7698ff6f6b" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-7698ff6f6b  deployment-3761  78df82b8-431b-493e-b83b-4532c32f9ca6 25655 1 2023-04-17 21:38:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8581d821-e75a-4f66-8979-126f23e39f68 0xc003d7ecb7 0xc003d7ecb8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8581d821-e75a-4f66-8979-126f23e39f68\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 7698ff6f6b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d7ed48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 21:38:30.298: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Apr 17 21:38:30.298: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-3761  bd9ff45c-2c0f-4d67-9daf-4bb31fb4b454 25654 1 2023-04-17 21:38:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 8581d821-e75a-4f66-8979-126f23e39f68 0xc003d7eb87 0xc003d7eb88}] [] [{e2e.test Update apps/v1 2023-04-17 21:38:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:38:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-04-17 21:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"8581d821-e75a-4f66-8979-126f23e39f68\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d7ec48 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 21:38:30.306: INFO: Pod "test-cleanup-controller-lfxcx" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-lfxcx test-cleanup-controller- deployment-3761  8698e03d-8f7b-47b0-b5cc-9c4e2f942714 25616 0 2023-04-17 21:38:25 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:46dc4ca1dae8125583c8593a9f618fc08a34813c681feef849ed3469b4973526 cni.projectcalico.org/podIP:192.168.163.239/32 cni.projectcalico.org/podIPs:192.168.163.239/32] [{apps/v1 ReplicaSet test-cleanup-controller bd9ff45c-2c0f-4d67-9daf-4bb31fb4b454 0xc003d7f2e7 0xc003d7f2e8}] [] [{calico Update v1 2023-04-17 21:38:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:38:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bd9ff45c-2c0f-4d67-9daf-4bb31fb4b454\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:38:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.239\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cf7r4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cf7r4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:38:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:38:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:38:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:38:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.239,StartTime:2023-04-17 21:38:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:38:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a7a2ee82ba22b3f285fd8e07175876c45683d4a71f1f1f017f24ad09ac535743,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.239,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 21:38:30.307: INFO: Pod "test-cleanup-deployment-7698ff6f6b-xct4k" is not available:
    &Pod{ObjectMeta:{test-cleanup-deployment-7698ff6f6b-xct4k test-cleanup-deployment-7698ff6f6b- deployment-3761  f8cb5c87-ef19-46cc-87bc-e2ac9835f412 25657 0 2023-04-17 21:38:30 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:7698ff6f6b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-7698ff6f6b 78df82b8-431b-493e-b83b-4532c32f9ca6 0xc003d7f4e7 0xc003d7f4e8}] [] [{kube-controller-manager Update v1 2023-04-17 21:38:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"78df82b8-431b-493e-b83b-4532c32f9ca6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98cxx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98cxx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:38:30.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3761" for this suite. 04/17/23 21:38:30.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:38:30.325
Apr 17 21:38:30.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename var-expansion 04/17/23 21:38:30.326
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:38:30.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:38:30.346
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Apr 17 21:38:30.356: INFO: Waiting up to 2m0s for pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca" in namespace "var-expansion-7077" to be "container 0 failed with reason CreateContainerConfigError"
Apr 17 21:38:30.361: INFO: Pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.705487ms
Apr 17 21:38:32.366: INFO: Pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00992876s
Apr 17 21:38:32.366: INFO: Pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Apr 17 21:38:32.366: INFO: Deleting pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca" in namespace "var-expansion-7077"
Apr 17 21:38:32.374: INFO: Wait up to 5m0s for pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 17 21:38:34.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7077" for this suite. 04/17/23 21:38:34.387
------------------------------
• [4.069 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:38:30.325
    Apr 17 21:38:30.325: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename var-expansion 04/17/23 21:38:30.326
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:38:30.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:38:30.346
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Apr 17 21:38:30.356: INFO: Waiting up to 2m0s for pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca" in namespace "var-expansion-7077" to be "container 0 failed with reason CreateContainerConfigError"
    Apr 17 21:38:30.361: INFO: Pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 4.705487ms
    Apr 17 21:38:32.366: INFO: Pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00992876s
    Apr 17 21:38:32.366: INFO: Pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Apr 17 21:38:32.366: INFO: Deleting pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca" in namespace "var-expansion-7077"
    Apr 17 21:38:32.374: INFO: Wait up to 5m0s for pod "var-expansion-6cf5066b-9671-4cd5-9ae2-76bedd04e2ca" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:38:34.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7077" for this suite. 04/17/23 21:38:34.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:38:34.396
Apr 17 21:38:34.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-pred 04/17/23 21:38:34.397
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:38:34.411
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:38:34.414
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 17 21:38:34.416: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 17 21:38:34.426: INFO: Waiting for terminating namespaces to be deleted...
Apr 17 21:38:34.429: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-106-231.us-west-2.compute.internal before test
Apr 17 21:38:34.439: INFO: calico-node-qwskb from calico-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.439: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:38:34.439: INFO: calico-typha-6dc6fbd9f5-tcn27 from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.439: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:38:34.439: INFO: csi-node-driver-pptcd from calico-system started at 2023-04-17 21:05:10 +0000 UTC (2 container statuses recorded)
Apr 17 21:38:34.439: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:38:34.439: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:38:34.439: INFO: test-cleanup-controller-lfxcx from deployment-3761 started at 2023-04-17 21:38:25 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.439: INFO: 	Container httpd ready: true, restart count 0
Apr 17 21:38:34.439: INFO: ebs-csi-node-lhxwm from kube-system started at 2023-04-17 21:05:10 +0000 UTC (3 container statuses recorded)
Apr 17 21:38:34.439: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:38:34.439: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:38:34.439: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:38:34.439: INFO: kube-proxy-h249s from kube-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.439: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:38:34.439: INFO: node-feature-discovery-worker-9bc5l from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.439: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:38:34.439: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:38:34.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:38:34.439: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:38:34.439: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-64-189.us-west-2.compute.internal before test
Apr 17 21:38:34.449: INFO: calico-node-b7r2c from calico-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.449: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:38:34.449: INFO: csi-node-driver-rzhbn from calico-system started at 2023-04-17 21:05:08 +0000 UTC (2 container statuses recorded)
Apr 17 21:38:34.449: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:38:34.449: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:38:34.449: INFO: test-cleanup-deployment-7698ff6f6b-xct4k from deployment-3761 started at 2023-04-17 21:38:30 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.449: INFO: 	Container agnhost ready: false, restart count 0
Apr 17 21:38:34.449: INFO: ebs-csi-node-7xssn from kube-system started at 2023-04-17 21:05:08 +0000 UTC (3 container statuses recorded)
Apr 17 21:38:34.449: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:38:34.449: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:38:34.449: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:38:34.449: INFO: kube-proxy-j772r from kube-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.449: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:38:34.449: INFO: node-feature-discovery-worker-cbvbq from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.449: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:38:34.449: INFO: sonobuoy-e2e-job-777eb8e1ef7a4686 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:38:34.449: INFO: 	Container e2e ready: true, restart count 0
Apr 17 21:38:34.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:38:34.449: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:38:34.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:38:34.449: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:38:34.449: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-74-52.us-west-2.compute.internal before test
Apr 17 21:38:34.458: INFO: calico-node-pqj88 from calico-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.458: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:38:34.458: INFO: calico-typha-6dc6fbd9f5-xbmcd from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.458: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:38:34.458: INFO: csi-node-driver-n474j from calico-system started at 2023-04-17 21:05:14 +0000 UTC (2 container statuses recorded)
Apr 17 21:38:34.458: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:38:34.458: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:38:34.458: INFO: ebs-csi-node-xhdpg from kube-system started at 2023-04-17 21:05:14 +0000 UTC (3 container statuses recorded)
Apr 17 21:38:34.458: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:38:34.458: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:38:34.458: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:38:34.458: INFO: kube-proxy-8tbrz from kube-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.458: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:38:34.458: INFO: node-feature-discovery-worker-bl2dc from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.458: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:38:34.458: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:38:34.458: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:38:34.458: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:38:34.458: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-93-18.us-west-2.compute.internal before test
Apr 17 21:38:34.474: INFO: calico-node-zbl7g from calico-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.474: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:38:34.474: INFO: csi-node-driver-58wnm from calico-system started at 2023-04-17 21:05:13 +0000 UTC (2 container statuses recorded)
Apr 17 21:38:34.474: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:38:34.474: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:38:34.474: INFO: ebs-csi-node-bqd6h from kube-system started at 2023-04-17 21:05:13 +0000 UTC (3 container statuses recorded)
Apr 17 21:38:34.474: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:38:34.474: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:38:34.474: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:38:34.474: INFO: kube-proxy-88qrm from kube-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.474: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:38:34.474: INFO: node-feature-discovery-worker-m4mrl from node-feature-discovery started at 2023-04-17 21:05:56 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.474: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:38:34.474: INFO: sonobuoy from sonobuoy started at 2023-04-17 21:09:00 +0000 UTC (1 container statuses recorded)
Apr 17 21:38:34.474: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 17 21:38:34.474: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:38:34.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:38:34.474: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/17/23 21:38:34.474
Apr 17 21:38:34.482: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3702" to be "running"
Apr 17 21:38:34.485: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.061536ms
Apr 17 21:38:36.489: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007389312s
Apr 17 21:38:36.489: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/17/23 21:38:36.492
STEP: Trying to apply a random label on the found node. 04/17/23 21:38:36.506
STEP: verifying the node has the label kubernetes.io/e2e-49a02b39-67c7-49c7-a6c9-48f422ce1d47 95 04/17/23 21:38:36.517
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 04/17/23 21:38:36.521
Apr 17 21:38:36.535: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3702" to be "not pending"
Apr 17 21:38:36.543: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.429559ms
Apr 17 21:38:38.547: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.012344043s
Apr 17 21:38:38.548: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.74.52 on the node which pod4 resides and expect not scheduled 04/17/23 21:38:38.548
Apr 17 21:38:38.554: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3702" to be "not pending"
Apr 17 21:38:38.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708209ms
Apr 17 21:38:40.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008314733s
Apr 17 21:38:42.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009019701s
Apr 17 21:38:44.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009874667s
Apr 17 21:38:46.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00901879s
Apr 17 21:38:48.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009563233s
Apr 17 21:38:50.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009534108s
Apr 17 21:38:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009505383s
Apr 17 21:38:54.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008357642s
Apr 17 21:38:56.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00875652s
Apr 17 21:38:58.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008665157s
Apr 17 21:39:00.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009547352s
Apr 17 21:39:02.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009557676s
Apr 17 21:39:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008896736s
Apr 17 21:39:06.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009020026s
Apr 17 21:39:08.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008120268s
Apr 17 21:39:10.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008429009s
Apr 17 21:39:12.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009410049s
Apr 17 21:39:14.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008530386s
Apr 17 21:39:16.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008436589s
Apr 17 21:39:18.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008502511s
Apr 17 21:39:20.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009275823s
Apr 17 21:39:22.565: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01107048s
Apr 17 21:39:24.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007968218s
Apr 17 21:39:26.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.0092092s
Apr 17 21:39:28.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.0079076s
Apr 17 21:39:30.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008789597s
Apr 17 21:39:32.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008655093s
Apr 17 21:39:34.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007579587s
Apr 17 21:39:36.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009454404s
Apr 17 21:39:38.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007852114s
Apr 17 21:39:40.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00862717s
Apr 17 21:39:42.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00872069s
Apr 17 21:39:44.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008708108s
Apr 17 21:39:46.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009375873s
Apr 17 21:39:48.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008389645s
Apr 17 21:39:50.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008638313s
Apr 17 21:39:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009578996s
Apr 17 21:39:54.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007898038s
Apr 17 21:39:56.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008548301s
Apr 17 21:39:58.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008461262s
Apr 17 21:40:00.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009336936s
Apr 17 21:40:02.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009240474s
Apr 17 21:40:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008750531s
Apr 17 21:40:06.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009269364s
Apr 17 21:40:08.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008506719s
Apr 17 21:40:10.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009346733s
Apr 17 21:40:12.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008961702s
Apr 17 21:40:14.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009039999s
Apr 17 21:40:16.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008772322s
Apr 17 21:40:18.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008963625s
Apr 17 21:40:20.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008834696s
Apr 17 21:40:22.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009073778s
Apr 17 21:40:24.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008961242s
Apr 17 21:40:26.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008625913s
Apr 17 21:40:28.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008666089s
Apr 17 21:40:30.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009228634s
Apr 17 21:40:32.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008952011s
Apr 17 21:40:34.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009135626s
Apr 17 21:40:36.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008828394s
Apr 17 21:40:38.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00871952s
Apr 17 21:40:40.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009659798s
Apr 17 21:40:42.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009399414s
Apr 17 21:40:44.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008121492s
Apr 17 21:40:46.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008669706s
Apr 17 21:40:48.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00761305s
Apr 17 21:40:50.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009562707s
Apr 17 21:40:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009721958s
Apr 17 21:40:54.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.008820747s
Apr 17 21:40:56.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008618789s
Apr 17 21:40:58.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008537281s
Apr 17 21:41:00.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.009346245s
Apr 17 21:41:02.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008750942s
Apr 17 21:41:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008497843s
Apr 17 21:41:06.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00931798s
Apr 17 21:41:08.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008057555s
Apr 17 21:41:10.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008787433s
Apr 17 21:41:12.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008199539s
Apr 17 21:41:14.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008137175s
Apr 17 21:41:16.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009126805s
Apr 17 21:41:18.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008482551s
Apr 17 21:41:20.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008437161s
Apr 17 21:41:22.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009377021s
Apr 17 21:41:24.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008531876s
Apr 17 21:41:26.564: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.010030335s
Apr 17 21:41:28.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008209549s
Apr 17 21:41:30.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009453001s
Apr 17 21:41:32.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009225348s
Apr 17 21:41:34.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009169547s
Apr 17 21:41:36.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009009533s
Apr 17 21:41:38.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008764008s
Apr 17 21:41:40.564: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009920337s
Apr 17 21:41:42.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008575971s
Apr 17 21:41:44.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008539255s
Apr 17 21:41:46.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.00926279s
Apr 17 21:41:48.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009412373s
Apr 17 21:41:50.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009430383s
Apr 17 21:41:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009305428s
Apr 17 21:41:54.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008249994s
Apr 17 21:41:56.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009009086s
Apr 17 21:41:58.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007816979s
Apr 17 21:42:00.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00850821s
Apr 17 21:42:02.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009158454s
Apr 17 21:42:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008231415s
Apr 17 21:42:06.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007809895s
Apr 17 21:42:08.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008092128s
Apr 17 21:42:10.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009052458s
Apr 17 21:42:12.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009164184s
Apr 17 21:42:14.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008177635s
Apr 17 21:42:16.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009158683s
Apr 17 21:42:18.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008107463s
Apr 17 21:42:20.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009043539s
Apr 17 21:42:22.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.0089208s
Apr 17 21:42:24.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009385595s
Apr 17 21:42:26.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009196726s
Apr 17 21:42:28.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008059322s
Apr 17 21:42:30.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.00876497s
Apr 17 21:42:32.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008882914s
Apr 17 21:42:34.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.00891838s
Apr 17 21:42:36.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.008863195s
Apr 17 21:42:38.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008472669s
Apr 17 21:42:40.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008571033s
Apr 17 21:42:42.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009232523s
Apr 17 21:42:44.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.008315657s
Apr 17 21:42:46.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009519246s
Apr 17 21:42:48.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008350898s
Apr 17 21:42:50.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009167126s
Apr 17 21:42:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009027692s
Apr 17 21:42:54.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008195119s
Apr 17 21:42:56.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00960757s
Apr 17 21:42:58.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00881469s
Apr 17 21:43:00.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007724343s
Apr 17 21:43:02.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009627448s
Apr 17 21:43:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008477881s
Apr 17 21:43:06.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009170141s
Apr 17 21:43:08.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007892019s
Apr 17 21:43:10.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009329488s
Apr 17 21:43:12.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009089941s
Apr 17 21:43:14.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009280145s
Apr 17 21:43:16.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009174507s
Apr 17 21:43:18.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.00792119s
Apr 17 21:43:20.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.007840406s
Apr 17 21:43:22.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008122154s
Apr 17 21:43:24.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008340039s
Apr 17 21:43:26.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.009042949s
Apr 17 21:43:28.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00781917s
Apr 17 21:43:30.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008571202s
Apr 17 21:43:32.609: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.055690016s
Apr 17 21:43:34.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008639273s
Apr 17 21:43:36.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009312151s
Apr 17 21:43:38.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008009207s
Apr 17 21:43:38.565: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011200864s
STEP: removing the label kubernetes.io/e2e-49a02b39-67c7-49c7-a6c9-48f422ce1d47 off the node ip-10-0-74-52.us-west-2.compute.internal 04/17/23 21:43:38.565
STEP: verifying the node doesn't have the label kubernetes.io/e2e-49a02b39-67c7-49c7-a6c9-48f422ce1d47 04/17/23 21:43:38.579
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:43:38.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-3702" for this suite. 04/17/23 21:43:38.589
------------------------------
• [SLOW TEST] [304.201 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:38:34.396
    Apr 17 21:38:34.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-pred 04/17/23 21:38:34.397
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:38:34.411
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:38:34.414
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 17 21:38:34.416: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 17 21:38:34.426: INFO: Waiting for terminating namespaces to be deleted...
    Apr 17 21:38:34.429: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-106-231.us-west-2.compute.internal before test
    Apr 17 21:38:34.439: INFO: calico-node-qwskb from calico-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.439: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: calico-typha-6dc6fbd9f5-tcn27 from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.439: INFO: 	Container calico-typha ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: csi-node-driver-pptcd from calico-system started at 2023-04-17 21:05:10 +0000 UTC (2 container statuses recorded)
    Apr 17 21:38:34.439: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: test-cleanup-controller-lfxcx from deployment-3761 started at 2023-04-17 21:38:25 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.439: INFO: 	Container httpd ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: ebs-csi-node-lhxwm from kube-system started at 2023-04-17 21:05:10 +0000 UTC (3 container statuses recorded)
    Apr 17 21:38:34.439: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: kube-proxy-h249s from kube-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.439: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: node-feature-discovery-worker-9bc5l from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.439: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:38:34.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:38:34.439: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-64-189.us-west-2.compute.internal before test
    Apr 17 21:38:34.449: INFO: calico-node-b7r2c from calico-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.449: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: csi-node-driver-rzhbn from calico-system started at 2023-04-17 21:05:08 +0000 UTC (2 container statuses recorded)
    Apr 17 21:38:34.449: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: test-cleanup-deployment-7698ff6f6b-xct4k from deployment-3761 started at 2023-04-17 21:38:30 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.449: INFO: 	Container agnhost ready: false, restart count 0
    Apr 17 21:38:34.449: INFO: ebs-csi-node-7xssn from kube-system started at 2023-04-17 21:05:08 +0000 UTC (3 container statuses recorded)
    Apr 17 21:38:34.449: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: kube-proxy-j772r from kube-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.449: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: node-feature-discovery-worker-cbvbq from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.449: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: sonobuoy-e2e-job-777eb8e1ef7a4686 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:38:34.449: INFO: 	Container e2e ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:38:34.449: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:38:34.449: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-74-52.us-west-2.compute.internal before test
    Apr 17 21:38:34.458: INFO: calico-node-pqj88 from calico-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.458: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: calico-typha-6dc6fbd9f5-xbmcd from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.458: INFO: 	Container calico-typha ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: csi-node-driver-n474j from calico-system started at 2023-04-17 21:05:14 +0000 UTC (2 container statuses recorded)
    Apr 17 21:38:34.458: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: ebs-csi-node-xhdpg from kube-system started at 2023-04-17 21:05:14 +0000 UTC (3 container statuses recorded)
    Apr 17 21:38:34.458: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: kube-proxy-8tbrz from kube-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.458: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: node-feature-discovery-worker-bl2dc from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.458: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:38:34.458: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:38:34.458: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-93-18.us-west-2.compute.internal before test
    Apr 17 21:38:34.474: INFO: calico-node-zbl7g from calico-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.474: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: csi-node-driver-58wnm from calico-system started at 2023-04-17 21:05:13 +0000 UTC (2 container statuses recorded)
    Apr 17 21:38:34.474: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: ebs-csi-node-bqd6h from kube-system started at 2023-04-17 21:05:13 +0000 UTC (3 container statuses recorded)
    Apr 17 21:38:34.474: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: kube-proxy-88qrm from kube-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.474: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: node-feature-discovery-worker-m4mrl from node-feature-discovery started at 2023-04-17 21:05:56 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.474: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: sonobuoy from sonobuoy started at 2023-04-17 21:09:00 +0000 UTC (1 container statuses recorded)
    Apr 17 21:38:34.474: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:38:34.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:38:34.474: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/17/23 21:38:34.474
    Apr 17 21:38:34.482: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3702" to be "running"
    Apr 17 21:38:34.485: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.061536ms
    Apr 17 21:38:36.489: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.007389312s
    Apr 17 21:38:36.489: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/17/23 21:38:36.492
    STEP: Trying to apply a random label on the found node. 04/17/23 21:38:36.506
    STEP: verifying the node has the label kubernetes.io/e2e-49a02b39-67c7-49c7-a6c9-48f422ce1d47 95 04/17/23 21:38:36.517
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 04/17/23 21:38:36.521
    Apr 17 21:38:36.535: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-3702" to be "not pending"
    Apr 17 21:38:36.543: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.429559ms
    Apr 17 21:38:38.547: INFO: Pod "pod4": Phase="Running", Reason="", readiness=false. Elapsed: 2.012344043s
    Apr 17 21:38:38.548: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.0.74.52 on the node which pod4 resides and expect not scheduled 04/17/23 21:38:38.548
    Apr 17 21:38:38.554: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-3702" to be "not pending"
    Apr 17 21:38:38.557: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.708209ms
    Apr 17 21:38:40.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008314733s
    Apr 17 21:38:42.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009019701s
    Apr 17 21:38:44.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009874667s
    Apr 17 21:38:46.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00901879s
    Apr 17 21:38:48.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009563233s
    Apr 17 21:38:50.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009534108s
    Apr 17 21:38:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009505383s
    Apr 17 21:38:54.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008357642s
    Apr 17 21:38:56.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.00875652s
    Apr 17 21:38:58.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008665157s
    Apr 17 21:39:00.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009547352s
    Apr 17 21:39:02.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009557676s
    Apr 17 21:39:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008896736s
    Apr 17 21:39:06.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.009020026s
    Apr 17 21:39:08.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.008120268s
    Apr 17 21:39:10.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008429009s
    Apr 17 21:39:12.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009410049s
    Apr 17 21:39:14.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008530386s
    Apr 17 21:39:16.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008436589s
    Apr 17 21:39:18.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.008502511s
    Apr 17 21:39:20.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009275823s
    Apr 17 21:39:22.565: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01107048s
    Apr 17 21:39:24.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007968218s
    Apr 17 21:39:26.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.0092092s
    Apr 17 21:39:28.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.0079076s
    Apr 17 21:39:30.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008789597s
    Apr 17 21:39:32.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.008655093s
    Apr 17 21:39:34.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007579587s
    Apr 17 21:39:36.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009454404s
    Apr 17 21:39:38.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007852114s
    Apr 17 21:39:40.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.00862717s
    Apr 17 21:39:42.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.00872069s
    Apr 17 21:39:44.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.008708108s
    Apr 17 21:39:46.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009375873s
    Apr 17 21:39:48.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008389645s
    Apr 17 21:39:50.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008638313s
    Apr 17 21:39:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009578996s
    Apr 17 21:39:54.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007898038s
    Apr 17 21:39:56.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008548301s
    Apr 17 21:39:58.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008461262s
    Apr 17 21:40:00.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009336936s
    Apr 17 21:40:02.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.009240474s
    Apr 17 21:40:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008750531s
    Apr 17 21:40:06.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009269364s
    Apr 17 21:40:08.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008506719s
    Apr 17 21:40:10.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009346733s
    Apr 17 21:40:12.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008961702s
    Apr 17 21:40:14.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009039999s
    Apr 17 21:40:16.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.008772322s
    Apr 17 21:40:18.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008963625s
    Apr 17 21:40:20.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008834696s
    Apr 17 21:40:22.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.009073778s
    Apr 17 21:40:24.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008961242s
    Apr 17 21:40:26.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008625913s
    Apr 17 21:40:28.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008666089s
    Apr 17 21:40:30.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009228634s
    Apr 17 21:40:32.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008952011s
    Apr 17 21:40:34.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009135626s
    Apr 17 21:40:36.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.008828394s
    Apr 17 21:40:38.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00871952s
    Apr 17 21:40:40.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.009659798s
    Apr 17 21:40:42.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009399414s
    Apr 17 21:40:44.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008121492s
    Apr 17 21:40:46.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008669706s
    Apr 17 21:40:48.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00761305s
    Apr 17 21:40:50.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009562707s
    Apr 17 21:40:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009721958s
    Apr 17 21:40:54.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.008820747s
    Apr 17 21:40:56.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008618789s
    Apr 17 21:40:58.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.008537281s
    Apr 17 21:41:00.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.009346245s
    Apr 17 21:41:02.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.008750942s
    Apr 17 21:41:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.008497843s
    Apr 17 21:41:06.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.00931798s
    Apr 17 21:41:08.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008057555s
    Apr 17 21:41:10.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008787433s
    Apr 17 21:41:12.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008199539s
    Apr 17 21:41:14.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008137175s
    Apr 17 21:41:16.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009126805s
    Apr 17 21:41:18.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.008482551s
    Apr 17 21:41:20.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008437161s
    Apr 17 21:41:22.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009377021s
    Apr 17 21:41:24.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.008531876s
    Apr 17 21:41:26.564: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.010030335s
    Apr 17 21:41:28.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008209549s
    Apr 17 21:41:30.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.009453001s
    Apr 17 21:41:32.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009225348s
    Apr 17 21:41:34.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.009169547s
    Apr 17 21:41:36.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009009533s
    Apr 17 21:41:38.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008764008s
    Apr 17 21:41:40.564: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.009920337s
    Apr 17 21:41:42.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.008575971s
    Apr 17 21:41:44.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.008539255s
    Apr 17 21:41:46.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.00926279s
    Apr 17 21:41:48.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009412373s
    Apr 17 21:41:50.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.009430383s
    Apr 17 21:41:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.009305428s
    Apr 17 21:41:54.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.008249994s
    Apr 17 21:41:56.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009009086s
    Apr 17 21:41:58.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007816979s
    Apr 17 21:42:00.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.00850821s
    Apr 17 21:42:02.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.009158454s
    Apr 17 21:42:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.008231415s
    Apr 17 21:42:06.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007809895s
    Apr 17 21:42:08.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008092128s
    Apr 17 21:42:10.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009052458s
    Apr 17 21:42:12.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009164184s
    Apr 17 21:42:14.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.008177635s
    Apr 17 21:42:16.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009158683s
    Apr 17 21:42:18.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008107463s
    Apr 17 21:42:20.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.009043539s
    Apr 17 21:42:22.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.0089208s
    Apr 17 21:42:24.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009385595s
    Apr 17 21:42:26.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.009196726s
    Apr 17 21:42:28.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.008059322s
    Apr 17 21:42:30.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.00876497s
    Apr 17 21:42:32.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.008882914s
    Apr 17 21:42:34.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.00891838s
    Apr 17 21:42:36.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.008863195s
    Apr 17 21:42:38.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.008472669s
    Apr 17 21:42:40.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008571033s
    Apr 17 21:42:42.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009232523s
    Apr 17 21:42:44.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.008315657s
    Apr 17 21:42:46.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009519246s
    Apr 17 21:42:48.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008350898s
    Apr 17 21:42:50.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.009167126s
    Apr 17 21:42:52.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009027692s
    Apr 17 21:42:54.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.008195119s
    Apr 17 21:42:56.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.00960757s
    Apr 17 21:42:58.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00881469s
    Apr 17 21:43:00.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.007724343s
    Apr 17 21:43:02.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.009627448s
    Apr 17 21:43:04.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.008477881s
    Apr 17 21:43:06.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.009170141s
    Apr 17 21:43:08.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.007892019s
    Apr 17 21:43:10.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009329488s
    Apr 17 21:43:12.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009089941s
    Apr 17 21:43:14.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009280145s
    Apr 17 21:43:16.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009174507s
    Apr 17 21:43:18.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.00792119s
    Apr 17 21:43:20.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.007840406s
    Apr 17 21:43:22.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.008122154s
    Apr 17 21:43:24.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008340039s
    Apr 17 21:43:26.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.009042949s
    Apr 17 21:43:28.561: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00781917s
    Apr 17 21:43:30.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.008571202s
    Apr 17 21:43:32.609: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.055690016s
    Apr 17 21:43:34.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008639273s
    Apr 17 21:43:36.563: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009312151s
    Apr 17 21:43:38.562: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.008009207s
    Apr 17 21:43:38.565: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.011200864s
    STEP: removing the label kubernetes.io/e2e-49a02b39-67c7-49c7-a6c9-48f422ce1d47 off the node ip-10-0-74-52.us-west-2.compute.internal 04/17/23 21:43:38.565
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-49a02b39-67c7-49c7-a6c9-48f422ce1d47 04/17/23 21:43:38.579
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:43:38.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-3702" for this suite. 04/17/23 21:43:38.589
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:43:38.598
Apr 17 21:43:38.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 21:43:38.598
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:38.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:38.616
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-27afba81-3b01-41d1-a349-dc8694a066f2 04/17/23 21:43:38.618
STEP: Creating a pod to test consume configMaps 04/17/23 21:43:38.624
Apr 17 21:43:38.632: INFO: Waiting up to 5m0s for pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72" in namespace "configmap-678" to be "Succeeded or Failed"
Apr 17 21:43:38.638: INFO: Pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.536894ms
Apr 17 21:43:40.643: INFO: Pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011152116s
Apr 17 21:43:42.643: INFO: Pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011246495s
STEP: Saw pod success 04/17/23 21:43:42.643
Apr 17 21:43:42.644: INFO: Pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72" satisfied condition "Succeeded or Failed"
Apr 17 21:43:42.647: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72 container configmap-volume-test: <nil>
STEP: delete the pod 04/17/23 21:43:42.662
Apr 17 21:43:42.673: INFO: Waiting for pod pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72 to disappear
Apr 17 21:43:42.676: INFO: Pod pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 21:43:42.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-678" for this suite. 04/17/23 21:43:42.681
------------------------------
• [4.090 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:43:38.598
    Apr 17 21:43:38.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 21:43:38.598
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:38.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:38.616
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-27afba81-3b01-41d1-a349-dc8694a066f2 04/17/23 21:43:38.618
    STEP: Creating a pod to test consume configMaps 04/17/23 21:43:38.624
    Apr 17 21:43:38.632: INFO: Waiting up to 5m0s for pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72" in namespace "configmap-678" to be "Succeeded or Failed"
    Apr 17 21:43:38.638: INFO: Pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72": Phase="Pending", Reason="", readiness=false. Elapsed: 5.536894ms
    Apr 17 21:43:40.643: INFO: Pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011152116s
    Apr 17 21:43:42.643: INFO: Pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011246495s
    STEP: Saw pod success 04/17/23 21:43:42.643
    Apr 17 21:43:42.644: INFO: Pod "pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72" satisfied condition "Succeeded or Failed"
    Apr 17 21:43:42.647: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72 container configmap-volume-test: <nil>
    STEP: delete the pod 04/17/23 21:43:42.662
    Apr 17 21:43:42.673: INFO: Waiting for pod pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72 to disappear
    Apr 17 21:43:42.676: INFO: Pod pod-configmaps-aad25522-70ce-4b3f-b391-58c47213ae72 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:43:42.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-678" for this suite. 04/17/23 21:43:42.681
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:43:42.687
Apr 17 21:43:42.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-webhook 04/17/23 21:43:42.688
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:42.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:42.705
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 04/17/23 21:43:42.707
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/17/23 21:43:42.83
STEP: Deploying the custom resource conversion webhook pod 04/17/23 21:43:42.838
STEP: Wait for the deployment to be ready 04/17/23 21:43:42.85
Apr 17 21:43:42.858: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 21:43:44.87
STEP: Verifying the service has paired with the endpoint 04/17/23 21:43:44.885
Apr 17 21:43:45.886: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Apr 17 21:43:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Creating a v1 custom resource 04/17/23 21:43:48.459
STEP: Create a v2 custom resource 04/17/23 21:43:48.475
STEP: List CRs in v1 04/17/23 21:43:48.517
STEP: List CRs in v2 04/17/23 21:43:48.522
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:43:49.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3073" for this suite. 04/17/23 21:43:49.209
------------------------------
• [SLOW TEST] [6.532 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:43:42.687
    Apr 17 21:43:42.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-webhook 04/17/23 21:43:42.688
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:42.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:42.705
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 04/17/23 21:43:42.707
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/17/23 21:43:42.83
    STEP: Deploying the custom resource conversion webhook pod 04/17/23 21:43:42.838
    STEP: Wait for the deployment to be ready 04/17/23 21:43:42.85
    Apr 17 21:43:42.858: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 21:43:44.87
    STEP: Verifying the service has paired with the endpoint 04/17/23 21:43:44.885
    Apr 17 21:43:45.886: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Apr 17 21:43:45.889: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Creating a v1 custom resource 04/17/23 21:43:48.459
    STEP: Create a v2 custom resource 04/17/23 21:43:48.475
    STEP: List CRs in v1 04/17/23 21:43:48.517
    STEP: List CRs in v2 04/17/23 21:43:48.522
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:43:49.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3073" for this suite. 04/17/23 21:43:49.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:43:49.22
Apr 17 21:43:49.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename runtimeclass 04/17/23 21:43:49.22
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:49.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:49.239
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-1159-delete-me 04/17/23 21:43:49.246
STEP: Waiting for the RuntimeClass to disappear 04/17/23 21:43:49.253
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 17 21:43:49.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1159" for this suite. 04/17/23 21:43:49.267
------------------------------
• [0.054 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:43:49.22
    Apr 17 21:43:49.220: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename runtimeclass 04/17/23 21:43:49.22
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:49.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:49.239
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-1159-delete-me 04/17/23 21:43:49.246
    STEP: Waiting for the RuntimeClass to disappear 04/17/23 21:43:49.253
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:43:49.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1159" for this suite. 04/17/23 21:43:49.267
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:43:49.274
Apr 17 21:43:49.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename containers 04/17/23 21:43:49.275
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:49.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:49.34
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 04/17/23 21:43:49.342
Apr 17 21:43:49.351: INFO: Waiting up to 5m0s for pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5" in namespace "containers-2597" to be "Succeeded or Failed"
Apr 17 21:43:49.355: INFO: Pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.801839ms
Apr 17 21:43:51.359: INFO: Pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007476343s
Apr 17 21:43:53.360: INFO: Pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008541001s
STEP: Saw pod success 04/17/23 21:43:53.36
Apr 17 21:43:53.360: INFO: Pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5" satisfied condition "Succeeded or Failed"
Apr 17 21:43:53.363: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 21:43:53.378
Apr 17 21:43:53.392: INFO: Waiting for pod client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5 to disappear
Apr 17 21:43:53.395: INFO: Pod client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 17 21:43:53.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-2597" for this suite. 04/17/23 21:43:53.4
------------------------------
• [4.133 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:43:49.274
    Apr 17 21:43:49.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename containers 04/17/23 21:43:49.275
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:49.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:49.34
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 04/17/23 21:43:49.342
    Apr 17 21:43:49.351: INFO: Waiting up to 5m0s for pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5" in namespace "containers-2597" to be "Succeeded or Failed"
    Apr 17 21:43:49.355: INFO: Pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.801839ms
    Apr 17 21:43:51.359: INFO: Pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007476343s
    Apr 17 21:43:53.360: INFO: Pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008541001s
    STEP: Saw pod success 04/17/23 21:43:53.36
    Apr 17 21:43:53.360: INFO: Pod "client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5" satisfied condition "Succeeded or Failed"
    Apr 17 21:43:53.363: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 21:43:53.378
    Apr 17 21:43:53.392: INFO: Waiting for pod client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5 to disappear
    Apr 17 21:43:53.395: INFO: Pod client-containers-49589b30-0269-4a69-9cb5-ca9f9adee6d5 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:43:53.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-2597" for this suite. 04/17/23 21:43:53.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:43:53.407
Apr 17 21:43:53.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 21:43:53.408
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:53.422
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:53.425
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-78f61c76-1d81-4978-a526-69101dc2c52d 04/17/23 21:43:53.428
STEP: Creating a pod to test consume configMaps 04/17/23 21:43:53.432
Apr 17 21:43:53.441: INFO: Waiting up to 5m0s for pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4" in namespace "configmap-2888" to be "Succeeded or Failed"
Apr 17 21:43:53.444: INFO: Pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233279ms
Apr 17 21:43:55.449: INFO: Pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008089466s
Apr 17 21:43:57.450: INFO: Pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008991068s
STEP: Saw pod success 04/17/23 21:43:57.45
Apr 17 21:43:57.450: INFO: Pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4" satisfied condition "Succeeded or Failed"
Apr 17 21:43:57.454: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 21:43:57.46
Apr 17 21:43:57.474: INFO: Waiting for pod pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4 to disappear
Apr 17 21:43:57.477: INFO: Pod pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 21:43:57.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2888" for this suite. 04/17/23 21:43:57.482
------------------------------
• [4.082 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:43:53.407
    Apr 17 21:43:53.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 21:43:53.408
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:53.422
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:53.425
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-78f61c76-1d81-4978-a526-69101dc2c52d 04/17/23 21:43:53.428
    STEP: Creating a pod to test consume configMaps 04/17/23 21:43:53.432
    Apr 17 21:43:53.441: INFO: Waiting up to 5m0s for pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4" in namespace "configmap-2888" to be "Succeeded or Failed"
    Apr 17 21:43:53.444: INFO: Pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.233279ms
    Apr 17 21:43:55.449: INFO: Pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008089466s
    Apr 17 21:43:57.450: INFO: Pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008991068s
    STEP: Saw pod success 04/17/23 21:43:57.45
    Apr 17 21:43:57.450: INFO: Pod "pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4" satisfied condition "Succeeded or Failed"
    Apr 17 21:43:57.454: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 21:43:57.46
    Apr 17 21:43:57.474: INFO: Waiting for pod pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4 to disappear
    Apr 17 21:43:57.477: INFO: Pod pod-configmaps-404d17d5-33cd-4b6c-96cc-a6f46bd319c4 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:43:57.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2888" for this suite. 04/17/23 21:43:57.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:43:57.49
Apr 17 21:43:57.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename dns 04/17/23 21:43:57.491
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:57.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:57.506
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 04/17/23 21:43:57.509
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
 04/17/23 21:43:57.513
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
 04/17/23 21:43:57.513
STEP: creating a pod to probe DNS 04/17/23 21:43:57.513
STEP: submitting the pod to kubernetes 04/17/23 21:43:57.513
Apr 17 21:43:57.522: INFO: Waiting up to 15m0s for pod "dns-test-7483d124-034c-4214-998a-0da644cfb3b4" in namespace "dns-6328" to be "running"
Apr 17 21:43:57.527: INFO: Pod "dns-test-7483d124-034c-4214-998a-0da644cfb3b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.198497ms
Apr 17 21:43:59.532: INFO: Pod "dns-test-7483d124-034c-4214-998a-0da644cfb3b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009386947s
Apr 17 21:43:59.532: INFO: Pod "dns-test-7483d124-034c-4214-998a-0da644cfb3b4" satisfied condition "running"
STEP: retrieving the pod 04/17/23 21:43:59.532
STEP: looking for the results for each expected name from probers 04/17/23 21:43:59.535
Apr 17 21:43:59.544: INFO: DNS probes using dns-test-7483d124-034c-4214-998a-0da644cfb3b4 succeeded

STEP: deleting the pod 04/17/23 21:43:59.544
STEP: changing the externalName to bar.example.com 04/17/23 21:43:59.559
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
 04/17/23 21:43:59.566
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
 04/17/23 21:43:59.566
STEP: creating a second pod to probe DNS 04/17/23 21:43:59.566
STEP: submitting the pod to kubernetes 04/17/23 21:43:59.566
Apr 17 21:43:59.573: INFO: Waiting up to 15m0s for pod "dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f" in namespace "dns-6328" to be "running"
Apr 17 21:43:59.578: INFO: Pod "dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963684ms
Apr 17 21:44:01.583: INFO: Pod "dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009530658s
Apr 17 21:44:01.583: INFO: Pod "dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f" satisfied condition "running"
STEP: retrieving the pod 04/17/23 21:44:01.583
STEP: looking for the results for each expected name from probers 04/17/23 21:44:01.586
Apr 17 21:44:01.591: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:01.595: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:01.595: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

Apr 17 21:44:06.602: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:06.606: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:06.606: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

Apr 17 21:44:11.600: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:11.604: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:11.604: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

Apr 17 21:44:16.603: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:16.607: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:16.607: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

Apr 17 21:44:21.603: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:21.607: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:21.607: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

Apr 17 21:44:26.603: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:26.607: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
' instead of 'bar.example.com.'
Apr 17 21:44:26.607: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

Apr 17 21:44:31.606: INFO: DNS probes using dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f succeeded

STEP: deleting the pod 04/17/23 21:44:31.606
STEP: changing the service to type=ClusterIP 04/17/23 21:44:31.622
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
 04/17/23 21:44:31.644
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
 04/17/23 21:44:31.645
STEP: creating a third pod to probe DNS 04/17/23 21:44:31.645
STEP: submitting the pod to kubernetes 04/17/23 21:44:31.648
Apr 17 21:44:31.658: INFO: Waiting up to 15m0s for pod "dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee" in namespace "dns-6328" to be "running"
Apr 17 21:44:31.662: INFO: Pod "dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.803705ms
Apr 17 21:44:33.666: INFO: Pod "dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee": Phase="Running", Reason="", readiness=true. Elapsed: 2.008182052s
Apr 17 21:44:33.666: INFO: Pod "dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee" satisfied condition "running"
STEP: retrieving the pod 04/17/23 21:44:33.666
STEP: looking for the results for each expected name from probers 04/17/23 21:44:33.67
Apr 17 21:44:33.679: INFO: DNS probes using dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee succeeded

STEP: deleting the pod 04/17/23 21:44:33.679
STEP: deleting the test externalName service 04/17/23 21:44:33.694
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 17 21:44:33.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6328" for this suite. 04/17/23 21:44:33.803
------------------------------
• [SLOW TEST] [36.321 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:43:57.49
    Apr 17 21:43:57.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename dns 04/17/23 21:43:57.491
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:43:57.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:43:57.506
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 04/17/23 21:43:57.509
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
     04/17/23 21:43:57.513
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
     04/17/23 21:43:57.513
    STEP: creating a pod to probe DNS 04/17/23 21:43:57.513
    STEP: submitting the pod to kubernetes 04/17/23 21:43:57.513
    Apr 17 21:43:57.522: INFO: Waiting up to 15m0s for pod "dns-test-7483d124-034c-4214-998a-0da644cfb3b4" in namespace "dns-6328" to be "running"
    Apr 17 21:43:57.527: INFO: Pod "dns-test-7483d124-034c-4214-998a-0da644cfb3b4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.198497ms
    Apr 17 21:43:59.532: INFO: Pod "dns-test-7483d124-034c-4214-998a-0da644cfb3b4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009386947s
    Apr 17 21:43:59.532: INFO: Pod "dns-test-7483d124-034c-4214-998a-0da644cfb3b4" satisfied condition "running"
    STEP: retrieving the pod 04/17/23 21:43:59.532
    STEP: looking for the results for each expected name from probers 04/17/23 21:43:59.535
    Apr 17 21:43:59.544: INFO: DNS probes using dns-test-7483d124-034c-4214-998a-0da644cfb3b4 succeeded

    STEP: deleting the pod 04/17/23 21:43:59.544
    STEP: changing the externalName to bar.example.com 04/17/23 21:43:59.559
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
     04/17/23 21:43:59.566
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
     04/17/23 21:43:59.566
    STEP: creating a second pod to probe DNS 04/17/23 21:43:59.566
    STEP: submitting the pod to kubernetes 04/17/23 21:43:59.566
    Apr 17 21:43:59.573: INFO: Waiting up to 15m0s for pod "dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f" in namespace "dns-6328" to be "running"
    Apr 17 21:43:59.578: INFO: Pod "dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.963684ms
    Apr 17 21:44:01.583: INFO: Pod "dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009530658s
    Apr 17 21:44:01.583: INFO: Pod "dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f" satisfied condition "running"
    STEP: retrieving the pod 04/17/23 21:44:01.583
    STEP: looking for the results for each expected name from probers 04/17/23 21:44:01.586
    Apr 17 21:44:01.591: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:01.595: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:01.595: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

    Apr 17 21:44:06.602: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:06.606: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:06.606: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

    Apr 17 21:44:11.600: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:11.604: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:11.604: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

    Apr 17 21:44:16.603: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:16.607: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:16.607: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

    Apr 17 21:44:21.603: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:21.607: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:21.607: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

    Apr 17 21:44:26.603: INFO: File wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:26.607: INFO: File jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local from pod  dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Apr 17 21:44:26.607: INFO: Lookups using dns-6328/dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f failed for: [wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local]

    Apr 17 21:44:31.606: INFO: DNS probes using dns-test-7a3f8808-140c-4bc7-a396-f15b4bd3782f succeeded

    STEP: deleting the pod 04/17/23 21:44:31.606
    STEP: changing the service to type=ClusterIP 04/17/23 21:44:31.622
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
     04/17/23 21:44:31.644
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6328.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6328.svc.cluster.local; sleep 1; done
     04/17/23 21:44:31.645
    STEP: creating a third pod to probe DNS 04/17/23 21:44:31.645
    STEP: submitting the pod to kubernetes 04/17/23 21:44:31.648
    Apr 17 21:44:31.658: INFO: Waiting up to 15m0s for pod "dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee" in namespace "dns-6328" to be "running"
    Apr 17 21:44:31.662: INFO: Pod "dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.803705ms
    Apr 17 21:44:33.666: INFO: Pod "dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee": Phase="Running", Reason="", readiness=true. Elapsed: 2.008182052s
    Apr 17 21:44:33.666: INFO: Pod "dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee" satisfied condition "running"
    STEP: retrieving the pod 04/17/23 21:44:33.666
    STEP: looking for the results for each expected name from probers 04/17/23 21:44:33.67
    Apr 17 21:44:33.679: INFO: DNS probes using dns-test-428aa198-29a8-41ec-a4c9-5dd5a23a2fee succeeded

    STEP: deleting the pod 04/17/23 21:44:33.679
    STEP: deleting the test externalName service 04/17/23 21:44:33.694
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:44:33.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6328" for this suite. 04/17/23 21:44:33.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:44:33.811
Apr 17 21:44:33.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 21:44:33.812
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:33.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:33.828
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 21:44:33.843
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:44:34.539
STEP: Deploying the webhook pod 04/17/23 21:44:34.547
STEP: Wait for the deployment to be ready 04/17/23 21:44:34.559
Apr 17 21:44:34.567: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 21:44:36.578
STEP: Verifying the service has paired with the endpoint 04/17/23 21:44:36.657
Apr 17 21:44:37.658: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 04/17/23 21:44:37.726
STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 21:44:37.821
STEP: Deleting the collection of validation webhooks 04/17/23 21:44:37.847
STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 21:44:37.903
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:44:37.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-722" for this suite. 04/17/23 21:44:37.971
STEP: Destroying namespace "webhook-722-markers" for this suite. 04/17/23 21:44:37.982
------------------------------
• [4.180 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:44:33.811
    Apr 17 21:44:33.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 21:44:33.812
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:33.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:33.828
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 21:44:33.843
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:44:34.539
    STEP: Deploying the webhook pod 04/17/23 21:44:34.547
    STEP: Wait for the deployment to be ready 04/17/23 21:44:34.559
    Apr 17 21:44:34.567: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 21:44:36.578
    STEP: Verifying the service has paired with the endpoint 04/17/23 21:44:36.657
    Apr 17 21:44:37.658: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 04/17/23 21:44:37.726
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 21:44:37.821
    STEP: Deleting the collection of validation webhooks 04/17/23 21:44:37.847
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 21:44:37.903
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:44:37.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-722" for this suite. 04/17/23 21:44:37.971
    STEP: Destroying namespace "webhook-722-markers" for this suite. 04/17/23 21:44:37.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:44:37.993
Apr 17 21:44:37.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename events 04/17/23 21:44:37.993
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:38.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:38.009
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 04/17/23 21:44:38.012
Apr 17 21:44:38.020: INFO: created test-event-1
Apr 17 21:44:38.028: INFO: created test-event-2
Apr 17 21:44:38.032: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 04/17/23 21:44:38.032
STEP: delete collection of events 04/17/23 21:44:38.035
Apr 17 21:44:38.035: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 04/17/23 21:44:38.057
Apr 17 21:44:38.057: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Apr 17 21:44:38.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-4171" for this suite. 04/17/23 21:44:38.065
------------------------------
• [0.079 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:44:37.993
    Apr 17 21:44:37.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename events 04/17/23 21:44:37.993
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:38.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:38.009
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 04/17/23 21:44:38.012
    Apr 17 21:44:38.020: INFO: created test-event-1
    Apr 17 21:44:38.028: INFO: created test-event-2
    Apr 17 21:44:38.032: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 04/17/23 21:44:38.032
    STEP: delete collection of events 04/17/23 21:44:38.035
    Apr 17 21:44:38.035: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 04/17/23 21:44:38.057
    Apr 17 21:44:38.057: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:44:38.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-4171" for this suite. 04/17/23 21:44:38.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:44:38.073
Apr 17 21:44:38.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pod-network-test 04/17/23 21:44:38.073
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:38.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:38.089
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-1635 04/17/23 21:44:38.091
STEP: creating a selector 04/17/23 21:44:38.091
STEP: Creating the service pods in kubernetes 04/17/23 21:44:38.091
Apr 17 21:44:38.091: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 17 21:44:38.136: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1635" to be "running and ready"
Apr 17 21:44:38.141: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.418447ms
Apr 17 21:44:38.141: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:44:40.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010530892s
Apr 17 21:44:40.146: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:44:42.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010345527s
Apr 17 21:44:42.146: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:44:44.147: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011259168s
Apr 17 21:44:44.147: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:44:46.147: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011328163s
Apr 17 21:44:46.147: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:44:48.145: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009770094s
Apr 17 21:44:48.145: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:44:50.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010122288s
Apr 17 21:44:50.146: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 17 21:44:50.146: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 17 21:44:50.152: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1635" to be "running and ready"
Apr 17 21:44:50.156: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.730903ms
Apr 17 21:44:50.156: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 17 21:44:50.156: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr 17 21:44:50.160: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1635" to be "running and ready"
Apr 17 21:44:50.163: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.144694ms
Apr 17 21:44:50.163: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr 17 21:44:50.163: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr 17 21:44:50.166: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1635" to be "running and ready"
Apr 17 21:44:50.169: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.961611ms
Apr 17 21:44:50.169: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr 17 21:44:50.169: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 04/17/23 21:44:50.172
Apr 17 21:44:50.178: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1635" to be "running"
Apr 17 21:44:50.182: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112278ms
Apr 17 21:44:52.187: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009271482s
Apr 17 21:44:52.187: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 17 21:44:52.191: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Apr 17 21:44:52.191: INFO: Breadth first check of 192.168.163.241 on host 10.0.106.231...
Apr 17 21:44:52.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.163.242:9080/dial?request=hostname&protocol=udp&host=192.168.163.241&port=8081&tries=1'] Namespace:pod-network-test-1635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:44:52.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:44:52.194: INFO: ExecWithOptions: Clientset creation
Apr 17 21:44:52.194: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1635/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.163.242%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.163.241%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 17 21:44:52.280: INFO: Waiting for responses: map[]
Apr 17 21:44:52.280: INFO: reached 192.168.163.241 after 0/1 tries
Apr 17 21:44:52.280: INFO: Breadth first check of 192.168.200.176 on host 10.0.64.189...
Apr 17 21:44:52.284: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.163.242:9080/dial?request=hostname&protocol=udp&host=192.168.200.176&port=8081&tries=1'] Namespace:pod-network-test-1635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:44:52.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:44:52.284: INFO: ExecWithOptions: Clientset creation
Apr 17 21:44:52.284: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1635/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.163.242%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.200.176%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 17 21:44:52.343: INFO: Waiting for responses: map[]
Apr 17 21:44:52.343: INFO: reached 192.168.200.176 after 0/1 tries
Apr 17 21:44:52.343: INFO: Breadth first check of 192.168.208.187 on host 10.0.74.52...
Apr 17 21:44:52.346: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.163.242:9080/dial?request=hostname&protocol=udp&host=192.168.208.187&port=8081&tries=1'] Namespace:pod-network-test-1635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:44:52.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:44:52.347: INFO: ExecWithOptions: Clientset creation
Apr 17 21:44:52.347: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1635/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.163.242%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.208.187%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 17 21:44:52.407: INFO: Waiting for responses: map[]
Apr 17 21:44:52.407: INFO: reached 192.168.208.187 after 0/1 tries
Apr 17 21:44:52.407: INFO: Breadth first check of 192.168.213.56 on host 10.0.93.18...
Apr 17 21:44:52.411: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.163.242:9080/dial?request=hostname&protocol=udp&host=192.168.213.56&port=8081&tries=1'] Namespace:pod-network-test-1635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:44:52.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:44:52.411: INFO: ExecWithOptions: Clientset creation
Apr 17 21:44:52.411: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1635/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.163.242%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.213.56%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Apr 17 21:44:52.470: INFO: Waiting for responses: map[]
Apr 17 21:44:52.470: INFO: reached 192.168.213.56 after 0/1 tries
Apr 17 21:44:52.470: INFO: Going to retry 0 out of 4 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 17 21:44:52.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1635" for this suite. 04/17/23 21:44:52.476
------------------------------
• [SLOW TEST] [14.411 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:44:38.073
    Apr 17 21:44:38.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pod-network-test 04/17/23 21:44:38.073
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:38.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:38.089
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-1635 04/17/23 21:44:38.091
    STEP: creating a selector 04/17/23 21:44:38.091
    STEP: Creating the service pods in kubernetes 04/17/23 21:44:38.091
    Apr 17 21:44:38.091: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 17 21:44:38.136: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1635" to be "running and ready"
    Apr 17 21:44:38.141: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.418447ms
    Apr 17 21:44:38.141: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:44:40.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010530892s
    Apr 17 21:44:40.146: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:44:42.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010345527s
    Apr 17 21:44:42.146: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:44:44.147: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011259168s
    Apr 17 21:44:44.147: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:44:46.147: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011328163s
    Apr 17 21:44:46.147: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:44:48.145: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009770094s
    Apr 17 21:44:48.145: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:44:50.146: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010122288s
    Apr 17 21:44:50.146: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 17 21:44:50.146: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 17 21:44:50.152: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1635" to be "running and ready"
    Apr 17 21:44:50.156: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.730903ms
    Apr 17 21:44:50.156: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 17 21:44:50.156: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr 17 21:44:50.160: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1635" to be "running and ready"
    Apr 17 21:44:50.163: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.144694ms
    Apr 17 21:44:50.163: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr 17 21:44:50.163: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr 17 21:44:50.166: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1635" to be "running and ready"
    Apr 17 21:44:50.169: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.961611ms
    Apr 17 21:44:50.169: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr 17 21:44:50.169: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 04/17/23 21:44:50.172
    Apr 17 21:44:50.178: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1635" to be "running"
    Apr 17 21:44:50.182: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112278ms
    Apr 17 21:44:52.187: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009271482s
    Apr 17 21:44:52.187: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 17 21:44:52.191: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Apr 17 21:44:52.191: INFO: Breadth first check of 192.168.163.241 on host 10.0.106.231...
    Apr 17 21:44:52.194: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.163.242:9080/dial?request=hostname&protocol=udp&host=192.168.163.241&port=8081&tries=1'] Namespace:pod-network-test-1635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:44:52.194: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:44:52.194: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:44:52.194: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1635/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.163.242%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.163.241%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 17 21:44:52.280: INFO: Waiting for responses: map[]
    Apr 17 21:44:52.280: INFO: reached 192.168.163.241 after 0/1 tries
    Apr 17 21:44:52.280: INFO: Breadth first check of 192.168.200.176 on host 10.0.64.189...
    Apr 17 21:44:52.284: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.163.242:9080/dial?request=hostname&protocol=udp&host=192.168.200.176&port=8081&tries=1'] Namespace:pod-network-test-1635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:44:52.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:44:52.284: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:44:52.284: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1635/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.163.242%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.200.176%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 17 21:44:52.343: INFO: Waiting for responses: map[]
    Apr 17 21:44:52.343: INFO: reached 192.168.200.176 after 0/1 tries
    Apr 17 21:44:52.343: INFO: Breadth first check of 192.168.208.187 on host 10.0.74.52...
    Apr 17 21:44:52.346: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.163.242:9080/dial?request=hostname&protocol=udp&host=192.168.208.187&port=8081&tries=1'] Namespace:pod-network-test-1635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:44:52.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:44:52.347: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:44:52.347: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1635/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.163.242%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.208.187%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 17 21:44:52.407: INFO: Waiting for responses: map[]
    Apr 17 21:44:52.407: INFO: reached 192.168.208.187 after 0/1 tries
    Apr 17 21:44:52.407: INFO: Breadth first check of 192.168.213.56 on host 10.0.93.18...
    Apr 17 21:44:52.411: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.163.242:9080/dial?request=hostname&protocol=udp&host=192.168.213.56&port=8081&tries=1'] Namespace:pod-network-test-1635 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:44:52.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:44:52.411: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:44:52.411: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1635/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.163.242%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.213.56%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Apr 17 21:44:52.470: INFO: Waiting for responses: map[]
    Apr 17 21:44:52.470: INFO: reached 192.168.213.56 after 0/1 tries
    Apr 17 21:44:52.470: INFO: Going to retry 0 out of 4 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:44:52.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1635" for this suite. 04/17/23 21:44:52.476
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:44:52.484
Apr 17 21:44:52.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 21:44:52.485
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:52.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:52.502
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 04/17/23 21:44:52.504
Apr 17 21:44:52.512: INFO: Waiting up to 5m0s for pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e" in namespace "emptydir-3792" to be "Succeeded or Failed"
Apr 17 21:44:52.518: INFO: Pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.497963ms
Apr 17 21:44:54.523: INFO: Pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010672564s
Apr 17 21:44:56.522: INFO: Pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010286401s
STEP: Saw pod success 04/17/23 21:44:56.522
Apr 17 21:44:56.522: INFO: Pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e" satisfied condition "Succeeded or Failed"
Apr 17 21:44:56.526: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e container test-container: <nil>
STEP: delete the pod 04/17/23 21:44:56.532
Apr 17 21:44:56.547: INFO: Waiting for pod pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e to disappear
Apr 17 21:44:56.550: INFO: Pod pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 21:44:56.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3792" for this suite. 04/17/23 21:44:56.555
------------------------------
• [4.078 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:44:52.484
    Apr 17 21:44:52.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 21:44:52.485
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:52.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:52.502
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 04/17/23 21:44:52.504
    Apr 17 21:44:52.512: INFO: Waiting up to 5m0s for pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e" in namespace "emptydir-3792" to be "Succeeded or Failed"
    Apr 17 21:44:52.518: INFO: Pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.497963ms
    Apr 17 21:44:54.523: INFO: Pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010672564s
    Apr 17 21:44:56.522: INFO: Pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010286401s
    STEP: Saw pod success 04/17/23 21:44:56.522
    Apr 17 21:44:56.522: INFO: Pod "pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e" satisfied condition "Succeeded or Failed"
    Apr 17 21:44:56.526: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e container test-container: <nil>
    STEP: delete the pod 04/17/23 21:44:56.532
    Apr 17 21:44:56.547: INFO: Waiting for pod pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e to disappear
    Apr 17 21:44:56.550: INFO: Pod pod-1c637910-3fc1-4ea8-ab5b-4c90b8ce962e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:44:56.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3792" for this suite. 04/17/23 21:44:56.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:44:56.563
Apr 17 21:44:56.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 21:44:56.564
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:56.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:56.581
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Apr 17 21:44:56.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/17/23 21:45:01.519
Apr 17 21:45:01.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 create -f -'
Apr 17 21:45:03.112: INFO: stderr: ""
Apr 17 21:45:03.112: INFO: stdout: "e2e-test-crd-publish-openapi-5642-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 17 21:45:03.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 delete e2e-test-crd-publish-openapi-5642-crds test-cr'
Apr 17 21:45:03.179: INFO: stderr: ""
Apr 17 21:45:03.179: INFO: stdout: "e2e-test-crd-publish-openapi-5642-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Apr 17 21:45:03.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 apply -f -'
Apr 17 21:45:04.609: INFO: stderr: ""
Apr 17 21:45:04.609: INFO: stdout: "e2e-test-crd-publish-openapi-5642-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Apr 17 21:45:04.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 delete e2e-test-crd-publish-openapi-5642-crds test-cr'
Apr 17 21:45:04.674: INFO: stderr: ""
Apr 17 21:45:04.674: INFO: stdout: "e2e-test-crd-publish-openapi-5642-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 04/17/23 21:45:04.674
Apr 17 21:45:04.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 explain e2e-test-crd-publish-openapi-5642-crds'
Apr 17 21:45:05.960: INFO: stderr: ""
Apr 17 21:45:05.960: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5642-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:45:10.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3090" for this suite. 04/17/23 21:45:10.008
------------------------------
• [SLOW TEST] [13.449 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:44:56.563
    Apr 17 21:44:56.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 21:44:56.564
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:44:56.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:44:56.581
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Apr 17 21:44:56.583: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/17/23 21:45:01.519
    Apr 17 21:45:01.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 create -f -'
    Apr 17 21:45:03.112: INFO: stderr: ""
    Apr 17 21:45:03.112: INFO: stdout: "e2e-test-crd-publish-openapi-5642-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Apr 17 21:45:03.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 delete e2e-test-crd-publish-openapi-5642-crds test-cr'
    Apr 17 21:45:03.179: INFO: stderr: ""
    Apr 17 21:45:03.179: INFO: stdout: "e2e-test-crd-publish-openapi-5642-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Apr 17 21:45:03.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 apply -f -'
    Apr 17 21:45:04.609: INFO: stderr: ""
    Apr 17 21:45:04.609: INFO: stdout: "e2e-test-crd-publish-openapi-5642-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Apr 17 21:45:04.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 --namespace=crd-publish-openapi-3090 delete e2e-test-crd-publish-openapi-5642-crds test-cr'
    Apr 17 21:45:04.674: INFO: stderr: ""
    Apr 17 21:45:04.674: INFO: stdout: "e2e-test-crd-publish-openapi-5642-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 04/17/23 21:45:04.674
    Apr 17 21:45:04.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3090 explain e2e-test-crd-publish-openapi-5642-crds'
    Apr 17 21:45:05.960: INFO: stderr: ""
    Apr 17 21:45:05.960: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5642-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:45:10.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3090" for this suite. 04/17/23 21:45:10.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:45:10.013
Apr 17 21:45:10.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 21:45:10.013
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:10.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:10.033
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Apr 17 21:45:10.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:45:10.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7803" for this suite. 04/17/23 21:45:10.571
------------------------------
• [0.565 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:45:10.013
    Apr 17 21:45:10.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 21:45:10.013
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:10.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:10.033
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Apr 17 21:45:10.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:45:10.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7803" for this suite. 04/17/23 21:45:10.571
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:45:10.578
Apr 17 21:45:10.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename runtimeclass 04/17/23 21:45:10.579
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:10.592
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:10.595
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 04/17/23 21:45:10.597
STEP: getting /apis/node.k8s.io 04/17/23 21:45:10.599
STEP: getting /apis/node.k8s.io/v1 04/17/23 21:45:10.601
STEP: creating 04/17/23 21:45:10.602
STEP: watching 04/17/23 21:45:10.614
Apr 17 21:45:10.614: INFO: starting watch
STEP: getting 04/17/23 21:45:10.618
STEP: listing 04/17/23 21:45:10.62
STEP: patching 04/17/23 21:45:10.622
STEP: updating 04/17/23 21:45:10.625
Apr 17 21:45:10.630: INFO: waiting for watch events with expected annotations
STEP: deleting 04/17/23 21:45:10.63
STEP: deleting a collection 04/17/23 21:45:10.638
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 17 21:45:10.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9708" for this suite. 04/17/23 21:45:10.652
------------------------------
• [0.079 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:45:10.578
    Apr 17 21:45:10.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename runtimeclass 04/17/23 21:45:10.579
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:10.592
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:10.595
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 04/17/23 21:45:10.597
    STEP: getting /apis/node.k8s.io 04/17/23 21:45:10.599
    STEP: getting /apis/node.k8s.io/v1 04/17/23 21:45:10.601
    STEP: creating 04/17/23 21:45:10.602
    STEP: watching 04/17/23 21:45:10.614
    Apr 17 21:45:10.614: INFO: starting watch
    STEP: getting 04/17/23 21:45:10.618
    STEP: listing 04/17/23 21:45:10.62
    STEP: patching 04/17/23 21:45:10.622
    STEP: updating 04/17/23 21:45:10.625
    Apr 17 21:45:10.630: INFO: waiting for watch events with expected annotations
    STEP: deleting 04/17/23 21:45:10.63
    STEP: deleting a collection 04/17/23 21:45:10.638
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:45:10.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9708" for this suite. 04/17/23 21:45:10.652
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:45:10.657
Apr 17 21:45:10.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replicaset 04/17/23 21:45:10.658
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:10.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:10.672
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 04/17/23 21:45:10.674
Apr 17 21:45:10.681: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1364" to be "running and ready"
Apr 17 21:45:10.683: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08056ms
Apr 17 21:45:10.683: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:45:12.686: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.005284695s
Apr 17 21:45:12.686: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Apr 17 21:45:12.686: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 04/17/23 21:45:12.688
STEP: Then the orphan pod is adopted 04/17/23 21:45:12.692
STEP: When the matched label of one of its pods change 04/17/23 21:45:13.698
Apr 17 21:45:13.700: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 04/17/23 21:45:13.709
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 17 21:45:14.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1364" for this suite. 04/17/23 21:45:14.718
------------------------------
• [4.065 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:45:10.657
    Apr 17 21:45:10.657: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replicaset 04/17/23 21:45:10.658
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:10.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:10.672
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 04/17/23 21:45:10.674
    Apr 17 21:45:10.681: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-1364" to be "running and ready"
    Apr 17 21:45:10.683: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 2.08056ms
    Apr 17 21:45:10.683: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:45:12.686: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.005284695s
    Apr 17 21:45:12.686: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Apr 17 21:45:12.686: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 04/17/23 21:45:12.688
    STEP: Then the orphan pod is adopted 04/17/23 21:45:12.692
    STEP: When the matched label of one of its pods change 04/17/23 21:45:13.698
    Apr 17 21:45:13.700: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 04/17/23 21:45:13.709
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:45:14.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1364" for this suite. 04/17/23 21:45:14.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:45:14.726
Apr 17 21:45:14.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 21:45:14.727
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:14.738
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:14.741
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2041 04/17/23 21:45:14.743
STEP: changing the ExternalName service to type=NodePort 04/17/23 21:45:14.747
STEP: creating replication controller externalname-service in namespace services-2041 04/17/23 21:45:14.769
I0417 21:45:14.774880      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2041, replica count: 2
I0417 21:45:17.825952      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 21:45:17.825: INFO: Creating new exec pod
Apr 17 21:45:17.833: INFO: Waiting up to 5m0s for pod "execpodx9knp" in namespace "services-2041" to be "running"
Apr 17 21:45:17.836: INFO: Pod "execpodx9knp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.93325ms
Apr 17 21:45:19.838: INFO: Pod "execpodx9knp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005897875s
Apr 17 21:45:19.839: INFO: Pod "execpodx9knp" satisfied condition "running"
Apr 17 21:45:20.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2041 exec execpodx9knp -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Apr 17 21:45:20.989: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 17 21:45:20.989: INFO: stdout: ""
Apr 17 21:45:20.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2041 exec execpodx9knp -- /bin/sh -x -c nc -v -z -w 2 10.96.230.150 80'
Apr 17 21:45:21.125: INFO: stderr: "+ nc -v -z -w 2 10.96.230.150 80\nConnection to 10.96.230.150 80 port [tcp/http] succeeded!\n"
Apr 17 21:45:21.125: INFO: stdout: ""
Apr 17 21:45:21.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2041 exec execpodx9knp -- /bin/sh -x -c nc -v -z -w 2 10.0.93.18 30373'
Apr 17 21:45:21.268: INFO: stderr: "+ nc -v -z -w 2 10.0.93.18 30373\nConnection to 10.0.93.18 30373 port [tcp/*] succeeded!\n"
Apr 17 21:45:21.268: INFO: stdout: ""
Apr 17 21:45:21.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2041 exec execpodx9knp -- /bin/sh -x -c nc -v -z -w 2 10.0.106.231 30373'
Apr 17 21:45:21.389: INFO: stderr: "+ nc -v -z -w 2 10.0.106.231 30373\nConnection to 10.0.106.231 30373 port [tcp/*] succeeded!\n"
Apr 17 21:45:21.389: INFO: stdout: ""
Apr 17 21:45:21.389: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 21:45:21.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2041" for this suite. 04/17/23 21:45:21.51
------------------------------
• [SLOW TEST] [6.789 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:45:14.726
    Apr 17 21:45:14.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 21:45:14.727
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:14.738
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:14.741
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2041 04/17/23 21:45:14.743
    STEP: changing the ExternalName service to type=NodePort 04/17/23 21:45:14.747
    STEP: creating replication controller externalname-service in namespace services-2041 04/17/23 21:45:14.769
    I0417 21:45:14.774880      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2041, replica count: 2
    I0417 21:45:17.825952      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 17 21:45:17.825: INFO: Creating new exec pod
    Apr 17 21:45:17.833: INFO: Waiting up to 5m0s for pod "execpodx9knp" in namespace "services-2041" to be "running"
    Apr 17 21:45:17.836: INFO: Pod "execpodx9knp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.93325ms
    Apr 17 21:45:19.838: INFO: Pod "execpodx9knp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005897875s
    Apr 17 21:45:19.839: INFO: Pod "execpodx9knp" satisfied condition "running"
    Apr 17 21:45:20.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2041 exec execpodx9knp -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Apr 17 21:45:20.989: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Apr 17 21:45:20.989: INFO: stdout: ""
    Apr 17 21:45:20.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2041 exec execpodx9knp -- /bin/sh -x -c nc -v -z -w 2 10.96.230.150 80'
    Apr 17 21:45:21.125: INFO: stderr: "+ nc -v -z -w 2 10.96.230.150 80\nConnection to 10.96.230.150 80 port [tcp/http] succeeded!\n"
    Apr 17 21:45:21.125: INFO: stdout: ""
    Apr 17 21:45:21.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2041 exec execpodx9knp -- /bin/sh -x -c nc -v -z -w 2 10.0.93.18 30373'
    Apr 17 21:45:21.268: INFO: stderr: "+ nc -v -z -w 2 10.0.93.18 30373\nConnection to 10.0.93.18 30373 port [tcp/*] succeeded!\n"
    Apr 17 21:45:21.268: INFO: stdout: ""
    Apr 17 21:45:21.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2041 exec execpodx9knp -- /bin/sh -x -c nc -v -z -w 2 10.0.106.231 30373'
    Apr 17 21:45:21.389: INFO: stderr: "+ nc -v -z -w 2 10.0.106.231 30373\nConnection to 10.0.106.231 30373 port [tcp/*] succeeded!\n"
    Apr 17 21:45:21.389: INFO: stdout: ""
    Apr 17 21:45:21.389: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:45:21.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2041" for this suite. 04/17/23 21:45:21.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:45:21.516
Apr 17 21:45:21.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename deployment 04/17/23 21:45:21.516
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:21.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:21.534
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 04/17/23 21:45:21.541
STEP: waiting for Deployment to be created 04/17/23 21:45:21.546
STEP: waiting for all Replicas to be Ready 04/17/23 21:45:21.547
Apr 17 21:45:21.548: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 17 21:45:21.548: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 17 21:45:21.559: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 17 21:45:21.559: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 17 21:45:21.571: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 17 21:45:21.571: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 17 21:45:21.593: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 17 21:45:21.593: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Apr 17 21:45:22.897: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 17 21:45:22.897: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Apr 17 21:45:23.074: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 04/17/23 21:45:23.074
W0417 21:45:23.084244      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr 17 21:45:23.085: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 04/17/23 21:45:23.085
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:23.097: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:23.097: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:23.112: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:23.112: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:23.124: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:23.124: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:23.136: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:23.136: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:24.085: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:24.085: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:24.101: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
STEP: listing Deployments 04/17/23 21:45:24.101
Apr 17 21:45:24.104: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 04/17/23 21:45:24.104
Apr 17 21:45:24.114: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 04/17/23 21:45:24.114
Apr 17 21:45:24.120: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 17 21:45:24.125: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 17 21:45:24.143: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 17 21:45:24.158: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 17 21:45:24.169: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Apr 17 21:45:25.122: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 17 21:45:25.186: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 17 21:45:25.200: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Apr 17 21:45:26.950: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 04/17/23 21:45:26.976
STEP: fetching the DeploymentStatus 04/17/23 21:45:26.981
Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 3
STEP: deleting the Deployment 04/17/23 21:45:26.985
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
Apr 17 21:45:26.992: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 17 21:45:26.994: INFO: Log out all the ReplicaSets if there is no deployment created
Apr 17 21:45:26.996: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-681  3389dc3b-1a29-42f2-a34b-0b1a278f43af 30268 2 2023-04-17 21:45:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 461ce0d8-189c-4d54-b9e9-4fac96e1c3f4 0xc0007c2b67 0xc0007c2b68}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"461ce0d8-189c-4d54-b9e9-4fac96e1c3f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:45:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c2bf0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Apr 17 21:45:27.001: INFO: pod: "test-deployment-7b7876f9d6-7g96z":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-7g96z test-deployment-7b7876f9d6- deployment-681  735d4c11-f1de-4e4e-b5df-7bee44d903f3 30189 0 2023-04-17 21:45:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:d78d828e6b4d57e75095339081baac650a87fe37f85826b23e64fca31c33d3c2 cni.projectcalico.org/podIP:192.168.163.247/32 cni.projectcalico.org/podIPs:192.168.163.247/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 3389dc3b-1a29-42f2-a34b-0b1a278f43af 0xc0007c3087 0xc0007c3088}] [] [{calico Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3389dc3b-1a29-42f2-a34b-0b1a278f43af\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8kb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8kb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.247,StartTime:2023-04-17 21:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:45:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3cb855ebf95210024ed9684100eaac27520c4acf9a86349a13e91c8674b3657d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 17 21:45:27.001: INFO: pod: "test-deployment-7b7876f9d6-zr6tl":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-zr6tl test-deployment-7b7876f9d6- deployment-681  d418d890-93c1-44f9-987b-53887eeda51a 30267 0 2023-04-17 21:45:25 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:af875cab2f9488eeb99313a9770dac3540ce28f5164df437064b46b46ba65d9d cni.projectcalico.org/podIP:192.168.213.60/32 cni.projectcalico.org/podIPs:192.168.213.60/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 3389dc3b-1a29-42f2-a34b-0b1a278f43af 0xc0007c3287 0xc0007c3288}] [] [{calico Update v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3389dc3b-1a29-42f2-a34b-0b1a278f43af\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:45:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5fsv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5fsv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.60,StartTime:2023-04-17 21:45:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:45:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f5e16460e8694cf923e0b4acd704d10243da9eea10a81ad7db32e72aca1e2ce5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 17 21:45:27.001: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-681  2101662c-2512-4da2-b605-18f07880eecf 30277 4 2023-04-17 21:45:23 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 461ce0d8-189c-4d54-b9e9-4fac96e1c3f4 0xc0007c2c57 0xc0007c2c58}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"461ce0d8-189c-4d54-b9e9-4fac96e1c3f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:45:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c2ce0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Apr 17 21:45:27.004: INFO: pod: "test-deployment-7df74c55ff-gq7fm":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-gq7fm test-deployment-7df74c55ff- deployment-681  21506a7e-553f-4285-b4b9-f41d0eac633f 30233 0 2023-04-17 21:45:24 +0000 UTC 2023-04-17 21:45:26 +0000 UTC 0xc004d4c6f8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:7e55c4d89634e7f55c324fae35e09c6ed1f81a6e6359c115267354ba982e4796 cni.projectcalico.org/podIP:192.168.208.189/32 cni.projectcalico.org/podIPs:192.168.208.189/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 2101662c-2512-4da2-b605-18f07880eecf 0xc004d4c727 0xc004d4c728}] [] [{calico Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2101662c-2512-4da2-b605-18f07880eecf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkdp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkdp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.189,StartTime:2023-04-17 21:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:45:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://98a77f48ac14586f1f608c81e8cf5ce613606cb9fe7ea3b62db31a0f34bd9788,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 17 21:45:27.004: INFO: pod: "test-deployment-7df74c55ff-qqrhk":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-qqrhk test-deployment-7df74c55ff- deployment-681  f1c6b703-83d0-41c0-bdf9-92e50054fa27 30271 0 2023-04-17 21:45:23 +0000 UTC 2023-04-17 21:45:27 +0000 UTC 0xc004d4c900 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:91be3dd7c2d6b5520bf604453484b5db984e5507657456827dc1e949d99df8cf cni.projectcalico.org/podIP:192.168.163.246/32 cni.projectcalico.org/podIPs:192.168.163.246/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 2101662c-2512-4da2-b605-18f07880eecf 0xc004d4c937 0xc004d4c938}] [] [{calico Update v1 2023-04-17 21:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2101662c-2512-4da2-b605-18f07880eecf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t69dc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t69dc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.246,StartTime:2023-04-17 21:45:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:45:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://911149e31dd971deaa1cfd2a89d01a84c856dfc1529c6aea58c3438c338348f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Apr 17 21:45:27.004: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-681  2a4aa507-e663-444c-aecc-ff5839062d2c 30141 3 2023-04-17 21:45:21 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 461ce0d8-189c-4d54-b9e9-4fac96e1c3f4 0xc0007c2d47 0xc0007c2d48}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"461ce0d8-189c-4d54-b9e9-4fac96e1c3f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c2dd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 17 21:45:27.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-681" for this suite. 04/17/23 21:45:27.015
------------------------------
• [SLOW TEST] [5.503 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:45:21.516
    Apr 17 21:45:21.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename deployment 04/17/23 21:45:21.516
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:21.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:21.534
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 04/17/23 21:45:21.541
    STEP: waiting for Deployment to be created 04/17/23 21:45:21.546
    STEP: waiting for all Replicas to be Ready 04/17/23 21:45:21.547
    Apr 17 21:45:21.548: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 17 21:45:21.548: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 17 21:45:21.559: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 17 21:45:21.559: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 17 21:45:21.571: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 17 21:45:21.571: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 17 21:45:21.593: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 17 21:45:21.593: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Apr 17 21:45:22.897: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Apr 17 21:45:22.897: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Apr 17 21:45:23.074: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 04/17/23 21:45:23.074
    W0417 21:45:23.084244      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr 17 21:45:23.085: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 04/17/23 21:45:23.085
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 0
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:23.086: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:23.097: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:23.097: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:23.112: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:23.112: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:23.124: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:23.124: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:23.136: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:23.136: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:24.085: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:24.085: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:24.101: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    STEP: listing Deployments 04/17/23 21:45:24.101
    Apr 17 21:45:24.104: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 04/17/23 21:45:24.104
    Apr 17 21:45:24.114: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 04/17/23 21:45:24.114
    Apr 17 21:45:24.120: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 17 21:45:24.125: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 17 21:45:24.143: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 17 21:45:24.158: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 17 21:45:24.169: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 17 21:45:25.122: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 17 21:45:25.186: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 17 21:45:25.200: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Apr 17 21:45:26.950: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 04/17/23 21:45:26.976
    STEP: fetching the DeploymentStatus 04/17/23 21:45:26.981
    Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 1
    Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 2
    Apr 17 21:45:26.985: INFO: observed Deployment test-deployment in namespace deployment-681 with ReadyReplicas 3
    STEP: deleting the Deployment 04/17/23 21:45:26.985
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    Apr 17 21:45:26.992: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 17 21:45:26.994: INFO: Log out all the ReplicaSets if there is no deployment created
    Apr 17 21:45:26.996: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-681  3389dc3b-1a29-42f2-a34b-0b1a278f43af 30268 2 2023-04-17 21:45:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 461ce0d8-189c-4d54-b9e9-4fac96e1c3f4 0xc0007c2b67 0xc0007c2b68}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"461ce0d8-189c-4d54-b9e9-4fac96e1c3f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:45:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c2bf0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Apr 17 21:45:27.001: INFO: pod: "test-deployment-7b7876f9d6-7g96z":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-7g96z test-deployment-7b7876f9d6- deployment-681  735d4c11-f1de-4e4e-b5df-7bee44d903f3 30189 0 2023-04-17 21:45:24 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:d78d828e6b4d57e75095339081baac650a87fe37f85826b23e64fca31c33d3c2 cni.projectcalico.org/podIP:192.168.163.247/32 cni.projectcalico.org/podIPs:192.168.163.247/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 3389dc3b-1a29-42f2-a34b-0b1a278f43af 0xc0007c3087 0xc0007c3088}] [] [{calico Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3389dc3b-1a29-42f2-a34b-0b1a278f43af\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.247\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8kb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8kb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.247,StartTime:2023-04-17 21:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:45:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3cb855ebf95210024ed9684100eaac27520c4acf9a86349a13e91c8674b3657d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.247,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 17 21:45:27.001: INFO: pod: "test-deployment-7b7876f9d6-zr6tl":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-zr6tl test-deployment-7b7876f9d6- deployment-681  d418d890-93c1-44f9-987b-53887eeda51a 30267 0 2023-04-17 21:45:25 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:af875cab2f9488eeb99313a9770dac3540ce28f5164df437064b46b46ba65d9d cni.projectcalico.org/podIP:192.168.213.60/32 cni.projectcalico.org/podIPs:192.168.213.60/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 3389dc3b-1a29-42f2-a34b-0b1a278f43af 0xc0007c3287 0xc0007c3288}] [] [{calico Update v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3389dc3b-1a29-42f2-a34b-0b1a278f43af\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:45:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.60\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5fsv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5fsv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.60,StartTime:2023-04-17 21:45:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:45:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f5e16460e8694cf923e0b4acd704d10243da9eea10a81ad7db32e72aca1e2ce5,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.60,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 17 21:45:27.001: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-681  2101662c-2512-4da2-b605-18f07880eecf 30277 4 2023-04-17 21:45:23 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 461ce0d8-189c-4d54-b9e9-4fac96e1c3f4 0xc0007c2c57 0xc0007c2c58}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:45:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"461ce0d8-189c-4d54-b9e9-4fac96e1c3f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:45:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c2ce0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Apr 17 21:45:27.004: INFO: pod: "test-deployment-7df74c55ff-gq7fm":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-gq7fm test-deployment-7df74c55ff- deployment-681  21506a7e-553f-4285-b4b9-f41d0eac633f 30233 0 2023-04-17 21:45:24 +0000 UTC 2023-04-17 21:45:26 +0000 UTC 0xc004d4c6f8 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:7e55c4d89634e7f55c324fae35e09c6ed1f81a6e6359c115267354ba982e4796 cni.projectcalico.org/podIP:192.168.208.189/32 cni.projectcalico.org/podIPs:192.168.208.189/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 2101662c-2512-4da2-b605-18f07880eecf 0xc004d4c727 0xc004d4c728}] [] [{calico Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2101662c-2512-4da2-b605-18f07880eecf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:45:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zkdp8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zkdp8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.189,StartTime:2023-04-17 21:45:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:45:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://98a77f48ac14586f1f608c81e8cf5ce613606cb9fe7ea3b62db31a0f34bd9788,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.189,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 17 21:45:27.004: INFO: pod: "test-deployment-7df74c55ff-qqrhk":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-qqrhk test-deployment-7df74c55ff- deployment-681  f1c6b703-83d0-41c0-bdf9-92e50054fa27 30271 0 2023-04-17 21:45:23 +0000 UTC 2023-04-17 21:45:27 +0000 UTC 0xc004d4c900 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:91be3dd7c2d6b5520bf604453484b5db984e5507657456827dc1e949d99df8cf cni.projectcalico.org/podIP:192.168.163.246/32 cni.projectcalico.org/podIPs:192.168.163.246/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 2101662c-2512-4da2-b605-18f07880eecf 0xc004d4c937 0xc004d4c938}] [] [{calico Update v1 2023-04-17 21:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 21:45:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2101662c-2512-4da2-b605-18f07880eecf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.246\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t69dc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t69dc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 21:45:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.246,StartTime:2023-04-17 21:45:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 21:45:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://911149e31dd971deaa1cfd2a89d01a84c856dfc1529c6aea58c3438c338348f3,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.246,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Apr 17 21:45:27.004: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-681  2a4aa507-e663-444c-aecc-ff5839062d2c 30141 3 2023-04-17 21:45:21 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 461ce0d8-189c-4d54-b9e9-4fac96e1c3f4 0xc0007c2d47 0xc0007c2d48}] [] [{kube-controller-manager Update apps/v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"461ce0d8-189c-4d54-b9e9-4fac96e1c3f4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 21:45:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c2dd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:45:27.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-681" for this suite. 04/17/23 21:45:27.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:45:27.019
Apr 17 21:45:27.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 21:45:27.02
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:27.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:27.035
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-08ded446-a644-4f41-a459-cb5977d28904 04/17/23 21:45:27.041
STEP: Creating the pod 04/17/23 21:45:27.044
Apr 17 21:45:27.051: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9" in namespace "projected-6335" to be "running and ready"
Apr 17 21:45:27.053: INFO: Pod "pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242267ms
Apr 17 21:45:27.053: INFO: The phase of Pod pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:45:29.056: INFO: Pod "pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005410102s
Apr 17 21:45:29.056: INFO: The phase of Pod pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9 is Running (Ready = true)
Apr 17 21:45:29.056: INFO: Pod "pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-08ded446-a644-4f41-a459-cb5977d28904 04/17/23 21:45:29.069
STEP: waiting to observe update in volume 04/17/23 21:45:29.074
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 17 21:46:37.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6335" for this suite. 04/17/23 21:46:37.292
------------------------------
• [SLOW TEST] [70.276 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:45:27.019
    Apr 17 21:45:27.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 21:45:27.02
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:45:27.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:45:27.035
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-08ded446-a644-4f41-a459-cb5977d28904 04/17/23 21:45:27.041
    STEP: Creating the pod 04/17/23 21:45:27.044
    Apr 17 21:45:27.051: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9" in namespace "projected-6335" to be "running and ready"
    Apr 17 21:45:27.053: INFO: Pod "pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242267ms
    Apr 17 21:45:27.053: INFO: The phase of Pod pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:45:29.056: INFO: Pod "pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005410102s
    Apr 17 21:45:29.056: INFO: The phase of Pod pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9 is Running (Ready = true)
    Apr 17 21:45:29.056: INFO: Pod "pod-projected-configmaps-33adb7ad-7c97-4f05-90a5-42ec54faecb9" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-08ded446-a644-4f41-a459-cb5977d28904 04/17/23 21:45:29.069
    STEP: waiting to observe update in volume 04/17/23 21:45:29.074
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:46:37.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6335" for this suite. 04/17/23 21:46:37.292
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:46:37.296
Apr 17 21:46:37.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename runtimeclass 04/17/23 21:46:37.297
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:46:37.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:46:37.313
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Apr 17 21:46:37.326: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8362 to be scheduled
Apr 17 21:46:37.328: INFO: 1 pods are not scheduled: [runtimeclass-8362/test-runtimeclass-runtimeclass-8362-preconfigured-handler-thxqx(1e7d06b6-be91-45e7-bb8f-2a49d0455f26)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 17 21:46:39.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-8362" for this suite. 04/17/23 21:46:39.339
------------------------------
• [2.048 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:46:37.296
    Apr 17 21:46:37.296: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename runtimeclass 04/17/23 21:46:37.297
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:46:37.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:46:37.313
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Apr 17 21:46:37.326: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-8362 to be scheduled
    Apr 17 21:46:37.328: INFO: 1 pods are not scheduled: [runtimeclass-8362/test-runtimeclass-runtimeclass-8362-preconfigured-handler-thxqx(1e7d06b6-be91-45e7-bb8f-2a49d0455f26)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:46:39.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-8362" for this suite. 04/17/23 21:46:39.339
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:46:39.344
Apr 17 21:46:39.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 21:46:39.345
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:46:39.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:46:39.36
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 04/17/23 21:46:39.362
STEP: Creating a ResourceQuota 04/17/23 21:46:44.366
STEP: Ensuring resource quota status is calculated 04/17/23 21:46:44.371
STEP: Creating a Pod that fits quota 04/17/23 21:46:46.375
STEP: Ensuring ResourceQuota status captures the pod usage 04/17/23 21:46:46.389
STEP: Not allowing a pod to be created that exceeds remaining quota 04/17/23 21:46:48.393
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 04/17/23 21:46:48.395
STEP: Ensuring a pod cannot update its resource requirements 04/17/23 21:46:48.397
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 04/17/23 21:46:48.4
STEP: Deleting the pod 04/17/23 21:46:50.404
STEP: Ensuring resource quota status released the pod usage 04/17/23 21:46:50.415
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 21:46:52.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1451" for this suite. 04/17/23 21:46:52.426
------------------------------
• [SLOW TEST] [13.088 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:46:39.344
    Apr 17 21:46:39.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 21:46:39.345
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:46:39.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:46:39.36
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 04/17/23 21:46:39.362
    STEP: Creating a ResourceQuota 04/17/23 21:46:44.366
    STEP: Ensuring resource quota status is calculated 04/17/23 21:46:44.371
    STEP: Creating a Pod that fits quota 04/17/23 21:46:46.375
    STEP: Ensuring ResourceQuota status captures the pod usage 04/17/23 21:46:46.389
    STEP: Not allowing a pod to be created that exceeds remaining quota 04/17/23 21:46:48.393
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 04/17/23 21:46:48.395
    STEP: Ensuring a pod cannot update its resource requirements 04/17/23 21:46:48.397
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 04/17/23 21:46:48.4
    STEP: Deleting the pod 04/17/23 21:46:50.404
    STEP: Ensuring resource quota status released the pod usage 04/17/23 21:46:50.415
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:46:52.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1451" for this suite. 04/17/23 21:46:52.426
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:46:52.432
Apr 17 21:46:52.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 21:46:52.433
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:46:52.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:46:52.448
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 21:46:52.459
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:46:52.791
STEP: Deploying the webhook pod 04/17/23 21:46:52.796
STEP: Wait for the deployment to be ready 04/17/23 21:46:52.808
Apr 17 21:46:52.814: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 21:46:54.823
STEP: Verifying the service has paired with the endpoint 04/17/23 21:46:54.833
Apr 17 21:46:55.833: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 04/17/23 21:46:55.836
STEP: create a pod that should be updated by the webhook 04/17/23 21:46:55.848
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:46:55.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2640" for this suite. 04/17/23 21:46:55.909
STEP: Destroying namespace "webhook-2640-markers" for this suite. 04/17/23 21:46:55.914
------------------------------
• [3.491 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:46:52.432
    Apr 17 21:46:52.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 21:46:52.433
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:46:52.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:46:52.448
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 21:46:52.459
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:46:52.791
    STEP: Deploying the webhook pod 04/17/23 21:46:52.796
    STEP: Wait for the deployment to be ready 04/17/23 21:46:52.808
    Apr 17 21:46:52.814: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 21:46:54.823
    STEP: Verifying the service has paired with the endpoint 04/17/23 21:46:54.833
    Apr 17 21:46:55.833: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 04/17/23 21:46:55.836
    STEP: create a pod that should be updated by the webhook 04/17/23 21:46:55.848
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:46:55.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2640" for this suite. 04/17/23 21:46:55.909
    STEP: Destroying namespace "webhook-2640-markers" for this suite. 04/17/23 21:46:55.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:46:55.925
Apr 17 21:46:55.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-preemption 04/17/23 21:46:55.925
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:46:55.94
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:46:55.943
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 17 21:46:55.955: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 17 21:47:56.003: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 04/17/23 21:47:56.005
Apr 17 21:47:56.029: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr 17 21:47:56.036: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr 17 21:47:56.058: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr 17 21:47:56.068: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Apr 17 21:47:56.091: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Apr 17 21:47:56.172: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Apr 17 21:47:56.199: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Apr 17 21:47:56.206: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 04/17/23 21:47:56.206
Apr 17 21:47:56.207: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7864" to be "running"
Apr 17 21:47:56.212: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.186535ms
Apr 17 21:47:58.215: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008806232s
Apr 17 21:47:58.215: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Apr 17 21:47:58.215: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
Apr 17 21:47:58.218: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.156608ms
Apr 17 21:47:58.218: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 21:47:58.218: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
Apr 17 21:47:58.220: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.129839ms
Apr 17 21:47:58.220: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 21:47:58.220: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
Apr 17 21:47:58.222: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.200721ms
Apr 17 21:47:58.222: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 21:47:58.222: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
Apr 17 21:47:58.224: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.93685ms
Apr 17 21:47:58.224: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 21:47:58.224: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
Apr 17 21:47:58.226: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.997553ms
Apr 17 21:47:58.226: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 21:47:58.226: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
Apr 17 21:47:58.228: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.226933ms
Apr 17 21:47:58.228: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 21:47:58.228: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
Apr 17 21:47:58.230: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.077851ms
Apr 17 21:47:58.230: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 04/17/23 21:47:58.23
Apr 17 21:47:58.234: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7864" to be "running"
Apr 17 21:47:58.237: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.460781ms
Apr 17 21:48:00.240: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005927614s
Apr 17 21:48:02.240: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005511604s
Apr 17 21:48:04.241: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006394048s
Apr 17 21:48:04.241: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:04.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7864" for this suite. 04/17/23 21:48:04.309
------------------------------
• [SLOW TEST] [68.389 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:46:55.925
    Apr 17 21:46:55.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-preemption 04/17/23 21:46:55.925
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:46:55.94
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:46:55.943
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 17 21:46:55.955: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 17 21:47:56.003: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 04/17/23 21:47:56.005
    Apr 17 21:47:56.029: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Apr 17 21:47:56.036: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Apr 17 21:47:56.058: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Apr 17 21:47:56.068: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Apr 17 21:47:56.091: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Apr 17 21:47:56.172: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Apr 17 21:47:56.199: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Apr 17 21:47:56.206: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 04/17/23 21:47:56.206
    Apr 17 21:47:56.207: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7864" to be "running"
    Apr 17 21:47:56.212: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 5.186535ms
    Apr 17 21:47:58.215: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008806232s
    Apr 17 21:47:58.215: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Apr 17 21:47:58.215: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
    Apr 17 21:47:58.218: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.156608ms
    Apr 17 21:47:58.218: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 21:47:58.218: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
    Apr 17 21:47:58.220: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.129839ms
    Apr 17 21:47:58.220: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 21:47:58.220: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
    Apr 17 21:47:58.222: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.200721ms
    Apr 17 21:47:58.222: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 21:47:58.222: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
    Apr 17 21:47:58.224: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.93685ms
    Apr 17 21:47:58.224: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 21:47:58.224: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
    Apr 17 21:47:58.226: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.997553ms
    Apr 17 21:47:58.226: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 21:47:58.226: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
    Apr 17 21:47:58.228: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.226933ms
    Apr 17 21:47:58.228: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 21:47:58.228: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-7864" to be "running"
    Apr 17 21:47:58.230: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.077851ms
    Apr 17 21:47:58.230: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 04/17/23 21:47:58.23
    Apr 17 21:47:58.234: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7864" to be "running"
    Apr 17 21:47:58.237: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.460781ms
    Apr 17 21:48:00.240: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005927614s
    Apr 17 21:48:02.240: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.005511604s
    Apr 17 21:48:04.241: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.006394048s
    Apr 17 21:48:04.241: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:04.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7864" for this suite. 04/17/23 21:48:04.309
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:04.314
Apr 17 21:48:04.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 21:48:04.315
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:04.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:04.329
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 04/17/23 21:48:04.331
Apr 17 21:48:04.337: INFO: Waiting up to 5m0s for pod "pod-933a04e3-9933-433d-a86a-c59552b447c8" in namespace "emptydir-2261" to be "Succeeded or Failed"
Apr 17 21:48:04.340: INFO: Pod "pod-933a04e3-9933-433d-a86a-c59552b447c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291385ms
Apr 17 21:48:06.344: INFO: Pod "pod-933a04e3-9933-433d-a86a-c59552b447c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006093056s
Apr 17 21:48:08.343: INFO: Pod "pod-933a04e3-9933-433d-a86a-c59552b447c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00596211s
STEP: Saw pod success 04/17/23 21:48:08.343
Apr 17 21:48:08.344: INFO: Pod "pod-933a04e3-9933-433d-a86a-c59552b447c8" satisfied condition "Succeeded or Failed"
Apr 17 21:48:08.346: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-933a04e3-9933-433d-a86a-c59552b447c8 container test-container: <nil>
STEP: delete the pod 04/17/23 21:48:08.358
Apr 17 21:48:08.369: INFO: Waiting for pod pod-933a04e3-9933-433d-a86a-c59552b447c8 to disappear
Apr 17 21:48:08.371: INFO: Pod pod-933a04e3-9933-433d-a86a-c59552b447c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:08.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2261" for this suite. 04/17/23 21:48:08.375
------------------------------
• [4.064 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:04.314
    Apr 17 21:48:04.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 21:48:04.315
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:04.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:04.329
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 04/17/23 21:48:04.331
    Apr 17 21:48:04.337: INFO: Waiting up to 5m0s for pod "pod-933a04e3-9933-433d-a86a-c59552b447c8" in namespace "emptydir-2261" to be "Succeeded or Failed"
    Apr 17 21:48:04.340: INFO: Pod "pod-933a04e3-9933-433d-a86a-c59552b447c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.291385ms
    Apr 17 21:48:06.344: INFO: Pod "pod-933a04e3-9933-433d-a86a-c59552b447c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006093056s
    Apr 17 21:48:08.343: INFO: Pod "pod-933a04e3-9933-433d-a86a-c59552b447c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00596211s
    STEP: Saw pod success 04/17/23 21:48:08.343
    Apr 17 21:48:08.344: INFO: Pod "pod-933a04e3-9933-433d-a86a-c59552b447c8" satisfied condition "Succeeded or Failed"
    Apr 17 21:48:08.346: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-933a04e3-9933-433d-a86a-c59552b447c8 container test-container: <nil>
    STEP: delete the pod 04/17/23 21:48:08.358
    Apr 17 21:48:08.369: INFO: Waiting for pod pod-933a04e3-9933-433d-a86a-c59552b447c8 to disappear
    Apr 17 21:48:08.371: INFO: Pod pod-933a04e3-9933-433d-a86a-c59552b447c8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:08.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2261" for this suite. 04/17/23 21:48:08.375
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:08.38
Apr 17 21:48:08.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 21:48:08.38
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:08.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:08.396
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 04/17/23 21:48:08.398
Apr 17 21:48:08.403: INFO: Waiting up to 5m0s for pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7" in namespace "downward-api-6247" to be "Succeeded or Failed"
Apr 17 21:48:08.406: INFO: Pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230761ms
Apr 17 21:48:10.410: INFO: Pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006644875s
Apr 17 21:48:12.410: INFO: Pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00666792s
STEP: Saw pod success 04/17/23 21:48:12.41
Apr 17 21:48:12.410: INFO: Pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7" satisfied condition "Succeeded or Failed"
Apr 17 21:48:12.413: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7 container dapi-container: <nil>
STEP: delete the pod 04/17/23 21:48:12.422
Apr 17 21:48:12.433: INFO: Waiting for pod downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7 to disappear
Apr 17 21:48:12.435: INFO: Pod downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:12.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6247" for this suite. 04/17/23 21:48:12.439
------------------------------
• [4.064 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:08.38
    Apr 17 21:48:08.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 21:48:08.38
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:08.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:08.396
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 04/17/23 21:48:08.398
    Apr 17 21:48:08.403: INFO: Waiting up to 5m0s for pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7" in namespace "downward-api-6247" to be "Succeeded or Failed"
    Apr 17 21:48:08.406: INFO: Pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.230761ms
    Apr 17 21:48:10.410: INFO: Pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006644875s
    Apr 17 21:48:12.410: INFO: Pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00666792s
    STEP: Saw pod success 04/17/23 21:48:12.41
    Apr 17 21:48:12.410: INFO: Pod "downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7" satisfied condition "Succeeded or Failed"
    Apr 17 21:48:12.413: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7 container dapi-container: <nil>
    STEP: delete the pod 04/17/23 21:48:12.422
    Apr 17 21:48:12.433: INFO: Waiting for pod downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7 to disappear
    Apr 17 21:48:12.435: INFO: Pod downward-api-4ebc0edf-5a5f-41fb-ada3-ddd55afca3b7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:12.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6247" for this suite. 04/17/23 21:48:12.439
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:12.444
Apr 17 21:48:12.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 21:48:12.445
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:12.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:12.459
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 21:48:12.471
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:48:12.768
STEP: Deploying the webhook pod 04/17/23 21:48:12.774
STEP: Wait for the deployment to be ready 04/17/23 21:48:12.784
Apr 17 21:48:12.790: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 04/17/23 21:48:14.798
STEP: Verifying the service has paired with the endpoint 04/17/23 21:48:14.807
Apr 17 21:48:15.808: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Apr 17 21:48:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Registering the custom resource webhook via the AdmissionRegistration API 04/17/23 21:48:16.318
STEP: Creating a custom resource that should be denied by the webhook 04/17/23 21:48:16.331
STEP: Creating a custom resource whose deletion would be denied by the webhook 04/17/23 21:48:18.357
STEP: Updating the custom resource with disallowed data should be denied 04/17/23 21:48:18.362
STEP: Deleting the custom resource should be denied 04/17/23 21:48:18.368
STEP: Remove the offending key and value from the custom resource data 04/17/23 21:48:18.372
STEP: Deleting the updated custom resource should be successful 04/17/23 21:48:18.379
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:18.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2989" for this suite. 04/17/23 21:48:18.976
STEP: Destroying namespace "webhook-2989-markers" for this suite. 04/17/23 21:48:18.982
------------------------------
• [SLOW TEST] [6.543 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:12.444
    Apr 17 21:48:12.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 21:48:12.445
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:12.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:12.459
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 21:48:12.471
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:48:12.768
    STEP: Deploying the webhook pod 04/17/23 21:48:12.774
    STEP: Wait for the deployment to be ready 04/17/23 21:48:12.784
    Apr 17 21:48:12.790: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 04/17/23 21:48:14.798
    STEP: Verifying the service has paired with the endpoint 04/17/23 21:48:14.807
    Apr 17 21:48:15.808: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Apr 17 21:48:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 04/17/23 21:48:16.318
    STEP: Creating a custom resource that should be denied by the webhook 04/17/23 21:48:16.331
    STEP: Creating a custom resource whose deletion would be denied by the webhook 04/17/23 21:48:18.357
    STEP: Updating the custom resource with disallowed data should be denied 04/17/23 21:48:18.362
    STEP: Deleting the custom resource should be denied 04/17/23 21:48:18.368
    STEP: Remove the offending key and value from the custom resource data 04/17/23 21:48:18.372
    STEP: Deleting the updated custom resource should be successful 04/17/23 21:48:18.379
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:18.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2989" for this suite. 04/17/23 21:48:18.976
    STEP: Destroying namespace "webhook-2989-markers" for this suite. 04/17/23 21:48:18.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:18.987
Apr 17 21:48:18.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 21:48:18.988
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:19.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:19.019
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 04/17/23 21:48:19.021
Apr 17 21:48:19.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452" in namespace "downward-api-9478" to be "Succeeded or Failed"
Apr 17 21:48:19.033: INFO: Pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452": Phase="Pending", Reason="", readiness=false. Elapsed: 3.623176ms
Apr 17 21:48:21.036: INFO: Pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006597774s
Apr 17 21:48:23.036: INFO: Pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006870771s
STEP: Saw pod success 04/17/23 21:48:23.036
Apr 17 21:48:23.036: INFO: Pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452" satisfied condition "Succeeded or Failed"
Apr 17 21:48:23.038: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452 container client-container: <nil>
STEP: delete the pod 04/17/23 21:48:23.048
Apr 17 21:48:23.059: INFO: Waiting for pod downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452 to disappear
Apr 17 21:48:23.061: INFO: Pod downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:23.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9478" for this suite. 04/17/23 21:48:23.065
------------------------------
• [4.082 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:18.987
    Apr 17 21:48:18.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 21:48:18.988
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:19.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:19.019
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 04/17/23 21:48:19.021
    Apr 17 21:48:19.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452" in namespace "downward-api-9478" to be "Succeeded or Failed"
    Apr 17 21:48:19.033: INFO: Pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452": Phase="Pending", Reason="", readiness=false. Elapsed: 3.623176ms
    Apr 17 21:48:21.036: INFO: Pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006597774s
    Apr 17 21:48:23.036: INFO: Pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006870771s
    STEP: Saw pod success 04/17/23 21:48:23.036
    Apr 17 21:48:23.036: INFO: Pod "downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452" satisfied condition "Succeeded or Failed"
    Apr 17 21:48:23.038: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452 container client-container: <nil>
    STEP: delete the pod 04/17/23 21:48:23.048
    Apr 17 21:48:23.059: INFO: Waiting for pod downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452 to disappear
    Apr 17 21:48:23.061: INFO: Pod downwardapi-volume-9b6e8398-be3a-4bcb-84ab-bc587940c452 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:23.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9478" for this suite. 04/17/23 21:48:23.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:23.07
Apr 17 21:48:23.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replicaset 04/17/23 21:48:23.071
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:23.084
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:23.086
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Apr 17 21:48:23.088: INFO: Creating ReplicaSet my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f
Apr 17 21:48:23.094: INFO: Pod name my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f: Found 0 pods out of 1
Apr 17 21:48:28.097: INFO: Pod name my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f: Found 1 pods out of 1
Apr 17 21:48:28.097: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f" is running
Apr 17 21:48:28.097: INFO: Waiting up to 5m0s for pod "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs" in namespace "replicaset-369" to be "running"
Apr 17 21:48:28.099: INFO: Pod "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs": Phase="Running", Reason="", readiness=true. Elapsed: 2.451289ms
Apr 17 21:48:28.099: INFO: Pod "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs" satisfied condition "running"
Apr 17 21:48:28.099: INFO: Pod "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:48:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:48:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:48:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:48:23 +0000 UTC Reason: Message:}])
Apr 17 21:48:28.099: INFO: Trying to dial the pod
Apr 17 21:48:33.109: INFO: Controller my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f: Got expected result from replica 1 [my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs]: "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:33.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-369" for this suite. 04/17/23 21:48:33.113
------------------------------
• [SLOW TEST] [10.048 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:23.07
    Apr 17 21:48:23.070: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replicaset 04/17/23 21:48:23.071
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:23.084
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:23.086
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Apr 17 21:48:23.088: INFO: Creating ReplicaSet my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f
    Apr 17 21:48:23.094: INFO: Pod name my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f: Found 0 pods out of 1
    Apr 17 21:48:28.097: INFO: Pod name my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f: Found 1 pods out of 1
    Apr 17 21:48:28.097: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f" is running
    Apr 17 21:48:28.097: INFO: Waiting up to 5m0s for pod "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs" in namespace "replicaset-369" to be "running"
    Apr 17 21:48:28.099: INFO: Pod "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs": Phase="Running", Reason="", readiness=true. Elapsed: 2.451289ms
    Apr 17 21:48:28.099: INFO: Pod "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs" satisfied condition "running"
    Apr 17 21:48:28.099: INFO: Pod "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:48:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:48:24 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:48:24 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-04-17 21:48:23 +0000 UTC Reason: Message:}])
    Apr 17 21:48:28.099: INFO: Trying to dial the pod
    Apr 17 21:48:33.109: INFO: Controller my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f: Got expected result from replica 1 [my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs]: "my-hostname-basic-47a220ba-f7ba-4c46-a4c6-fe7fa096b73f-twxgs", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:33.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-369" for this suite. 04/17/23 21:48:33.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:33.119
Apr 17 21:48:33.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 21:48:33.12
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:33.143
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:33.145
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 04/17/23 21:48:33.147
Apr 17 21:48:33.154: INFO: Waiting up to 5m0s for pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1" in namespace "projected-9195" to be "running and ready"
Apr 17 21:48:33.157: INFO: Pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.686182ms
Apr 17 21:48:33.157: INFO: The phase of Pod annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:48:35.160: INFO: Pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00615102s
Apr 17 21:48:35.160: INFO: The phase of Pod annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1 is Running (Ready = true)
Apr 17 21:48:35.160: INFO: Pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1" satisfied condition "running and ready"
Apr 17 21:48:35.677: INFO: Successfully updated pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:39.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9195" for this suite. 04/17/23 21:48:39.71
------------------------------
• [SLOW TEST] [6.595 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:33.119
    Apr 17 21:48:33.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 21:48:33.12
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:33.143
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:33.145
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 04/17/23 21:48:33.147
    Apr 17 21:48:33.154: INFO: Waiting up to 5m0s for pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1" in namespace "projected-9195" to be "running and ready"
    Apr 17 21:48:33.157: INFO: Pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.686182ms
    Apr 17 21:48:33.157: INFO: The phase of Pod annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:48:35.160: INFO: Pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00615102s
    Apr 17 21:48:35.160: INFO: The phase of Pod annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1 is Running (Ready = true)
    Apr 17 21:48:35.160: INFO: Pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1" satisfied condition "running and ready"
    Apr 17 21:48:35.677: INFO: Successfully updated pod "annotationupdate001b36b2-3151-4e2e-9a8b-98a9b0364fd1"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:39.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9195" for this suite. 04/17/23 21:48:39.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:39.715
Apr 17 21:48:39.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 21:48:39.716
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:39.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:39.731
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-cfbd7331-8187-4372-a168-cb41e4f2f74b 04/17/23 21:48:39.733
STEP: Creating a pod to test consume secrets 04/17/23 21:48:39.737
Apr 17 21:48:39.744: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5" in namespace "projected-2373" to be "Succeeded or Failed"
Apr 17 21:48:39.746: INFO: Pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289819ms
Apr 17 21:48:41.750: INFO: Pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005948612s
Apr 17 21:48:43.750: INFO: Pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006184201s
STEP: Saw pod success 04/17/23 21:48:43.75
Apr 17 21:48:43.750: INFO: Pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5" satisfied condition "Succeeded or Failed"
Apr 17 21:48:43.753: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/17/23 21:48:43.757
Apr 17 21:48:43.769: INFO: Waiting for pod pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5 to disappear
Apr 17 21:48:43.771: INFO: Pod pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:43.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2373" for this suite. 04/17/23 21:48:43.776
------------------------------
• [4.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:39.715
    Apr 17 21:48:39.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 21:48:39.716
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:39.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:39.731
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-cfbd7331-8187-4372-a168-cb41e4f2f74b 04/17/23 21:48:39.733
    STEP: Creating a pod to test consume secrets 04/17/23 21:48:39.737
    Apr 17 21:48:39.744: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5" in namespace "projected-2373" to be "Succeeded or Failed"
    Apr 17 21:48:39.746: INFO: Pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289819ms
    Apr 17 21:48:41.750: INFO: Pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005948612s
    Apr 17 21:48:43.750: INFO: Pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006184201s
    STEP: Saw pod success 04/17/23 21:48:43.75
    Apr 17 21:48:43.750: INFO: Pod "pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5" satisfied condition "Succeeded or Failed"
    Apr 17 21:48:43.753: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 21:48:43.757
    Apr 17 21:48:43.769: INFO: Waiting for pod pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5 to disappear
    Apr 17 21:48:43.771: INFO: Pod pod-projected-secrets-a76a8f3d-ed35-4b28-be64-6aa930b05fa5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:43.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2373" for this suite. 04/17/23 21:48:43.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:43.782
Apr 17 21:48:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename job 04/17/23 21:48:43.783
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:43.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:43.797
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 04/17/23 21:48:43.799
STEP: Ensure pods equal to parallelism count is attached to the job 04/17/23 21:48:43.804
STEP: patching /status 04/17/23 21:48:45.809
STEP: updating /status 04/17/23 21:48:45.814
STEP: get /status 04/17/23 21:48:45.821
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:45.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-6366" for this suite. 04/17/23 21:48:45.827
------------------------------
• [2.049 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:43.782
    Apr 17 21:48:43.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename job 04/17/23 21:48:43.783
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:43.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:43.797
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 04/17/23 21:48:43.799
    STEP: Ensure pods equal to parallelism count is attached to the job 04/17/23 21:48:43.804
    STEP: patching /status 04/17/23 21:48:45.809
    STEP: updating /status 04/17/23 21:48:45.814
    STEP: get /status 04/17/23 21:48:45.821
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:45.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-6366" for this suite. 04/17/23 21:48:45.827
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:45.831
Apr 17 21:48:45.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 21:48:45.832
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:45.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:45.847
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-559f2e2e-8b26-4f83-9fd9-5af3d831ac60 04/17/23 21:48:45.849
STEP: Creating a pod to test consume secrets 04/17/23 21:48:45.852
Apr 17 21:48:45.859: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114" in namespace "projected-2951" to be "Succeeded or Failed"
Apr 17 21:48:45.861: INFO: Pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255992ms
Apr 17 21:48:47.864: INFO: Pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005066828s
Apr 17 21:48:49.865: INFO: Pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00601784s
STEP: Saw pod success 04/17/23 21:48:49.865
Apr 17 21:48:49.865: INFO: Pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114" satisfied condition "Succeeded or Failed"
Apr 17 21:48:49.867: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/17/23 21:48:49.871
Apr 17 21:48:49.905: INFO: Waiting for pod pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114 to disappear
Apr 17 21:48:49.912: INFO: Pod pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:49.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2951" for this suite. 04/17/23 21:48:49.923
------------------------------
• [4.113 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:45.831
    Apr 17 21:48:45.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 21:48:45.832
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:45.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:45.847
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-559f2e2e-8b26-4f83-9fd9-5af3d831ac60 04/17/23 21:48:45.849
    STEP: Creating a pod to test consume secrets 04/17/23 21:48:45.852
    Apr 17 21:48:45.859: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114" in namespace "projected-2951" to be "Succeeded or Failed"
    Apr 17 21:48:45.861: INFO: Pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.255992ms
    Apr 17 21:48:47.864: INFO: Pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005066828s
    Apr 17 21:48:49.865: INFO: Pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00601784s
    STEP: Saw pod success 04/17/23 21:48:49.865
    Apr 17 21:48:49.865: INFO: Pod "pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114" satisfied condition "Succeeded or Failed"
    Apr 17 21:48:49.867: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 21:48:49.871
    Apr 17 21:48:49.905: INFO: Waiting for pod pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114 to disappear
    Apr 17 21:48:49.912: INFO: Pod pod-projected-secrets-e2278a46-db73-4705-8b1e-54eba14dd114 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:49.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2951" for this suite. 04/17/23 21:48:49.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:49.945
Apr 17 21:48:49.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 21:48:49.946
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:49.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:49.987
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-3adc9016-8c6a-43f9-ad19-b197c4c2b3eb 04/17/23 21:48:49.99
STEP: Creating a pod to test consume configMaps 04/17/23 21:48:50
Apr 17 21:48:50.009: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1" in namespace "projected-292" to be "Succeeded or Failed"
Apr 17 21:48:50.012: INFO: Pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084356ms
Apr 17 21:48:52.016: INFO: Pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006510892s
Apr 17 21:48:54.016: INFO: Pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006986864s
STEP: Saw pod success 04/17/23 21:48:54.016
Apr 17 21:48:54.017: INFO: Pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1" satisfied condition "Succeeded or Failed"
Apr 17 21:48:54.019: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 21:48:54.026
Apr 17 21:48:54.037: INFO: Waiting for pod pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1 to disappear
Apr 17 21:48:54.039: INFO: Pod pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 17 21:48:54.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-292" for this suite. 04/17/23 21:48:54.043
------------------------------
• [4.102 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:49.945
    Apr 17 21:48:49.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 21:48:49.946
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:49.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:49.987
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-3adc9016-8c6a-43f9-ad19-b197c4c2b3eb 04/17/23 21:48:49.99
    STEP: Creating a pod to test consume configMaps 04/17/23 21:48:50
    Apr 17 21:48:50.009: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1" in namespace "projected-292" to be "Succeeded or Failed"
    Apr 17 21:48:50.012: INFO: Pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.084356ms
    Apr 17 21:48:52.016: INFO: Pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006510892s
    Apr 17 21:48:54.016: INFO: Pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006986864s
    STEP: Saw pod success 04/17/23 21:48:54.016
    Apr 17 21:48:54.017: INFO: Pod "pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1" satisfied condition "Succeeded or Failed"
    Apr 17 21:48:54.019: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 21:48:54.026
    Apr 17 21:48:54.037: INFO: Waiting for pod pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1 to disappear
    Apr 17 21:48:54.039: INFO: Pod pod-projected-configmaps-cac9e7b3-f607-4b76-97c9-c339e2be4bc1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:48:54.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-292" for this suite. 04/17/23 21:48:54.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:48:54.048
Apr 17 21:48:54.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename job 04/17/23 21:48:54.049
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:54.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:54.063
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 04/17/23 21:48:54.065
STEP: Ensuring job reaches completions 04/17/23 21:48:54.069
STEP: Ensuring pods with index for job exist 04/17/23 21:49:02.073
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 17 21:49:02.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4272" for this suite. 04/17/23 21:49:02.081
------------------------------
• [SLOW TEST] [8.037 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:48:54.048
    Apr 17 21:48:54.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename job 04/17/23 21:48:54.049
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:48:54.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:48:54.063
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 04/17/23 21:48:54.065
    STEP: Ensuring job reaches completions 04/17/23 21:48:54.069
    STEP: Ensuring pods with index for job exist 04/17/23 21:49:02.073
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:49:02.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4272" for this suite. 04/17/23 21:49:02.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:49:02.086
Apr 17 21:49:02.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 21:49:02.087
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:49:02.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:49:02.108
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/17/23 21:49:02.11
Apr 17 21:49:02.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-5018 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Apr 17 21:49:02.168: INFO: stderr: ""
Apr 17 21:49:02.168: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 04/17/23 21:49:02.168
STEP: verifying the pod e2e-test-httpd-pod was created 04/17/23 21:49:07.221
Apr 17 21:49:07.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-5018 get pod e2e-test-httpd-pod -o json'
Apr 17 21:49:07.278: INFO: stderr: ""
Apr 17 21:49:07.278: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"76a28e36d625ac2d2559f7e178005d034bc71927515e53e6527117c58eb92c81\",\n            \"cni.projectcalico.org/podIP\": \"192.168.213.25/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.213.25/32\"\n        },\n        \"creationTimestamp\": \"2023-04-17T21:49:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5018\",\n        \"resourceVersion\": \"33112\",\n        \"uid\": \"b9d695b7-73d8-4558-8d6f-5158dbc17709\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-27mm4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-93-18.us-west-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-27mm4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-17T21:49:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-17T21:49:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-17T21:49:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-17T21:49:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://bbf6a4e1ba7402d70a7a56fc6acd0c415fbe7017a3642ecd17dc8a136e891e63\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-17T21:49:02Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.93.18\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.213.25\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.213.25\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-17T21:49:02Z\"\n    }\n}\n"
STEP: replace the image in the pod 04/17/23 21:49:07.278
Apr 17 21:49:07.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-5018 replace -f -'
Apr 17 21:49:09.416: INFO: stderr: ""
Apr 17 21:49:09.416: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 04/17/23 21:49:09.416
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Apr 17 21:49:09.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-5018 delete pods e2e-test-httpd-pod'
Apr 17 21:49:11.320: INFO: stderr: ""
Apr 17 21:49:11.320: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 21:49:11.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5018" for this suite. 04/17/23 21:49:11.325
------------------------------
• [SLOW TEST] [9.243 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:49:02.086
    Apr 17 21:49:02.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 21:49:02.087
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:49:02.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:49:02.108
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/17/23 21:49:02.11
    Apr 17 21:49:02.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-5018 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Apr 17 21:49:02.168: INFO: stderr: ""
    Apr 17 21:49:02.168: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 04/17/23 21:49:02.168
    STEP: verifying the pod e2e-test-httpd-pod was created 04/17/23 21:49:07.221
    Apr 17 21:49:07.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-5018 get pod e2e-test-httpd-pod -o json'
    Apr 17 21:49:07.278: INFO: stderr: ""
    Apr 17 21:49:07.278: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"76a28e36d625ac2d2559f7e178005d034bc71927515e53e6527117c58eb92c81\",\n            \"cni.projectcalico.org/podIP\": \"192.168.213.25/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.213.25/32\"\n        },\n        \"creationTimestamp\": \"2023-04-17T21:49:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5018\",\n        \"resourceVersion\": \"33112\",\n        \"uid\": \"b9d695b7-73d8-4558-8d6f-5158dbc17709\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-27mm4\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-93-18.us-west-2.compute.internal\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-27mm4\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-17T21:49:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-17T21:49:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-17T21:49:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-04-17T21:49:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://bbf6a4e1ba7402d70a7a56fc6acd0c415fbe7017a3642ecd17dc8a136e891e63\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-04-17T21:49:02Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.93.18\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.213.25\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.213.25\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-04-17T21:49:02Z\"\n    }\n}\n"
    STEP: replace the image in the pod 04/17/23 21:49:07.278
    Apr 17 21:49:07.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-5018 replace -f -'
    Apr 17 21:49:09.416: INFO: stderr: ""
    Apr 17 21:49:09.416: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 04/17/23 21:49:09.416
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Apr 17 21:49:09.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-5018 delete pods e2e-test-httpd-pod'
    Apr 17 21:49:11.320: INFO: stderr: ""
    Apr 17 21:49:11.320: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:49:11.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5018" for this suite. 04/17/23 21:49:11.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:49:11.33
Apr 17 21:49:11.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename watch 04/17/23 21:49:11.331
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:49:11.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:49:11.344
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 04/17/23 21:49:11.346
STEP: modifying the configmap once 04/17/23 21:49:11.349
STEP: modifying the configmap a second time 04/17/23 21:49:11.356
STEP: deleting the configmap 04/17/23 21:49:11.361
STEP: creating a watch on configmaps from the resource version returned by the first update 04/17/23 21:49:11.365
STEP: Expecting to observe notifications for all changes to the configmap after the first update 04/17/23 21:49:11.366
Apr 17 21:49:11.366: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3235  bd665da4-6db0-4d0f-b3f5-e6d428df640d 33237 0 2023-04-17 21:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-17 21:49:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 21:49:11.366: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3235  bd665da4-6db0-4d0f-b3f5-e6d428df640d 33238 0 2023-04-17 21:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-17 21:49:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 17 21:49:11.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3235" for this suite. 04/17/23 21:49:11.37
------------------------------
• [0.044 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:49:11.33
    Apr 17 21:49:11.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename watch 04/17/23 21:49:11.331
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:49:11.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:49:11.344
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 04/17/23 21:49:11.346
    STEP: modifying the configmap once 04/17/23 21:49:11.349
    STEP: modifying the configmap a second time 04/17/23 21:49:11.356
    STEP: deleting the configmap 04/17/23 21:49:11.361
    STEP: creating a watch on configmaps from the resource version returned by the first update 04/17/23 21:49:11.365
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 04/17/23 21:49:11.366
    Apr 17 21:49:11.366: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3235  bd665da4-6db0-4d0f-b3f5-e6d428df640d 33237 0 2023-04-17 21:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-17 21:49:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 21:49:11.366: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-3235  bd665da4-6db0-4d0f-b3f5-e6d428df640d 33238 0 2023-04-17 21:49:11 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-04-17 21:49:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:49:11.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3235" for this suite. 04/17/23 21:49:11.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:49:11.376
Apr 17 21:49:11.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename cronjob 04/17/23 21:49:11.376
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:49:11.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:49:11.391
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 04/17/23 21:49:11.393
STEP: Ensuring a job is scheduled 04/17/23 21:49:11.396
STEP: Ensuring exactly one is scheduled 04/17/23 21:50:01.401
STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/17/23 21:50:01.403
STEP: Ensuring no more jobs are scheduled 04/17/23 21:50:01.406
STEP: Removing cronjob 04/17/23 21:55:01.413
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 17 21:55:01.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3389" for this suite. 04/17/23 21:55:01.422
------------------------------
• [SLOW TEST] [350.052 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:49:11.376
    Apr 17 21:49:11.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename cronjob 04/17/23 21:49:11.376
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:49:11.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:49:11.391
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 04/17/23 21:49:11.393
    STEP: Ensuring a job is scheduled 04/17/23 21:49:11.396
    STEP: Ensuring exactly one is scheduled 04/17/23 21:50:01.401
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 04/17/23 21:50:01.403
    STEP: Ensuring no more jobs are scheduled 04/17/23 21:50:01.406
    STEP: Removing cronjob 04/17/23 21:55:01.413
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:55:01.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3389" for this suite. 04/17/23 21:55:01.422
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:55:01.428
Apr 17 21:55:01.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pod-network-test 04/17/23 21:55:01.429
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:55:01.453
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:55:01.457
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-49 04/17/23 21:55:01.459
STEP: creating a selector 04/17/23 21:55:01.459
STEP: Creating the service pods in kubernetes 04/17/23 21:55:01.459
Apr 17 21:55:01.460: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 17 21:55:01.494: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-49" to be "running and ready"
Apr 17 21:55:01.502: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.159449ms
Apr 17 21:55:01.502: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:55:03.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011310044s
Apr 17 21:55:03.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:05.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011216377s
Apr 17 21:55:05.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:07.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011082336s
Apr 17 21:55:07.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:09.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011969309s
Apr 17 21:55:09.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:11.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011656313s
Apr 17 21:55:11.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:13.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011373228s
Apr 17 21:55:13.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:15.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012076214s
Apr 17 21:55:15.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:17.507: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012761537s
Apr 17 21:55:17.507: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:19.507: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012604825s
Apr 17 21:55:19.507: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:21.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012288565s
Apr 17 21:55:21.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 21:55:23.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011346431s
Apr 17 21:55:23.505: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 17 21:55:23.505: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 17 21:55:23.508: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-49" to be "running and ready"
Apr 17 21:55:23.510: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.111116ms
Apr 17 21:55:23.510: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 17 21:55:23.510: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr 17 21:55:23.512: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-49" to be "running and ready"
Apr 17 21:55:23.514: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.950868ms
Apr 17 21:55:23.514: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr 17 21:55:23.514: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr 17 21:55:23.516: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-49" to be "running and ready"
Apr 17 21:55:23.518: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018383ms
Apr 17 21:55:23.518: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr 17 21:55:23.518: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 04/17/23 21:55:23.52
Apr 17 21:55:23.529: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-49" to be "running"
Apr 17 21:55:23.532: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434713ms
Apr 17 21:55:25.536: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006671018s
Apr 17 21:55:25.536: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 17 21:55:25.539: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-49" to be "running"
Apr 17 21:55:25.541: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.111087ms
Apr 17 21:55:25.541: INFO: Pod "host-test-container-pod" satisfied condition "running"
Apr 17 21:55:25.543: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Apr 17 21:55:25.543: INFO: Going to poll 192.168.163.198 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Apr 17 21:55:25.545: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.163.198:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-49 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:55:25.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:55:25.546: INFO: ExecWithOptions: Clientset creation
Apr 17 21:55:25.546: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-49/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.163.198%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 17 21:55:25.635: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 17 21:55:25.635: INFO: Going to poll 192.168.200.180 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Apr 17 21:55:25.638: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.200.180:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-49 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:55:25.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:55:25.638: INFO: ExecWithOptions: Clientset creation
Apr 17 21:55:25.638: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-49/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.200.180%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 17 21:55:25.731: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 17 21:55:25.731: INFO: Going to poll 192.168.208.152 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Apr 17 21:55:25.734: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.208.152:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-49 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:55:25.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:55:25.734: INFO: ExecWithOptions: Clientset creation
Apr 17 21:55:25.734: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-49/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.208.152%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 17 21:55:25.839: INFO: Found all 1 expected endpoints: [netserver-2]
Apr 17 21:55:25.839: INFO: Going to poll 192.168.213.22 on port 8083 at least 0 times, with a maximum of 46 tries before failing
Apr 17 21:55:25.841: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.213.22:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-49 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 21:55:25.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 21:55:25.842: INFO: ExecWithOptions: Clientset creation
Apr 17 21:55:25.842: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-49/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.213.22%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 17 21:55:25.935: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 17 21:55:25.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-49" for this suite. 04/17/23 21:55:25.94
------------------------------
• [SLOW TEST] [24.516 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:55:01.428
    Apr 17 21:55:01.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pod-network-test 04/17/23 21:55:01.429
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:55:01.453
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:55:01.457
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-49 04/17/23 21:55:01.459
    STEP: creating a selector 04/17/23 21:55:01.459
    STEP: Creating the service pods in kubernetes 04/17/23 21:55:01.459
    Apr 17 21:55:01.460: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 17 21:55:01.494: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-49" to be "running and ready"
    Apr 17 21:55:01.502: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.159449ms
    Apr 17 21:55:01.502: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:55:03.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011310044s
    Apr 17 21:55:03.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:05.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011216377s
    Apr 17 21:55:05.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:07.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011082336s
    Apr 17 21:55:07.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:09.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011969309s
    Apr 17 21:55:09.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:11.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011656313s
    Apr 17 21:55:11.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:13.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011373228s
    Apr 17 21:55:13.505: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:15.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.012076214s
    Apr 17 21:55:15.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:17.507: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012761537s
    Apr 17 21:55:17.507: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:19.507: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.012604825s
    Apr 17 21:55:19.507: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:21.506: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.012288565s
    Apr 17 21:55:21.506: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 21:55:23.505: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011346431s
    Apr 17 21:55:23.505: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 17 21:55:23.505: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 17 21:55:23.508: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-49" to be "running and ready"
    Apr 17 21:55:23.510: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.111116ms
    Apr 17 21:55:23.510: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 17 21:55:23.510: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr 17 21:55:23.512: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-49" to be "running and ready"
    Apr 17 21:55:23.514: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.950868ms
    Apr 17 21:55:23.514: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr 17 21:55:23.514: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr 17 21:55:23.516: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-49" to be "running and ready"
    Apr 17 21:55:23.518: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.018383ms
    Apr 17 21:55:23.518: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr 17 21:55:23.518: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 04/17/23 21:55:23.52
    Apr 17 21:55:23.529: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-49" to be "running"
    Apr 17 21:55:23.532: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.434713ms
    Apr 17 21:55:25.536: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006671018s
    Apr 17 21:55:25.536: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 17 21:55:25.539: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-49" to be "running"
    Apr 17 21:55:25.541: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.111087ms
    Apr 17 21:55:25.541: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Apr 17 21:55:25.543: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Apr 17 21:55:25.543: INFO: Going to poll 192.168.163.198 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Apr 17 21:55:25.545: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.163.198:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-49 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:55:25.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:55:25.546: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:55:25.546: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-49/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.163.198%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 17 21:55:25.635: INFO: Found all 1 expected endpoints: [netserver-0]
    Apr 17 21:55:25.635: INFO: Going to poll 192.168.200.180 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Apr 17 21:55:25.638: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.200.180:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-49 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:55:25.638: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:55:25.638: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:55:25.638: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-49/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.200.180%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 17 21:55:25.731: INFO: Found all 1 expected endpoints: [netserver-1]
    Apr 17 21:55:25.731: INFO: Going to poll 192.168.208.152 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Apr 17 21:55:25.734: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.208.152:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-49 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:55:25.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:55:25.734: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:55:25.734: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-49/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.208.152%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 17 21:55:25.839: INFO: Found all 1 expected endpoints: [netserver-2]
    Apr 17 21:55:25.839: INFO: Going to poll 192.168.213.22 on port 8083 at least 0 times, with a maximum of 46 tries before failing
    Apr 17 21:55:25.841: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.213.22:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-49 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 21:55:25.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 21:55:25.842: INFO: ExecWithOptions: Clientset creation
    Apr 17 21:55:25.842: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-49/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.213.22%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 17 21:55:25.935: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:55:25.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-49" for this suite. 04/17/23 21:55:25.94
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:55:25.945
Apr 17 21:55:25.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 21:55:25.945
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:55:25.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:55:25.96
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 04/17/23 21:55:25.963
STEP: Creating a ResourceQuota 04/17/23 21:55:30.966
STEP: Ensuring resource quota status is calculated 04/17/23 21:55:30.971
STEP: Creating a ReplicationController 04/17/23 21:55:32.975
STEP: Ensuring resource quota status captures replication controller creation 04/17/23 21:55:32.985
STEP: Deleting a ReplicationController 04/17/23 21:55:34.988
STEP: Ensuring resource quota status released usage 04/17/23 21:55:34.993
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 21:55:36.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4305" for this suite. 04/17/23 21:55:37.002
------------------------------
• [SLOW TEST] [11.061 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:55:25.945
    Apr 17 21:55:25.945: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 21:55:25.945
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:55:25.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:55:25.96
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 04/17/23 21:55:25.963
    STEP: Creating a ResourceQuota 04/17/23 21:55:30.966
    STEP: Ensuring resource quota status is calculated 04/17/23 21:55:30.971
    STEP: Creating a ReplicationController 04/17/23 21:55:32.975
    STEP: Ensuring resource quota status captures replication controller creation 04/17/23 21:55:32.985
    STEP: Deleting a ReplicationController 04/17/23 21:55:34.988
    STEP: Ensuring resource quota status released usage 04/17/23 21:55:34.993
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:55:36.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4305" for this suite. 04/17/23 21:55:37.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:55:37.007
Apr 17 21:55:37.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 21:55:37.008
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:55:37.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:55:37.023
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 04/17/23 21:55:37.025
STEP: Getting a ResourceQuota 04/17/23 21:55:37.029
STEP: Listing all ResourceQuotas with LabelSelector 04/17/23 21:55:37.031
STEP: Patching the ResourceQuota 04/17/23 21:55:37.033
STEP: Deleting a Collection of ResourceQuotas 04/17/23 21:55:37.038
STEP: Verifying the deleted ResourceQuota 04/17/23 21:55:37.045
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 21:55:37.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9881" for this suite. 04/17/23 21:55:37.051
------------------------------
• [0.049 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:55:37.007
    Apr 17 21:55:37.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 21:55:37.008
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:55:37.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:55:37.023
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 04/17/23 21:55:37.025
    STEP: Getting a ResourceQuota 04/17/23 21:55:37.029
    STEP: Listing all ResourceQuotas with LabelSelector 04/17/23 21:55:37.031
    STEP: Patching the ResourceQuota 04/17/23 21:55:37.033
    STEP: Deleting a Collection of ResourceQuotas 04/17/23 21:55:37.038
    STEP: Verifying the deleted ResourceQuota 04/17/23 21:55:37.045
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:55:37.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9881" for this suite. 04/17/23 21:55:37.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:55:37.056
Apr 17 21:55:37.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename subpath 04/17/23 21:55:37.057
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:55:37.07
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:55:37.073
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/17/23 21:55:37.075
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-w87r 04/17/23 21:55:37.081
STEP: Creating a pod to test atomic-volume-subpath 04/17/23 21:55:37.081
Apr 17 21:55:37.087: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w87r" in namespace "subpath-9386" to be "Succeeded or Failed"
Apr 17 21:55:37.089: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.322715ms
Apr 17 21:55:39.092: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005858673s
Apr 17 21:55:41.094: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 4.00707233s
Apr 17 21:55:43.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 6.006055442s
Apr 17 21:55:45.092: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 8.005155615s
Apr 17 21:55:47.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 10.006137539s
Apr 17 21:55:49.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 12.006087402s
Apr 17 21:55:51.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 14.006807848s
Apr 17 21:55:53.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 16.006087458s
Apr 17 21:55:55.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 18.006779247s
Apr 17 21:55:57.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 20.006338281s
Apr 17 21:55:59.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=false. Elapsed: 22.006100296s
Apr 17 21:56:01.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006130431s
STEP: Saw pod success 04/17/23 21:56:01.093
Apr 17 21:56:01.093: INFO: Pod "pod-subpath-test-configmap-w87r" satisfied condition "Succeeded or Failed"
Apr 17 21:56:01.095: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-subpath-test-configmap-w87r container test-container-subpath-configmap-w87r: <nil>
STEP: delete the pod 04/17/23 21:56:01.107
Apr 17 21:56:01.118: INFO: Waiting for pod pod-subpath-test-configmap-w87r to disappear
Apr 17 21:56:01.120: INFO: Pod pod-subpath-test-configmap-w87r no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w87r 04/17/23 21:56:01.12
Apr 17 21:56:01.120: INFO: Deleting pod "pod-subpath-test-configmap-w87r" in namespace "subpath-9386"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 17 21:56:01.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-9386" for this suite. 04/17/23 21:56:01.127
------------------------------
• [SLOW TEST] [24.075 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:55:37.056
    Apr 17 21:55:37.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename subpath 04/17/23 21:55:37.057
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:55:37.07
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:55:37.073
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/17/23 21:55:37.075
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-w87r 04/17/23 21:55:37.081
    STEP: Creating a pod to test atomic-volume-subpath 04/17/23 21:55:37.081
    Apr 17 21:55:37.087: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w87r" in namespace "subpath-9386" to be "Succeeded or Failed"
    Apr 17 21:55:37.089: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.322715ms
    Apr 17 21:55:39.092: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005858673s
    Apr 17 21:55:41.094: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 4.00707233s
    Apr 17 21:55:43.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 6.006055442s
    Apr 17 21:55:45.092: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 8.005155615s
    Apr 17 21:55:47.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 10.006137539s
    Apr 17 21:55:49.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 12.006087402s
    Apr 17 21:55:51.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 14.006807848s
    Apr 17 21:55:53.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 16.006087458s
    Apr 17 21:55:55.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 18.006779247s
    Apr 17 21:55:57.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=true. Elapsed: 20.006338281s
    Apr 17 21:55:59.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Running", Reason="", readiness=false. Elapsed: 22.006100296s
    Apr 17 21:56:01.093: INFO: Pod "pod-subpath-test-configmap-w87r": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006130431s
    STEP: Saw pod success 04/17/23 21:56:01.093
    Apr 17 21:56:01.093: INFO: Pod "pod-subpath-test-configmap-w87r" satisfied condition "Succeeded or Failed"
    Apr 17 21:56:01.095: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-subpath-test-configmap-w87r container test-container-subpath-configmap-w87r: <nil>
    STEP: delete the pod 04/17/23 21:56:01.107
    Apr 17 21:56:01.118: INFO: Waiting for pod pod-subpath-test-configmap-w87r to disappear
    Apr 17 21:56:01.120: INFO: Pod pod-subpath-test-configmap-w87r no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-w87r 04/17/23 21:56:01.12
    Apr 17 21:56:01.120: INFO: Deleting pod "pod-subpath-test-configmap-w87r" in namespace "subpath-9386"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:56:01.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-9386" for this suite. 04/17/23 21:56:01.127
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:56:01.131
Apr 17 21:56:01.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename dns 04/17/23 21:56:01.132
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:01.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:01.147
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 04/17/23 21:56:01.149
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9840.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9840.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 103.182.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.182.103_udp@PTR;check="$$(dig +tcp +noall +answer +search 103.182.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.182.103_tcp@PTR;sleep 1; done
 04/17/23 21:56:01.165
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9840.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9840.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 103.182.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.182.103_udp@PTR;check="$$(dig +tcp +noall +answer +search 103.182.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.182.103_tcp@PTR;sleep 1; done
 04/17/23 21:56:01.165
STEP: creating a pod to probe DNS 04/17/23 21:56:01.165
STEP: submitting the pod to kubernetes 04/17/23 21:56:01.165
Apr 17 21:56:01.176: INFO: Waiting up to 15m0s for pod "dns-test-6097918e-c691-43aa-a378-8703e0ff9854" in namespace "dns-9840" to be "running"
Apr 17 21:56:01.180: INFO: Pod "dns-test-6097918e-c691-43aa-a378-8703e0ff9854": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094759ms
Apr 17 21:56:03.184: INFO: Pod "dns-test-6097918e-c691-43aa-a378-8703e0ff9854": Phase="Running", Reason="", readiness=true. Elapsed: 2.008067989s
Apr 17 21:56:03.184: INFO: Pod "dns-test-6097918e-c691-43aa-a378-8703e0ff9854" satisfied condition "running"
STEP: retrieving the pod 04/17/23 21:56:03.184
STEP: looking for the results for each expected name from probers 04/17/23 21:56:03.187
Apr 17 21:56:03.190: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:03.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:03.195: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:03.198: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:03.211: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:03.213: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:03.216: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:03.218: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:03.228: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

Apr 17 21:56:08.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:08.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:08.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:08.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:08.252: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:08.255: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:08.257: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:08.260: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:08.270: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

Apr 17 21:56:13.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:13.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:13.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:13.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:13.252: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:13.254: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:13.257: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:13.259: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:13.269: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

Apr 17 21:56:18.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:18.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:18.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:18.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:18.253: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:18.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:18.258: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:18.261: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:18.270: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

Apr 17 21:56:23.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:23.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:23.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:23.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:23.252: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:23.255: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:23.257: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:23.259: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:23.269: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

Apr 17 21:56:28.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:28.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:28.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:28.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:28.251: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:28.254: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:28.256: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:28.258: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
Apr 17 21:56:28.268: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

Apr 17 21:56:33.269: INFO: DNS probes using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 succeeded

STEP: deleting the pod 04/17/23 21:56:33.269
STEP: deleting the test service 04/17/23 21:56:33.281
STEP: deleting the test headless service 04/17/23 21:56:33.373
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 17 21:56:33.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9840" for this suite. 04/17/23 21:56:33.387
------------------------------
• [SLOW TEST] [32.261 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:56:01.131
    Apr 17 21:56:01.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename dns 04/17/23 21:56:01.132
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:01.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:01.147
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 04/17/23 21:56:01.149
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9840.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9840.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 103.182.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.182.103_udp@PTR;check="$$(dig +tcp +noall +answer +search 103.182.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.182.103_tcp@PTR;sleep 1; done
     04/17/23 21:56:01.165
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9840.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9840.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9840.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9840.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9840.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 103.182.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.182.103_udp@PTR;check="$$(dig +tcp +noall +answer +search 103.182.108.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.108.182.103_tcp@PTR;sleep 1; done
     04/17/23 21:56:01.165
    STEP: creating a pod to probe DNS 04/17/23 21:56:01.165
    STEP: submitting the pod to kubernetes 04/17/23 21:56:01.165
    Apr 17 21:56:01.176: INFO: Waiting up to 15m0s for pod "dns-test-6097918e-c691-43aa-a378-8703e0ff9854" in namespace "dns-9840" to be "running"
    Apr 17 21:56:01.180: INFO: Pod "dns-test-6097918e-c691-43aa-a378-8703e0ff9854": Phase="Pending", Reason="", readiness=false. Elapsed: 4.094759ms
    Apr 17 21:56:03.184: INFO: Pod "dns-test-6097918e-c691-43aa-a378-8703e0ff9854": Phase="Running", Reason="", readiness=true. Elapsed: 2.008067989s
    Apr 17 21:56:03.184: INFO: Pod "dns-test-6097918e-c691-43aa-a378-8703e0ff9854" satisfied condition "running"
    STEP: retrieving the pod 04/17/23 21:56:03.184
    STEP: looking for the results for each expected name from probers 04/17/23 21:56:03.187
    Apr 17 21:56:03.190: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:03.193: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:03.195: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:03.198: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:03.211: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:03.213: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:03.216: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:03.218: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:03.228: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

    Apr 17 21:56:08.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:08.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:08.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:08.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:08.252: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:08.255: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:08.257: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:08.260: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:08.270: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

    Apr 17 21:56:13.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:13.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:13.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:13.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:13.252: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:13.254: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:13.257: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:13.259: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:13.269: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

    Apr 17 21:56:18.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:18.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:18.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:18.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:18.253: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:18.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:18.258: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:18.261: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:18.270: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

    Apr 17 21:56:23.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:23.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:23.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:23.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:23.252: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:23.255: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:23.257: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:23.259: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:23.269: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

    Apr 17 21:56:28.232: INFO: Unable to read wheezy_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:28.235: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:28.237: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:28.240: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:28.251: INFO: Unable to read jessie_udp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:28.254: INFO: Unable to read jessie_tcp@dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:28.256: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:28.258: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local from pod dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854: the server could not find the requested resource (get pods dns-test-6097918e-c691-43aa-a378-8703e0ff9854)
    Apr 17 21:56:28.268: INFO: Lookups using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 failed for: [wheezy_udp@dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@dns-test-service.dns-9840.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_udp@dns-test-service.dns-9840.svc.cluster.local jessie_tcp@dns-test-service.dns-9840.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-9840.svc.cluster.local]

    Apr 17 21:56:33.269: INFO: DNS probes using dns-9840/dns-test-6097918e-c691-43aa-a378-8703e0ff9854 succeeded

    STEP: deleting the pod 04/17/23 21:56:33.269
    STEP: deleting the test service 04/17/23 21:56:33.281
    STEP: deleting the test headless service 04/17/23 21:56:33.373
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:56:33.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9840" for this suite. 04/17/23 21:56:33.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:56:33.393
Apr 17 21:56:33.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 21:56:33.394
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:33.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:33.409
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 21:56:33.432
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:56:33.926
STEP: Deploying the webhook pod 04/17/23 21:56:33.933
STEP: Wait for the deployment to be ready 04/17/23 21:56:33.943
Apr 17 21:56:33.947: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 04/17/23 21:56:35.955
STEP: Verifying the service has paired with the endpoint 04/17/23 21:56:35.967
Apr 17 21:56:36.967: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 04/17/23 21:56:36.97
Apr 17 21:56:36.983: INFO: Waiting for webhook configuration to be ready...
STEP: Creating a custom resource definition that should be denied by the webhook 04/17/23 21:56:37.09
Apr 17 21:56:37.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:56:37.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3458" for this suite. 04/17/23 21:56:37.138
STEP: Destroying namespace "webhook-3458-markers" for this suite. 04/17/23 21:56:37.144
------------------------------
• [3.756 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:56:33.393
    Apr 17 21:56:33.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 21:56:33.394
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:33.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:33.409
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 21:56:33.432
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 21:56:33.926
    STEP: Deploying the webhook pod 04/17/23 21:56:33.933
    STEP: Wait for the deployment to be ready 04/17/23 21:56:33.943
    Apr 17 21:56:33.947: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 04/17/23 21:56:35.955
    STEP: Verifying the service has paired with the endpoint 04/17/23 21:56:35.967
    Apr 17 21:56:36.967: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 04/17/23 21:56:36.97
    Apr 17 21:56:36.983: INFO: Waiting for webhook configuration to be ready...
    STEP: Creating a custom resource definition that should be denied by the webhook 04/17/23 21:56:37.09
    Apr 17 21:56:37.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:56:37.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3458" for this suite. 04/17/23 21:56:37.138
    STEP: Destroying namespace "webhook-3458-markers" for this suite. 04/17/23 21:56:37.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:56:37.15
Apr 17 21:56:37.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename csiinlinevolumes 04/17/23 21:56:37.15
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:37.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:37.165
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 04/17/23 21:56:37.167
STEP: getting 04/17/23 21:56:37.185
STEP: listing in namespace 04/17/23 21:56:37.187
STEP: patching 04/17/23 21:56:37.19
STEP: deleting 04/17/23 21:56:37.203
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Apr 17 21:56:37.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-5470" for this suite. 04/17/23 21:56:37.216
------------------------------
• [0.072 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:56:37.15
    Apr 17 21:56:37.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename csiinlinevolumes 04/17/23 21:56:37.15
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:37.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:37.165
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 04/17/23 21:56:37.167
    STEP: getting 04/17/23 21:56:37.185
    STEP: listing in namespace 04/17/23 21:56:37.187
    STEP: patching 04/17/23 21:56:37.19
    STEP: deleting 04/17/23 21:56:37.203
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:56:37.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-5470" for this suite. 04/17/23 21:56:37.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:56:37.222
Apr 17 21:56:37.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename daemonsets 04/17/23 21:56:37.223
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:37.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:37.236
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Apr 17 21:56:37.256: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 21:56:37.259
Apr 17 21:56:37.264: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:37.264: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:37.264: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:37.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 21:56:37.266: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:56:38.271: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:38.271: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:38.271: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:38.273: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 21:56:38.273: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:56:39.272: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:39.272: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:39.272: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:39.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr 17 21:56:39.275: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Update daemon pods image. 04/17/23 21:56:39.285
STEP: Check that daemon pods images are updated. 04/17/23 21:56:39.297
Apr 17 21:56:39.299: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:39.299: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:39.299: INFO: Wrong image for pod: daemon-set-ngxt5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:39.299: INFO: Wrong image for pod: daemon-set-r4dsj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:39.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:39.303: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:39.303: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:40.307: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:40.307: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:40.307: INFO: Wrong image for pod: daemon-set-r4dsj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:40.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:40.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:40.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:41.306: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:41.306: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:41.306: INFO: Wrong image for pod: daemon-set-r4dsj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:41.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:41.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:41.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:42.307: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:42.307: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:42.307: INFO: Wrong image for pod: daemon-set-r4dsj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:42.307: INFO: Pod daemon-set-snftp is not available
Apr 17 21:56:42.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:42.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:42.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:43.307: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:43.307: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:43.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:43.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:43.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:44.306: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:44.306: INFO: Pod daemon-set-6m84c is not available
Apr 17 21:56:44.306: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:44.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:44.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:44.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:45.307: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:45.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:45.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:45.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:46.306: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Apr 17 21:56:46.306: INFO: Pod daemon-set-srsgz is not available
Apr 17 21:56:46.310: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:46.310: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:46.310: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:47.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:47.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:47.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:48.306: INFO: Pod daemon-set-jcn8x is not available
Apr 17 21:56:48.310: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:48.310: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:48.310: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 04/17/23 21:56:48.31
Apr 17 21:56:48.314: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:48.314: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:48.314: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:48.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Apr 17 21:56:48.317: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 21:56:49.322: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:49.322: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:49.322: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 21:56:49.325: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr 17 21:56:49.325: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/17/23 21:56:49.336
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-257, will wait for the garbage collector to delete the pods 04/17/23 21:56:49.336
Apr 17 21:56:49.393: INFO: Deleting DaemonSet.extensions daemon-set took: 4.667979ms
Apr 17 21:56:49.493: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.078059ms
Apr 17 21:56:52.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 21:56:52.296: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 17 21:56:52.298: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37816"},"items":null}

Apr 17 21:56:52.300: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37816"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:56:52.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-257" for this suite. 04/17/23 21:56:52.318
------------------------------
• [SLOW TEST] [15.100 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:56:37.222
    Apr 17 21:56:37.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename daemonsets 04/17/23 21:56:37.223
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:37.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:37.236
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Apr 17 21:56:37.256: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 21:56:37.259
    Apr 17 21:56:37.264: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:37.264: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:37.264: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:37.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 21:56:37.266: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:56:38.271: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:38.271: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:38.271: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:38.273: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 21:56:38.273: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:56:39.272: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:39.272: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:39.272: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:39.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr 17 21:56:39.275: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Update daemon pods image. 04/17/23 21:56:39.285
    STEP: Check that daemon pods images are updated. 04/17/23 21:56:39.297
    Apr 17 21:56:39.299: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:39.299: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:39.299: INFO: Wrong image for pod: daemon-set-ngxt5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:39.299: INFO: Wrong image for pod: daemon-set-r4dsj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:39.303: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:39.303: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:39.303: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:40.307: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:40.307: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:40.307: INFO: Wrong image for pod: daemon-set-r4dsj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:40.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:40.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:40.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:41.306: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:41.306: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:41.306: INFO: Wrong image for pod: daemon-set-r4dsj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:41.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:41.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:41.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:42.307: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:42.307: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:42.307: INFO: Wrong image for pod: daemon-set-r4dsj. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:42.307: INFO: Pod daemon-set-snftp is not available
    Apr 17 21:56:42.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:42.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:42.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:43.307: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:43.307: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:43.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:43.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:43.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:44.306: INFO: Wrong image for pod: daemon-set-2pfqd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:44.306: INFO: Pod daemon-set-6m84c is not available
    Apr 17 21:56:44.306: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:44.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:44.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:44.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:45.307: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:45.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:45.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:45.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:46.306: INFO: Wrong image for pod: daemon-set-j5wjw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Apr 17 21:56:46.306: INFO: Pod daemon-set-srsgz is not available
    Apr 17 21:56:46.310: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:46.310: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:46.310: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:47.311: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:47.311: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:47.311: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:48.306: INFO: Pod daemon-set-jcn8x is not available
    Apr 17 21:56:48.310: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:48.310: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:48.310: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 04/17/23 21:56:48.31
    Apr 17 21:56:48.314: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:48.314: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:48.314: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:48.317: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Apr 17 21:56:48.317: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 21:56:49.322: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:49.322: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:49.322: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 21:56:49.325: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr 17 21:56:49.325: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/17/23 21:56:49.336
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-257, will wait for the garbage collector to delete the pods 04/17/23 21:56:49.336
    Apr 17 21:56:49.393: INFO: Deleting DaemonSet.extensions daemon-set took: 4.667979ms
    Apr 17 21:56:49.493: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.078059ms
    Apr 17 21:56:52.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 21:56:52.296: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 17 21:56:52.298: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37816"},"items":null}

    Apr 17 21:56:52.300: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37816"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:56:52.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-257" for this suite. 04/17/23 21:56:52.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:56:52.323
Apr 17 21:56:52.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename gc 04/17/23 21:56:52.323
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:52.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:52.338
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 04/17/23 21:56:52.345
STEP: create the rc2 04/17/23 21:56:52.348
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 04/17/23 21:56:57.356
STEP: delete the rc simpletest-rc-to-be-deleted 04/17/23 21:56:57.845
STEP: wait for the rc to be deleted 04/17/23 21:56:57.85
Apr 17 21:57:02.862: INFO: 65 pods remaining
Apr 17 21:57:02.862: INFO: 65 pods has nil DeletionTimestamp
Apr 17 21:57:02.862: INFO: 
STEP: Gathering metrics 04/17/23 21:57:07.869
Apr 17 21:57:07.889: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
Apr 17 21:57:07.892: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.295174ms
Apr 17 21:57:07.892: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
Apr 17 21:57:07.892: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
Apr 17 21:57:07.947: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 17 21:57:07.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ptf7" in namespace "gc-6977"
Apr 17 21:57:07.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-48gsl" in namespace "gc-6977"
Apr 17 21:57:07.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-49bkf" in namespace "gc-6977"
Apr 17 21:57:07.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-49fp2" in namespace "gc-6977"
Apr 17 21:57:07.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kfff" in namespace "gc-6977"
Apr 17 21:57:08.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qgqj" in namespace "gc-6977"
Apr 17 21:57:08.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vd8l" in namespace "gc-6977"
Apr 17 21:57:08.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hpsf" in namespace "gc-6977"
Apr 17 21:57:08.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-67dmd" in namespace "gc-6977"
Apr 17 21:57:08.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cf74" in namespace "gc-6977"
Apr 17 21:57:08.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-6j8fw" in namespace "gc-6977"
Apr 17 21:57:08.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lc64" in namespace "gc-6977"
Apr 17 21:57:08.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wh5z" in namespace "gc-6977"
Apr 17 21:57:08.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h87j" in namespace "gc-6977"
Apr 17 21:57:08.087: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xxh6" in namespace "gc-6977"
Apr 17 21:57:08.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cfcl" in namespace "gc-6977"
Apr 17 21:57:08.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dqmh" in namespace "gc-6977"
Apr 17 21:57:08.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dqz4" in namespace "gc-6977"
Apr 17 21:57:08.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jkp2" in namespace "gc-6977"
Apr 17 21:57:08.136: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7hpz" in namespace "gc-6977"
Apr 17 21:57:08.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7r6j" in namespace "gc-6977"
Apr 17 21:57:08.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-b95xq" in namespace "gc-6977"
Apr 17 21:57:08.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfc6b" in namespace "gc-6977"
Apr 17 21:57:08.174: INFO: Deleting pod "simpletest-rc-to-be-deleted-bs486" in namespace "gc-6977"
Apr 17 21:57:08.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvjt9" in namespace "gc-6977"
Apr 17 21:57:08.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-cv52v" in namespace "gc-6977"
Apr 17 21:57:08.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc26m" in namespace "gc-6977"
Apr 17 21:57:08.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddgtk" in namespace "gc-6977"
Apr 17 21:57:08.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl4j8" in namespace "gc-6977"
Apr 17 21:57:08.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmjkv" in namespace "gc-6977"
Apr 17 21:57:08.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-dngbr" in namespace "gc-6977"
Apr 17 21:57:08.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr5kr" in namespace "gc-6977"
Apr 17 21:57:08.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwmgl" in namespace "gc-6977"
Apr 17 21:57:08.280: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwwch" in namespace "gc-6977"
Apr 17 21:57:08.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd9bw" in namespace "gc-6977"
Apr 17 21:57:08.299: INFO: Deleting pod "simpletest-rc-to-be-deleted-fh2dj" in namespace "gc-6977"
Apr 17 21:57:08.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjgsc" in namespace "gc-6977"
Apr 17 21:57:08.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4mlg" in namespace "gc-6977"
Apr 17 21:57:08.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9q9q" in namespace "gc-6977"
Apr 17 21:57:08.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-hb824" in namespace "gc-6977"
Apr 17 21:57:08.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjpbt" in namespace "gc-6977"
Apr 17 21:57:08.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmlkj" in namespace "gc-6977"
Apr 17 21:57:08.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp25v" in namespace "gc-6977"
Apr 17 21:57:08.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6694" in namespace "gc-6977"
Apr 17 21:57:08.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7cft" in namespace "gc-6977"
Apr 17 21:57:08.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgwh7" in namespace "gc-6977"
Apr 17 21:57:08.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-jj2xh" in namespace "gc-6977"
Apr 17 21:57:08.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk8wx" in namespace "gc-6977"
Apr 17 21:57:08.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-kcpf7" in namespace "gc-6977"
Apr 17 21:57:08.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfx45" in namespace "gc-6977"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 17 21:57:08.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6977" for this suite. 04/17/23 21:57:08.455
------------------------------
• [SLOW TEST] [16.138 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:56:52.323
    Apr 17 21:56:52.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename gc 04/17/23 21:56:52.323
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:56:52.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:56:52.338
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 04/17/23 21:56:52.345
    STEP: create the rc2 04/17/23 21:56:52.348
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 04/17/23 21:56:57.356
    STEP: delete the rc simpletest-rc-to-be-deleted 04/17/23 21:56:57.845
    STEP: wait for the rc to be deleted 04/17/23 21:56:57.85
    Apr 17 21:57:02.862: INFO: 65 pods remaining
    Apr 17 21:57:02.862: INFO: 65 pods has nil DeletionTimestamp
    Apr 17 21:57:02.862: INFO: 
    STEP: Gathering metrics 04/17/23 21:57:07.869
    Apr 17 21:57:07.889: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Apr 17 21:57:07.892: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.295174ms
    Apr 17 21:57:07.892: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
    Apr 17 21:57:07.892: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
    Apr 17 21:57:07.947: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Apr 17 21:57:07.947: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ptf7" in namespace "gc-6977"
    Apr 17 21:57:07.961: INFO: Deleting pod "simpletest-rc-to-be-deleted-48gsl" in namespace "gc-6977"
    Apr 17 21:57:07.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-49bkf" in namespace "gc-6977"
    Apr 17 21:57:07.978: INFO: Deleting pod "simpletest-rc-to-be-deleted-49fp2" in namespace "gc-6977"
    Apr 17 21:57:07.989: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kfff" in namespace "gc-6977"
    Apr 17 21:57:08.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qgqj" in namespace "gc-6977"
    Apr 17 21:57:08.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vd8l" in namespace "gc-6977"
    Apr 17 21:57:08.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hpsf" in namespace "gc-6977"
    Apr 17 21:57:08.034: INFO: Deleting pod "simpletest-rc-to-be-deleted-67dmd" in namespace "gc-6977"
    Apr 17 21:57:08.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cf74" in namespace "gc-6977"
    Apr 17 21:57:08.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-6j8fw" in namespace "gc-6977"
    Apr 17 21:57:08.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lc64" in namespace "gc-6977"
    Apr 17 21:57:08.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wh5z" in namespace "gc-6977"
    Apr 17 21:57:08.079: INFO: Deleting pod "simpletest-rc-to-be-deleted-7h87j" in namespace "gc-6977"
    Apr 17 21:57:08.087: INFO: Deleting pod "simpletest-rc-to-be-deleted-7xxh6" in namespace "gc-6977"
    Apr 17 21:57:08.095: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cfcl" in namespace "gc-6977"
    Apr 17 21:57:08.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dqmh" in namespace "gc-6977"
    Apr 17 21:57:08.116: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dqz4" in namespace "gc-6977"
    Apr 17 21:57:08.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jkp2" in namespace "gc-6977"
    Apr 17 21:57:08.136: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7hpz" in namespace "gc-6977"
    Apr 17 21:57:08.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7r6j" in namespace "gc-6977"
    Apr 17 21:57:08.155: INFO: Deleting pod "simpletest-rc-to-be-deleted-b95xq" in namespace "gc-6977"
    Apr 17 21:57:08.165: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfc6b" in namespace "gc-6977"
    Apr 17 21:57:08.174: INFO: Deleting pod "simpletest-rc-to-be-deleted-bs486" in namespace "gc-6977"
    Apr 17 21:57:08.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvjt9" in namespace "gc-6977"
    Apr 17 21:57:08.196: INFO: Deleting pod "simpletest-rc-to-be-deleted-cv52v" in namespace "gc-6977"
    Apr 17 21:57:08.207: INFO: Deleting pod "simpletest-rc-to-be-deleted-dc26m" in namespace "gc-6977"
    Apr 17 21:57:08.217: INFO: Deleting pod "simpletest-rc-to-be-deleted-ddgtk" in namespace "gc-6977"
    Apr 17 21:57:08.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-dl4j8" in namespace "gc-6977"
    Apr 17 21:57:08.240: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmjkv" in namespace "gc-6977"
    Apr 17 21:57:08.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-dngbr" in namespace "gc-6977"
    Apr 17 21:57:08.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-dr5kr" in namespace "gc-6977"
    Apr 17 21:57:08.270: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwmgl" in namespace "gc-6977"
    Apr 17 21:57:08.280: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwwch" in namespace "gc-6977"
    Apr 17 21:57:08.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-fd9bw" in namespace "gc-6977"
    Apr 17 21:57:08.299: INFO: Deleting pod "simpletest-rc-to-be-deleted-fh2dj" in namespace "gc-6977"
    Apr 17 21:57:08.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjgsc" in namespace "gc-6977"
    Apr 17 21:57:08.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-h4mlg" in namespace "gc-6977"
    Apr 17 21:57:08.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9q9q" in namespace "gc-6977"
    Apr 17 21:57:08.339: INFO: Deleting pod "simpletest-rc-to-be-deleted-hb824" in namespace "gc-6977"
    Apr 17 21:57:08.348: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjpbt" in namespace "gc-6977"
    Apr 17 21:57:08.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-hmlkj" in namespace "gc-6977"
    Apr 17 21:57:08.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-hp25v" in namespace "gc-6977"
    Apr 17 21:57:08.382: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6694" in namespace "gc-6977"
    Apr 17 21:57:08.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7cft" in namespace "gc-6977"
    Apr 17 21:57:08.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgwh7" in namespace "gc-6977"
    Apr 17 21:57:08.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-jj2xh" in namespace "gc-6977"
    Apr 17 21:57:08.419: INFO: Deleting pod "simpletest-rc-to-be-deleted-jk8wx" in namespace "gc-6977"
    Apr 17 21:57:08.428: INFO: Deleting pod "simpletest-rc-to-be-deleted-kcpf7" in namespace "gc-6977"
    Apr 17 21:57:08.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfx45" in namespace "gc-6977"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:57:08.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6977" for this suite. 04/17/23 21:57:08.455
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:57:08.464
Apr 17 21:57:08.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename job 04/17/23 21:57:08.465
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:57:08.478
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:57:08.481
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 04/17/23 21:57:08.483
STEP: Ensuring active pods == parallelism 04/17/23 21:57:08.487
STEP: delete a job 04/17/23 21:57:12.492
STEP: deleting Job.batch foo in namespace job-1573, will wait for the garbage collector to delete the pods 04/17/23 21:57:12.492
Apr 17 21:57:12.550: INFO: Deleting Job.batch foo took: 5.481282ms
Apr 17 21:57:12.650: INFO: Terminating Job.batch foo pods took: 100.423865ms
STEP: Ensuring job was deleted 04/17/23 21:57:43.551
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 17 21:57:43.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-1573" for this suite. 04/17/23 21:57:43.559
------------------------------
• [SLOW TEST] [35.101 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:57:08.464
    Apr 17 21:57:08.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename job 04/17/23 21:57:08.465
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:57:08.478
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:57:08.481
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 04/17/23 21:57:08.483
    STEP: Ensuring active pods == parallelism 04/17/23 21:57:08.487
    STEP: delete a job 04/17/23 21:57:12.492
    STEP: deleting Job.batch foo in namespace job-1573, will wait for the garbage collector to delete the pods 04/17/23 21:57:12.492
    Apr 17 21:57:12.550: INFO: Deleting Job.batch foo took: 5.481282ms
    Apr 17 21:57:12.650: INFO: Terminating Job.batch foo pods took: 100.423865ms
    STEP: Ensuring job was deleted 04/17/23 21:57:43.551
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:57:43.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-1573" for this suite. 04/17/23 21:57:43.559
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:57:43.566
Apr 17 21:57:43.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-pred 04/17/23 21:57:43.566
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:57:43.579
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:57:43.582
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 17 21:57:43.584: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 17 21:57:43.592: INFO: Waiting for terminating namespaces to be deleted...
Apr 17 21:57:43.594: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-106-231.us-west-2.compute.internal before test
Apr 17 21:57:43.603: INFO: calico-node-qwskb from calico-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.603: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:57:43.603: INFO: calico-typha-6dc6fbd9f5-tcn27 from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.603: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:57:43.603: INFO: csi-node-driver-pptcd from calico-system started at 2023-04-17 21:05:10 +0000 UTC (2 container statuses recorded)
Apr 17 21:57:43.603: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:57:43.603: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:57:43.603: INFO: ebs-csi-node-lhxwm from kube-system started at 2023-04-17 21:05:10 +0000 UTC (3 container statuses recorded)
Apr 17 21:57:43.603: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:57:43.603: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:57:43.603: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:57:43.603: INFO: kube-proxy-h249s from kube-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.603: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:57:43.603: INFO: node-feature-discovery-worker-9bc5l from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.603: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:57:43.603: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:57:43.603: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:57:43.603: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:57:43.603: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-64-189.us-west-2.compute.internal before test
Apr 17 21:57:43.613: INFO: calico-node-b7r2c from calico-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.613: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:57:43.613: INFO: csi-node-driver-rzhbn from calico-system started at 2023-04-17 21:05:08 +0000 UTC (2 container statuses recorded)
Apr 17 21:57:43.613: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:57:43.613: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:57:43.613: INFO: ebs-csi-node-7xssn from kube-system started at 2023-04-17 21:05:08 +0000 UTC (3 container statuses recorded)
Apr 17 21:57:43.613: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:57:43.613: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:57:43.613: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:57:43.613: INFO: kube-proxy-j772r from kube-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.613: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:57:43.613: INFO: node-feature-discovery-worker-cbvbq from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.613: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:57:43.613: INFO: sonobuoy-e2e-job-777eb8e1ef7a4686 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:57:43.613: INFO: 	Container e2e ready: true, restart count 0
Apr 17 21:57:43.613: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:57:43.613: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:57:43.613: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:57:43.613: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:57:43.613: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-74-52.us-west-2.compute.internal before test
Apr 17 21:57:43.622: INFO: calico-node-pqj88 from calico-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.622: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:57:43.622: INFO: calico-typha-6dc6fbd9f5-xbmcd from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.622: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:57:43.622: INFO: csi-node-driver-n474j from calico-system started at 2023-04-17 21:05:14 +0000 UTC (2 container statuses recorded)
Apr 17 21:57:43.622: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:57:43.622: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:57:43.622: INFO: pod-csi-inline-volumes from csiinlinevolumes-5470 started at 2023-04-17 21:56:37 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.622: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
Apr 17 21:57:43.622: INFO: ebs-csi-node-xhdpg from kube-system started at 2023-04-17 21:05:14 +0000 UTC (3 container statuses recorded)
Apr 17 21:57:43.622: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:57:43.622: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:57:43.622: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:57:43.622: INFO: kube-proxy-8tbrz from kube-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.622: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:57:43.622: INFO: node-feature-discovery-worker-bl2dc from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.622: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:57:43.622: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:57:43.622: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:57:43.622: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:57:43.622: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-93-18.us-west-2.compute.internal before test
Apr 17 21:57:43.630: INFO: calico-node-zbl7g from calico-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.630: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:57:43.630: INFO: csi-node-driver-58wnm from calico-system started at 2023-04-17 21:05:13 +0000 UTC (2 container statuses recorded)
Apr 17 21:57:43.630: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:57:43.630: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:57:43.630: INFO: ebs-csi-node-bqd6h from kube-system started at 2023-04-17 21:05:13 +0000 UTC (3 container statuses recorded)
Apr 17 21:57:43.630: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:57:43.630: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:57:43.630: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:57:43.630: INFO: kube-proxy-88qrm from kube-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.630: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:57:43.630: INFO: node-feature-discovery-worker-m4mrl from node-feature-discovery started at 2023-04-17 21:05:56 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.630: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:57:43.630: INFO: sonobuoy from sonobuoy started at 2023-04-17 21:09:00 +0000 UTC (1 container statuses recorded)
Apr 17 21:57:43.630: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 17 21:57:43.630: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:57:43.630: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:57:43.630: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 04/17/23 21:57:43.631
Apr 17 21:57:43.638: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2952" to be "running"
Apr 17 21:57:43.640: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.408379ms
Apr 17 21:57:45.643: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005400816s
Apr 17 21:57:45.643: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 04/17/23 21:57:45.646
STEP: Trying to apply a random label on the found node. 04/17/23 21:57:45.655
STEP: verifying the node has the label kubernetes.io/e2e-65afdcf0-e8ea-4c16-bfe4-1ab419bd4c78 42 04/17/23 21:57:45.666
STEP: Trying to relaunch the pod, now with labels. 04/17/23 21:57:45.67
Apr 17 21:57:45.675: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2952" to be "not pending"
Apr 17 21:57:45.678: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110401ms
Apr 17 21:57:47.681: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.006587619s
Apr 17 21:57:47.681: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-65afdcf0-e8ea-4c16-bfe4-1ab419bd4c78 off the node ip-10-0-93-18.us-west-2.compute.internal 04/17/23 21:57:47.684
STEP: verifying the node doesn't have the label kubernetes.io/e2e-65afdcf0-e8ea-4c16-bfe4-1ab419bd4c78 04/17/23 21:57:47.702
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:57:47.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2952" for this suite. 04/17/23 21:57:47.713
------------------------------
• [4.154 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:57:43.566
    Apr 17 21:57:43.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-pred 04/17/23 21:57:43.566
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:57:43.579
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:57:43.582
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 17 21:57:43.584: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 17 21:57:43.592: INFO: Waiting for terminating namespaces to be deleted...
    Apr 17 21:57:43.594: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-106-231.us-west-2.compute.internal before test
    Apr 17 21:57:43.603: INFO: calico-node-qwskb from calico-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.603: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: calico-typha-6dc6fbd9f5-tcn27 from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.603: INFO: 	Container calico-typha ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: csi-node-driver-pptcd from calico-system started at 2023-04-17 21:05:10 +0000 UTC (2 container statuses recorded)
    Apr 17 21:57:43.603: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: ebs-csi-node-lhxwm from kube-system started at 2023-04-17 21:05:10 +0000 UTC (3 container statuses recorded)
    Apr 17 21:57:43.603: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: kube-proxy-h249s from kube-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.603: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: node-feature-discovery-worker-9bc5l from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.603: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:57:43.603: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:57:43.603: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-64-189.us-west-2.compute.internal before test
    Apr 17 21:57:43.613: INFO: calico-node-b7r2c from calico-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.613: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: csi-node-driver-rzhbn from calico-system started at 2023-04-17 21:05:08 +0000 UTC (2 container statuses recorded)
    Apr 17 21:57:43.613: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: ebs-csi-node-7xssn from kube-system started at 2023-04-17 21:05:08 +0000 UTC (3 container statuses recorded)
    Apr 17 21:57:43.613: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: kube-proxy-j772r from kube-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.613: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: node-feature-discovery-worker-cbvbq from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.613: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: sonobuoy-e2e-job-777eb8e1ef7a4686 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:57:43.613: INFO: 	Container e2e ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:57:43.613: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:57:43.613: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-74-52.us-west-2.compute.internal before test
    Apr 17 21:57:43.622: INFO: calico-node-pqj88 from calico-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.622: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: calico-typha-6dc6fbd9f5-xbmcd from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.622: INFO: 	Container calico-typha ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: csi-node-driver-n474j from calico-system started at 2023-04-17 21:05:14 +0000 UTC (2 container statuses recorded)
    Apr 17 21:57:43.622: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: pod-csi-inline-volumes from csiinlinevolumes-5470 started at 2023-04-17 21:56:37 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.622: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
    Apr 17 21:57:43.622: INFO: ebs-csi-node-xhdpg from kube-system started at 2023-04-17 21:05:14 +0000 UTC (3 container statuses recorded)
    Apr 17 21:57:43.622: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: kube-proxy-8tbrz from kube-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.622: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: node-feature-discovery-worker-bl2dc from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.622: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:57:43.622: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:57:43.622: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-93-18.us-west-2.compute.internal before test
    Apr 17 21:57:43.630: INFO: calico-node-zbl7g from calico-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.630: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: csi-node-driver-58wnm from calico-system started at 2023-04-17 21:05:13 +0000 UTC (2 container statuses recorded)
    Apr 17 21:57:43.630: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: ebs-csi-node-bqd6h from kube-system started at 2023-04-17 21:05:13 +0000 UTC (3 container statuses recorded)
    Apr 17 21:57:43.630: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: kube-proxy-88qrm from kube-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.630: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: node-feature-discovery-worker-m4mrl from node-feature-discovery started at 2023-04-17 21:05:56 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.630: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: sonobuoy from sonobuoy started at 2023-04-17 21:09:00 +0000 UTC (1 container statuses recorded)
    Apr 17 21:57:43.630: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:57:43.630: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:57:43.630: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 04/17/23 21:57:43.631
    Apr 17 21:57:43.638: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2952" to be "running"
    Apr 17 21:57:43.640: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.408379ms
    Apr 17 21:57:45.643: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.005400816s
    Apr 17 21:57:45.643: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 04/17/23 21:57:45.646
    STEP: Trying to apply a random label on the found node. 04/17/23 21:57:45.655
    STEP: verifying the node has the label kubernetes.io/e2e-65afdcf0-e8ea-4c16-bfe4-1ab419bd4c78 42 04/17/23 21:57:45.666
    STEP: Trying to relaunch the pod, now with labels. 04/17/23 21:57:45.67
    Apr 17 21:57:45.675: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2952" to be "not pending"
    Apr 17 21:57:45.678: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110401ms
    Apr 17 21:57:47.681: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.006587619s
    Apr 17 21:57:47.681: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-65afdcf0-e8ea-4c16-bfe4-1ab419bd4c78 off the node ip-10-0-93-18.us-west-2.compute.internal 04/17/23 21:57:47.684
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-65afdcf0-e8ea-4c16-bfe4-1ab419bd4c78 04/17/23 21:57:47.702
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:57:47.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2952" for this suite. 04/17/23 21:57:47.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:57:47.72
Apr 17 21:57:47.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replicaset 04/17/23 21:57:47.721
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:57:47.735
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:57:47.737
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Apr 17 21:57:47.752: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 17 21:57:52.758: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/17/23 21:57:52.758
STEP: Scaling up "test-rs" replicaset  04/17/23 21:57:52.758
Apr 17 21:57:52.765: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 04/17/23 21:57:52.765
W0417 21:57:52.772545      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr 17 21:57:52.773: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 1, AvailableReplicas 1
Apr 17 21:57:52.788: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 1, AvailableReplicas 1
Apr 17 21:57:52.801: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 1, AvailableReplicas 1
Apr 17 21:57:52.807: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 1, AvailableReplicas 1
Apr 17 21:57:54.285: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 2, AvailableReplicas 2
Apr 17 21:57:54.444: INFO: observed Replicaset test-rs in namespace replicaset-9662 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 17 21:57:54.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9662" for this suite. 04/17/23 21:57:54.449
------------------------------
• [SLOW TEST] [6.734 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:57:47.72
    Apr 17 21:57:47.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replicaset 04/17/23 21:57:47.721
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:57:47.735
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:57:47.737
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Apr 17 21:57:47.752: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 17 21:57:52.758: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/17/23 21:57:52.758
    STEP: Scaling up "test-rs" replicaset  04/17/23 21:57:52.758
    Apr 17 21:57:52.765: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 04/17/23 21:57:52.765
    W0417 21:57:52.772545      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr 17 21:57:52.773: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 1, AvailableReplicas 1
    Apr 17 21:57:52.788: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 1, AvailableReplicas 1
    Apr 17 21:57:52.801: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 1, AvailableReplicas 1
    Apr 17 21:57:52.807: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 1, AvailableReplicas 1
    Apr 17 21:57:54.285: INFO: observed ReplicaSet test-rs in namespace replicaset-9662 with ReadyReplicas 2, AvailableReplicas 2
    Apr 17 21:57:54.444: INFO: observed Replicaset test-rs in namespace replicaset-9662 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:57:54.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9662" for this suite. 04/17/23 21:57:54.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:57:54.455
Apr 17 21:57:54.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 21:57:54.456
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:57:54.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:57:54.472
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-9947 04/17/23 21:57:54.474
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[] 04/17/23 21:57:54.617
Apr 17 21:57:54.623: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Apr 17 21:57:55.629: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9947 04/17/23 21:57:55.629
Apr 17 21:57:55.637: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9947" to be "running and ready"
Apr 17 21:57:55.639: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.436628ms
Apr 17 21:57:55.639: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:57:57.642: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005788103s
Apr 17 21:57:57.643: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr 17 21:57:57.643: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[pod1:[80]] 04/17/23 21:57:57.645
Apr 17 21:57:57.652: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 04/17/23 21:57:57.652
Apr 17 21:57:57.652: INFO: Creating new exec pod
Apr 17 21:57:57.656: INFO: Waiting up to 5m0s for pod "execpodc78t2" in namespace "services-9947" to be "running"
Apr 17 21:57:57.659: INFO: Pod "execpodc78t2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.302855ms
Apr 17 21:57:59.665: INFO: Pod "execpodc78t2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008918473s
Apr 17 21:57:59.665: INFO: Pod "execpodc78t2" satisfied condition "running"
Apr 17 21:58:00.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Apr 17 21:58:00.815: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 17 21:58:00.815: INFO: stdout: ""
Apr 17 21:58:00.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 10.105.17.253 80'
Apr 17 21:58:00.953: INFO: stderr: "+ nc -v -z -w 2 10.105.17.253 80\nConnection to 10.105.17.253 80 port [tcp/http] succeeded!\n"
Apr 17 21:58:00.953: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-9947 04/17/23 21:58:00.953
Apr 17 21:58:00.959: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9947" to be "running and ready"
Apr 17 21:58:00.962: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024945ms
Apr 17 21:58:00.962: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 21:58:02.966: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006807178s
Apr 17 21:58:02.966: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr 17 21:58:02.966: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[pod1:[80] pod2:[80]] 04/17/23 21:58:02.968
Apr 17 21:58:02.977: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 04/17/23 21:58:02.977
Apr 17 21:58:03.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Apr 17 21:58:04.118: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 17 21:58:04.118: INFO: stdout: ""
Apr 17 21:58:04.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 10.105.17.253 80'
Apr 17 21:58:04.245: INFO: stderr: "+ nc -v -z -w 2 10.105.17.253 80\nConnection to 10.105.17.253 80 port [tcp/http] succeeded!\n"
Apr 17 21:58:04.245: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9947 04/17/23 21:58:04.245
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[pod2:[80]] 04/17/23 21:58:04.258
Apr 17 21:58:04.269: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 04/17/23 21:58:04.269
Apr 17 21:58:05.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Apr 17 21:58:05.414: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Apr 17 21:58:05.414: INFO: stdout: ""
Apr 17 21:58:05.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 10.105.17.253 80'
Apr 17 21:58:05.557: INFO: stderr: "+ nc -v -z -w 2 10.105.17.253 80\nConnection to 10.105.17.253 80 port [tcp/http] succeeded!\n"
Apr 17 21:58:05.557: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-9947 04/17/23 21:58:05.557
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[] 04/17/23 21:58:05.571
Apr 17 21:58:05.577: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 21:58:05.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9947" for this suite. 04/17/23 21:58:05.6
------------------------------
• [SLOW TEST] [11.150 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:57:54.455
    Apr 17 21:57:54.455: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 21:57:54.456
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:57:54.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:57:54.472
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-9947 04/17/23 21:57:54.474
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[] 04/17/23 21:57:54.617
    Apr 17 21:57:54.623: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Apr 17 21:57:55.629: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9947 04/17/23 21:57:55.629
    Apr 17 21:57:55.637: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9947" to be "running and ready"
    Apr 17 21:57:55.639: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.436628ms
    Apr 17 21:57:55.639: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:57:57.642: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005788103s
    Apr 17 21:57:57.643: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr 17 21:57:57.643: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[pod1:[80]] 04/17/23 21:57:57.645
    Apr 17 21:57:57.652: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 04/17/23 21:57:57.652
    Apr 17 21:57:57.652: INFO: Creating new exec pod
    Apr 17 21:57:57.656: INFO: Waiting up to 5m0s for pod "execpodc78t2" in namespace "services-9947" to be "running"
    Apr 17 21:57:57.659: INFO: Pod "execpodc78t2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.302855ms
    Apr 17 21:57:59.665: INFO: Pod "execpodc78t2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008918473s
    Apr 17 21:57:59.665: INFO: Pod "execpodc78t2" satisfied condition "running"
    Apr 17 21:58:00.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Apr 17 21:58:00.815: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr 17 21:58:00.815: INFO: stdout: ""
    Apr 17 21:58:00.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 10.105.17.253 80'
    Apr 17 21:58:00.953: INFO: stderr: "+ nc -v -z -w 2 10.105.17.253 80\nConnection to 10.105.17.253 80 port [tcp/http] succeeded!\n"
    Apr 17 21:58:00.953: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-9947 04/17/23 21:58:00.953
    Apr 17 21:58:00.959: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9947" to be "running and ready"
    Apr 17 21:58:00.962: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024945ms
    Apr 17 21:58:00.962: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 21:58:02.966: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006807178s
    Apr 17 21:58:02.966: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr 17 21:58:02.966: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[pod1:[80] pod2:[80]] 04/17/23 21:58:02.968
    Apr 17 21:58:02.977: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 04/17/23 21:58:02.977
    Apr 17 21:58:03.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Apr 17 21:58:04.118: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr 17 21:58:04.118: INFO: stdout: ""
    Apr 17 21:58:04.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 10.105.17.253 80'
    Apr 17 21:58:04.245: INFO: stderr: "+ nc -v -z -w 2 10.105.17.253 80\nConnection to 10.105.17.253 80 port [tcp/http] succeeded!\n"
    Apr 17 21:58:04.245: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9947 04/17/23 21:58:04.245
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[pod2:[80]] 04/17/23 21:58:04.258
    Apr 17 21:58:04.269: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 04/17/23 21:58:04.269
    Apr 17 21:58:05.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Apr 17 21:58:05.414: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Apr 17 21:58:05.414: INFO: stdout: ""
    Apr 17 21:58:05.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9947 exec execpodc78t2 -- /bin/sh -x -c nc -v -z -w 2 10.105.17.253 80'
    Apr 17 21:58:05.557: INFO: stderr: "+ nc -v -z -w 2 10.105.17.253 80\nConnection to 10.105.17.253 80 port [tcp/http] succeeded!\n"
    Apr 17 21:58:05.557: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-9947 04/17/23 21:58:05.557
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9947 to expose endpoints map[] 04/17/23 21:58:05.571
    Apr 17 21:58:05.577: INFO: successfully validated that service endpoint-test2 in namespace services-9947 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:58:05.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9947" for this suite. 04/17/23 21:58:05.6
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:58:05.606
Apr 17 21:58:05.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replication-controller 04/17/23 21:58:05.606
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:05.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:05.625
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 04/17/23 21:58:05.627
STEP: When the matched label of one of its pods change 04/17/23 21:58:05.632
Apr 17 21:58:05.634: INFO: Pod name pod-release: Found 0 pods out of 1
Apr 17 21:58:10.639: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 04/17/23 21:58:10.647
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 17 21:58:11.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5270" for this suite. 04/17/23 21:58:11.657
------------------------------
• [SLOW TEST] [6.056 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:58:05.606
    Apr 17 21:58:05.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replication-controller 04/17/23 21:58:05.606
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:05.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:05.625
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 04/17/23 21:58:05.627
    STEP: When the matched label of one of its pods change 04/17/23 21:58:05.632
    Apr 17 21:58:05.634: INFO: Pod name pod-release: Found 0 pods out of 1
    Apr 17 21:58:10.639: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 04/17/23 21:58:10.647
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:58:11.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5270" for this suite. 04/17/23 21:58:11.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:58:11.662
Apr 17 21:58:11.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-pred 04/17/23 21:58:11.663
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:11.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:11.678
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Apr 17 21:58:11.680: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr 17 21:58:11.687: INFO: Waiting for terminating namespaces to be deleted...
Apr 17 21:58:11.690: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-106-231.us-west-2.compute.internal before test
Apr 17 21:58:11.699: INFO: calico-node-qwskb from calico-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.699: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:58:11.699: INFO: calico-typha-6dc6fbd9f5-tcn27 from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.699: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:58:11.699: INFO: csi-node-driver-pptcd from calico-system started at 2023-04-17 21:05:10 +0000 UTC (2 container statuses recorded)
Apr 17 21:58:11.699: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:58:11.699: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:58:11.699: INFO: ebs-csi-node-lhxwm from kube-system started at 2023-04-17 21:05:10 +0000 UTC (3 container statuses recorded)
Apr 17 21:58:11.699: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:58:11.699: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:58:11.699: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:58:11.699: INFO: kube-proxy-h249s from kube-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.699: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:58:11.699: INFO: node-feature-discovery-worker-9bc5l from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.699: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:58:11.699: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:58:11.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:58:11.699: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:58:11.699: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-64-189.us-west-2.compute.internal before test
Apr 17 21:58:11.709: INFO: calico-node-b7r2c from calico-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.709: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:58:11.709: INFO: csi-node-driver-rzhbn from calico-system started at 2023-04-17 21:05:08 +0000 UTC (2 container statuses recorded)
Apr 17 21:58:11.709: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:58:11.709: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:58:11.709: INFO: ebs-csi-node-7xssn from kube-system started at 2023-04-17 21:05:08 +0000 UTC (3 container statuses recorded)
Apr 17 21:58:11.709: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:58:11.709: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:58:11.709: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:58:11.709: INFO: kube-proxy-j772r from kube-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.709: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:58:11.709: INFO: node-feature-discovery-worker-cbvbq from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.709: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:58:11.709: INFO: sonobuoy-e2e-job-777eb8e1ef7a4686 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:58:11.709: INFO: 	Container e2e ready: true, restart count 0
Apr 17 21:58:11.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:58:11.709: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:58:11.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:58:11.709: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:58:11.709: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-74-52.us-west-2.compute.internal before test
Apr 17 21:58:11.720: INFO: calico-node-pqj88 from calico-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.720: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:58:11.720: INFO: calico-typha-6dc6fbd9f5-xbmcd from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.720: INFO: 	Container calico-typha ready: true, restart count 0
Apr 17 21:58:11.720: INFO: csi-node-driver-n474j from calico-system started at 2023-04-17 21:05:14 +0000 UTC (2 container statuses recorded)
Apr 17 21:58:11.720: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:58:11.720: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:58:11.720: INFO: pod-csi-inline-volumes from csiinlinevolumes-5470 started at 2023-04-17 21:56:37 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.720: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
Apr 17 21:58:11.720: INFO: ebs-csi-node-xhdpg from kube-system started at 2023-04-17 21:05:14 +0000 UTC (3 container statuses recorded)
Apr 17 21:58:11.720: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:58:11.720: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:58:11.720: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:58:11.720: INFO: kube-proxy-8tbrz from kube-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.720: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:58:11.720: INFO: node-feature-discovery-worker-bl2dc from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.720: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:58:11.720: INFO: pod-release-gzkvq from replication-controller-5270 started at 2023-04-17 21:58:05 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.720: INFO: 	Container pod-release ready: true, restart count 0
Apr 17 21:58:11.720: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:58:11.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:58:11.720: INFO: 	Container systemd-logs ready: true, restart count 0
Apr 17 21:58:11.720: INFO: 
Logging pods the apiserver thinks is on node ip-10-0-93-18.us-west-2.compute.internal before test
Apr 17 21:58:11.731: INFO: calico-node-zbl7g from calico-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.731: INFO: 	Container calico-node ready: true, restart count 0
Apr 17 21:58:11.731: INFO: csi-node-driver-58wnm from calico-system started at 2023-04-17 21:05:13 +0000 UTC (2 container statuses recorded)
Apr 17 21:58:11.731: INFO: 	Container calico-csi ready: true, restart count 0
Apr 17 21:58:11.731: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
Apr 17 21:58:11.731: INFO: ebs-csi-node-bqd6h from kube-system started at 2023-04-17 21:05:13 +0000 UTC (3 container statuses recorded)
Apr 17 21:58:11.731: INFO: 	Container ebs-plugin ready: true, restart count 0
Apr 17 21:58:11.731: INFO: 	Container liveness-probe ready: true, restart count 0
Apr 17 21:58:11.731: INFO: 	Container node-driver-registrar ready: true, restart count 0
Apr 17 21:58:11.731: INFO: kube-proxy-88qrm from kube-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.731: INFO: 	Container kube-proxy ready: true, restart count 0
Apr 17 21:58:11.731: INFO: node-feature-discovery-worker-m4mrl from node-feature-discovery started at 2023-04-17 21:05:56 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.731: INFO: 	Container worker ready: true, restart count 0
Apr 17 21:58:11.731: INFO: pod-release-qqz2k from replication-controller-5270 started at 2023-04-17 21:58:10 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.731: INFO: 	Container pod-release ready: false, restart count 0
Apr 17 21:58:11.731: INFO: sonobuoy from sonobuoy started at 2023-04-17 21:09:00 +0000 UTC (1 container statuses recorded)
Apr 17 21:58:11.731: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr 17 21:58:11.731: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
Apr 17 21:58:11.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr 17 21:58:11.731: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 04/17/23 21:58:11.731
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1756d7b6fad42173], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/7 nodes are available: 7 Preemption is not helpful for scheduling..] 04/17/23 21:58:11.765
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:58:12.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-4535" for this suite. 04/17/23 21:58:12.767
------------------------------
• [1.109 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:58:11.662
    Apr 17 21:58:11.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-pred 04/17/23 21:58:11.663
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:11.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:11.678
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Apr 17 21:58:11.680: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Apr 17 21:58:11.687: INFO: Waiting for terminating namespaces to be deleted...
    Apr 17 21:58:11.690: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-106-231.us-west-2.compute.internal before test
    Apr 17 21:58:11.699: INFO: calico-node-qwskb from calico-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.699: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: calico-typha-6dc6fbd9f5-tcn27 from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.699: INFO: 	Container calico-typha ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: csi-node-driver-pptcd from calico-system started at 2023-04-17 21:05:10 +0000 UTC (2 container statuses recorded)
    Apr 17 21:58:11.699: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: ebs-csi-node-lhxwm from kube-system started at 2023-04-17 21:05:10 +0000 UTC (3 container statuses recorded)
    Apr 17 21:58:11.699: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: kube-proxy-h249s from kube-system started at 2023-04-17 21:05:10 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.699: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: node-feature-discovery-worker-9bc5l from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.699: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:58:11.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:58:11.699: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-64-189.us-west-2.compute.internal before test
    Apr 17 21:58:11.709: INFO: calico-node-b7r2c from calico-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.709: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: csi-node-driver-rzhbn from calico-system started at 2023-04-17 21:05:08 +0000 UTC (2 container statuses recorded)
    Apr 17 21:58:11.709: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: ebs-csi-node-7xssn from kube-system started at 2023-04-17 21:05:08 +0000 UTC (3 container statuses recorded)
    Apr 17 21:58:11.709: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: kube-proxy-j772r from kube-system started at 2023-04-17 21:05:08 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.709: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: node-feature-discovery-worker-cbvbq from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.709: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: sonobuoy-e2e-job-777eb8e1ef7a4686 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:58:11.709: INFO: 	Container e2e ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-b2zcv from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:58:11.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:58:11.709: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-74-52.us-west-2.compute.internal before test
    Apr 17 21:58:11.720: INFO: calico-node-pqj88 from calico-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.720: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: calico-typha-6dc6fbd9f5-xbmcd from calico-system started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.720: INFO: 	Container calico-typha ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: csi-node-driver-n474j from calico-system started at 2023-04-17 21:05:14 +0000 UTC (2 container statuses recorded)
    Apr 17 21:58:11.720: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: pod-csi-inline-volumes from csiinlinevolumes-5470 started at 2023-04-17 21:56:37 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.720: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
    Apr 17 21:58:11.720: INFO: ebs-csi-node-xhdpg from kube-system started at 2023-04-17 21:05:14 +0000 UTC (3 container statuses recorded)
    Apr 17 21:58:11.720: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: kube-proxy-8tbrz from kube-system started at 2023-04-17 21:05:14 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.720: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: node-feature-discovery-worker-bl2dc from node-feature-discovery started at 2023-04-17 21:05:55 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.720: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: pod-release-gzkvq from replication-controller-5270 started at 2023-04-17 21:58:05 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.720: INFO: 	Container pod-release ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-zrvdz from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:58:11.720: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: 	Container systemd-logs ready: true, restart count 0
    Apr 17 21:58:11.720: INFO: 
    Logging pods the apiserver thinks is on node ip-10-0-93-18.us-west-2.compute.internal before test
    Apr 17 21:58:11.731: INFO: calico-node-zbl7g from calico-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.731: INFO: 	Container calico-node ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: csi-node-driver-58wnm from calico-system started at 2023-04-17 21:05:13 +0000 UTC (2 container statuses recorded)
    Apr 17 21:58:11.731: INFO: 	Container calico-csi ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: 	Container csi-node-driver-registrar ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: ebs-csi-node-bqd6h from kube-system started at 2023-04-17 21:05:13 +0000 UTC (3 container statuses recorded)
    Apr 17 21:58:11.731: INFO: 	Container ebs-plugin ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: 	Container liveness-probe ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: 	Container node-driver-registrar ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: kube-proxy-88qrm from kube-system started at 2023-04-17 21:05:13 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.731: INFO: 	Container kube-proxy ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: node-feature-discovery-worker-m4mrl from node-feature-discovery started at 2023-04-17 21:05:56 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.731: INFO: 	Container worker ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: pod-release-qqz2k from replication-controller-5270 started at 2023-04-17 21:58:10 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.731: INFO: 	Container pod-release ready: false, restart count 0
    Apr 17 21:58:11.731: INFO: sonobuoy from sonobuoy started at 2023-04-17 21:09:00 +0000 UTC (1 container statuses recorded)
    Apr 17 21:58:11.731: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-4gzc5 from sonobuoy started at 2023-04-17 21:09:05 +0000 UTC (2 container statuses recorded)
    Apr 17 21:58:11.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Apr 17 21:58:11.731: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 04/17/23 21:58:11.731
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.1756d7b6fad42173], Reason = [FailedScheduling], Message = [0/7 nodes are available: 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 4 node(s) didn't match Pod's node affinity/selector. preemption: 0/7 nodes are available: 7 Preemption is not helpful for scheduling..] 04/17/23 21:58:11.765
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:58:12.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-4535" for this suite. 04/17/23 21:58:12.767
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:58:12.772
Apr 17 21:58:12.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 21:58:12.772
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:12.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:12.788
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9479 04/17/23 21:58:12.79
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/17/23 21:58:12.8
STEP: creating service externalsvc in namespace services-9479 04/17/23 21:58:12.8
STEP: creating replication controller externalsvc in namespace services-9479 04/17/23 21:58:12.817
I0417 21:58:12.824460      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9479, replica count: 2
I0417 21:58:15.876475      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 04/17/23 21:58:15.879
Apr 17 21:58:15.894: INFO: Creating new exec pod
Apr 17 21:58:15.901: INFO: Waiting up to 5m0s for pod "execpods97nf" in namespace "services-9479" to be "running"
Apr 17 21:58:15.903: INFO: Pod "execpods97nf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.636992ms
Apr 17 21:58:17.907: INFO: Pod "execpods97nf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006236414s
Apr 17 21:58:17.907: INFO: Pod "execpods97nf" satisfied condition "running"
Apr 17 21:58:17.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9479 exec execpods97nf -- /bin/sh -x -c nslookup clusterip-service.services-9479.svc.cluster.local'
Apr 17 21:58:18.062: INFO: stderr: "+ nslookup clusterip-service.services-9479.svc.cluster.local\n"
Apr 17 21:58:18.062: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9479.svc.cluster.local\tcanonical name = externalsvc.services-9479.svc.cluster.local.\nName:\texternalsvc.services-9479.svc.cluster.local\nAddress: 10.101.83.140\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9479, will wait for the garbage collector to delete the pods 04/17/23 21:58:18.062
Apr 17 21:58:18.120: INFO: Deleting ReplicationController externalsvc took: 4.922232ms
Apr 17 21:58:18.221: INFO: Terminating ReplicationController externalsvc pods took: 100.918739ms
Apr 17 21:58:20.437: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 21:58:20.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9479" for this suite. 04/17/23 21:58:20.451
------------------------------
• [SLOW TEST] [7.685 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:58:12.772
    Apr 17 21:58:12.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 21:58:12.772
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:12.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:12.788
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9479 04/17/23 21:58:12.79
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/17/23 21:58:12.8
    STEP: creating service externalsvc in namespace services-9479 04/17/23 21:58:12.8
    STEP: creating replication controller externalsvc in namespace services-9479 04/17/23 21:58:12.817
    I0417 21:58:12.824460      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9479, replica count: 2
    I0417 21:58:15.876475      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 04/17/23 21:58:15.879
    Apr 17 21:58:15.894: INFO: Creating new exec pod
    Apr 17 21:58:15.901: INFO: Waiting up to 5m0s for pod "execpods97nf" in namespace "services-9479" to be "running"
    Apr 17 21:58:15.903: INFO: Pod "execpods97nf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.636992ms
    Apr 17 21:58:17.907: INFO: Pod "execpods97nf": Phase="Running", Reason="", readiness=true. Elapsed: 2.006236414s
    Apr 17 21:58:17.907: INFO: Pod "execpods97nf" satisfied condition "running"
    Apr 17 21:58:17.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9479 exec execpods97nf -- /bin/sh -x -c nslookup clusterip-service.services-9479.svc.cluster.local'
    Apr 17 21:58:18.062: INFO: stderr: "+ nslookup clusterip-service.services-9479.svc.cluster.local\n"
    Apr 17 21:58:18.062: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9479.svc.cluster.local\tcanonical name = externalsvc.services-9479.svc.cluster.local.\nName:\texternalsvc.services-9479.svc.cluster.local\nAddress: 10.101.83.140\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9479, will wait for the garbage collector to delete the pods 04/17/23 21:58:18.062
    Apr 17 21:58:18.120: INFO: Deleting ReplicationController externalsvc took: 4.922232ms
    Apr 17 21:58:18.221: INFO: Terminating ReplicationController externalsvc pods took: 100.918739ms
    Apr 17 21:58:20.437: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:58:20.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9479" for this suite. 04/17/23 21:58:20.451
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:58:20.457
Apr 17 21:58:20.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename subpath 04/17/23 21:58:20.458
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:20.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:20.472
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/17/23 21:58:20.474
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-nmvt 04/17/23 21:58:20.481
STEP: Creating a pod to test atomic-volume-subpath 04/17/23 21:58:20.481
Apr 17 21:58:20.486: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nmvt" in namespace "subpath-5956" to be "Succeeded or Failed"
Apr 17 21:58:20.489: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421771ms
Apr 17 21:58:22.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 2.005516946s
Apr 17 21:58:24.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 4.005845092s
Apr 17 21:58:26.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 6.006010411s
Apr 17 21:58:28.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 8.005430166s
Apr 17 21:58:30.493: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 10.006211382s
Apr 17 21:58:32.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 12.006030864s
Apr 17 21:58:34.493: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 14.006377716s
Apr 17 21:58:36.495: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 16.008169239s
Apr 17 21:58:38.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 18.005542482s
Apr 17 21:58:40.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 20.005834509s
Apr 17 21:58:42.493: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=false. Elapsed: 22.006347622s
Apr 17 21:58:44.493: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006439702s
STEP: Saw pod success 04/17/23 21:58:44.493
Apr 17 21:58:44.493: INFO: Pod "pod-subpath-test-secret-nmvt" satisfied condition "Succeeded or Failed"
Apr 17 21:58:44.495: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-subpath-test-secret-nmvt container test-container-subpath-secret-nmvt: <nil>
STEP: delete the pod 04/17/23 21:58:44.507
Apr 17 21:58:44.516: INFO: Waiting for pod pod-subpath-test-secret-nmvt to disappear
Apr 17 21:58:44.519: INFO: Pod pod-subpath-test-secret-nmvt no longer exists
STEP: Deleting pod pod-subpath-test-secret-nmvt 04/17/23 21:58:44.519
Apr 17 21:58:44.519: INFO: Deleting pod "pod-subpath-test-secret-nmvt" in namespace "subpath-5956"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 17 21:58:44.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5956" for this suite. 04/17/23 21:58:44.525
------------------------------
• [SLOW TEST] [24.074 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:58:20.457
    Apr 17 21:58:20.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename subpath 04/17/23 21:58:20.458
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:20.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:20.472
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/17/23 21:58:20.474
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-nmvt 04/17/23 21:58:20.481
    STEP: Creating a pod to test atomic-volume-subpath 04/17/23 21:58:20.481
    Apr 17 21:58:20.486: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-nmvt" in namespace "subpath-5956" to be "Succeeded or Failed"
    Apr 17 21:58:20.489: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421771ms
    Apr 17 21:58:22.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 2.005516946s
    Apr 17 21:58:24.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 4.005845092s
    Apr 17 21:58:26.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 6.006010411s
    Apr 17 21:58:28.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 8.005430166s
    Apr 17 21:58:30.493: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 10.006211382s
    Apr 17 21:58:32.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 12.006030864s
    Apr 17 21:58:34.493: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 14.006377716s
    Apr 17 21:58:36.495: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 16.008169239s
    Apr 17 21:58:38.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 18.005542482s
    Apr 17 21:58:40.492: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=true. Elapsed: 20.005834509s
    Apr 17 21:58:42.493: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Running", Reason="", readiness=false. Elapsed: 22.006347622s
    Apr 17 21:58:44.493: INFO: Pod "pod-subpath-test-secret-nmvt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006439702s
    STEP: Saw pod success 04/17/23 21:58:44.493
    Apr 17 21:58:44.493: INFO: Pod "pod-subpath-test-secret-nmvt" satisfied condition "Succeeded or Failed"
    Apr 17 21:58:44.495: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-subpath-test-secret-nmvt container test-container-subpath-secret-nmvt: <nil>
    STEP: delete the pod 04/17/23 21:58:44.507
    Apr 17 21:58:44.516: INFO: Waiting for pod pod-subpath-test-secret-nmvt to disappear
    Apr 17 21:58:44.519: INFO: Pod pod-subpath-test-secret-nmvt no longer exists
    STEP: Deleting pod pod-subpath-test-secret-nmvt 04/17/23 21:58:44.519
    Apr 17 21:58:44.519: INFO: Deleting pod "pod-subpath-test-secret-nmvt" in namespace "subpath-5956"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:58:44.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5956" for this suite. 04/17/23 21:58:44.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:58:44.531
Apr 17 21:58:44.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 21:58:44.532
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:44.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:44.545
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Apr 17 21:58:44.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 21:58:51.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8321" for this suite. 04/17/23 21:58:51.015
------------------------------
• [SLOW TEST] [6.488 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:58:44.531
    Apr 17 21:58:44.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 21:58:44.532
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:44.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:44.545
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Apr 17 21:58:44.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 21:58:51.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8321" for this suite. 04/17/23 21:58:51.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 21:58:51.02
Apr 17 21:58:51.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-probe 04/17/23 21:58:51.021
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:51.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:51.037
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec in namespace container-probe-838 04/17/23 21:58:51.039
Apr 17 21:58:51.045: INFO: Waiting up to 5m0s for pod "busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec" in namespace "container-probe-838" to be "not pending"
Apr 17 21:58:51.050: INFO: Pod "busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.054172ms
Apr 17 21:58:53.054: INFO: Pod "busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008126774s
Apr 17 21:58:53.054: INFO: Pod "busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec" satisfied condition "not pending"
Apr 17 21:58:53.054: INFO: Started pod busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec in namespace container-probe-838
STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 21:58:53.054
Apr 17 21:58:53.056: INFO: Initial restart count of pod busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec is 0
STEP: deleting the pod 04/17/23 22:02:53.496
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 17 22:02:53.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-838" for this suite. 04/17/23 22:02:53.513
------------------------------
• [SLOW TEST] [242.498 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 21:58:51.02
    Apr 17 21:58:51.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-probe 04/17/23 21:58:51.021
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 21:58:51.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 21:58:51.037
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec in namespace container-probe-838 04/17/23 21:58:51.039
    Apr 17 21:58:51.045: INFO: Waiting up to 5m0s for pod "busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec" in namespace "container-probe-838" to be "not pending"
    Apr 17 21:58:51.050: INFO: Pod "busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.054172ms
    Apr 17 21:58:53.054: INFO: Pod "busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008126774s
    Apr 17 21:58:53.054: INFO: Pod "busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec" satisfied condition "not pending"
    Apr 17 21:58:53.054: INFO: Started pod busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec in namespace container-probe-838
    STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 21:58:53.054
    Apr 17 21:58:53.056: INFO: Initial restart count of pod busybox-94dd4401-8016-406b-adef-8eb7e9bf2dec is 0
    STEP: deleting the pod 04/17/23 22:02:53.496
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:02:53.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-838" for this suite. 04/17/23 22:02:53.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:02:53.519
Apr 17 22:02:53.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 22:02:53.519
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:02:53.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:02:53.533
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-4101 04/17/23 22:02:53.535
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/17/23 22:02:53.661
STEP: creating service externalsvc in namespace services-4101 04/17/23 22:02:53.661
STEP: creating replication controller externalsvc in namespace services-4101 04/17/23 22:02:53.68
I0417 22:02:53.687647      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4101, replica count: 2
I0417 22:02:56.738560      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 04/17/23 22:02:56.741
Apr 17 22:02:56.757: INFO: Creating new exec pod
Apr 17 22:02:56.766: INFO: Waiting up to 5m0s for pod "execpodfmjmg" in namespace "services-4101" to be "running"
Apr 17 22:02:56.769: INFO: Pod "execpodfmjmg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.151432ms
Apr 17 22:02:58.772: INFO: Pod "execpodfmjmg": Phase="Running", Reason="", readiness=true. Elapsed: 2.006414918s
Apr 17 22:02:58.772: INFO: Pod "execpodfmjmg" satisfied condition "running"
Apr 17 22:02:58.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4101 exec execpodfmjmg -- /bin/sh -x -c nslookup nodeport-service.services-4101.svc.cluster.local'
Apr 17 22:02:58.959: INFO: stderr: "+ nslookup nodeport-service.services-4101.svc.cluster.local\n"
Apr 17 22:02:58.959: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4101.svc.cluster.local\tcanonical name = externalsvc.services-4101.svc.cluster.local.\nName:\texternalsvc.services-4101.svc.cluster.local\nAddress: 10.100.214.52\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-4101, will wait for the garbage collector to delete the pods 04/17/23 22:02:58.959
Apr 17 22:02:59.018: INFO: Deleting ReplicationController externalsvc took: 5.208225ms
Apr 17 22:02:59.119: INFO: Terminating ReplicationController externalsvc pods took: 100.939119ms
Apr 17 22:03:01.120: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 22:03:01.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4101" for this suite. 04/17/23 22:03:01.133
------------------------------
• [SLOW TEST] [7.620 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:02:53.519
    Apr 17 22:02:53.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 22:02:53.519
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:02:53.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:02:53.533
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-4101 04/17/23 22:02:53.535
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 04/17/23 22:02:53.661
    STEP: creating service externalsvc in namespace services-4101 04/17/23 22:02:53.661
    STEP: creating replication controller externalsvc in namespace services-4101 04/17/23 22:02:53.68
    I0417 22:02:53.687647      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-4101, replica count: 2
    I0417 22:02:56.738560      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 04/17/23 22:02:56.741
    Apr 17 22:02:56.757: INFO: Creating new exec pod
    Apr 17 22:02:56.766: INFO: Waiting up to 5m0s for pod "execpodfmjmg" in namespace "services-4101" to be "running"
    Apr 17 22:02:56.769: INFO: Pod "execpodfmjmg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.151432ms
    Apr 17 22:02:58.772: INFO: Pod "execpodfmjmg": Phase="Running", Reason="", readiness=true. Elapsed: 2.006414918s
    Apr 17 22:02:58.772: INFO: Pod "execpodfmjmg" satisfied condition "running"
    Apr 17 22:02:58.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4101 exec execpodfmjmg -- /bin/sh -x -c nslookup nodeport-service.services-4101.svc.cluster.local'
    Apr 17 22:02:58.959: INFO: stderr: "+ nslookup nodeport-service.services-4101.svc.cluster.local\n"
    Apr 17 22:02:58.959: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-4101.svc.cluster.local\tcanonical name = externalsvc.services-4101.svc.cluster.local.\nName:\texternalsvc.services-4101.svc.cluster.local\nAddress: 10.100.214.52\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-4101, will wait for the garbage collector to delete the pods 04/17/23 22:02:58.959
    Apr 17 22:02:59.018: INFO: Deleting ReplicationController externalsvc took: 5.208225ms
    Apr 17 22:02:59.119: INFO: Terminating ReplicationController externalsvc pods took: 100.939119ms
    Apr 17 22:03:01.120: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:03:01.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4101" for this suite. 04/17/23 22:03:01.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:03:01.139
Apr 17 22:03:01.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename var-expansion 04/17/23 22:03:01.139
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:03:01.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:03:01.155
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 04/17/23 22:03:01.158
Apr 17 22:03:01.164: INFO: Waiting up to 2m0s for pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" in namespace "var-expansion-5626" to be "running"
Apr 17 22:03:01.166: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463751ms
Apr 17 22:03:03.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005726145s
Apr 17 22:03:05.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006416955s
Apr 17 22:03:07.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007389371s
Apr 17 22:03:09.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006602852s
Apr 17 22:03:11.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005680252s
Apr 17 22:03:13.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006683241s
Apr 17 22:03:15.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006388783s
Apr 17 22:03:17.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005449461s
Apr 17 22:03:19.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006384872s
Apr 17 22:03:21.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007101787s
Apr 17 22:03:23.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006059629s
Apr 17 22:03:25.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006832627s
Apr 17 22:03:27.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00662383s
Apr 17 22:03:29.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006017579s
Apr 17 22:03:31.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005647244s
Apr 17 22:03:33.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00579916s
Apr 17 22:03:35.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006732465s
Apr 17 22:03:37.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006642125s
Apr 17 22:03:39.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00650459s
Apr 17 22:03:41.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007232446s
Apr 17 22:03:43.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006381337s
Apr 17 22:03:45.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007233714s
Apr 17 22:03:47.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005736499s
Apr 17 22:03:49.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005949184s
Apr 17 22:03:51.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006867012s
Apr 17 22:03:53.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006058461s
Apr 17 22:03:55.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007262143s
Apr 17 22:03:57.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006401638s
Apr 17 22:03:59.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006165058s
Apr 17 22:04:01.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00583735s
Apr 17 22:04:03.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006141177s
Apr 17 22:04:05.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007111799s
Apr 17 22:04:07.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007009509s
Apr 17 22:04:09.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005990622s
Apr 17 22:04:11.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006520044s
Apr 17 22:04:13.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005541027s
Apr 17 22:04:15.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007329959s
Apr 17 22:04:17.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00742489s
Apr 17 22:04:19.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007403317s
Apr 17 22:04:21.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006937033s
Apr 17 22:04:23.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006419322s
Apr 17 22:04:25.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007196552s
Apr 17 22:04:27.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006894055s
Apr 17 22:04:29.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006067608s
Apr 17 22:04:31.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006474613s
Apr 17 22:04:33.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005515938s
Apr 17 22:04:35.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007335843s
Apr 17 22:04:37.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007046334s
Apr 17 22:04:39.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005872219s
Apr 17 22:04:41.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00697498s
Apr 17 22:04:43.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006040718s
Apr 17 22:04:45.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006804499s
Apr 17 22:04:47.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006476006s
Apr 17 22:04:49.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006455721s
Apr 17 22:04:51.172: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008119101s
Apr 17 22:04:53.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006255805s
Apr 17 22:04:55.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006810253s
Apr 17 22:04:57.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00564033s
Apr 17 22:04:59.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006599273s
Apr 17 22:05:01.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005674744s
Apr 17 22:05:01.172: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008316536s
STEP: updating the pod 04/17/23 22:05:01.172
Apr 17 22:05:01.684: INFO: Successfully updated pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25"
STEP: waiting for pod running 04/17/23 22:05:01.684
Apr 17 22:05:01.684: INFO: Waiting up to 2m0s for pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" in namespace "var-expansion-5626" to be "running"
Apr 17 22:05:01.687: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.768228ms
Apr 17 22:05:03.692: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Running", Reason="", readiness=true. Elapsed: 2.007907679s
Apr 17 22:05:03.692: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" satisfied condition "running"
STEP: deleting the pod gracefully 04/17/23 22:05:03.692
Apr 17 22:05:03.692: INFO: Deleting pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" in namespace "var-expansion-5626"
Apr 17 22:05:03.698: INFO: Wait up to 5m0s for pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 17 22:05:35.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5626" for this suite. 04/17/23 22:05:35.71
------------------------------
• [SLOW TEST] [154.575 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:03:01.139
    Apr 17 22:03:01.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename var-expansion 04/17/23 22:03:01.139
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:03:01.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:03:01.155
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 04/17/23 22:03:01.158
    Apr 17 22:03:01.164: INFO: Waiting up to 2m0s for pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" in namespace "var-expansion-5626" to be "running"
    Apr 17 22:03:01.166: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463751ms
    Apr 17 22:03:03.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005726145s
    Apr 17 22:03:05.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006416955s
    Apr 17 22:03:07.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007389371s
    Apr 17 22:03:09.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006602852s
    Apr 17 22:03:11.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005680252s
    Apr 17 22:03:13.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 12.006683241s
    Apr 17 22:03:15.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 14.006388783s
    Apr 17 22:03:17.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 16.005449461s
    Apr 17 22:03:19.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006384872s
    Apr 17 22:03:21.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 20.007101787s
    Apr 17 22:03:23.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 22.006059629s
    Apr 17 22:03:25.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 24.006832627s
    Apr 17 22:03:27.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00662383s
    Apr 17 22:03:29.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 28.006017579s
    Apr 17 22:03:31.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 30.005647244s
    Apr 17 22:03:33.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00579916s
    Apr 17 22:03:35.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006732465s
    Apr 17 22:03:37.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006642125s
    Apr 17 22:03:39.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00650459s
    Apr 17 22:03:41.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007232446s
    Apr 17 22:03:43.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 42.006381337s
    Apr 17 22:03:45.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 44.007233714s
    Apr 17 22:03:47.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 46.005736499s
    Apr 17 22:03:49.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 48.005949184s
    Apr 17 22:03:51.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 50.006867012s
    Apr 17 22:03:53.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006058461s
    Apr 17 22:03:55.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 54.007262143s
    Apr 17 22:03:57.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 56.006401638s
    Apr 17 22:03:59.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 58.006165058s
    Apr 17 22:04:01.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.00583735s
    Apr 17 22:04:03.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.006141177s
    Apr 17 22:04:05.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.007111799s
    Apr 17 22:04:07.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007009509s
    Apr 17 22:04:09.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.005990622s
    Apr 17 22:04:11.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.006520044s
    Apr 17 22:04:13.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.005541027s
    Apr 17 22:04:15.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007329959s
    Apr 17 22:04:17.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.00742489s
    Apr 17 22:04:19.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007403317s
    Apr 17 22:04:21.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.006937033s
    Apr 17 22:04:23.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.006419322s
    Apr 17 22:04:25.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007196552s
    Apr 17 22:04:27.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006894055s
    Apr 17 22:04:29.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006067608s
    Apr 17 22:04:31.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.006474613s
    Apr 17 22:04:33.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.005515938s
    Apr 17 22:04:35.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.007335843s
    Apr 17 22:04:37.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.007046334s
    Apr 17 22:04:39.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.005872219s
    Apr 17 22:04:41.171: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.00697498s
    Apr 17 22:04:43.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.006040718s
    Apr 17 22:04:45.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.006804499s
    Apr 17 22:04:47.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.006476006s
    Apr 17 22:04:49.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.006455721s
    Apr 17 22:04:51.172: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008119101s
    Apr 17 22:04:53.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.006255805s
    Apr 17 22:04:55.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.006810253s
    Apr 17 22:04:57.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.00564033s
    Apr 17 22:04:59.170: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.006599273s
    Apr 17 22:05:01.169: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.005674744s
    Apr 17 22:05:01.172: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008316536s
    STEP: updating the pod 04/17/23 22:05:01.172
    Apr 17 22:05:01.684: INFO: Successfully updated pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25"
    STEP: waiting for pod running 04/17/23 22:05:01.684
    Apr 17 22:05:01.684: INFO: Waiting up to 2m0s for pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" in namespace "var-expansion-5626" to be "running"
    Apr 17 22:05:01.687: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.768228ms
    Apr 17 22:05:03.692: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25": Phase="Running", Reason="", readiness=true. Elapsed: 2.007907679s
    Apr 17 22:05:03.692: INFO: Pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" satisfied condition "running"
    STEP: deleting the pod gracefully 04/17/23 22:05:03.692
    Apr 17 22:05:03.692: INFO: Deleting pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" in namespace "var-expansion-5626"
    Apr 17 22:05:03.698: INFO: Wait up to 5m0s for pod "var-expansion-83858394-c932-4347-8c42-42cb28813c25" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:05:35.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5626" for this suite. 04/17/23 22:05:35.71
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:05:35.715
Apr 17 22:05:35.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename subpath 04/17/23 22:05:35.716
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:05:35.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:05:35.731
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/17/23 22:05:35.733
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-wgmh 04/17/23 22:05:35.741
STEP: Creating a pod to test atomic-volume-subpath 04/17/23 22:05:35.741
Apr 17 22:05:35.748: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-wgmh" in namespace "subpath-8548" to be "Succeeded or Failed"
Apr 17 22:05:35.750: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.227923ms
Apr 17 22:05:37.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 2.005868337s
Apr 17 22:05:39.755: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 4.006591864s
Apr 17 22:05:41.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 6.006310822s
Apr 17 22:05:43.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 8.005623215s
Apr 17 22:05:45.755: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 10.006452536s
Apr 17 22:05:47.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 12.005403926s
Apr 17 22:05:49.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 14.006157886s
Apr 17 22:05:51.755: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 16.006635391s
Apr 17 22:05:53.753: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 18.005344106s
Apr 17 22:05:55.755: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 20.00711551s
Apr 17 22:05:57.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=false. Elapsed: 22.006068149s
Apr 17 22:05:59.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005813712s
STEP: Saw pod success 04/17/23 22:05:59.754
Apr 17 22:05:59.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh" satisfied condition "Succeeded or Failed"
Apr 17 22:05:59.756: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-subpath-test-downwardapi-wgmh container test-container-subpath-downwardapi-wgmh: <nil>
STEP: delete the pod 04/17/23 22:05:59.768
Apr 17 22:05:59.783: INFO: Waiting for pod pod-subpath-test-downwardapi-wgmh to disappear
Apr 17 22:05:59.786: INFO: Pod pod-subpath-test-downwardapi-wgmh no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-wgmh 04/17/23 22:05:59.786
Apr 17 22:05:59.786: INFO: Deleting pod "pod-subpath-test-downwardapi-wgmh" in namespace "subpath-8548"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 17 22:05:59.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8548" for this suite. 04/17/23 22:05:59.792
------------------------------
• [SLOW TEST] [24.082 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:05:35.715
    Apr 17 22:05:35.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename subpath 04/17/23 22:05:35.716
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:05:35.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:05:35.731
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/17/23 22:05:35.733
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-wgmh 04/17/23 22:05:35.741
    STEP: Creating a pod to test atomic-volume-subpath 04/17/23 22:05:35.741
    Apr 17 22:05:35.748: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-wgmh" in namespace "subpath-8548" to be "Succeeded or Failed"
    Apr 17 22:05:35.750: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.227923ms
    Apr 17 22:05:37.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 2.005868337s
    Apr 17 22:05:39.755: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 4.006591864s
    Apr 17 22:05:41.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 6.006310822s
    Apr 17 22:05:43.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 8.005623215s
    Apr 17 22:05:45.755: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 10.006452536s
    Apr 17 22:05:47.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 12.005403926s
    Apr 17 22:05:49.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 14.006157886s
    Apr 17 22:05:51.755: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 16.006635391s
    Apr 17 22:05:53.753: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 18.005344106s
    Apr 17 22:05:55.755: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=true. Elapsed: 20.00711551s
    Apr 17 22:05:57.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Running", Reason="", readiness=false. Elapsed: 22.006068149s
    Apr 17 22:05:59.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005813712s
    STEP: Saw pod success 04/17/23 22:05:59.754
    Apr 17 22:05:59.754: INFO: Pod "pod-subpath-test-downwardapi-wgmh" satisfied condition "Succeeded or Failed"
    Apr 17 22:05:59.756: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-subpath-test-downwardapi-wgmh container test-container-subpath-downwardapi-wgmh: <nil>
    STEP: delete the pod 04/17/23 22:05:59.768
    Apr 17 22:05:59.783: INFO: Waiting for pod pod-subpath-test-downwardapi-wgmh to disappear
    Apr 17 22:05:59.786: INFO: Pod pod-subpath-test-downwardapi-wgmh no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-wgmh 04/17/23 22:05:59.786
    Apr 17 22:05:59.786: INFO: Deleting pod "pod-subpath-test-downwardapi-wgmh" in namespace "subpath-8548"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:05:59.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8548" for this suite. 04/17/23 22:05:59.792
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:05:59.797
Apr 17 22:05:59.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:05:59.798
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:05:59.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:05:59.813
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 04/17/23 22:05:59.815
Apr 17 22:05:59.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 04/17/23 22:06:11.778
Apr 17 22:06:11.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:06:15.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:06:27.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8980" for this suite. 04/17/23 22:06:27.672
------------------------------
• [SLOW TEST] [27.881 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:05:59.797
    Apr 17 22:05:59.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:05:59.798
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:05:59.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:05:59.813
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 04/17/23 22:05:59.815
    Apr 17 22:05:59.815: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 04/17/23 22:06:11.778
    Apr 17 22:06:11.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:06:15.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:06:27.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8980" for this suite. 04/17/23 22:06:27.672
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:06:27.678
Apr 17 22:06:27.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename dns 04/17/23 22:06:27.679
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:27.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:27.697
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 04/17/23 22:06:27.699
Apr 17 22:06:27.707: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4262  eda3b663-793f-4fbd-a167-f866a73cd681 45908 0 2023-04-17 22:06:27 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-17 22:06:27 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hx7c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hx7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:06:27.707: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4262" to be "running and ready"
Apr 17 22:06:27.712: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.067971ms
Apr 17 22:06:27.712: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:06:29.778: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.070515246s
Apr 17 22:06:29.778: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Apr 17 22:06:29.778: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 04/17/23 22:06:29.778
Apr 17 22:06:29.778: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4262 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:06:29.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:06:29.778: INFO: ExecWithOptions: Clientset creation
Apr 17 22:06:29.778: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4262/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 04/17/23 22:06:29.869
Apr 17 22:06:29.869: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4262 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:06:29.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:06:29.870: INFO: ExecWithOptions: Clientset creation
Apr 17 22:06:29.870: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4262/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 17 22:06:29.941: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 17 22:06:29.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4262" for this suite. 04/17/23 22:06:29.961
------------------------------
• [2.289 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:06:27.678
    Apr 17 22:06:27.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename dns 04/17/23 22:06:27.679
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:27.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:27.697
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 04/17/23 22:06:27.699
    Apr 17 22:06:27.707: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4262  eda3b663-793f-4fbd-a167-f866a73cd681 45908 0 2023-04-17 22:06:27 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-04-17 22:06:27 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2hx7c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2hx7c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:06:27.707: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-4262" to be "running and ready"
    Apr 17 22:06:27.712: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 5.067971ms
    Apr 17 22:06:27.712: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:06:29.778: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.070515246s
    Apr 17 22:06:29.778: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Apr 17 22:06:29.778: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 04/17/23 22:06:29.778
    Apr 17 22:06:29.778: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4262 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:06:29.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:06:29.778: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:06:29.778: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4262/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 04/17/23 22:06:29.869
    Apr 17 22:06:29.869: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4262 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:06:29.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:06:29.870: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:06:29.870: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-4262/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 17 22:06:29.941: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:06:29.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4262" for this suite. 04/17/23 22:06:29.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:06:29.969
Apr 17 22:06:29.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:06:29.97
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:29.985
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:29.987
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/17/23 22:06:29.99
Apr 17 22:06:29.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-861 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Apr 17 22:06:30.050: INFO: stderr: ""
Apr 17 22:06:30.050: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 04/17/23 22:06:30.05
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Apr 17 22:06:30.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-861 delete pods e2e-test-httpd-pod'
Apr 17 22:06:31.974: INFO: stderr: ""
Apr 17 22:06:31.974: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:06:31.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-861" for this suite. 04/17/23 22:06:31.98
------------------------------
• [2.017 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:06:29.969
    Apr 17 22:06:29.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:06:29.97
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:29.985
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:29.987
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 04/17/23 22:06:29.99
    Apr 17 22:06:29.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-861 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Apr 17 22:06:30.050: INFO: stderr: ""
    Apr 17 22:06:30.050: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 04/17/23 22:06:30.05
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Apr 17 22:06:30.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-861 delete pods e2e-test-httpd-pod'
    Apr 17 22:06:31.974: INFO: stderr: ""
    Apr 17 22:06:31.974: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:06:31.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-861" for this suite. 04/17/23 22:06:31.98
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:06:31.987
Apr 17 22:06:31.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 22:06:31.987
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:32.003
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:32.005
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 04/17/23 22:06:32.008
STEP: Counting existing ResourceQuota 04/17/23 22:06:37.012
STEP: Creating a ResourceQuota 04/17/23 22:06:42.017
STEP: Ensuring resource quota status is calculated 04/17/23 22:06:42.022
STEP: Creating a Secret 04/17/23 22:06:44.026
STEP: Ensuring resource quota status captures secret creation 04/17/23 22:06:44.038
STEP: Deleting a secret 04/17/23 22:06:46.043
STEP: Ensuring resource quota status released usage 04/17/23 22:06:46.05
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 22:06:48.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2587" for this suite. 04/17/23 22:06:48.06
------------------------------
• [SLOW TEST] [16.081 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:06:31.987
    Apr 17 22:06:31.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 22:06:31.987
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:32.003
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:32.005
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 04/17/23 22:06:32.008
    STEP: Counting existing ResourceQuota 04/17/23 22:06:37.012
    STEP: Creating a ResourceQuota 04/17/23 22:06:42.017
    STEP: Ensuring resource quota status is calculated 04/17/23 22:06:42.022
    STEP: Creating a Secret 04/17/23 22:06:44.026
    STEP: Ensuring resource quota status captures secret creation 04/17/23 22:06:44.038
    STEP: Deleting a secret 04/17/23 22:06:46.043
    STEP: Ensuring resource quota status released usage 04/17/23 22:06:46.05
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:06:48.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2587" for this suite. 04/17/23 22:06:48.06
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:06:48.068
Apr 17 22:06:48.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename var-expansion 04/17/23 22:06:48.069
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:48.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:48.085
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 04/17/23 22:06:48.088
Apr 17 22:06:48.096: INFO: Waiting up to 5m0s for pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3" in namespace "var-expansion-1064" to be "Succeeded or Failed"
Apr 17 22:06:48.100: INFO: Pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.407729ms
Apr 17 22:06:50.105: INFO: Pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008507571s
Apr 17 22:06:52.104: INFO: Pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008415211s
STEP: Saw pod success 04/17/23 22:06:52.104
Apr 17 22:06:52.105: INFO: Pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3" satisfied condition "Succeeded or Failed"
Apr 17 22:06:52.108: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3 container dapi-container: <nil>
STEP: delete the pod 04/17/23 22:06:52.122
Apr 17 22:06:52.138: INFO: Waiting for pod var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3 to disappear
Apr 17 22:06:52.141: INFO: Pod var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 17 22:06:52.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1064" for this suite. 04/17/23 22:06:52.146
------------------------------
• [4.087 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:06:48.068
    Apr 17 22:06:48.068: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename var-expansion 04/17/23 22:06:48.069
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:48.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:48.085
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 04/17/23 22:06:48.088
    Apr 17 22:06:48.096: INFO: Waiting up to 5m0s for pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3" in namespace "var-expansion-1064" to be "Succeeded or Failed"
    Apr 17 22:06:48.100: INFO: Pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.407729ms
    Apr 17 22:06:50.105: INFO: Pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008507571s
    Apr 17 22:06:52.104: INFO: Pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008415211s
    STEP: Saw pod success 04/17/23 22:06:52.104
    Apr 17 22:06:52.105: INFO: Pod "var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3" satisfied condition "Succeeded or Failed"
    Apr 17 22:06:52.108: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3 container dapi-container: <nil>
    STEP: delete the pod 04/17/23 22:06:52.122
    Apr 17 22:06:52.138: INFO: Waiting for pod var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3 to disappear
    Apr 17 22:06:52.141: INFO: Pod var-expansion-5da574a4-082e-40d5-8739-fc18ad11a8c3 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:06:52.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1064" for this suite. 04/17/23 22:06:52.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:06:52.156
Apr 17 22:06:52.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:06:52.157
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:52.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:52.173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-d12d72da-44d2-48ae-bb34-20cab47bdb1e 04/17/23 22:06:52.175
STEP: Creating a pod to test consume configMaps 04/17/23 22:06:52.18
Apr 17 22:06:52.187: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80" in namespace "projected-6873" to be "Succeeded or Failed"
Apr 17 22:06:52.192: INFO: Pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935568ms
Apr 17 22:06:54.197: INFO: Pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80": Phase="Running", Reason="", readiness=false. Elapsed: 2.010121032s
Apr 17 22:06:56.197: INFO: Pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009969997s
STEP: Saw pod success 04/17/23 22:06:56.197
Apr 17 22:06:56.197: INFO: Pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80" satisfied condition "Succeeded or Failed"
Apr 17 22:06:56.201: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:06:56.215
Apr 17 22:06:56.232: INFO: Waiting for pod pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80 to disappear
Apr 17 22:06:56.235: INFO: Pod pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:06:56.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6873" for this suite. 04/17/23 22:06:56.241
------------------------------
• [4.093 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:06:52.156
    Apr 17 22:06:52.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:06:52.157
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:52.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:52.173
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-d12d72da-44d2-48ae-bb34-20cab47bdb1e 04/17/23 22:06:52.175
    STEP: Creating a pod to test consume configMaps 04/17/23 22:06:52.18
    Apr 17 22:06:52.187: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80" in namespace "projected-6873" to be "Succeeded or Failed"
    Apr 17 22:06:52.192: INFO: Pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935568ms
    Apr 17 22:06:54.197: INFO: Pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80": Phase="Running", Reason="", readiness=false. Elapsed: 2.010121032s
    Apr 17 22:06:56.197: INFO: Pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009969997s
    STEP: Saw pod success 04/17/23 22:06:56.197
    Apr 17 22:06:56.197: INFO: Pod "pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80" satisfied condition "Succeeded or Failed"
    Apr 17 22:06:56.201: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:06:56.215
    Apr 17 22:06:56.232: INFO: Waiting for pod pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80 to disappear
    Apr 17 22:06:56.235: INFO: Pod pod-projected-configmaps-9fa9707e-f14b-4746-a2c2-6c5ca2a7ca80 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:06:56.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6873" for this suite. 04/17/23 22:06:56.241
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:06:56.249
Apr 17 22:06:56.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename disruption 04/17/23 22:06:56.25
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:56.264
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:56.266
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 04/17/23 22:06:56.269
STEP: Waiting for the pdb to be processed 04/17/23 22:06:56.274
STEP: First trying to evict a pod which shouldn't be evictable 04/17/23 22:06:58.287
STEP: Waiting for all pods to be running 04/17/23 22:06:58.287
Apr 17 22:06:58.290: INFO: pods: 0 < 3
STEP: locating a running pod 04/17/23 22:07:00.296
STEP: Updating the pdb to allow a pod to be evicted 04/17/23 22:07:00.306
STEP: Waiting for the pdb to be processed 04/17/23 22:07:00.315
STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/17/23 22:07:02.322
STEP: Waiting for all pods to be running 04/17/23 22:07:02.322
STEP: Waiting for the pdb to observed all healthy pods 04/17/23 22:07:02.326
STEP: Patching the pdb to disallow a pod to be evicted 04/17/23 22:07:02.355
STEP: Waiting for the pdb to be processed 04/17/23 22:07:02.383
STEP: Waiting for all pods to be running 04/17/23 22:07:02.387
Apr 17 22:07:02.390: INFO: running pods: 2 < 3
STEP: locating a running pod 04/17/23 22:07:04.395
STEP: Deleting the pdb to allow a pod to be evicted 04/17/23 22:07:04.404
STEP: Waiting for the pdb to be deleted 04/17/23 22:07:04.412
STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/17/23 22:07:04.415
STEP: Waiting for all pods to be running 04/17/23 22:07:04.415
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 17 22:07:04.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6281" for this suite. 04/17/23 22:07:04.442
------------------------------
• [SLOW TEST] [8.206 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:06:56.249
    Apr 17 22:06:56.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename disruption 04/17/23 22:06:56.25
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:06:56.264
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:06:56.266
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 04/17/23 22:06:56.269
    STEP: Waiting for the pdb to be processed 04/17/23 22:06:56.274
    STEP: First trying to evict a pod which shouldn't be evictable 04/17/23 22:06:58.287
    STEP: Waiting for all pods to be running 04/17/23 22:06:58.287
    Apr 17 22:06:58.290: INFO: pods: 0 < 3
    STEP: locating a running pod 04/17/23 22:07:00.296
    STEP: Updating the pdb to allow a pod to be evicted 04/17/23 22:07:00.306
    STEP: Waiting for the pdb to be processed 04/17/23 22:07:00.315
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/17/23 22:07:02.322
    STEP: Waiting for all pods to be running 04/17/23 22:07:02.322
    STEP: Waiting for the pdb to observed all healthy pods 04/17/23 22:07:02.326
    STEP: Patching the pdb to disallow a pod to be evicted 04/17/23 22:07:02.355
    STEP: Waiting for the pdb to be processed 04/17/23 22:07:02.383
    STEP: Waiting for all pods to be running 04/17/23 22:07:02.387
    Apr 17 22:07:02.390: INFO: running pods: 2 < 3
    STEP: locating a running pod 04/17/23 22:07:04.395
    STEP: Deleting the pdb to allow a pod to be evicted 04/17/23 22:07:04.404
    STEP: Waiting for the pdb to be deleted 04/17/23 22:07:04.412
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 04/17/23 22:07:04.415
    STEP: Waiting for all pods to be running 04/17/23 22:07:04.415
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:07:04.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6281" for this suite. 04/17/23 22:07:04.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:07:04.457
Apr 17 22:07:04.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename containers 04/17/23 22:07:04.457
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:04.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:04.474
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Apr 17 22:07:04.485: INFO: Waiting up to 5m0s for pod "client-containers-60a6fb17-d01f-4a93-9b53-068b3f5dad0c" in namespace "containers-1832" to be "running"
Apr 17 22:07:04.491: INFO: Pod "client-containers-60a6fb17-d01f-4a93-9b53-068b3f5dad0c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295334ms
Apr 17 22:07:06.494: INFO: Pod "client-containers-60a6fb17-d01f-4a93-9b53-068b3f5dad0c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009034628s
Apr 17 22:07:06.494: INFO: Pod "client-containers-60a6fb17-d01f-4a93-9b53-068b3f5dad0c" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 17 22:07:06.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1832" for this suite. 04/17/23 22:07:06.512
------------------------------
• [2.062 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:07:04.457
    Apr 17 22:07:04.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename containers 04/17/23 22:07:04.457
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:04.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:04.474
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Apr 17 22:07:04.485: INFO: Waiting up to 5m0s for pod "client-containers-60a6fb17-d01f-4a93-9b53-068b3f5dad0c" in namespace "containers-1832" to be "running"
    Apr 17 22:07:04.491: INFO: Pod "client-containers-60a6fb17-d01f-4a93-9b53-068b3f5dad0c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295334ms
    Apr 17 22:07:06.494: INFO: Pod "client-containers-60a6fb17-d01f-4a93-9b53-068b3f5dad0c": Phase="Running", Reason="", readiness=true. Elapsed: 2.009034628s
    Apr 17 22:07:06.494: INFO: Pod "client-containers-60a6fb17-d01f-4a93-9b53-068b3f5dad0c" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:07:06.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1832" for this suite. 04/17/23 22:07:06.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:07:06.519
Apr 17 22:07:06.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename watch 04/17/23 22:07:06.52
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:06.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:06.537
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 04/17/23 22:07:06.54
STEP: creating a watch on configmaps with label B 04/17/23 22:07:06.541
STEP: creating a watch on configmaps with label A or B 04/17/23 22:07:06.542
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 04/17/23 22:07:06.543
Apr 17 22:07:06.547: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46509 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:07:06.547: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46509 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 04/17/23 22:07:06.547
Apr 17 22:07:06.555: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46510 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:07:06.556: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46510 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 04/17/23 22:07:06.556
Apr 17 22:07:06.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46511 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:07:06.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46511 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 04/17/23 22:07:06.564
Apr 17 22:07:06.570: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46512 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:07:06.570: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46512 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 04/17/23 22:07:06.57
Apr 17 22:07:06.574: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2913  d21d474d-7355-4150-a0cd-5b75f37b575b 46513 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:07:06.574: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2913  d21d474d-7355-4150-a0cd-5b75f37b575b 46513 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 04/17/23 22:07:16.574
Apr 17 22:07:16.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2913  d21d474d-7355-4150-a0cd-5b75f37b575b 46665 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:07:16.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2913  d21d474d-7355-4150-a0cd-5b75f37b575b 46665 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 17 22:07:26.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-2913" for this suite. 04/17/23 22:07:26.59
------------------------------
• [SLOW TEST] [20.079 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:07:06.519
    Apr 17 22:07:06.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename watch 04/17/23 22:07:06.52
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:06.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:06.537
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 04/17/23 22:07:06.54
    STEP: creating a watch on configmaps with label B 04/17/23 22:07:06.541
    STEP: creating a watch on configmaps with label A or B 04/17/23 22:07:06.542
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 04/17/23 22:07:06.543
    Apr 17 22:07:06.547: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46509 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:07:06.547: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46509 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 04/17/23 22:07:06.547
    Apr 17 22:07:06.555: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46510 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:07:06.556: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46510 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 04/17/23 22:07:06.556
    Apr 17 22:07:06.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46511 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:07:06.564: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46511 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 04/17/23 22:07:06.564
    Apr 17 22:07:06.570: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46512 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:07:06.570: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2913  bd565d83-c353-4ce7-a816-9bf66c8cd6d0 46512 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 04/17/23 22:07:06.57
    Apr 17 22:07:06.574: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2913  d21d474d-7355-4150-a0cd-5b75f37b575b 46513 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:07:06.574: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2913  d21d474d-7355-4150-a0cd-5b75f37b575b 46513 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 04/17/23 22:07:16.574
    Apr 17 22:07:16.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2913  d21d474d-7355-4150-a0cd-5b75f37b575b 46665 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:07:16.581: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2913  d21d474d-7355-4150-a0cd-5b75f37b575b 46665 0 2023-04-17 22:07:06 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-04-17 22:07:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:07:26.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-2913" for this suite. 04/17/23 22:07:26.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:07:26.6
Apr 17 22:07:26.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 22:07:26.601
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:26.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:26.615
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 04/17/23 22:07:26.618
Apr 17 22:07:26.627: INFO: Waiting up to 5m0s for pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b" in namespace "emptydir-9412" to be "Succeeded or Failed"
Apr 17 22:07:26.630: INFO: Pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.93401ms
Apr 17 22:07:28.635: INFO: Pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008621753s
Apr 17 22:07:30.635: INFO: Pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008263801s
STEP: Saw pod success 04/17/23 22:07:30.635
Apr 17 22:07:30.635: INFO: Pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b" satisfied condition "Succeeded or Failed"
Apr 17 22:07:30.638: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b container test-container: <nil>
STEP: delete the pod 04/17/23 22:07:30.645
Apr 17 22:07:30.659: INFO: Waiting for pod pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b to disappear
Apr 17 22:07:30.662: INFO: Pod pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:07:30.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9412" for this suite. 04/17/23 22:07:30.667
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:07:26.6
    Apr 17 22:07:26.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 22:07:26.601
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:26.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:26.615
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 04/17/23 22:07:26.618
    Apr 17 22:07:26.627: INFO: Waiting up to 5m0s for pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b" in namespace "emptydir-9412" to be "Succeeded or Failed"
    Apr 17 22:07:26.630: INFO: Pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.93401ms
    Apr 17 22:07:28.635: INFO: Pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008621753s
    Apr 17 22:07:30.635: INFO: Pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008263801s
    STEP: Saw pod success 04/17/23 22:07:30.635
    Apr 17 22:07:30.635: INFO: Pod "pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b" satisfied condition "Succeeded or Failed"
    Apr 17 22:07:30.638: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b container test-container: <nil>
    STEP: delete the pod 04/17/23 22:07:30.645
    Apr 17 22:07:30.659: INFO: Waiting for pod pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b to disappear
    Apr 17 22:07:30.662: INFO: Pod pod-e9c9437a-8568-44fb-a8e2-d9d57df7ff5b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:07:30.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9412" for this suite. 04/17/23 22:07:30.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:07:30.677
Apr 17 22:07:30.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename deployment 04/17/23 22:07:30.678
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:30.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:30.694
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 04/17/23 22:07:30.7
Apr 17 22:07:30.700: INFO: Creating simple deployment test-deployment-dmwk4
Apr 17 22:07:30.715: INFO: deployment "test-deployment-dmwk4" doesn't have the required revision set
STEP: Getting /status 04/17/23 22:07:32.73
Apr 17 22:07:32.734: INFO: Deployment test-deployment-dmwk4 has Conditions: [{Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 04/17/23 22:07:32.734
Apr 17 22:07:32.744: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 7, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 7, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 7, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 7, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-dmwk4-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 04/17/23 22:07:32.744
Apr 17 22:07:32.745: INFO: Observed &Deployment event: ADDED
Apr 17 22:07:32.745: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dmwk4-54bc444df"}
Apr 17 22:07:32.745: INFO: Observed &Deployment event: MODIFIED
Apr 17 22:07:32.745: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dmwk4-54bc444df"}
Apr 17 22:07:32.745: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 17 22:07:32.746: INFO: Observed &Deployment event: MODIFIED
Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dmwk4-54bc444df" is progressing.}
Apr 17 22:07:32.746: INFO: Observed &Deployment event: MODIFIED
Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}
Apr 17 22:07:32.746: INFO: Observed &Deployment event: MODIFIED
Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}
Apr 17 22:07:32.746: INFO: Found Deployment test-deployment-dmwk4 in namespace deployment-2614 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 17 22:07:32.746: INFO: Deployment test-deployment-dmwk4 has an updated status
STEP: patching the Statefulset Status 04/17/23 22:07:32.746
Apr 17 22:07:32.746: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Apr 17 22:07:32.753: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 04/17/23 22:07:32.753
Apr 17 22:07:32.755: INFO: Observed &Deployment event: ADDED
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dmwk4-54bc444df"}
Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dmwk4-54bc444df"}
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dmwk4-54bc444df" is progressing.}
Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}
Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}
Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
Apr 17 22:07:32.755: INFO: Found deployment test-deployment-dmwk4 in namespace deployment-2614 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Apr 17 22:07:32.755: INFO: Deployment test-deployment-dmwk4 has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 17 22:07:32.759: INFO: Deployment "test-deployment-dmwk4":
&Deployment{ObjectMeta:{test-deployment-dmwk4  deployment-2614  641e7b78-7bc1-4eaa-a4e7-e6673e4eaa23 46857 1 2023-04-17 22:07:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-17 22:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-17 22:07:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-17 22:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0065efea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-dmwk4-54bc444df",LastUpdateTime:2023-04-17 22:07:32 +0000 UTC,LastTransitionTime:2023-04-17 22:07:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 17 22:07:32.762: INFO: New ReplicaSet "test-deployment-dmwk4-54bc444df" of Deployment "test-deployment-dmwk4":
&ReplicaSet{ObjectMeta:{test-deployment-dmwk4-54bc444df  deployment-2614  2625e91e-6ed2-42c4-b3a6-845398b449d6 46850 1 2023-04-17 22:07:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-dmwk4 641e7b78-7bc1-4eaa-a4e7-e6673e4eaa23 0xc006a66290 0xc006a66291}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"641e7b78-7bc1-4eaa-a4e7-e6673e4eaa23\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:07:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a66338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:07:32.766: INFO: Pod "test-deployment-dmwk4-54bc444df-lvnq6" is available:
&Pod{ObjectMeta:{test-deployment-dmwk4-54bc444df-lvnq6 test-deployment-dmwk4-54bc444df- deployment-2614  024b53b7-6126-496b-bb6a-01c6acce9cc7 46849 0 2023-04-17 22:07:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:e3251a36314a75dea7cc55c73c0ac87561829b6a603f5ef251a331d4fb20f382 cni.projectcalico.org/podIP:192.168.200.154/32 cni.projectcalico.org/podIPs:192.168.200.154/32] [{apps/v1 ReplicaSet test-deployment-dmwk4-54bc444df 2625e91e-6ed2-42c4-b3a6-845398b449d6 0xc003b10a37 0xc003b10a38}] [] [{kube-controller-manager Update v1 2023-04-17 22:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2625e91e-6ed2-42c4-b3a6-845398b449d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:07:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.200.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5p98j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5p98j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:07:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:192.168.200.154,StartTime:2023-04-17 22:07:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:07:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://351f0fd825c659b151cbe4a82b35d44e1aab7cf7daa1afd2aa9e0ceade992be7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.200.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 17 22:07:32.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2614" for this suite. 04/17/23 22:07:32.772
------------------------------
• [2.101 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:07:30.677
    Apr 17 22:07:30.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename deployment 04/17/23 22:07:30.678
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:30.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:30.694
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 04/17/23 22:07:30.7
    Apr 17 22:07:30.700: INFO: Creating simple deployment test-deployment-dmwk4
    Apr 17 22:07:30.715: INFO: deployment "test-deployment-dmwk4" doesn't have the required revision set
    STEP: Getting /status 04/17/23 22:07:32.73
    Apr 17 22:07:32.734: INFO: Deployment test-deployment-dmwk4 has Conditions: [{Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 04/17/23 22:07:32.734
    Apr 17 22:07:32.744: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 7, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 7, 32, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 7, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 7, 30, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-dmwk4-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 04/17/23 22:07:32.744
    Apr 17 22:07:32.745: INFO: Observed &Deployment event: ADDED
    Apr 17 22:07:32.745: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dmwk4-54bc444df"}
    Apr 17 22:07:32.745: INFO: Observed &Deployment event: MODIFIED
    Apr 17 22:07:32.745: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dmwk4-54bc444df"}
    Apr 17 22:07:32.745: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 17 22:07:32.746: INFO: Observed &Deployment event: MODIFIED
    Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dmwk4-54bc444df" is progressing.}
    Apr 17 22:07:32.746: INFO: Observed &Deployment event: MODIFIED
    Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}
    Apr 17 22:07:32.746: INFO: Observed &Deployment event: MODIFIED
    Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 17 22:07:32.746: INFO: Observed Deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}
    Apr 17 22:07:32.746: INFO: Found Deployment test-deployment-dmwk4 in namespace deployment-2614 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 17 22:07:32.746: INFO: Deployment test-deployment-dmwk4 has an updated status
    STEP: patching the Statefulset Status 04/17/23 22:07:32.746
    Apr 17 22:07:32.746: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Apr 17 22:07:32.753: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 04/17/23 22:07:32.753
    Apr 17 22:07:32.755: INFO: Observed &Deployment event: ADDED
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dmwk4-54bc444df"}
    Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dmwk4-54bc444df"}
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:30 +0000 UTC 2023-04-17 22:07:30 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dmwk4-54bc444df" is progressing.}
    Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}
    Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-04-17 22:07:32 +0000 UTC 2023-04-17 22:07:30 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dmwk4-54bc444df" has successfully progressed.}
    Apr 17 22:07:32.755: INFO: Observed deployment test-deployment-dmwk4 in namespace deployment-2614 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Apr 17 22:07:32.755: INFO: Observed &Deployment event: MODIFIED
    Apr 17 22:07:32.755: INFO: Found deployment test-deployment-dmwk4 in namespace deployment-2614 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Apr 17 22:07:32.755: INFO: Deployment test-deployment-dmwk4 has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 17 22:07:32.759: INFO: Deployment "test-deployment-dmwk4":
    &Deployment{ObjectMeta:{test-deployment-dmwk4  deployment-2614  641e7b78-7bc1-4eaa-a4e7-e6673e4eaa23 46857 1 2023-04-17 22:07:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-04-17 22:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-04-17 22:07:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-04-17 22:07:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0065efea8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-dmwk4-54bc444df",LastUpdateTime:2023-04-17 22:07:32 +0000 UTC,LastTransitionTime:2023-04-17 22:07:32 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 17 22:07:32.762: INFO: New ReplicaSet "test-deployment-dmwk4-54bc444df" of Deployment "test-deployment-dmwk4":
    &ReplicaSet{ObjectMeta:{test-deployment-dmwk4-54bc444df  deployment-2614  2625e91e-6ed2-42c4-b3a6-845398b449d6 46850 1 2023-04-17 22:07:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-dmwk4 641e7b78-7bc1-4eaa-a4e7-e6673e4eaa23 0xc006a66290 0xc006a66291}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"641e7b78-7bc1-4eaa-a4e7-e6673e4eaa23\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:07:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006a66338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 22:07:32.766: INFO: Pod "test-deployment-dmwk4-54bc444df-lvnq6" is available:
    &Pod{ObjectMeta:{test-deployment-dmwk4-54bc444df-lvnq6 test-deployment-dmwk4-54bc444df- deployment-2614  024b53b7-6126-496b-bb6a-01c6acce9cc7 46849 0 2023-04-17 22:07:30 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:e3251a36314a75dea7cc55c73c0ac87561829b6a603f5ef251a331d4fb20f382 cni.projectcalico.org/podIP:192.168.200.154/32 cni.projectcalico.org/podIPs:192.168.200.154/32] [{apps/v1 ReplicaSet test-deployment-dmwk4-54bc444df 2625e91e-6ed2-42c4-b3a6-845398b449d6 0xc003b10a37 0xc003b10a38}] [] [{kube-controller-manager Update v1 2023-04-17 22:07:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2625e91e-6ed2-42c4-b3a6-845398b449d6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:07:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:07:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.200.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5p98j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5p98j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:07:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:07:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:07:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:192.168.200.154,StartTime:2023-04-17 22:07:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:07:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://351f0fd825c659b151cbe4a82b35d44e1aab7cf7daa1afd2aa9e0ceade992be7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.200.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:07:32.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2614" for this suite. 04/17/23 22:07:32.772
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:07:32.779
Apr 17 22:07:32.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:07:32.78
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:32.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:32.809
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-0e1ad6ee-2e7d-4996-b3f7-37bce8a7ef64 04/17/23 22:07:32.812
STEP: Creating a pod to test consume secrets 04/17/23 22:07:32.824
Apr 17 22:07:32.846: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e" in namespace "projected-8199" to be "Succeeded or Failed"
Apr 17 22:07:32.876: INFO: Pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.134969ms
Apr 17 22:07:34.884: INFO: Pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037553627s
Apr 17 22:07:36.881: INFO: Pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034805426s
STEP: Saw pod success 04/17/23 22:07:36.881
Apr 17 22:07:36.881: INFO: Pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e" satisfied condition "Succeeded or Failed"
Apr 17 22:07:36.884: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e container projected-secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:07:36.891
Apr 17 22:07:36.902: INFO: Waiting for pod pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e to disappear
Apr 17 22:07:36.905: INFO: Pod pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 17 22:07:36.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8199" for this suite. 04/17/23 22:07:36.91
------------------------------
• [4.137 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:07:32.779
    Apr 17 22:07:32.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:07:32.78
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:32.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:32.809
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-0e1ad6ee-2e7d-4996-b3f7-37bce8a7ef64 04/17/23 22:07:32.812
    STEP: Creating a pod to test consume secrets 04/17/23 22:07:32.824
    Apr 17 22:07:32.846: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e" in namespace "projected-8199" to be "Succeeded or Failed"
    Apr 17 22:07:32.876: INFO: Pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.134969ms
    Apr 17 22:07:34.884: INFO: Pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037553627s
    Apr 17 22:07:36.881: INFO: Pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.034805426s
    STEP: Saw pod success 04/17/23 22:07:36.881
    Apr 17 22:07:36.881: INFO: Pod "pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e" satisfied condition "Succeeded or Failed"
    Apr 17 22:07:36.884: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:07:36.891
    Apr 17 22:07:36.902: INFO: Waiting for pod pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e to disappear
    Apr 17 22:07:36.905: INFO: Pod pod-projected-secrets-6ca1d458-25c0-4ada-834c-4b7155a8258e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:07:36.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8199" for this suite. 04/17/23 22:07:36.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:07:36.918
Apr 17 22:07:36.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:07:36.919
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:36.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:36.937
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 04/17/23 22:07:36.939
Apr 17 22:07:36.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 create -f -'
Apr 17 22:07:38.646: INFO: stderr: ""
Apr 17 22:07:38.646: INFO: stdout: "pod/pause created\n"
Apr 17 22:07:38.646: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr 17 22:07:38.646: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8467" to be "running and ready"
Apr 17 22:07:38.650: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751874ms
Apr 17 22:07:38.650: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-93-18.us-west-2.compute.internal' to be 'Running' but was 'Pending'
Apr 17 22:07:40.655: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008561236s
Apr 17 22:07:40.655: INFO: Pod "pause" satisfied condition "running and ready"
Apr 17 22:07:40.655: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 04/17/23 22:07:40.655
Apr 17 22:07:40.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 label pods pause testing-label=testing-label-value'
Apr 17 22:07:40.722: INFO: stderr: ""
Apr 17 22:07:40.723: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 04/17/23 22:07:40.723
Apr 17 22:07:40.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 get pod pause -L testing-label'
Apr 17 22:07:40.781: INFO: stderr: ""
Apr 17 22:07:40.781: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 04/17/23 22:07:40.781
Apr 17 22:07:40.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 label pods pause testing-label-'
Apr 17 22:07:40.850: INFO: stderr: ""
Apr 17 22:07:40.850: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 04/17/23 22:07:40.85
Apr 17 22:07:40.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 get pod pause -L testing-label'
Apr 17 22:07:40.909: INFO: stderr: ""
Apr 17 22:07:40.909: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 04/17/23 22:07:40.909
Apr 17 22:07:40.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 delete --grace-period=0 --force -f -'
Apr 17 22:07:40.974: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 22:07:40.974: INFO: stdout: "pod \"pause\" force deleted\n"
Apr 17 22:07:40.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 get rc,svc -l name=pause --no-headers'
Apr 17 22:07:41.040: INFO: stderr: "No resources found in kubectl-8467 namespace.\n"
Apr 17 22:07:41.040: INFO: stdout: ""
Apr 17 22:07:41.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 17 22:07:41.098: INFO: stderr: ""
Apr 17 22:07:41.098: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:07:41.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8467" for this suite. 04/17/23 22:07:41.104
------------------------------
• [4.192 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:07:36.918
    Apr 17 22:07:36.918: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:07:36.919
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:36.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:36.937
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 04/17/23 22:07:36.939
    Apr 17 22:07:36.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 create -f -'
    Apr 17 22:07:38.646: INFO: stderr: ""
    Apr 17 22:07:38.646: INFO: stdout: "pod/pause created\n"
    Apr 17 22:07:38.646: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Apr 17 22:07:38.646: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8467" to be "running and ready"
    Apr 17 22:07:38.650: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.751874ms
    Apr 17 22:07:38.650: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'ip-10-0-93-18.us-west-2.compute.internal' to be 'Running' but was 'Pending'
    Apr 17 22:07:40.655: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008561236s
    Apr 17 22:07:40.655: INFO: Pod "pause" satisfied condition "running and ready"
    Apr 17 22:07:40.655: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 04/17/23 22:07:40.655
    Apr 17 22:07:40.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 label pods pause testing-label=testing-label-value'
    Apr 17 22:07:40.722: INFO: stderr: ""
    Apr 17 22:07:40.723: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 04/17/23 22:07:40.723
    Apr 17 22:07:40.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 get pod pause -L testing-label'
    Apr 17 22:07:40.781: INFO: stderr: ""
    Apr 17 22:07:40.781: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 04/17/23 22:07:40.781
    Apr 17 22:07:40.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 label pods pause testing-label-'
    Apr 17 22:07:40.850: INFO: stderr: ""
    Apr 17 22:07:40.850: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 04/17/23 22:07:40.85
    Apr 17 22:07:40.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 get pod pause -L testing-label'
    Apr 17 22:07:40.909: INFO: stderr: ""
    Apr 17 22:07:40.909: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 04/17/23 22:07:40.909
    Apr 17 22:07:40.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 delete --grace-period=0 --force -f -'
    Apr 17 22:07:40.974: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 17 22:07:40.974: INFO: stdout: "pod \"pause\" force deleted\n"
    Apr 17 22:07:40.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 get rc,svc -l name=pause --no-headers'
    Apr 17 22:07:41.040: INFO: stderr: "No resources found in kubectl-8467 namespace.\n"
    Apr 17 22:07:41.040: INFO: stdout: ""
    Apr 17 22:07:41.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8467 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr 17 22:07:41.098: INFO: stderr: ""
    Apr 17 22:07:41.098: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:07:41.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8467" for this suite. 04/17/23 22:07:41.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:07:41.112
Apr 17 22:07:41.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replicaset 04/17/23 22:07:41.113
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:41.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:41.132
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 04/17/23 22:07:41.134
Apr 17 22:07:41.143: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr 17 22:07:46.150: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/17/23 22:07:46.15
STEP: getting scale subresource 04/17/23 22:07:46.15
STEP: updating a scale subresource 04/17/23 22:07:46.154
STEP: verifying the replicaset Spec.Replicas was modified 04/17/23 22:07:46.159
STEP: Patch a scale subresource 04/17/23 22:07:46.164
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:07:46.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4120" for this suite. 04/17/23 22:07:46.196
------------------------------
• [SLOW TEST] [5.098 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:07:41.112
    Apr 17 22:07:41.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replicaset 04/17/23 22:07:41.113
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:41.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:41.132
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 04/17/23 22:07:41.134
    Apr 17 22:07:41.143: INFO: Pod name sample-pod: Found 0 pods out of 1
    Apr 17 22:07:46.150: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/17/23 22:07:46.15
    STEP: getting scale subresource 04/17/23 22:07:46.15
    STEP: updating a scale subresource 04/17/23 22:07:46.154
    STEP: verifying the replicaset Spec.Replicas was modified 04/17/23 22:07:46.159
    STEP: Patch a scale subresource 04/17/23 22:07:46.164
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:07:46.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4120" for this suite. 04/17/23 22:07:46.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:07:46.211
Apr 17 22:07:46.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:07:46.212
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:46.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:46.231
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:07:46.246
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:07:46.422
STEP: Deploying the webhook pod 04/17/23 22:07:46.43
STEP: Wait for the deployment to be ready 04/17/23 22:07:46.442
Apr 17 22:07:46.450: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:07:48.461
STEP: Verifying the service has paired with the endpoint 04/17/23 22:07:48.476
Apr 17 22:07:49.476: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 04/17/23 22:07:49.479
STEP: Updating a mutating webhook configuration's rules to not include the create operation 04/17/23 22:07:49.499
STEP: Creating a configMap that should not be mutated 04/17/23 22:07:49.506
STEP: Patching a mutating webhook configuration's rules to include the create operation 04/17/23 22:07:49.517
STEP: Creating a configMap that should be mutated 04/17/23 22:07:49.524
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:07:49.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1361" for this suite. 04/17/23 22:07:49.682
STEP: Destroying namespace "webhook-1361-markers" for this suite. 04/17/23 22:07:49.69
------------------------------
• [3.486 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:07:46.211
    Apr 17 22:07:46.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:07:46.212
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:46.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:46.231
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:07:46.246
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:07:46.422
    STEP: Deploying the webhook pod 04/17/23 22:07:46.43
    STEP: Wait for the deployment to be ready 04/17/23 22:07:46.442
    Apr 17 22:07:46.450: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:07:48.461
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:07:48.476
    Apr 17 22:07:49.476: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 04/17/23 22:07:49.479
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 04/17/23 22:07:49.499
    STEP: Creating a configMap that should not be mutated 04/17/23 22:07:49.506
    STEP: Patching a mutating webhook configuration's rules to include the create operation 04/17/23 22:07:49.517
    STEP: Creating a configMap that should be mutated 04/17/23 22:07:49.524
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:07:49.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1361" for this suite. 04/17/23 22:07:49.682
    STEP: Destroying namespace "webhook-1361-markers" for this suite. 04/17/23 22:07:49.69
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:07:49.697
Apr 17 22:07:49.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:07:49.697
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:49.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:49.713
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 04/17/23 22:07:49.716
Apr 17 22:07:49.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: mark a version not serverd 04/17/23 22:07:56.364
STEP: check the unserved version gets removed 04/17/23 22:07:56.381
STEP: check the other version is not changed 04/17/23 22:08:00.398
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:08:06.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3010" for this suite. 04/17/23 22:08:06.188
------------------------------
• [SLOW TEST] [16.499 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:07:49.697
    Apr 17 22:07:49.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:07:49.697
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:07:49.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:07:49.713
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 04/17/23 22:07:49.716
    Apr 17 22:07:49.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: mark a version not serverd 04/17/23 22:07:56.364
    STEP: check the unserved version gets removed 04/17/23 22:07:56.381
    STEP: check the other version is not changed 04/17/23 22:08:00.398
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:08:06.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3010" for this suite. 04/17/23 22:08:06.188
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:08:06.196
Apr 17 22:08:06.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:08:06.197
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:06.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:06.218
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:08:06.22
Apr 17 22:08:06.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42" in namespace "projected-373" to be "Succeeded or Failed"
Apr 17 22:08:06.234: INFO: Pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42": Phase="Pending", Reason="", readiness=false. Elapsed: 3.860346ms
Apr 17 22:08:08.239: INFO: Pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00832617s
Apr 17 22:08:10.239: INFO: Pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008529631s
STEP: Saw pod success 04/17/23 22:08:10.239
Apr 17 22:08:10.239: INFO: Pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42" satisfied condition "Succeeded or Failed"
Apr 17 22:08:10.243: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42 container client-container: <nil>
STEP: delete the pod 04/17/23 22:08:10.249
Apr 17 22:08:10.260: INFO: Waiting for pod downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42 to disappear
Apr 17 22:08:10.263: INFO: Pod downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 22:08:10.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-373" for this suite. 04/17/23 22:08:10.268
------------------------------
• [4.079 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:08:06.196
    Apr 17 22:08:06.196: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:08:06.197
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:06.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:06.218
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:08:06.22
    Apr 17 22:08:06.230: INFO: Waiting up to 5m0s for pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42" in namespace "projected-373" to be "Succeeded or Failed"
    Apr 17 22:08:06.234: INFO: Pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42": Phase="Pending", Reason="", readiness=false. Elapsed: 3.860346ms
    Apr 17 22:08:08.239: INFO: Pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00832617s
    Apr 17 22:08:10.239: INFO: Pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008529631s
    STEP: Saw pod success 04/17/23 22:08:10.239
    Apr 17 22:08:10.239: INFO: Pod "downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42" satisfied condition "Succeeded or Failed"
    Apr 17 22:08:10.243: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42 container client-container: <nil>
    STEP: delete the pod 04/17/23 22:08:10.249
    Apr 17 22:08:10.260: INFO: Waiting for pod downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42 to disappear
    Apr 17 22:08:10.263: INFO: Pod downwardapi-volume-525388c4-7032-4ebb-a9d0-3d1803fe0d42 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:08:10.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-373" for this suite. 04/17/23 22:08:10.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:08:10.275
Apr 17 22:08:10.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:08:10.276
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:10.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:10.293
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Apr 17 22:08:10.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 create -f -'
Apr 17 22:08:11.533: INFO: stderr: ""
Apr 17 22:08:11.533: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Apr 17 22:08:11.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 create -f -'
Apr 17 22:08:12.898: INFO: stderr: ""
Apr 17 22:08:12.898: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/17/23 22:08:12.898
Apr 17 22:08:13.903: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 17 22:08:13.903: INFO: Found 1 / 1
Apr 17 22:08:13.903: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 17 22:08:13.906: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 17 22:08:13.906: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 17 22:08:13.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe pod agnhost-primary-mfg2m'
Apr 17 22:08:13.973: INFO: stderr: ""
Apr 17 22:08:13.973: INFO: stdout: "Name:             agnhost-primary-mfg2m\nNamespace:        kubectl-1100\nPriority:         0\nService Account:  default\nNode:             ip-10-0-74-52.us-west-2.compute.internal/10.0.74.52\nStart Time:       Mon, 17 Apr 2023 22:08:11 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 27f348365a5d4e8df5b7e0d5d0dfaf3629c767fd4e6cf20a25a92b09c56e34b0\n                  cni.projectcalico.org/podIP: 192.168.208.178/32\n                  cni.projectcalico.org/podIPs: 192.168.208.178/32\nStatus:           Running\nIP:               192.168.208.178\nIPs:\n  IP:           192.168.208.178\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c18c66ce1d56b4e8ba4475bb92dba2372ddb75e1274c2fcdc347ef05d8dbf464\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 17 Apr 2023 22:08:12 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fz5d6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fz5d6:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1100/agnhost-primary-mfg2m to ip-10-0-74-52.us-west-2.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Apr 17 22:08:13.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe rc agnhost-primary'
Apr 17 22:08:14.044: INFO: stderr: ""
Apr 17 22:08:14.044: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1100\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-mfg2m\n"
Apr 17 22:08:14.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe service agnhost-primary'
Apr 17 22:08:14.110: INFO: stderr: ""
Apr 17 22:08:14.110: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1100\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.98.220.104\nIPs:               10.98.220.104\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.208.178:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr 17 22:08:14.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe node ip-10-0-106-231.us-west-2.compute.internal'
Apr 17 22:08:14.204: INFO: stderr: ""
Apr 17 22:08:14.204: INFO: stdout: "Name:               ip-10-0-106-231.us-west-2.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2a\n                    feature.node.kubernetes.io/cpu-cpuid.ADX=true\n                    feature.node.kubernetes.io/cpu-cpuid.AESNI=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX2=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512BW=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512CD=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512DQ=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512F=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512VL=true\n                    feature.node.kubernetes.io/cpu-cpuid.CMPXCHG8=true\n                    feature.node.kubernetes.io/cpu-cpuid.FMA3=true\n                    feature.node.kubernetes.io/cpu-cpuid.FXSR=true\n                    feature.node.kubernetes.io/cpu-cpuid.FXSROPT=true\n                    feature.node.kubernetes.io/cpu-cpuid.HLE=true\n                    feature.node.kubernetes.io/cpu-cpuid.HYPERVISOR=true\n                    feature.node.kubernetes.io/cpu-cpuid.LAHF=true\n                    feature.node.kubernetes.io/cpu-cpuid.MOVBE=true\n                    feature.node.kubernetes.io/cpu-cpuid.MPX=true\n                    feature.node.kubernetes.io/cpu-cpuid.OSXSAVE=true\n                    feature.node.kubernetes.io/cpu-cpuid.RTM=true\n                    feature.node.kubernetes.io/cpu-cpuid.SYSCALL=true\n                    feature.node.kubernetes.io/cpu-cpuid.SYSEE=true\n                    feature.node.kubernetes.io/cpu-cpuid.X87=true\n                    feature.node.kubernetes.io/cpu-cpuid.XGETBV1=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVE=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVEC=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVEOPT=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVES=true\n                    feature.node.kubernetes.io/cpu-hardware_multithreading=true\n                    feature.node.kubernetes.io/cpu-model.family=6\n                    feature.node.kubernetes.io/cpu-model.id=85\n                    feature.node.kubernetes.io/cpu-model.vendor_id=Intel\n                    feature.node.kubernetes.io/kernel-config.NO_HZ=true\n                    feature.node.kubernetes.io/kernel-config.NO_HZ_IDLE=true\n                    feature.node.kubernetes.io/kernel-version.full=5.15.0-1031-aws\n                    feature.node.kubernetes.io/kernel-version.major=5\n                    feature.node.kubernetes.io/kernel-version.minor=15\n                    feature.node.kubernetes.io/kernel-version.revision=0\n                    feature.node.kubernetes.io/pci-0300_1d0f.present=true\n                    feature.node.kubernetes.io/storage-nonrotationaldisk=true\n                    feature.node.kubernetes.io/system-os_release.ID=ubuntu\n                    feature.node.kubernetes.io/system-os_release.VERSION_ID=20.04\n                    feature.node.kubernetes.io/system-os_release.VERSION_ID.major=20\n                    feature.node.kubernetes.io/system-os_release.VERSION_ID.minor=04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-106-231.us-west-2.compute.internal\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=m5.2xlarge\n                    topology.ebs.csi.aws.com/zone=us-west-2a\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2a\nAnnotations:        cluster.x-k8s.io/cluster-name: cncf-conformance-v2.6.0-dev.2\n                    cluster.x-k8s.io/cluster-namespace: default\n                    cluster.x-k8s.io/machine: cncf-conformance-v2.6.0-dev.2-md-0-5f4c6f8747-6fvzb\n                    cluster.x-k8s.io/owner-kind: MachineSet\n                    cluster.x-k8s.io/owner-name: cncf-conformance-v2.6.0-dev.2-md-0-5f4c6f8747\n                    csi.volume.kubernetes.io/nodeid: {\"csi.tigera.io\":\"ip-10-0-106-231.us-west-2.compute.internal\",\"ebs.csi.aws.com\":\"i-07b114fd7de8d5e8f\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    nfd.node.kubernetes.io/extended-resources: \n                    nfd.node.kubernetes.io/feature-labels:\n                      cpu-cpuid.ADX,cpu-cpuid.AESNI,cpu-cpuid.AVX,cpu-cpuid.AVX2,cpu-cpuid.AVX512BW,cpu-cpuid.AVX512CD,cpu-cpuid.AVX512DQ,cpu-cpuid.AVX512F,cpu-...\n                    nfd.node.kubernetes.io/worker.version: v0.12.1\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.106.231/18\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.163.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 17 Apr 2023 21:05:06 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-106-231.us-west-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 17 Apr 2023 22:08:12 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 17 Apr 2023 21:05:48 +0000   Mon, 17 Apr 2023 21:05:48 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 17 Apr 2023 22:03:44 +0000   Mon, 17 Apr 2023 21:05:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 17 Apr 2023 22:03:44 +0000   Mon, 17 Apr 2023 21:05:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 17 Apr 2023 22:03:44 +0000   Mon, 17 Apr 2023 21:05:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 17 Apr 2023 22:03:44 +0000   Mon, 17 Apr 2023 21:05:39 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.0.106.231\n  Hostname:     ip-10-0-106-231.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-106-231.us-west-2.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           81106868Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      32055880Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       5\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           74748089426\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      31953480Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       5\nSystem Info:\n  Machine ID:                 ec2fcc816aabf45ed22b5593003f4ec2\n  System UUID:                ec2fcc81-6aab-f45e-d22b-5593003f4ec2\n  Boot ID:                    228572fb-7957-42ac-9d17-07a426138928\n  Kernel Version:             5.15.0-1031-aws\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19\n  Kubelet Version:            v1.26.3\n  Kube-Proxy Version:         v1.26.3\nPodCIDR:                      192.168.1.0/24\nPodCIDRs:                     192.168.1.0/24\nProviderID:                   aws:///us-west-2a/i-07b114fd7de8d5e8f\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-node-qwskb                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  calico-system               calico-typha-6dc6fbd9f5-tcn27                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         62m\n  calico-system               csi-node-driver-pptcd                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  kube-system                 ebs-csi-node-lhxwm                                         30m (0%)      300m (3%)   120Mi (0%)       768Mi (2%)     63m\n  kube-system                 kube-proxy-h249s                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  node-feature-discovery      node-feature-discovery-worker-9bc5l                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         62m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         30m (0%)    300m (3%)\n  memory                      120Mi (0%)  768Mi (2%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\n  scheduling.k8s.io/foo       0           0\nEvents:                       <none>\n"
Apr 17 22:08:14.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe namespace kubectl-1100'
Apr 17 22:08:14.272: INFO: stderr: ""
Apr 17 22:08:14.272: INFO: stdout: "Name:         kubectl-1100\nLabels:       e2e-framework=kubectl\n              e2e-run=a8206136-e860-42f7-86bb-baf1eab83f33\n              kubernetes.io/metadata.name=kubectl-1100\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:08:14.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1100" for this suite. 04/17/23 22:08:14.277
------------------------------
• [4.008 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:08:10.275
    Apr 17 22:08:10.275: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:08:10.276
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:10.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:10.293
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Apr 17 22:08:10.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 create -f -'
    Apr 17 22:08:11.533: INFO: stderr: ""
    Apr 17 22:08:11.533: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Apr 17 22:08:11.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 create -f -'
    Apr 17 22:08:12.898: INFO: stderr: ""
    Apr 17 22:08:12.898: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/17/23 22:08:12.898
    Apr 17 22:08:13.903: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 17 22:08:13.903: INFO: Found 1 / 1
    Apr 17 22:08:13.903: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Apr 17 22:08:13.906: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 17 22:08:13.906: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr 17 22:08:13.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe pod agnhost-primary-mfg2m'
    Apr 17 22:08:13.973: INFO: stderr: ""
    Apr 17 22:08:13.973: INFO: stdout: "Name:             agnhost-primary-mfg2m\nNamespace:        kubectl-1100\nPriority:         0\nService Account:  default\nNode:             ip-10-0-74-52.us-west-2.compute.internal/10.0.74.52\nStart Time:       Mon, 17 Apr 2023 22:08:11 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 27f348365a5d4e8df5b7e0d5d0dfaf3629c767fd4e6cf20a25a92b09c56e34b0\n                  cni.projectcalico.org/podIP: 192.168.208.178/32\n                  cni.projectcalico.org/podIPs: 192.168.208.178/32\nStatus:           Running\nIP:               192.168.208.178\nIPs:\n  IP:           192.168.208.178\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://c18c66ce1d56b4e8ba4475bb92dba2372ddb75e1274c2fcdc347ef05d8dbf464\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 17 Apr 2023 22:08:12 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-fz5d6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-fz5d6:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-1100/agnhost-primary-mfg2m to ip-10-0-74-52.us-west-2.compute.internal\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Apr 17 22:08:13.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe rc agnhost-primary'
    Apr 17 22:08:14.044: INFO: stderr: ""
    Apr 17 22:08:14.044: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-1100\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-mfg2m\n"
    Apr 17 22:08:14.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe service agnhost-primary'
    Apr 17 22:08:14.110: INFO: stderr: ""
    Apr 17 22:08:14.110: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-1100\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.98.220.104\nIPs:               10.98.220.104\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.208.178:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Apr 17 22:08:14.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe node ip-10-0-106-231.us-west-2.compute.internal'
    Apr 17 22:08:14.204: INFO: stderr: ""
    Apr 17 22:08:14.204: INFO: stdout: "Name:               ip-10-0-106-231.us-west-2.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=m5.2xlarge\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-west-2\n                    failure-domain.beta.kubernetes.io/zone=us-west-2a\n                    feature.node.kubernetes.io/cpu-cpuid.ADX=true\n                    feature.node.kubernetes.io/cpu-cpuid.AESNI=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX2=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512BW=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512CD=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512DQ=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512F=true\n                    feature.node.kubernetes.io/cpu-cpuid.AVX512VL=true\n                    feature.node.kubernetes.io/cpu-cpuid.CMPXCHG8=true\n                    feature.node.kubernetes.io/cpu-cpuid.FMA3=true\n                    feature.node.kubernetes.io/cpu-cpuid.FXSR=true\n                    feature.node.kubernetes.io/cpu-cpuid.FXSROPT=true\n                    feature.node.kubernetes.io/cpu-cpuid.HLE=true\n                    feature.node.kubernetes.io/cpu-cpuid.HYPERVISOR=true\n                    feature.node.kubernetes.io/cpu-cpuid.LAHF=true\n                    feature.node.kubernetes.io/cpu-cpuid.MOVBE=true\n                    feature.node.kubernetes.io/cpu-cpuid.MPX=true\n                    feature.node.kubernetes.io/cpu-cpuid.OSXSAVE=true\n                    feature.node.kubernetes.io/cpu-cpuid.RTM=true\n                    feature.node.kubernetes.io/cpu-cpuid.SYSCALL=true\n                    feature.node.kubernetes.io/cpu-cpuid.SYSEE=true\n                    feature.node.kubernetes.io/cpu-cpuid.X87=true\n                    feature.node.kubernetes.io/cpu-cpuid.XGETBV1=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVE=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVEC=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVEOPT=true\n                    feature.node.kubernetes.io/cpu-cpuid.XSAVES=true\n                    feature.node.kubernetes.io/cpu-hardware_multithreading=true\n                    feature.node.kubernetes.io/cpu-model.family=6\n                    feature.node.kubernetes.io/cpu-model.id=85\n                    feature.node.kubernetes.io/cpu-model.vendor_id=Intel\n                    feature.node.kubernetes.io/kernel-config.NO_HZ=true\n                    feature.node.kubernetes.io/kernel-config.NO_HZ_IDLE=true\n                    feature.node.kubernetes.io/kernel-version.full=5.15.0-1031-aws\n                    feature.node.kubernetes.io/kernel-version.major=5\n                    feature.node.kubernetes.io/kernel-version.minor=15\n                    feature.node.kubernetes.io/kernel-version.revision=0\n                    feature.node.kubernetes.io/pci-0300_1d0f.present=true\n                    feature.node.kubernetes.io/storage-nonrotationaldisk=true\n                    feature.node.kubernetes.io/system-os_release.ID=ubuntu\n                    feature.node.kubernetes.io/system-os_release.VERSION_ID=20.04\n                    feature.node.kubernetes.io/system-os_release.VERSION_ID.major=20\n                    feature.node.kubernetes.io/system-os_release.VERSION_ID.minor=04\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-106-231.us-west-2.compute.internal\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=m5.2xlarge\n                    topology.ebs.csi.aws.com/zone=us-west-2a\n                    topology.kubernetes.io/region=us-west-2\n                    topology.kubernetes.io/zone=us-west-2a\nAnnotations:        cluster.x-k8s.io/cluster-name: cncf-conformance-v2.6.0-dev.2\n                    cluster.x-k8s.io/cluster-namespace: default\n                    cluster.x-k8s.io/machine: cncf-conformance-v2.6.0-dev.2-md-0-5f4c6f8747-6fvzb\n                    cluster.x-k8s.io/owner-kind: MachineSet\n                    cluster.x-k8s.io/owner-name: cncf-conformance-v2.6.0-dev.2-md-0-5f4c6f8747\n                    csi.volume.kubernetes.io/nodeid: {\"csi.tigera.io\":\"ip-10-0-106-231.us-west-2.compute.internal\",\"ebs.csi.aws.com\":\"i-07b114fd7de8d5e8f\"}\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    nfd.node.kubernetes.io/extended-resources: \n                    nfd.node.kubernetes.io/feature-labels:\n                      cpu-cpuid.ADX,cpu-cpuid.AESNI,cpu-cpuid.AVX,cpu-cpuid.AVX2,cpu-cpuid.AVX512BW,cpu-cpuid.AVX512CD,cpu-cpuid.AVX512DQ,cpu-cpuid.AVX512F,cpu-...\n                    nfd.node.kubernetes.io/worker.version: v0.12.1\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.106.231/18\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.163.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 17 Apr 2023 21:05:06 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ip-10-0-106-231.us-west-2.compute.internal\n  AcquireTime:     <unset>\n  RenewTime:       Mon, 17 Apr 2023 22:08:12 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 17 Apr 2023 21:05:48 +0000   Mon, 17 Apr 2023 21:05:48 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 17 Apr 2023 22:03:44 +0000   Mon, 17 Apr 2023 21:05:06 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 17 Apr 2023 22:03:44 +0000   Mon, 17 Apr 2023 21:05:06 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 17 Apr 2023 22:03:44 +0000   Mon, 17 Apr 2023 21:05:06 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 17 Apr 2023 22:03:44 +0000   Mon, 17 Apr 2023 21:05:39 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   10.0.106.231\n  Hostname:     ip-10-0-106-231.us-west-2.compute.internal\n  InternalDNS:  ip-10-0-106-231.us-west-2.compute.internal\nCapacity:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           81106868Ki\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      32055880Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       5\nAllocatable:\n  attachable-volumes-aws-ebs:  25\n  cpu:                         8\n  ephemeral-storage:           74748089426\n  hugepages-1Gi:               0\n  hugepages-2Mi:               0\n  memory:                      31953480Ki\n  pods:                        110\n  scheduling.k8s.io/foo:       5\nSystem Info:\n  Machine ID:                 ec2fcc816aabf45ed22b5593003f4ec2\n  System UUID:                ec2fcc81-6aab-f45e-d22b-5593003f4ec2\n  Boot ID:                    228572fb-7957-42ac-9d17-07a426138928\n  Kernel Version:             5.15.0-1031-aws\n  OS Image:                   Ubuntu 20.04.6 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19\n  Kubelet Version:            v1.26.3\n  Kube-Proxy Version:         v1.26.3\nPodCIDR:                      192.168.1.0/24\nPodCIDRs:                     192.168.1.0/24\nProviderID:                   aws:///us-west-2a/i-07b114fd7de8d5e8f\nNon-terminated Pods:          (7 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system               calico-node-qwskb                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  calico-system               calico-typha-6dc6fbd9f5-tcn27                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         62m\n  calico-system               csi-node-driver-pptcd                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  kube-system                 ebs-csi-node-lhxwm                                         30m (0%)      300m (3%)   120Mi (0%)       768Mi (2%)     63m\n  kube-system                 kube-proxy-h249s                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         63m\n  node-feature-discovery      node-feature-discovery-worker-9bc5l                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         62m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-9578e18701e8402b-xb2qr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         59m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests    Limits\n  --------                    --------    ------\n  cpu                         30m (0%)    300m (3%)\n  memory                      120Mi (0%)  768Mi (2%)\n  ephemeral-storage           0 (0%)      0 (0%)\n  hugepages-1Gi               0 (0%)      0 (0%)\n  hugepages-2Mi               0 (0%)      0 (0%)\n  attachable-volumes-aws-ebs  0           0\n  scheduling.k8s.io/foo       0           0\nEvents:                       <none>\n"
    Apr 17 22:08:14.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1100 describe namespace kubectl-1100'
    Apr 17 22:08:14.272: INFO: stderr: ""
    Apr 17 22:08:14.272: INFO: stdout: "Name:         kubectl-1100\nLabels:       e2e-framework=kubectl\n              e2e-run=a8206136-e860-42f7-86bb-baf1eab83f33\n              kubernetes.io/metadata.name=kubectl-1100\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:08:14.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1100" for this suite. 04/17/23 22:08:14.277
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:08:14.285
Apr 17 22:08:14.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename subpath 04/17/23 22:08:14.285
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:14.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:14.301
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/17/23 22:08:14.304
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-6fgn 04/17/23 22:08:14.313
STEP: Creating a pod to test atomic-volume-subpath 04/17/23 22:08:14.313
Apr 17 22:08:14.321: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6fgn" in namespace "subpath-8734" to be "Succeeded or Failed"
Apr 17 22:08:14.324: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.130324ms
Apr 17 22:08:16.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 2.007417504s
Apr 17 22:08:18.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 4.007253687s
Apr 17 22:08:20.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 6.007253871s
Apr 17 22:08:22.329: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 8.007779805s
Apr 17 22:08:24.329: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 10.007496229s
Apr 17 22:08:26.329: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 12.007782726s
Apr 17 22:08:28.329: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 14.007565306s
Apr 17 22:08:30.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 16.006908398s
Apr 17 22:08:32.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 18.007388846s
Apr 17 22:08:34.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 20.007258448s
Apr 17 22:08:36.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=false. Elapsed: 22.007239815s
Apr 17 22:08:38.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007110341s
STEP: Saw pod success 04/17/23 22:08:38.328
Apr 17 22:08:38.328: INFO: Pod "pod-subpath-test-configmap-6fgn" satisfied condition "Succeeded or Failed"
Apr 17 22:08:38.331: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-subpath-test-configmap-6fgn container test-container-subpath-configmap-6fgn: <nil>
STEP: delete the pod 04/17/23 22:08:38.347
Apr 17 22:08:38.358: INFO: Waiting for pod pod-subpath-test-configmap-6fgn to disappear
Apr 17 22:08:38.361: INFO: Pod pod-subpath-test-configmap-6fgn no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6fgn 04/17/23 22:08:38.361
Apr 17 22:08:38.361: INFO: Deleting pod "pod-subpath-test-configmap-6fgn" in namespace "subpath-8734"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 17 22:08:38.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8734" for this suite. 04/17/23 22:08:38.369
------------------------------
• [SLOW TEST] [24.090 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:08:14.285
    Apr 17 22:08:14.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename subpath 04/17/23 22:08:14.285
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:14.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:14.301
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/17/23 22:08:14.304
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-6fgn 04/17/23 22:08:14.313
    STEP: Creating a pod to test atomic-volume-subpath 04/17/23 22:08:14.313
    Apr 17 22:08:14.321: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6fgn" in namespace "subpath-8734" to be "Succeeded or Failed"
    Apr 17 22:08:14.324: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.130324ms
    Apr 17 22:08:16.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 2.007417504s
    Apr 17 22:08:18.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 4.007253687s
    Apr 17 22:08:20.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 6.007253871s
    Apr 17 22:08:22.329: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 8.007779805s
    Apr 17 22:08:24.329: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 10.007496229s
    Apr 17 22:08:26.329: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 12.007782726s
    Apr 17 22:08:28.329: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 14.007565306s
    Apr 17 22:08:30.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 16.006908398s
    Apr 17 22:08:32.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 18.007388846s
    Apr 17 22:08:34.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=true. Elapsed: 20.007258448s
    Apr 17 22:08:36.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Running", Reason="", readiness=false. Elapsed: 22.007239815s
    Apr 17 22:08:38.328: INFO: Pod "pod-subpath-test-configmap-6fgn": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007110341s
    STEP: Saw pod success 04/17/23 22:08:38.328
    Apr 17 22:08:38.328: INFO: Pod "pod-subpath-test-configmap-6fgn" satisfied condition "Succeeded or Failed"
    Apr 17 22:08:38.331: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-subpath-test-configmap-6fgn container test-container-subpath-configmap-6fgn: <nil>
    STEP: delete the pod 04/17/23 22:08:38.347
    Apr 17 22:08:38.358: INFO: Waiting for pod pod-subpath-test-configmap-6fgn to disappear
    Apr 17 22:08:38.361: INFO: Pod pod-subpath-test-configmap-6fgn no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-6fgn 04/17/23 22:08:38.361
    Apr 17 22:08:38.361: INFO: Deleting pod "pod-subpath-test-configmap-6fgn" in namespace "subpath-8734"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:08:38.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8734" for this suite. 04/17/23 22:08:38.369
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:08:38.375
Apr 17 22:08:38.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename conformance-tests 04/17/23 22:08:38.376
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:38.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:38.393
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 04/17/23 22:08:38.395
Apr 17 22:08:38.395: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Apr 17 22:08:38.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-4333" for this suite. 04/17/23 22:08:38.407
------------------------------
• [0.038 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:08:38.375
    Apr 17 22:08:38.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename conformance-tests 04/17/23 22:08:38.376
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:38.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:38.393
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 04/17/23 22:08:38.395
    Apr 17 22:08:38.395: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:08:38.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-4333" for this suite. 04/17/23 22:08:38.407
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:08:38.413
Apr 17 22:08:38.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-lifecycle-hook 04/17/23 22:08:38.414
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:38.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:38.43
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/17/23 22:08:38.44
Apr 17 22:08:38.448: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9411" to be "running and ready"
Apr 17 22:08:38.453: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.429654ms
Apr 17 22:08:38.453: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:08:40.458: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010212615s
Apr 17 22:08:40.458: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 17 22:08:40.458: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 04/17/23 22:08:40.461
Apr 17 22:08:40.468: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9411" to be "running and ready"
Apr 17 22:08:40.473: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912821ms
Apr 17 22:08:40.473: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:08:42.478: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009745428s
Apr 17 22:08:42.478: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Apr 17 22:08:42.478: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 04/17/23 22:08:42.481
Apr 17 22:08:42.488: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 17 22:08:42.492: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 17 22:08:44.493: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 17 22:08:44.497: INFO: Pod pod-with-prestop-exec-hook still exists
Apr 17 22:08:46.494: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr 17 22:08:46.498: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 04/17/23 22:08:46.498
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 17 22:08:46.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9411" for this suite. 04/17/23 22:08:46.51
------------------------------
• [SLOW TEST] [8.105 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:08:38.413
    Apr 17 22:08:38.413: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/17/23 22:08:38.414
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:38.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:38.43
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/17/23 22:08:38.44
    Apr 17 22:08:38.448: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9411" to be "running and ready"
    Apr 17 22:08:38.453: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.429654ms
    Apr 17 22:08:38.453: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:08:40.458: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010212615s
    Apr 17 22:08:40.458: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 17 22:08:40.458: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 04/17/23 22:08:40.461
    Apr 17 22:08:40.468: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-9411" to be "running and ready"
    Apr 17 22:08:40.473: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.912821ms
    Apr 17 22:08:40.473: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:08:42.478: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009745428s
    Apr 17 22:08:42.478: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Apr 17 22:08:42.478: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 04/17/23 22:08:42.481
    Apr 17 22:08:42.488: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr 17 22:08:42.492: INFO: Pod pod-with-prestop-exec-hook still exists
    Apr 17 22:08:44.493: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr 17 22:08:44.497: INFO: Pod pod-with-prestop-exec-hook still exists
    Apr 17 22:08:46.494: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Apr 17 22:08:46.498: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 04/17/23 22:08:46.498
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:08:46.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9411" for this suite. 04/17/23 22:08:46.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:08:46.519
Apr 17 22:08:46.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:08:46.52
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:46.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:46.536
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-afc20a0f-e7fb-4d84-9244-f406ca8802c0 04/17/23 22:08:46.538
STEP: Creating a pod to test consume configMaps 04/17/23 22:08:46.544
Apr 17 22:08:46.552: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f" in namespace "projected-5596" to be "Succeeded or Failed"
Apr 17 22:08:46.557: INFO: Pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489097ms
Apr 17 22:08:48.561: INFO: Pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008954252s
Apr 17 22:08:50.562: INFO: Pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009780787s
STEP: Saw pod success 04/17/23 22:08:50.562
Apr 17 22:08:50.562: INFO: Pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f" satisfied condition "Succeeded or Failed"
Apr 17 22:08:50.565: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:08:50.572
Apr 17 22:08:50.587: INFO: Waiting for pod pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f to disappear
Apr 17 22:08:50.590: INFO: Pod pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:08:50.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5596" for this suite. 04/17/23 22:08:50.596
------------------------------
• [4.084 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:08:46.519
    Apr 17 22:08:46.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:08:46.52
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:46.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:46.536
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-afc20a0f-e7fb-4d84-9244-f406ca8802c0 04/17/23 22:08:46.538
    STEP: Creating a pod to test consume configMaps 04/17/23 22:08:46.544
    Apr 17 22:08:46.552: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f" in namespace "projected-5596" to be "Succeeded or Failed"
    Apr 17 22:08:46.557: INFO: Pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.489097ms
    Apr 17 22:08:48.561: INFO: Pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008954252s
    Apr 17 22:08:50.562: INFO: Pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009780787s
    STEP: Saw pod success 04/17/23 22:08:50.562
    Apr 17 22:08:50.562: INFO: Pod "pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f" satisfied condition "Succeeded or Failed"
    Apr 17 22:08:50.565: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:08:50.572
    Apr 17 22:08:50.587: INFO: Waiting for pod pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f to disappear
    Apr 17 22:08:50.590: INFO: Pod pod-projected-configmaps-3ae52422-b324-4a44-a202-c3292c73e63f no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:08:50.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5596" for this suite. 04/17/23 22:08:50.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:08:50.603
Apr 17 22:08:50.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename proxy 04/17/23 22:08:50.604
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:50.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:50.623
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Apr 17 22:08:50.625: INFO: Creating pod...
Apr 17 22:08:50.633: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9950" to be "running"
Apr 17 22:08:50.637: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.423521ms
Apr 17 22:08:52.642: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00911566s
Apr 17 22:08:52.642: INFO: Pod "agnhost" satisfied condition "running"
Apr 17 22:08:52.642: INFO: Creating service...
Apr 17 22:08:52.656: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=DELETE
Apr 17 22:08:52.662: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 17 22:08:52.662: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=OPTIONS
Apr 17 22:08:52.666: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 17 22:08:52.666: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=PATCH
Apr 17 22:08:52.671: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 17 22:08:52.671: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=POST
Apr 17 22:08:52.675: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 17 22:08:52.675: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=PUT
Apr 17 22:08:52.679: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 17 22:08:52.679: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=DELETE
Apr 17 22:08:52.685: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 17 22:08:52.685: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=OPTIONS
Apr 17 22:08:52.691: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 17 22:08:52.691: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=PATCH
Apr 17 22:08:52.697: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 17 22:08:52.697: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=POST
Apr 17 22:08:52.703: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 17 22:08:52.703: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=PUT
Apr 17 22:08:52.709: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 17 22:08:52.709: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=GET
Apr 17 22:08:52.712: INFO: http.Client request:GET StatusCode:301
Apr 17 22:08:52.712: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=GET
Apr 17 22:08:52.718: INFO: http.Client request:GET StatusCode:301
Apr 17 22:08:52.718: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=HEAD
Apr 17 22:08:52.721: INFO: http.Client request:HEAD StatusCode:301
Apr 17 22:08:52.721: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=HEAD
Apr 17 22:08:52.728: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Apr 17 22:08:52.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-9950" for this suite. 04/17/23 22:08:52.733
------------------------------
• [2.138 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:08:50.603
    Apr 17 22:08:50.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename proxy 04/17/23 22:08:50.604
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:50.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:50.623
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Apr 17 22:08:50.625: INFO: Creating pod...
    Apr 17 22:08:50.633: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9950" to be "running"
    Apr 17 22:08:50.637: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.423521ms
    Apr 17 22:08:52.642: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.00911566s
    Apr 17 22:08:52.642: INFO: Pod "agnhost" satisfied condition "running"
    Apr 17 22:08:52.642: INFO: Creating service...
    Apr 17 22:08:52.656: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=DELETE
    Apr 17 22:08:52.662: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 17 22:08:52.662: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=OPTIONS
    Apr 17 22:08:52.666: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 17 22:08:52.666: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=PATCH
    Apr 17 22:08:52.671: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 17 22:08:52.671: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=POST
    Apr 17 22:08:52.675: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 17 22:08:52.675: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=PUT
    Apr 17 22:08:52.679: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr 17 22:08:52.679: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=DELETE
    Apr 17 22:08:52.685: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 17 22:08:52.685: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Apr 17 22:08:52.691: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 17 22:08:52.691: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=PATCH
    Apr 17 22:08:52.697: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 17 22:08:52.697: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=POST
    Apr 17 22:08:52.703: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 17 22:08:52.703: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=PUT
    Apr 17 22:08:52.709: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr 17 22:08:52.709: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=GET
    Apr 17 22:08:52.712: INFO: http.Client request:GET StatusCode:301
    Apr 17 22:08:52.712: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=GET
    Apr 17 22:08:52.718: INFO: http.Client request:GET StatusCode:301
    Apr 17 22:08:52.718: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/pods/agnhost/proxy?method=HEAD
    Apr 17 22:08:52.721: INFO: http.Client request:HEAD StatusCode:301
    Apr 17 22:08:52.721: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9950/services/e2e-proxy-test-service/proxy?method=HEAD
    Apr 17 22:08:52.728: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:08:52.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-9950" for this suite. 04/17/23 22:08:52.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:08:52.742
Apr 17 22:08:52.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename init-container 04/17/23 22:08:52.743
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:52.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:52.762
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 04/17/23 22:08:52.765
Apr 17 22:08:52.765: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:08:56.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3912" for this suite. 04/17/23 22:08:56.314
------------------------------
• [3.578 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:08:52.742
    Apr 17 22:08:52.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename init-container 04/17/23 22:08:52.743
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:52.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:52.762
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 04/17/23 22:08:52.765
    Apr 17 22:08:52.765: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:08:56.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3912" for this suite. 04/17/23 22:08:56.314
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:08:56.32
Apr 17 22:08:56.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-lifecycle-hook 04/17/23 22:08:56.321
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:56.335
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:56.338
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/17/23 22:08:56.346
Apr 17 22:08:56.354: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1963" to be "running and ready"
Apr 17 22:08:56.359: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.730212ms
Apr 17 22:08:56.359: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:08:58.363: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008791877s
Apr 17 22:08:58.363: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 17 22:08:58.363: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 04/17/23 22:08:58.367
Apr 17 22:08:58.375: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1963" to be "running and ready"
Apr 17 22:08:58.380: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.618958ms
Apr 17 22:08:58.380: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:09:00.385: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010586649s
Apr 17 22:09:00.385: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Apr 17 22:09:00.385: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 04/17/23 22:09:00.389
STEP: delete the pod with lifecycle hook 04/17/23 22:09:00.395
Apr 17 22:09:00.403: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 17 22:09:00.406: INFO: Pod pod-with-poststart-exec-hook still exists
Apr 17 22:09:02.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr 17 22:09:02.410: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 17 22:09:02.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-1963" for this suite. 04/17/23 22:09:02.417
------------------------------
• [SLOW TEST] [6.103 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:08:56.32
    Apr 17 22:08:56.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/17/23 22:08:56.321
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:08:56.335
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:08:56.338
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/17/23 22:08:56.346
    Apr 17 22:08:56.354: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-1963" to be "running and ready"
    Apr 17 22:08:56.359: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.730212ms
    Apr 17 22:08:56.359: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:08:58.363: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008791877s
    Apr 17 22:08:58.363: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 17 22:08:58.363: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 04/17/23 22:08:58.367
    Apr 17 22:08:58.375: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-1963" to be "running and ready"
    Apr 17 22:08:58.380: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 5.618958ms
    Apr 17 22:08:58.380: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:09:00.385: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010586649s
    Apr 17 22:09:00.385: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Apr 17 22:09:00.385: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 04/17/23 22:09:00.389
    STEP: delete the pod with lifecycle hook 04/17/23 22:09:00.395
    Apr 17 22:09:00.403: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Apr 17 22:09:00.406: INFO: Pod pod-with-poststart-exec-hook still exists
    Apr 17 22:09:02.406: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Apr 17 22:09:02.410: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:09:02.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-1963" for this suite. 04/17/23 22:09:02.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:09:02.424
Apr 17 22:09:02.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename var-expansion 04/17/23 22:09:02.425
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:02.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:02.443
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 04/17/23 22:09:02.445
Apr 17 22:09:02.455: INFO: Waiting up to 5m0s for pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee" in namespace "var-expansion-8685" to be "Succeeded or Failed"
Apr 17 22:09:02.459: INFO: Pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.761395ms
Apr 17 22:09:04.463: INFO: Pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee": Phase="Running", Reason="", readiness=false. Elapsed: 2.008110644s
Apr 17 22:09:06.464: INFO: Pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008737254s
STEP: Saw pod success 04/17/23 22:09:06.464
Apr 17 22:09:06.464: INFO: Pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee" satisfied condition "Succeeded or Failed"
Apr 17 22:09:06.467: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee container dapi-container: <nil>
STEP: delete the pod 04/17/23 22:09:06.474
Apr 17 22:09:06.489: INFO: Waiting for pod var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee to disappear
Apr 17 22:09:06.492: INFO: Pod var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 17 22:09:06.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8685" for this suite. 04/17/23 22:09:06.497
------------------------------
• [4.080 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:09:02.424
    Apr 17 22:09:02.424: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename var-expansion 04/17/23 22:09:02.425
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:02.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:02.443
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 04/17/23 22:09:02.445
    Apr 17 22:09:02.455: INFO: Waiting up to 5m0s for pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee" in namespace "var-expansion-8685" to be "Succeeded or Failed"
    Apr 17 22:09:02.459: INFO: Pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee": Phase="Pending", Reason="", readiness=false. Elapsed: 3.761395ms
    Apr 17 22:09:04.463: INFO: Pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee": Phase="Running", Reason="", readiness=false. Elapsed: 2.008110644s
    Apr 17 22:09:06.464: INFO: Pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008737254s
    STEP: Saw pod success 04/17/23 22:09:06.464
    Apr 17 22:09:06.464: INFO: Pod "var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee" satisfied condition "Succeeded or Failed"
    Apr 17 22:09:06.467: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee container dapi-container: <nil>
    STEP: delete the pod 04/17/23 22:09:06.474
    Apr 17 22:09:06.489: INFO: Waiting for pod var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee to disappear
    Apr 17 22:09:06.492: INFO: Pod var-expansion-a421aa74-74f0-4329-823a-1c7abe639eee no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:09:06.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8685" for this suite. 04/17/23 22:09:06.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:09:06.504
Apr 17 22:09:06.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename endpointslice 04/17/23 22:09:06.505
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:06.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:06.521
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 04/17/23 22:09:11.701
STEP: referencing matching pods with named port 04/17/23 22:09:16.71
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 04/17/23 22:09:21.719
STEP: recreating EndpointSlices after they've been deleted 04/17/23 22:09:26.729
Apr 17 22:09:26.750: INFO: EndpointSlice for Service endpointslice-1853/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 17 22:09:36.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1853" for this suite. 04/17/23 22:09:36.765
------------------------------
• [SLOW TEST] [30.268 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:09:06.504
    Apr 17 22:09:06.504: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename endpointslice 04/17/23 22:09:06.505
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:06.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:06.521
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 04/17/23 22:09:11.701
    STEP: referencing matching pods with named port 04/17/23 22:09:16.71
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 04/17/23 22:09:21.719
    STEP: recreating EndpointSlices after they've been deleted 04/17/23 22:09:26.729
    Apr 17 22:09:26.750: INFO: EndpointSlice for Service endpointslice-1853/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:09:36.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1853" for this suite. 04/17/23 22:09:36.765
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:09:36.772
Apr 17 22:09:36.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:09:36.773
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:36.788
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:36.791
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:09:36.805
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:09:37.588
STEP: Deploying the webhook pod 04/17/23 22:09:37.596
STEP: Wait for the deployment to be ready 04/17/23 22:09:37.609
Apr 17 22:09:37.616: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:09:39.628
STEP: Verifying the service has paired with the endpoint 04/17/23 22:09:39.643
Apr 17 22:09:40.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Apr 17 22:09:40.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8158-crds.webhook.example.com via the AdmissionRegistration API 04/17/23 22:09:41.158
STEP: Creating a custom resource that should be mutated by the webhook 04/17/23 22:09:41.172
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:09:43.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5552" for this suite. 04/17/23 22:09:43.772
STEP: Destroying namespace "webhook-5552-markers" for this suite. 04/17/23 22:09:43.783
------------------------------
• [SLOW TEST] [7.018 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:09:36.772
    Apr 17 22:09:36.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:09:36.773
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:36.788
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:36.791
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:09:36.805
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:09:37.588
    STEP: Deploying the webhook pod 04/17/23 22:09:37.596
    STEP: Wait for the deployment to be ready 04/17/23 22:09:37.609
    Apr 17 22:09:37.616: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:09:39.628
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:09:39.643
    Apr 17 22:09:40.644: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Apr 17 22:09:40.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8158-crds.webhook.example.com via the AdmissionRegistration API 04/17/23 22:09:41.158
    STEP: Creating a custom resource that should be mutated by the webhook 04/17/23 22:09:41.172
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:09:43.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5552" for this suite. 04/17/23 22:09:43.772
    STEP: Destroying namespace "webhook-5552-markers" for this suite. 04/17/23 22:09:43.783
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:09:43.794
Apr 17 22:09:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:09:43.794
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:43.808
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:43.811
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 04/17/23 22:09:43.813
Apr 17 22:09:43.821: INFO: Waiting up to 5m0s for pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578" in namespace "downward-api-7049" to be "Succeeded or Failed"
Apr 17 22:09:43.826: INFO: Pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578": Phase="Pending", Reason="", readiness=false. Elapsed: 5.42885ms
Apr 17 22:09:45.831: INFO: Pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009577356s
Apr 17 22:09:47.831: INFO: Pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009695239s
STEP: Saw pod success 04/17/23 22:09:47.831
Apr 17 22:09:47.831: INFO: Pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578" satisfied condition "Succeeded or Failed"
Apr 17 22:09:47.834: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578 container dapi-container: <nil>
STEP: delete the pod 04/17/23 22:09:47.841
Apr 17 22:09:47.856: INFO: Waiting for pod downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578 to disappear
Apr 17 22:09:47.859: INFO: Pod downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 17 22:09:47.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7049" for this suite. 04/17/23 22:09:47.865
------------------------------
• [4.078 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:09:43.794
    Apr 17 22:09:43.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:09:43.794
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:43.808
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:43.811
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 04/17/23 22:09:43.813
    Apr 17 22:09:43.821: INFO: Waiting up to 5m0s for pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578" in namespace "downward-api-7049" to be "Succeeded or Failed"
    Apr 17 22:09:43.826: INFO: Pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578": Phase="Pending", Reason="", readiness=false. Elapsed: 5.42885ms
    Apr 17 22:09:45.831: INFO: Pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009577356s
    Apr 17 22:09:47.831: INFO: Pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009695239s
    STEP: Saw pod success 04/17/23 22:09:47.831
    Apr 17 22:09:47.831: INFO: Pod "downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578" satisfied condition "Succeeded or Failed"
    Apr 17 22:09:47.834: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578 container dapi-container: <nil>
    STEP: delete the pod 04/17/23 22:09:47.841
    Apr 17 22:09:47.856: INFO: Waiting for pod downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578 to disappear
    Apr 17 22:09:47.859: INFO: Pod downward-api-d9ac1c73-f9ca-4063-af14-2812b1d3f578 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:09:47.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7049" for this suite. 04/17/23 22:09:47.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:09:47.872
Apr 17 22:09:47.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 22:09:47.873
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:47.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:47.891
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 04/17/23 22:09:47.894
Apr 17 22:09:47.903: INFO: Waiting up to 5m0s for pod "pod-62adfec1-031c-4eef-8697-2842f0240a55" in namespace "emptydir-5861" to be "Succeeded or Failed"
Apr 17 22:09:47.908: INFO: Pod "pod-62adfec1-031c-4eef-8697-2842f0240a55": Phase="Pending", Reason="", readiness=false. Elapsed: 5.565749ms
Apr 17 22:09:49.912: INFO: Pod "pod-62adfec1-031c-4eef-8697-2842f0240a55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009232639s
Apr 17 22:09:51.913: INFO: Pod "pod-62adfec1-031c-4eef-8697-2842f0240a55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009864742s
STEP: Saw pod success 04/17/23 22:09:51.913
Apr 17 22:09:51.913: INFO: Pod "pod-62adfec1-031c-4eef-8697-2842f0240a55" satisfied condition "Succeeded or Failed"
Apr 17 22:09:51.916: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-62adfec1-031c-4eef-8697-2842f0240a55 container test-container: <nil>
STEP: delete the pod 04/17/23 22:09:51.928
Apr 17 22:09:51.943: INFO: Waiting for pod pod-62adfec1-031c-4eef-8697-2842f0240a55 to disappear
Apr 17 22:09:51.946: INFO: Pod pod-62adfec1-031c-4eef-8697-2842f0240a55 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:09:51.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5861" for this suite. 04/17/23 22:09:51.951
------------------------------
• [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:09:47.872
    Apr 17 22:09:47.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 22:09:47.873
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:47.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:47.891
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 04/17/23 22:09:47.894
    Apr 17 22:09:47.903: INFO: Waiting up to 5m0s for pod "pod-62adfec1-031c-4eef-8697-2842f0240a55" in namespace "emptydir-5861" to be "Succeeded or Failed"
    Apr 17 22:09:47.908: INFO: Pod "pod-62adfec1-031c-4eef-8697-2842f0240a55": Phase="Pending", Reason="", readiness=false. Elapsed: 5.565749ms
    Apr 17 22:09:49.912: INFO: Pod "pod-62adfec1-031c-4eef-8697-2842f0240a55": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009232639s
    Apr 17 22:09:51.913: INFO: Pod "pod-62adfec1-031c-4eef-8697-2842f0240a55": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009864742s
    STEP: Saw pod success 04/17/23 22:09:51.913
    Apr 17 22:09:51.913: INFO: Pod "pod-62adfec1-031c-4eef-8697-2842f0240a55" satisfied condition "Succeeded or Failed"
    Apr 17 22:09:51.916: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-62adfec1-031c-4eef-8697-2842f0240a55 container test-container: <nil>
    STEP: delete the pod 04/17/23 22:09:51.928
    Apr 17 22:09:51.943: INFO: Waiting for pod pod-62adfec1-031c-4eef-8697-2842f0240a55 to disappear
    Apr 17 22:09:51.946: INFO: Pod pod-62adfec1-031c-4eef-8697-2842f0240a55 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:09:51.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5861" for this suite. 04/17/23 22:09:51.951
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:09:51.958
Apr 17 22:09:51.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 22:09:51.959
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:51.973
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:51.975
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 04/17/23 22:09:51.978
Apr 17 22:09:51.986: INFO: Waiting up to 5m0s for pod "pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78" in namespace "pods-3003" to be "running and ready"
Apr 17 22:09:51.989: INFO: Pod "pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78": Phase="Pending", Reason="", readiness=false. Elapsed: 3.261433ms
Apr 17 22:09:51.989: INFO: The phase of Pod pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:09:53.994: INFO: Pod "pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78": Phase="Running", Reason="", readiness=true. Elapsed: 2.007771818s
Apr 17 22:09:53.994: INFO: The phase of Pod pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78 is Running (Ready = true)
Apr 17 22:09:53.994: INFO: Pod "pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78" satisfied condition "running and ready"
Apr 17 22:09:54.000: INFO: Pod pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78 has hostIP: 10.0.64.189
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 22:09:54.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3003" for this suite. 04/17/23 22:09:54.005
------------------------------
• [2.054 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:09:51.958
    Apr 17 22:09:51.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 22:09:51.959
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:51.973
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:51.975
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 04/17/23 22:09:51.978
    Apr 17 22:09:51.986: INFO: Waiting up to 5m0s for pod "pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78" in namespace "pods-3003" to be "running and ready"
    Apr 17 22:09:51.989: INFO: Pod "pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78": Phase="Pending", Reason="", readiness=false. Elapsed: 3.261433ms
    Apr 17 22:09:51.989: INFO: The phase of Pod pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:09:53.994: INFO: Pod "pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78": Phase="Running", Reason="", readiness=true. Elapsed: 2.007771818s
    Apr 17 22:09:53.994: INFO: The phase of Pod pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78 is Running (Ready = true)
    Apr 17 22:09:53.994: INFO: Pod "pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78" satisfied condition "running and ready"
    Apr 17 22:09:54.000: INFO: Pod pod-hostip-3232ea45-7a04-499b-84ab-fea5e22f8e78 has hostIP: 10.0.64.189
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:09:54.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3003" for this suite. 04/17/23 22:09:54.005
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:09:54.012
Apr 17 22:09:54.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:09:54.013
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:54.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:54.033
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-f7e3c40e-1d5b-49df-ab6d-5d66a2bcd57c 04/17/23 22:09:54.035
STEP: Creating a pod to test consume configMaps 04/17/23 22:09:54.04
Apr 17 22:09:54.049: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974" in namespace "projected-2279" to be "Succeeded or Failed"
Apr 17 22:09:54.052: INFO: Pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974": Phase="Pending", Reason="", readiness=false. Elapsed: 3.102527ms
Apr 17 22:09:56.057: INFO: Pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00776791s
Apr 17 22:09:58.056: INFO: Pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007693077s
STEP: Saw pod success 04/17/23 22:09:58.057
Apr 17 22:09:58.057: INFO: Pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974" satisfied condition "Succeeded or Failed"
Apr 17 22:09:58.060: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:09:58.066
Apr 17 22:09:58.080: INFO: Waiting for pod pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974 to disappear
Apr 17 22:09:58.083: INFO: Pod pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:09:58.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2279" for this suite. 04/17/23 22:09:58.088
------------------------------
• [4.082 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:09:54.012
    Apr 17 22:09:54.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:09:54.013
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:54.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:54.033
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-f7e3c40e-1d5b-49df-ab6d-5d66a2bcd57c 04/17/23 22:09:54.035
    STEP: Creating a pod to test consume configMaps 04/17/23 22:09:54.04
    Apr 17 22:09:54.049: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974" in namespace "projected-2279" to be "Succeeded or Failed"
    Apr 17 22:09:54.052: INFO: Pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974": Phase="Pending", Reason="", readiness=false. Elapsed: 3.102527ms
    Apr 17 22:09:56.057: INFO: Pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00776791s
    Apr 17 22:09:58.056: INFO: Pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007693077s
    STEP: Saw pod success 04/17/23 22:09:58.057
    Apr 17 22:09:58.057: INFO: Pod "pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974" satisfied condition "Succeeded or Failed"
    Apr 17 22:09:58.060: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:09:58.066
    Apr 17 22:09:58.080: INFO: Waiting for pod pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974 to disappear
    Apr 17 22:09:58.083: INFO: Pod pod-projected-configmaps-e97814e0-0b70-41c5-bb9b-dda1c99e0974 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:09:58.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2279" for this suite. 04/17/23 22:09:58.088
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:09:58.095
Apr 17 22:09:58.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 22:09:58.096
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:58.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:58.115
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 04/17/23 22:09:58.117
Apr 17 22:09:58.125: INFO: Waiting up to 5m0s for pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301" in namespace "emptydir-2692" to be "Succeeded or Failed"
Apr 17 22:09:58.130: INFO: Pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301": Phase="Pending", Reason="", readiness=false. Elapsed: 5.210936ms
Apr 17 22:10:00.135: INFO: Pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010405433s
Apr 17 22:10:02.134: INFO: Pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009443572s
STEP: Saw pod success 04/17/23 22:10:02.134
Apr 17 22:10:02.134: INFO: Pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301" satisfied condition "Succeeded or Failed"
Apr 17 22:10:02.138: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-1eb4bb49-c747-44a6-be92-7ab051e08301 container test-container: <nil>
STEP: delete the pod 04/17/23 22:10:02.145
Apr 17 22:10:02.157: INFO: Waiting for pod pod-1eb4bb49-c747-44a6-be92-7ab051e08301 to disappear
Apr 17 22:10:02.160: INFO: Pod pod-1eb4bb49-c747-44a6-be92-7ab051e08301 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:02.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2692" for this suite. 04/17/23 22:10:02.165
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:09:58.095
    Apr 17 22:09:58.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 22:09:58.096
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:09:58.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:09:58.115
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 04/17/23 22:09:58.117
    Apr 17 22:09:58.125: INFO: Waiting up to 5m0s for pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301" in namespace "emptydir-2692" to be "Succeeded or Failed"
    Apr 17 22:09:58.130: INFO: Pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301": Phase="Pending", Reason="", readiness=false. Elapsed: 5.210936ms
    Apr 17 22:10:00.135: INFO: Pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010405433s
    Apr 17 22:10:02.134: INFO: Pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009443572s
    STEP: Saw pod success 04/17/23 22:10:02.134
    Apr 17 22:10:02.134: INFO: Pod "pod-1eb4bb49-c747-44a6-be92-7ab051e08301" satisfied condition "Succeeded or Failed"
    Apr 17 22:10:02.138: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-1eb4bb49-c747-44a6-be92-7ab051e08301 container test-container: <nil>
    STEP: delete the pod 04/17/23 22:10:02.145
    Apr 17 22:10:02.157: INFO: Waiting for pod pod-1eb4bb49-c747-44a6-be92-7ab051e08301 to disappear
    Apr 17 22:10:02.160: INFO: Pod pod-1eb4bb49-c747-44a6-be92-7ab051e08301 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:02.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2692" for this suite. 04/17/23 22:10:02.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:02.174
Apr 17 22:10:02.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-runtime 04/17/23 22:10:02.175
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:02.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:02.191
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 04/17/23 22:10:02.194
STEP: wait for the container to reach Failed 04/17/23 22:10:02.203
STEP: get the container status 04/17/23 22:10:07.228
STEP: the container should be terminated 04/17/23 22:10:07.231
STEP: the termination message should be set 04/17/23 22:10:07.231
Apr 17 22:10:07.231: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 04/17/23 22:10:07.231
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:07.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7381" for this suite. 04/17/23 22:10:07.256
------------------------------
• [SLOW TEST] [5.088 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:02.174
    Apr 17 22:10:02.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-runtime 04/17/23 22:10:02.175
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:02.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:02.191
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 04/17/23 22:10:02.194
    STEP: wait for the container to reach Failed 04/17/23 22:10:02.203
    STEP: get the container status 04/17/23 22:10:07.228
    STEP: the container should be terminated 04/17/23 22:10:07.231
    STEP: the termination message should be set 04/17/23 22:10:07.231
    Apr 17 22:10:07.231: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 04/17/23 22:10:07.231
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:07.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7381" for this suite. 04/17/23 22:10:07.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:07.263
Apr 17 22:10:07.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:10:07.263
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:07.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:07.281
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Apr 17 22:10:07.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 04/17/23 22:10:12.256
Apr 17 22:10:12.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 create -f -'
Apr 17 22:10:13.627: INFO: stderr: ""
Apr 17 22:10:13.627: INFO: stdout: "e2e-test-crd-publish-openapi-5007-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 17 22:10:13.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 delete e2e-test-crd-publish-openapi-5007-crds test-foo'
Apr 17 22:10:13.688: INFO: stderr: ""
Apr 17 22:10:13.688: INFO: stdout: "e2e-test-crd-publish-openapi-5007-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Apr 17 22:10:13.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 apply -f -'
Apr 17 22:10:14.949: INFO: stderr: ""
Apr 17 22:10:14.949: INFO: stdout: "e2e-test-crd-publish-openapi-5007-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Apr 17 22:10:14.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 delete e2e-test-crd-publish-openapi-5007-crds test-foo'
Apr 17 22:10:15.013: INFO: stderr: ""
Apr 17 22:10:15.013: INFO: stdout: "e2e-test-crd-publish-openapi-5007-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 04/17/23 22:10:15.013
Apr 17 22:10:15.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 create -f -'
Apr 17 22:10:15.399: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 04/17/23 22:10:15.399
Apr 17 22:10:15.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 create -f -'
Apr 17 22:10:15.786: INFO: rc: 1
Apr 17 22:10:15.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 apply -f -'
Apr 17 22:10:16.184: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 04/17/23 22:10:16.184
Apr 17 22:10:16.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 create -f -'
Apr 17 22:10:17.406: INFO: rc: 1
Apr 17 22:10:17.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 apply -f -'
Apr 17 22:10:17.787: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 04/17/23 22:10:17.787
Apr 17 22:10:17.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds'
Apr 17 22:10:18.172: INFO: stderr: ""
Apr 17 22:10:18.172: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5007-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 04/17/23 22:10:18.172
Apr 17 22:10:18.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds.metadata'
Apr 17 22:10:18.585: INFO: stderr: ""
Apr 17 22:10:18.585: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5007-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Apr 17 22:10:18.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds.spec'
Apr 17 22:10:18.978: INFO: stderr: ""
Apr 17 22:10:18.978: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5007-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Apr 17 22:10:18.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds.spec.bars'
Apr 17 22:10:19.358: INFO: stderr: ""
Apr 17 22:10:19.358: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5007-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 04/17/23 22:10:19.358
Apr 17 22:10:19.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds.spec.bars2'
Apr 17 22:10:19.748: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:23.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9946" for this suite. 04/17/23 22:10:23.513
------------------------------
• [SLOW TEST] [16.256 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:07.263
    Apr 17 22:10:07.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:10:07.263
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:07.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:07.281
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Apr 17 22:10:07.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 04/17/23 22:10:12.256
    Apr 17 22:10:12.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 create -f -'
    Apr 17 22:10:13.627: INFO: stderr: ""
    Apr 17 22:10:13.627: INFO: stdout: "e2e-test-crd-publish-openapi-5007-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Apr 17 22:10:13.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 delete e2e-test-crd-publish-openapi-5007-crds test-foo'
    Apr 17 22:10:13.688: INFO: stderr: ""
    Apr 17 22:10:13.688: INFO: stdout: "e2e-test-crd-publish-openapi-5007-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Apr 17 22:10:13.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 apply -f -'
    Apr 17 22:10:14.949: INFO: stderr: ""
    Apr 17 22:10:14.949: INFO: stdout: "e2e-test-crd-publish-openapi-5007-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Apr 17 22:10:14.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 delete e2e-test-crd-publish-openapi-5007-crds test-foo'
    Apr 17 22:10:15.013: INFO: stderr: ""
    Apr 17 22:10:15.013: INFO: stdout: "e2e-test-crd-publish-openapi-5007-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 04/17/23 22:10:15.013
    Apr 17 22:10:15.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 create -f -'
    Apr 17 22:10:15.399: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 04/17/23 22:10:15.399
    Apr 17 22:10:15.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 create -f -'
    Apr 17 22:10:15.786: INFO: rc: 1
    Apr 17 22:10:15.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 apply -f -'
    Apr 17 22:10:16.184: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 04/17/23 22:10:16.184
    Apr 17 22:10:16.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 create -f -'
    Apr 17 22:10:17.406: INFO: rc: 1
    Apr 17 22:10:17.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 --namespace=crd-publish-openapi-9946 apply -f -'
    Apr 17 22:10:17.787: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 04/17/23 22:10:17.787
    Apr 17 22:10:17.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds'
    Apr 17 22:10:18.172: INFO: stderr: ""
    Apr 17 22:10:18.172: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5007-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 04/17/23 22:10:18.172
    Apr 17 22:10:18.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds.metadata'
    Apr 17 22:10:18.585: INFO: stderr: ""
    Apr 17 22:10:18.585: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5007-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Apr 17 22:10:18.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds.spec'
    Apr 17 22:10:18.978: INFO: stderr: ""
    Apr 17 22:10:18.978: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5007-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Apr 17 22:10:18.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds.spec.bars'
    Apr 17 22:10:19.358: INFO: stderr: ""
    Apr 17 22:10:19.358: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5007-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 04/17/23 22:10:19.358
    Apr 17 22:10:19.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-9946 explain e2e-test-crd-publish-openapi-5007-crds.spec.bars2'
    Apr 17 22:10:19.748: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:23.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9946" for this suite. 04/17/23 22:10:23.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:23.519
Apr 17 22:10:23.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-runtime 04/17/23 22:10:23.52
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:23.532
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:23.534
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 04/17/23 22:10:23.536
STEP: wait for the container to reach Succeeded 04/17/23 22:10:23.543
STEP: get the container status 04/17/23 22:10:27.618
STEP: the container should be terminated 04/17/23 22:10:27.621
STEP: the termination message should be set 04/17/23 22:10:27.621
Apr 17 22:10:27.621: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 04/17/23 22:10:27.621
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:27.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5211" for this suite. 04/17/23 22:10:27.642
------------------------------
• [4.128 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:23.519
    Apr 17 22:10:23.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-runtime 04/17/23 22:10:23.52
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:23.532
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:23.534
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 04/17/23 22:10:23.536
    STEP: wait for the container to reach Succeeded 04/17/23 22:10:23.543
    STEP: get the container status 04/17/23 22:10:27.618
    STEP: the container should be terminated 04/17/23 22:10:27.621
    STEP: the termination message should be set 04/17/23 22:10:27.621
    Apr 17 22:10:27.621: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 04/17/23 22:10:27.621
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:27.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5211" for this suite. 04/17/23 22:10:27.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:27.649
Apr 17 22:10:27.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename watch 04/17/23 22:10:27.649
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:27.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:27.66
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 04/17/23 22:10:27.662
STEP: creating a new configmap 04/17/23 22:10:27.663
STEP: modifying the configmap once 04/17/23 22:10:27.669
STEP: changing the label value of the configmap 04/17/23 22:10:27.675
STEP: Expecting to observe a delete notification for the watched object 04/17/23 22:10:27.68
Apr 17 22:10:27.680: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49416 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:10:27.680: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49417 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:10:27.680: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49418 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 04/17/23 22:10:27.68
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 04/17/23 22:10:27.685
STEP: changing the label value of the configmap back 04/17/23 22:10:37.686
STEP: modifying the configmap a third time 04/17/23 22:10:37.693
STEP: deleting the configmap 04/17/23 22:10:37.699
STEP: Expecting to observe an add notification for the watched object when the label value was restored 04/17/23 22:10:37.705
Apr 17 22:10:37.705: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49519 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:10:37.705: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49520 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Apr 17 22:10:37.705: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49521 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:37.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5038" for this suite. 04/17/23 22:10:37.709
------------------------------
• [SLOW TEST] [10.066 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:27.649
    Apr 17 22:10:27.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename watch 04/17/23 22:10:27.649
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:27.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:27.66
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 04/17/23 22:10:27.662
    STEP: creating a new configmap 04/17/23 22:10:27.663
    STEP: modifying the configmap once 04/17/23 22:10:27.669
    STEP: changing the label value of the configmap 04/17/23 22:10:27.675
    STEP: Expecting to observe a delete notification for the watched object 04/17/23 22:10:27.68
    Apr 17 22:10:27.680: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49416 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:10:27.680: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49417 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:10:27.680: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49418 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:27 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 04/17/23 22:10:27.68
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 04/17/23 22:10:27.685
    STEP: changing the label value of the configmap back 04/17/23 22:10:37.686
    STEP: modifying the configmap a third time 04/17/23 22:10:37.693
    STEP: deleting the configmap 04/17/23 22:10:37.699
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 04/17/23 22:10:37.705
    Apr 17 22:10:37.705: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49519 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:10:37.705: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49520 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Apr 17 22:10:37.705: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5038  ecb9d73c-bad5-4241-b945-111fcb48aeb6 49521 0 2023-04-17 22:10:27 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-04-17 22:10:37 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:37.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5038" for this suite. 04/17/23 22:10:37.709
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:37.715
Apr 17 22:10:37.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename disruption 04/17/23 22:10:37.715
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:37.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:37.727
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:37.728
Apr 17 22:10:37.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename disruption-2 04/17/23 22:10:37.729
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:37.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:37.741
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 04/17/23 22:10:37.746
STEP: Waiting for the pdb to be processed 04/17/23 22:10:39.757
STEP: Waiting for the pdb to be processed 04/17/23 22:10:41.766
STEP: listing a collection of PDBs across all namespaces 04/17/23 22:10:43.772
STEP: listing a collection of PDBs in namespace disruption-7013 04/17/23 22:10:43.774
STEP: deleting a collection of PDBs 04/17/23 22:10:43.777
STEP: Waiting for the PDB collection to be deleted 04/17/23 22:10:43.788
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:43.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:43.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-3836" for this suite. 04/17/23 22:10:43.799
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7013" for this suite. 04/17/23 22:10:43.803
------------------------------
• [SLOW TEST] [6.093 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:37.715
    Apr 17 22:10:37.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename disruption 04/17/23 22:10:37.715
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:37.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:37.727
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:37.728
    Apr 17 22:10:37.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename disruption-2 04/17/23 22:10:37.729
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:37.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:37.741
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 04/17/23 22:10:37.746
    STEP: Waiting for the pdb to be processed 04/17/23 22:10:39.757
    STEP: Waiting for the pdb to be processed 04/17/23 22:10:41.766
    STEP: listing a collection of PDBs across all namespaces 04/17/23 22:10:43.772
    STEP: listing a collection of PDBs in namespace disruption-7013 04/17/23 22:10:43.774
    STEP: deleting a collection of PDBs 04/17/23 22:10:43.777
    STEP: Waiting for the PDB collection to be deleted 04/17/23 22:10:43.788
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:43.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:43.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-3836" for this suite. 04/17/23 22:10:43.799
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7013" for this suite. 04/17/23 22:10:43.803
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:43.808
Apr 17 22:10:43.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename disruption 04/17/23 22:10:43.809
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:43.821
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:43.823
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 04/17/23 22:10:43.824
STEP: Waiting for the pdb to be processed 04/17/23 22:10:43.828
STEP: updating the pdb 04/17/23 22:10:45.834
STEP: Waiting for the pdb to be processed 04/17/23 22:10:45.84
STEP: patching the pdb 04/17/23 22:10:47.846
STEP: Waiting for the pdb to be processed 04/17/23 22:10:47.855
STEP: Waiting for the pdb to be deleted 04/17/23 22:10:49.865
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:49.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-557" for this suite. 04/17/23 22:10:49.871
------------------------------
• [SLOW TEST] [6.067 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:43.808
    Apr 17 22:10:43.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename disruption 04/17/23 22:10:43.809
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:43.821
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:43.823
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 04/17/23 22:10:43.824
    STEP: Waiting for the pdb to be processed 04/17/23 22:10:43.828
    STEP: updating the pdb 04/17/23 22:10:45.834
    STEP: Waiting for the pdb to be processed 04/17/23 22:10:45.84
    STEP: patching the pdb 04/17/23 22:10:47.846
    STEP: Waiting for the pdb to be processed 04/17/23 22:10:47.855
    STEP: Waiting for the pdb to be deleted 04/17/23 22:10:49.865
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:49.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-557" for this suite. 04/17/23 22:10:49.871
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:49.876
Apr 17 22:10:49.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename runtimeclass 04/17/23 22:10:49.877
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:49.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:49.889
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Apr 17 22:10:49.902: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1552 to be scheduled
Apr 17 22:10:49.904: INFO: 1 pods are not scheduled: [runtimeclass-1552/test-runtimeclass-runtimeclass-1552-preconfigured-handler-rvzg7(cca1c3e3-48a6-4a79-b6f6-ee3ec7121660)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:51.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1552" for this suite. 04/17/23 22:10:51.917
------------------------------
• [2.046 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:49.876
    Apr 17 22:10:49.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename runtimeclass 04/17/23 22:10:49.877
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:49.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:49.889
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Apr 17 22:10:49.902: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1552 to be scheduled
    Apr 17 22:10:49.904: INFO: 1 pods are not scheduled: [runtimeclass-1552/test-runtimeclass-runtimeclass-1552-preconfigured-handler-rvzg7(cca1c3e3-48a6-4a79-b6f6-ee3ec7121660)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:51.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1552" for this suite. 04/17/23 22:10:51.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:51.924
Apr 17 22:10:51.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 22:10:51.925
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:51.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:51.937
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 04/17/23 22:10:51.939
STEP: setting up watch 04/17/23 22:10:51.939
STEP: submitting the pod to kubernetes 04/17/23 22:10:52.042
STEP: verifying the pod is in kubernetes 04/17/23 22:10:52.05
STEP: verifying pod creation was observed 04/17/23 22:10:52.053
Apr 17 22:10:52.053: INFO: Waiting up to 5m0s for pod "pod-submit-remove-c0cbbf54-31b0-45ec-a0ed-28e9699cfec9" in namespace "pods-3152" to be "running"
Apr 17 22:10:52.056: INFO: Pod "pod-submit-remove-c0cbbf54-31b0-45ec-a0ed-28e9699cfec9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.633173ms
Apr 17 22:10:54.060: INFO: Pod "pod-submit-remove-c0cbbf54-31b0-45ec-a0ed-28e9699cfec9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007044593s
Apr 17 22:10:54.060: INFO: Pod "pod-submit-remove-c0cbbf54-31b0-45ec-a0ed-28e9699cfec9" satisfied condition "running"
STEP: deleting the pod gracefully 04/17/23 22:10:54.062
STEP: verifying pod deletion was observed 04/17/23 22:10:54.069
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 22:10:56.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-3152" for this suite. 04/17/23 22:10:56.527
------------------------------
• [4.607 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:51.924
    Apr 17 22:10:51.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 22:10:51.925
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:51.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:51.937
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 04/17/23 22:10:51.939
    STEP: setting up watch 04/17/23 22:10:51.939
    STEP: submitting the pod to kubernetes 04/17/23 22:10:52.042
    STEP: verifying the pod is in kubernetes 04/17/23 22:10:52.05
    STEP: verifying pod creation was observed 04/17/23 22:10:52.053
    Apr 17 22:10:52.053: INFO: Waiting up to 5m0s for pod "pod-submit-remove-c0cbbf54-31b0-45ec-a0ed-28e9699cfec9" in namespace "pods-3152" to be "running"
    Apr 17 22:10:52.056: INFO: Pod "pod-submit-remove-c0cbbf54-31b0-45ec-a0ed-28e9699cfec9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.633173ms
    Apr 17 22:10:54.060: INFO: Pod "pod-submit-remove-c0cbbf54-31b0-45ec-a0ed-28e9699cfec9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007044593s
    Apr 17 22:10:54.060: INFO: Pod "pod-submit-remove-c0cbbf54-31b0-45ec-a0ed-28e9699cfec9" satisfied condition "running"
    STEP: deleting the pod gracefully 04/17/23 22:10:54.062
    STEP: verifying pod deletion was observed 04/17/23 22:10:54.069
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:10:56.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-3152" for this suite. 04/17/23 22:10:56.527
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:10:56.531
Apr 17 22:10:56.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename disruption 04/17/23 22:10:56.532
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:56.543
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:56.545
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 04/17/23 22:10:56.55
STEP: Updating PodDisruptionBudget status 04/17/23 22:10:58.555
STEP: Waiting for all pods to be running 04/17/23 22:10:58.563
Apr 17 22:10:58.565: INFO: running pods: 0 < 1
STEP: locating a running pod 04/17/23 22:11:00.568
STEP: Waiting for the pdb to be processed 04/17/23 22:11:00.577
STEP: Patching PodDisruptionBudget status 04/17/23 22:11:00.582
STEP: Waiting for the pdb to be processed 04/17/23 22:11:00.59
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 17 22:11:00.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6034" for this suite. 04/17/23 22:11:00.599
------------------------------
• [4.073 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:10:56.531
    Apr 17 22:10:56.532: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename disruption 04/17/23 22:10:56.532
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:10:56.543
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:10:56.545
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 04/17/23 22:10:56.55
    STEP: Updating PodDisruptionBudget status 04/17/23 22:10:58.555
    STEP: Waiting for all pods to be running 04/17/23 22:10:58.563
    Apr 17 22:10:58.565: INFO: running pods: 0 < 1
    STEP: locating a running pod 04/17/23 22:11:00.568
    STEP: Waiting for the pdb to be processed 04/17/23 22:11:00.577
    STEP: Patching PodDisruptionBudget status 04/17/23 22:11:00.582
    STEP: Waiting for the pdb to be processed 04/17/23 22:11:00.59
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:11:00.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6034" for this suite. 04/17/23 22:11:00.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:11:00.605
Apr 17 22:11:00.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir-wrapper 04/17/23 22:11:00.606
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:11:00.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:11:00.619
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 04/17/23 22:11:00.621
STEP: Creating RC which spawns configmap-volume pods 04/17/23 22:11:00.861
Apr 17 22:11:00.996: INFO: Pod name wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb: Found 1 pods out of 5
Apr 17 22:11:06.001: INFO: Pod name wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/17/23 22:11:06.001
Apr 17 22:11:06.001: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:06.004: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.629521ms
Apr 17 22:11:08.007: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006084873s
Apr 17 22:11:10.008: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006820683s
Apr 17 22:11:12.008: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00661546s
Apr 17 22:11:14.008: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006683935s
Apr 17 22:11:16.009: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Running", Reason="", readiness=true. Elapsed: 10.007629046s
Apr 17 22:11:16.009: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699" satisfied condition "running"
Apr 17 22:11:16.009: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-hggpg" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:16.011: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-hggpg": Phase="Running", Reason="", readiness=true. Elapsed: 2.720427ms
Apr 17 22:11:16.011: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-hggpg" satisfied condition "running"
Apr 17 22:11:16.011: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-j2cl4" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:16.014: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-j2cl4": Phase="Running", Reason="", readiness=true. Elapsed: 2.760236ms
Apr 17 22:11:16.014: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-j2cl4" satisfied condition "running"
Apr 17 22:11:16.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-jgv7z" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:16.017: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-jgv7z": Phase="Running", Reason="", readiness=true. Elapsed: 2.611864ms
Apr 17 22:11:16.017: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-jgv7z" satisfied condition "running"
Apr 17 22:11:16.017: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-prwzz" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:16.019: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-prwzz": Phase="Running", Reason="", readiness=true. Elapsed: 2.466584ms
Apr 17 22:11:16.019: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-prwzz" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb in namespace emptydir-wrapper-6233, will wait for the garbage collector to delete the pods 04/17/23 22:11:16.019
Apr 17 22:11:16.078: INFO: Deleting ReplicationController wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb took: 5.146697ms
Apr 17 22:11:16.179: INFO: Terminating ReplicationController wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb pods took: 101.027109ms
STEP: Creating RC which spawns configmap-volume pods 04/17/23 22:11:18.585
Apr 17 22:11:18.598: INFO: Pod name wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54: Found 0 pods out of 5
Apr 17 22:11:23.604: INFO: Pod name wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/17/23 22:11:23.604
Apr 17 22:11:23.604: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:23.607: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665622ms
Apr 17 22:11:25.611: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007209173s
Apr 17 22:11:27.611: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006752672s
Apr 17 22:11:29.611: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006863487s
Apr 17 22:11:31.611: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006875875s
Apr 17 22:11:33.610: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Running", Reason="", readiness=true. Elapsed: 10.006217215s
Apr 17 22:11:33.610: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t" satisfied condition "running"
Apr 17 22:11:33.610: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-6d7pw" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:33.613: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-6d7pw": Phase="Running", Reason="", readiness=true. Elapsed: 2.641902ms
Apr 17 22:11:33.613: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-6d7pw" satisfied condition "running"
Apr 17 22:11:33.613: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-7dq5s" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:33.615: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-7dq5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.507826ms
Apr 17 22:11:33.615: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-7dq5s" satisfied condition "running"
Apr 17 22:11:33.615: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-8bjcc" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:33.618: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-8bjcc": Phase="Running", Reason="", readiness=true. Elapsed: 2.402574ms
Apr 17 22:11:33.618: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-8bjcc" satisfied condition "running"
Apr 17 22:11:33.618: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-txltn" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:33.620: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-txltn": Phase="Running", Reason="", readiness=true. Elapsed: 2.483795ms
Apr 17 22:11:33.620: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-txltn" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54 in namespace emptydir-wrapper-6233, will wait for the garbage collector to delete the pods 04/17/23 22:11:33.62
Apr 17 22:11:33.679: INFO: Deleting ReplicationController wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54 took: 5.787044ms
Apr 17 22:11:33.780: INFO: Terminating ReplicationController wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54 pods took: 100.520889ms
STEP: Creating RC which spawns configmap-volume pods 04/17/23 22:11:36.085
Apr 17 22:11:36.099: INFO: Pod name wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2: Found 0 pods out of 5
Apr 17 22:11:41.106: INFO: Pod name wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2: Found 5 pods out of 5
STEP: Ensuring each pod is running 04/17/23 22:11:41.106
Apr 17 22:11:41.106: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:41.108: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580789ms
Apr 17 22:11:43.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005994946s
Apr 17 22:11:45.113: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007116425s
Apr 17 22:11:47.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00665009s
Apr 17 22:11:49.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006608763s
Apr 17 22:11:51.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Running", Reason="", readiness=true. Elapsed: 10.006228398s
Apr 17 22:11:51.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n" satisfied condition "running"
Apr 17 22:11:51.112: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-fqn4k" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:51.114: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-fqn4k": Phase="Running", Reason="", readiness=true. Elapsed: 2.515535ms
Apr 17 22:11:51.114: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-fqn4k" satisfied condition "running"
Apr 17 22:11:51.115: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-frxqq" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:51.117: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-frxqq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340402ms
Apr 17 22:11:53.121: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-frxqq": Phase="Running", Reason="", readiness=true. Elapsed: 2.006178311s
Apr 17 22:11:53.121: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-frxqq" satisfied condition "running"
Apr 17 22:11:53.121: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-hnfbz" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:53.123: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-hnfbz": Phase="Running", Reason="", readiness=true. Elapsed: 2.717395ms
Apr 17 22:11:53.123: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-hnfbz" satisfied condition "running"
Apr 17 22:11:53.123: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-stcxp" in namespace "emptydir-wrapper-6233" to be "running"
Apr 17 22:11:53.126: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-stcxp": Phase="Running", Reason="", readiness=true. Elapsed: 2.876817ms
Apr 17 22:11:53.126: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-stcxp" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2 in namespace emptydir-wrapper-6233, will wait for the garbage collector to delete the pods 04/17/23 22:11:53.126
Apr 17 22:11:53.186: INFO: Deleting ReplicationController wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2 took: 6.280525ms
Apr 17 22:11:53.387: INFO: Terminating ReplicationController wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2 pods took: 200.880823ms
STEP: Cleaning up the configMaps 04/17/23 22:11:55.888
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:11:56.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-6233" for this suite. 04/17/23 22:11:56.117
------------------------------
• [SLOW TEST] [55.516 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:11:00.605
    Apr 17 22:11:00.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir-wrapper 04/17/23 22:11:00.606
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:11:00.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:11:00.619
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 04/17/23 22:11:00.621
    STEP: Creating RC which spawns configmap-volume pods 04/17/23 22:11:00.861
    Apr 17 22:11:00.996: INFO: Pod name wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb: Found 1 pods out of 5
    Apr 17 22:11:06.001: INFO: Pod name wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/17/23 22:11:06.001
    Apr 17 22:11:06.001: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:06.004: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.629521ms
    Apr 17 22:11:08.007: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006084873s
    Apr 17 22:11:10.008: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006820683s
    Apr 17 22:11:12.008: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00661546s
    Apr 17 22:11:14.008: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006683935s
    Apr 17 22:11:16.009: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699": Phase="Running", Reason="", readiness=true. Elapsed: 10.007629046s
    Apr 17 22:11:16.009: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-fj699" satisfied condition "running"
    Apr 17 22:11:16.009: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-hggpg" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:16.011: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-hggpg": Phase="Running", Reason="", readiness=true. Elapsed: 2.720427ms
    Apr 17 22:11:16.011: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-hggpg" satisfied condition "running"
    Apr 17 22:11:16.011: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-j2cl4" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:16.014: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-j2cl4": Phase="Running", Reason="", readiness=true. Elapsed: 2.760236ms
    Apr 17 22:11:16.014: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-j2cl4" satisfied condition "running"
    Apr 17 22:11:16.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-jgv7z" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:16.017: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-jgv7z": Phase="Running", Reason="", readiness=true. Elapsed: 2.611864ms
    Apr 17 22:11:16.017: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-jgv7z" satisfied condition "running"
    Apr 17 22:11:16.017: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-prwzz" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:16.019: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-prwzz": Phase="Running", Reason="", readiness=true. Elapsed: 2.466584ms
    Apr 17 22:11:16.019: INFO: Pod "wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb-prwzz" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb in namespace emptydir-wrapper-6233, will wait for the garbage collector to delete the pods 04/17/23 22:11:16.019
    Apr 17 22:11:16.078: INFO: Deleting ReplicationController wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb took: 5.146697ms
    Apr 17 22:11:16.179: INFO: Terminating ReplicationController wrapped-volume-race-793d5dae-53ef-47e3-ab26-269afe019feb pods took: 101.027109ms
    STEP: Creating RC which spawns configmap-volume pods 04/17/23 22:11:18.585
    Apr 17 22:11:18.598: INFO: Pod name wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54: Found 0 pods out of 5
    Apr 17 22:11:23.604: INFO: Pod name wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/17/23 22:11:23.604
    Apr 17 22:11:23.604: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:23.607: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665622ms
    Apr 17 22:11:25.611: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007209173s
    Apr 17 22:11:27.611: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006752672s
    Apr 17 22:11:29.611: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 6.006863487s
    Apr 17 22:11:31.611: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006875875s
    Apr 17 22:11:33.610: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t": Phase="Running", Reason="", readiness=true. Elapsed: 10.006217215s
    Apr 17 22:11:33.610: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-2v94t" satisfied condition "running"
    Apr 17 22:11:33.610: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-6d7pw" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:33.613: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-6d7pw": Phase="Running", Reason="", readiness=true. Elapsed: 2.641902ms
    Apr 17 22:11:33.613: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-6d7pw" satisfied condition "running"
    Apr 17 22:11:33.613: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-7dq5s" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:33.615: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-7dq5s": Phase="Running", Reason="", readiness=true. Elapsed: 2.507826ms
    Apr 17 22:11:33.615: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-7dq5s" satisfied condition "running"
    Apr 17 22:11:33.615: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-8bjcc" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:33.618: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-8bjcc": Phase="Running", Reason="", readiness=true. Elapsed: 2.402574ms
    Apr 17 22:11:33.618: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-8bjcc" satisfied condition "running"
    Apr 17 22:11:33.618: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-txltn" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:33.620: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-txltn": Phase="Running", Reason="", readiness=true. Elapsed: 2.483795ms
    Apr 17 22:11:33.620: INFO: Pod "wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54-txltn" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54 in namespace emptydir-wrapper-6233, will wait for the garbage collector to delete the pods 04/17/23 22:11:33.62
    Apr 17 22:11:33.679: INFO: Deleting ReplicationController wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54 took: 5.787044ms
    Apr 17 22:11:33.780: INFO: Terminating ReplicationController wrapped-volume-race-1a539cfb-98e5-4545-bbb9-4a8eb7512b54 pods took: 100.520889ms
    STEP: Creating RC which spawns configmap-volume pods 04/17/23 22:11:36.085
    Apr 17 22:11:36.099: INFO: Pod name wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2: Found 0 pods out of 5
    Apr 17 22:11:41.106: INFO: Pod name wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2: Found 5 pods out of 5
    STEP: Ensuring each pod is running 04/17/23 22:11:41.106
    Apr 17 22:11:41.106: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:41.108: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.580789ms
    Apr 17 22:11:43.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005994946s
    Apr 17 22:11:45.113: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007116425s
    Apr 17 22:11:47.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00665009s
    Apr 17 22:11:49.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006608763s
    Apr 17 22:11:51.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n": Phase="Running", Reason="", readiness=true. Elapsed: 10.006228398s
    Apr 17 22:11:51.112: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-cn54n" satisfied condition "running"
    Apr 17 22:11:51.112: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-fqn4k" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:51.114: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-fqn4k": Phase="Running", Reason="", readiness=true. Elapsed: 2.515535ms
    Apr 17 22:11:51.114: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-fqn4k" satisfied condition "running"
    Apr 17 22:11:51.115: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-frxqq" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:51.117: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-frxqq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340402ms
    Apr 17 22:11:53.121: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-frxqq": Phase="Running", Reason="", readiness=true. Elapsed: 2.006178311s
    Apr 17 22:11:53.121: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-frxqq" satisfied condition "running"
    Apr 17 22:11:53.121: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-hnfbz" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:53.123: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-hnfbz": Phase="Running", Reason="", readiness=true. Elapsed: 2.717395ms
    Apr 17 22:11:53.123: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-hnfbz" satisfied condition "running"
    Apr 17 22:11:53.123: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-stcxp" in namespace "emptydir-wrapper-6233" to be "running"
    Apr 17 22:11:53.126: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-stcxp": Phase="Running", Reason="", readiness=true. Elapsed: 2.876817ms
    Apr 17 22:11:53.126: INFO: Pod "wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2-stcxp" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2 in namespace emptydir-wrapper-6233, will wait for the garbage collector to delete the pods 04/17/23 22:11:53.126
    Apr 17 22:11:53.186: INFO: Deleting ReplicationController wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2 took: 6.280525ms
    Apr 17 22:11:53.387: INFO: Terminating ReplicationController wrapped-volume-race-d28edc99-15df-44cc-97d5-3f1c9f4773c2 pods took: 200.880823ms
    STEP: Cleaning up the configMaps 04/17/23 22:11:55.888
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:11:56.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-6233" for this suite. 04/17/23 22:11:56.117
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:11:56.122
Apr 17 22:11:56.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 22:11:56.122
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:11:56.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:11:56.135
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 04/17/23 22:11:56.137
Apr 17 22:11:56.143: INFO: Waiting up to 5m0s for pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2" in namespace "emptydir-8693" to be "Succeeded or Failed"
Apr 17 22:11:56.145: INFO: Pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286902ms
Apr 17 22:11:58.149: INFO: Pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006067711s
Apr 17 22:12:00.150: INFO: Pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007119093s
STEP: Saw pod success 04/17/23 22:12:00.15
Apr 17 22:12:00.150: INFO: Pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2" satisfied condition "Succeeded or Failed"
Apr 17 22:12:00.152: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-91629bd8-f667-45ce-bf68-680b35796fa2 container test-container: <nil>
STEP: delete the pod 04/17/23 22:12:00.165
Apr 17 22:12:00.175: INFO: Waiting for pod pod-91629bd8-f667-45ce-bf68-680b35796fa2 to disappear
Apr 17 22:12:00.177: INFO: Pod pod-91629bd8-f667-45ce-bf68-680b35796fa2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:12:00.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8693" for this suite. 04/17/23 22:12:00.181
------------------------------
• [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:11:56.122
    Apr 17 22:11:56.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 22:11:56.122
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:11:56.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:11:56.135
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 04/17/23 22:11:56.137
    Apr 17 22:11:56.143: INFO: Waiting up to 5m0s for pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2" in namespace "emptydir-8693" to be "Succeeded or Failed"
    Apr 17 22:11:56.145: INFO: Pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.286902ms
    Apr 17 22:11:58.149: INFO: Pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006067711s
    Apr 17 22:12:00.150: INFO: Pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007119093s
    STEP: Saw pod success 04/17/23 22:12:00.15
    Apr 17 22:12:00.150: INFO: Pod "pod-91629bd8-f667-45ce-bf68-680b35796fa2" satisfied condition "Succeeded or Failed"
    Apr 17 22:12:00.152: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-91629bd8-f667-45ce-bf68-680b35796fa2 container test-container: <nil>
    STEP: delete the pod 04/17/23 22:12:00.165
    Apr 17 22:12:00.175: INFO: Waiting for pod pod-91629bd8-f667-45ce-bf68-680b35796fa2 to disappear
    Apr 17 22:12:00.177: INFO: Pod pod-91629bd8-f667-45ce-bf68-680b35796fa2 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:12:00.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8693" for this suite. 04/17/23 22:12:00.181
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:12:00.187
Apr 17 22:12:00.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename namespaces 04/17/23 22:12:00.187
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:12:00.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:12:00.201
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-q9w8v" 04/17/23 22:12:00.203
Apr 17 22:12:00.212: INFO: Namespace "e2e-ns-q9w8v-2123" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-q9w8v-2123" 04/17/23 22:12:00.212
Apr 17 22:12:00.219: INFO: Namespace "e2e-ns-q9w8v-2123" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-q9w8v-2123" 04/17/23 22:12:00.219
Apr 17 22:12:00.224: INFO: Namespace "e2e-ns-q9w8v-2123" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:12:00.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4903" for this suite. 04/17/23 22:12:00.228
STEP: Destroying namespace "e2e-ns-q9w8v-2123" for this suite. 04/17/23 22:12:00.233
------------------------------
• [0.051 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:12:00.187
    Apr 17 22:12:00.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename namespaces 04/17/23 22:12:00.187
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:12:00.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:12:00.201
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-q9w8v" 04/17/23 22:12:00.203
    Apr 17 22:12:00.212: INFO: Namespace "e2e-ns-q9w8v-2123" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-q9w8v-2123" 04/17/23 22:12:00.212
    Apr 17 22:12:00.219: INFO: Namespace "e2e-ns-q9w8v-2123" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-q9w8v-2123" 04/17/23 22:12:00.219
    Apr 17 22:12:00.224: INFO: Namespace "e2e-ns-q9w8v-2123" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:12:00.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4903" for this suite. 04/17/23 22:12:00.228
    STEP: Destroying namespace "e2e-ns-q9w8v-2123" for this suite. 04/17/23 22:12:00.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:12:00.238
Apr 17 22:12:00.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:12:00.239
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:12:00.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:12:00.27
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 04/17/23 22:12:00.271
Apr 17 22:12:00.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 create -f -'
Apr 17 22:12:01.594: INFO: stderr: ""
Apr 17 22:12:01.594: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/17/23 22:12:01.594
Apr 17 22:12:01.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 17 22:12:01.653: INFO: stderr: ""
Apr 17 22:12:01.653: INFO: stdout: "update-demo-nautilus-lvtcp update-demo-nautilus-vgfzz "
Apr 17 22:12:01.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-lvtcp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:12:01.707: INFO: stderr: ""
Apr 17 22:12:01.707: INFO: stdout: ""
Apr 17 22:12:01.707: INFO: update-demo-nautilus-lvtcp is created but not running
Apr 17 22:12:06.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 17 22:12:06.767: INFO: stderr: ""
Apr 17 22:12:06.767: INFO: stdout: "update-demo-nautilus-lvtcp update-demo-nautilus-vgfzz "
Apr 17 22:12:06.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-lvtcp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:12:06.822: INFO: stderr: ""
Apr 17 22:12:06.822: INFO: stdout: "true"
Apr 17 22:12:06.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-lvtcp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 17 22:12:06.876: INFO: stderr: ""
Apr 17 22:12:06.876: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 17 22:12:06.876: INFO: validating pod update-demo-nautilus-lvtcp
Apr 17 22:12:06.880: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 22:12:06.880: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 22:12:06.880: INFO: update-demo-nautilus-lvtcp is verified up and running
Apr 17 22:12:06.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:12:06.937: INFO: stderr: ""
Apr 17 22:12:06.937: INFO: stdout: "true"
Apr 17 22:12:06.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 17 22:12:06.993: INFO: stderr: ""
Apr 17 22:12:06.993: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 17 22:12:06.993: INFO: validating pod update-demo-nautilus-vgfzz
Apr 17 22:12:06.997: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 22:12:06.997: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 22:12:06.997: INFO: update-demo-nautilus-vgfzz is verified up and running
STEP: scaling down the replication controller 04/17/23 22:12:06.997
Apr 17 22:12:06.999: INFO: scanned /root for discovery docs: <nil>
Apr 17 22:12:06.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Apr 17 22:12:08.072: INFO: stderr: ""
Apr 17 22:12:08.072: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/17/23 22:12:08.072
Apr 17 22:12:08.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 17 22:12:08.132: INFO: stderr: ""
Apr 17 22:12:08.132: INFO: stdout: "update-demo-nautilus-lvtcp update-demo-nautilus-vgfzz "
STEP: Replicas for name=update-demo: expected=1 actual=2 04/17/23 22:12:08.132
Apr 17 22:12:13.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 17 22:12:13.189: INFO: stderr: ""
Apr 17 22:12:13.189: INFO: stdout: "update-demo-nautilus-vgfzz "
Apr 17 22:12:13.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:12:13.245: INFO: stderr: ""
Apr 17 22:12:13.245: INFO: stdout: "true"
Apr 17 22:12:13.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 17 22:12:13.301: INFO: stderr: ""
Apr 17 22:12:13.301: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 17 22:12:13.301: INFO: validating pod update-demo-nautilus-vgfzz
Apr 17 22:12:13.304: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 22:12:13.304: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 22:12:13.304: INFO: update-demo-nautilus-vgfzz is verified up and running
STEP: scaling up the replication controller 04/17/23 22:12:13.304
Apr 17 22:12:13.306: INFO: scanned /root for discovery docs: <nil>
Apr 17 22:12:13.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Apr 17 22:12:14.383: INFO: stderr: ""
Apr 17 22:12:14.383: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/17/23 22:12:14.383
Apr 17 22:12:14.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 17 22:12:14.444: INFO: stderr: ""
Apr 17 22:12:14.444: INFO: stdout: "update-demo-nautilus-f25qp update-demo-nautilus-vgfzz "
Apr 17 22:12:14.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-f25qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:12:14.500: INFO: stderr: ""
Apr 17 22:12:14.500: INFO: stdout: ""
Apr 17 22:12:14.500: INFO: update-demo-nautilus-f25qp is created but not running
Apr 17 22:12:19.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 17 22:12:19.560: INFO: stderr: ""
Apr 17 22:12:19.560: INFO: stdout: "update-demo-nautilus-f25qp update-demo-nautilus-vgfzz "
Apr 17 22:12:19.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-f25qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:12:19.616: INFO: stderr: ""
Apr 17 22:12:19.616: INFO: stdout: "true"
Apr 17 22:12:19.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-f25qp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 17 22:12:19.674: INFO: stderr: ""
Apr 17 22:12:19.674: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 17 22:12:19.674: INFO: validating pod update-demo-nautilus-f25qp
Apr 17 22:12:19.678: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 22:12:19.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 22:12:19.678: INFO: update-demo-nautilus-f25qp is verified up and running
Apr 17 22:12:19.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:12:19.732: INFO: stderr: ""
Apr 17 22:12:19.732: INFO: stdout: "true"
Apr 17 22:12:19.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 17 22:12:19.788: INFO: stderr: ""
Apr 17 22:12:19.788: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 17 22:12:19.788: INFO: validating pod update-demo-nautilus-vgfzz
Apr 17 22:12:19.791: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 22:12:19.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 22:12:19.791: INFO: update-demo-nautilus-vgfzz is verified up and running
STEP: using delete to clean up resources 04/17/23 22:12:19.791
Apr 17 22:12:19.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 delete --grace-period=0 --force -f -'
Apr 17 22:12:19.849: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 22:12:19.849: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 17 22:12:19.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get rc,svc -l name=update-demo --no-headers'
Apr 17 22:12:19.914: INFO: stderr: "No resources found in kubectl-2346 namespace.\n"
Apr 17 22:12:19.914: INFO: stdout: ""
Apr 17 22:12:19.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 17 22:12:19.973: INFO: stderr: ""
Apr 17 22:12:19.973: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:12:19.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2346" for this suite. 04/17/23 22:12:19.978
------------------------------
• [SLOW TEST] [19.745 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:12:00.238
    Apr 17 22:12:00.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:12:00.239
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:12:00.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:12:00.27
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 04/17/23 22:12:00.271
    Apr 17 22:12:00.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 create -f -'
    Apr 17 22:12:01.594: INFO: stderr: ""
    Apr 17 22:12:01.594: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/17/23 22:12:01.594
    Apr 17 22:12:01.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 17 22:12:01.653: INFO: stderr: ""
    Apr 17 22:12:01.653: INFO: stdout: "update-demo-nautilus-lvtcp update-demo-nautilus-vgfzz "
    Apr 17 22:12:01.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-lvtcp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:12:01.707: INFO: stderr: ""
    Apr 17 22:12:01.707: INFO: stdout: ""
    Apr 17 22:12:01.707: INFO: update-demo-nautilus-lvtcp is created but not running
    Apr 17 22:12:06.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 17 22:12:06.767: INFO: stderr: ""
    Apr 17 22:12:06.767: INFO: stdout: "update-demo-nautilus-lvtcp update-demo-nautilus-vgfzz "
    Apr 17 22:12:06.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-lvtcp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:12:06.822: INFO: stderr: ""
    Apr 17 22:12:06.822: INFO: stdout: "true"
    Apr 17 22:12:06.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-lvtcp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 17 22:12:06.876: INFO: stderr: ""
    Apr 17 22:12:06.876: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 17 22:12:06.876: INFO: validating pod update-demo-nautilus-lvtcp
    Apr 17 22:12:06.880: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 17 22:12:06.880: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 17 22:12:06.880: INFO: update-demo-nautilus-lvtcp is verified up and running
    Apr 17 22:12:06.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:12:06.937: INFO: stderr: ""
    Apr 17 22:12:06.937: INFO: stdout: "true"
    Apr 17 22:12:06.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 17 22:12:06.993: INFO: stderr: ""
    Apr 17 22:12:06.993: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 17 22:12:06.993: INFO: validating pod update-demo-nautilus-vgfzz
    Apr 17 22:12:06.997: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 17 22:12:06.997: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 17 22:12:06.997: INFO: update-demo-nautilus-vgfzz is verified up and running
    STEP: scaling down the replication controller 04/17/23 22:12:06.997
    Apr 17 22:12:06.999: INFO: scanned /root for discovery docs: <nil>
    Apr 17 22:12:06.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Apr 17 22:12:08.072: INFO: stderr: ""
    Apr 17 22:12:08.072: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/17/23 22:12:08.072
    Apr 17 22:12:08.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 17 22:12:08.132: INFO: stderr: ""
    Apr 17 22:12:08.132: INFO: stdout: "update-demo-nautilus-lvtcp update-demo-nautilus-vgfzz "
    STEP: Replicas for name=update-demo: expected=1 actual=2 04/17/23 22:12:08.132
    Apr 17 22:12:13.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 17 22:12:13.189: INFO: stderr: ""
    Apr 17 22:12:13.189: INFO: stdout: "update-demo-nautilus-vgfzz "
    Apr 17 22:12:13.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:12:13.245: INFO: stderr: ""
    Apr 17 22:12:13.245: INFO: stdout: "true"
    Apr 17 22:12:13.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 17 22:12:13.301: INFO: stderr: ""
    Apr 17 22:12:13.301: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 17 22:12:13.301: INFO: validating pod update-demo-nautilus-vgfzz
    Apr 17 22:12:13.304: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 17 22:12:13.304: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 17 22:12:13.304: INFO: update-demo-nautilus-vgfzz is verified up and running
    STEP: scaling up the replication controller 04/17/23 22:12:13.304
    Apr 17 22:12:13.306: INFO: scanned /root for discovery docs: <nil>
    Apr 17 22:12:13.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Apr 17 22:12:14.383: INFO: stderr: ""
    Apr 17 22:12:14.383: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/17/23 22:12:14.383
    Apr 17 22:12:14.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 17 22:12:14.444: INFO: stderr: ""
    Apr 17 22:12:14.444: INFO: stdout: "update-demo-nautilus-f25qp update-demo-nautilus-vgfzz "
    Apr 17 22:12:14.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-f25qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:12:14.500: INFO: stderr: ""
    Apr 17 22:12:14.500: INFO: stdout: ""
    Apr 17 22:12:14.500: INFO: update-demo-nautilus-f25qp is created but not running
    Apr 17 22:12:19.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 17 22:12:19.560: INFO: stderr: ""
    Apr 17 22:12:19.560: INFO: stdout: "update-demo-nautilus-f25qp update-demo-nautilus-vgfzz "
    Apr 17 22:12:19.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-f25qp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:12:19.616: INFO: stderr: ""
    Apr 17 22:12:19.616: INFO: stdout: "true"
    Apr 17 22:12:19.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-f25qp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 17 22:12:19.674: INFO: stderr: ""
    Apr 17 22:12:19.674: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 17 22:12:19.674: INFO: validating pod update-demo-nautilus-f25qp
    Apr 17 22:12:19.678: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 17 22:12:19.678: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 17 22:12:19.678: INFO: update-demo-nautilus-f25qp is verified up and running
    Apr 17 22:12:19.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:12:19.732: INFO: stderr: ""
    Apr 17 22:12:19.732: INFO: stdout: "true"
    Apr 17 22:12:19.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods update-demo-nautilus-vgfzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 17 22:12:19.788: INFO: stderr: ""
    Apr 17 22:12:19.788: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 17 22:12:19.788: INFO: validating pod update-demo-nautilus-vgfzz
    Apr 17 22:12:19.791: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 17 22:12:19.791: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 17 22:12:19.791: INFO: update-demo-nautilus-vgfzz is verified up and running
    STEP: using delete to clean up resources 04/17/23 22:12:19.791
    Apr 17 22:12:19.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 delete --grace-period=0 --force -f -'
    Apr 17 22:12:19.849: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 17 22:12:19.849: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Apr 17 22:12:19.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get rc,svc -l name=update-demo --no-headers'
    Apr 17 22:12:19.914: INFO: stderr: "No resources found in kubectl-2346 namespace.\n"
    Apr 17 22:12:19.914: INFO: stdout: ""
    Apr 17 22:12:19.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2346 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr 17 22:12:19.973: INFO: stderr: ""
    Apr 17 22:12:19.973: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:12:19.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2346" for this suite. 04/17/23 22:12:19.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:12:19.984
Apr 17 22:12:19.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename prestop 04/17/23 22:12:19.984
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:12:19.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:12:19.997
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-2998 04/17/23 22:12:19.999
STEP: Waiting for pods to come up. 04/17/23 22:12:20.005
Apr 17 22:12:20.005: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2998" to be "running"
Apr 17 22:12:20.007: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084415ms
Apr 17 22:12:22.011: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.005924026s
Apr 17 22:12:22.011: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-2998 04/17/23 22:12:22.014
Apr 17 22:12:22.019: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2998" to be "running"
Apr 17 22:12:22.022: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.027904ms
Apr 17 22:12:24.025: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006018029s
Apr 17 22:12:24.025: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 04/17/23 22:12:24.025
Apr 17 22:12:29.036: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 04/17/23 22:12:29.036
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Apr 17 22:12:29.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-2998" for this suite. 04/17/23 22:12:29.053
------------------------------
• [SLOW TEST] [9.076 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:12:19.984
    Apr 17 22:12:19.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename prestop 04/17/23 22:12:19.984
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:12:19.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:12:19.997
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-2998 04/17/23 22:12:19.999
    STEP: Waiting for pods to come up. 04/17/23 22:12:20.005
    Apr 17 22:12:20.005: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-2998" to be "running"
    Apr 17 22:12:20.007: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.084415ms
    Apr 17 22:12:22.011: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.005924026s
    Apr 17 22:12:22.011: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-2998 04/17/23 22:12:22.014
    Apr 17 22:12:22.019: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-2998" to be "running"
    Apr 17 22:12:22.022: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 3.027904ms
    Apr 17 22:12:24.025: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006018029s
    Apr 17 22:12:24.025: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 04/17/23 22:12:24.025
    Apr 17 22:12:29.036: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 04/17/23 22:12:29.036
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:12:29.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-2998" for this suite. 04/17/23 22:12:29.053
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:12:29.06
Apr 17 22:12:29.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-watch 04/17/23 22:12:29.061
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:12:29.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:12:29.072
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Apr 17 22:12:29.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Creating first CR  04/17/23 22:12:31.612
Apr 17 22:12:31.616: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:12:31Z]] name:name1 resourceVersion:51773 uid:e6031dda-20a7-4c31-aef6-05e3697e37a1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 04/17/23 22:12:41.617
Apr 17 22:12:41.622: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:12:41Z]] name:name2 resourceVersion:51872 uid:3ace6b8c-0da7-4f0d-9333-158c4e055a3a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 04/17/23 22:12:51.623
Apr 17 22:12:51.629: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:12:51Z]] name:name1 resourceVersion:51959 uid:e6031dda-20a7-4c31-aef6-05e3697e37a1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 04/17/23 22:13:01.63
Apr 17 22:13:01.634: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:13:01Z]] name:name2 resourceVersion:52053 uid:3ace6b8c-0da7-4f0d-9333-158c4e055a3a] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 04/17/23 22:13:11.637
Apr 17 22:13:11.643: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:12:51Z]] name:name1 resourceVersion:52137 uid:e6031dda-20a7-4c31-aef6-05e3697e37a1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 04/17/23 22:13:21.644
Apr 17 22:13:21.651: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:13:01Z]] name:name2 resourceVersion:52225 uid:3ace6b8c-0da7-4f0d-9333-158c4e055a3a] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:13:32.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-8170" for this suite. 04/17/23 22:13:32.165
------------------------------
• [SLOW TEST] [63.110 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:12:29.06
    Apr 17 22:12:29.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-watch 04/17/23 22:12:29.061
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:12:29.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:12:29.072
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Apr 17 22:12:29.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Creating first CR  04/17/23 22:12:31.612
    Apr 17 22:12:31.616: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:31Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:12:31Z]] name:name1 resourceVersion:51773 uid:e6031dda-20a7-4c31-aef6-05e3697e37a1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 04/17/23 22:12:41.617
    Apr 17 22:12:41.622: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:41Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:12:41Z]] name:name2 resourceVersion:51872 uid:3ace6b8c-0da7-4f0d-9333-158c4e055a3a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 04/17/23 22:12:51.623
    Apr 17 22:12:51.629: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:12:51Z]] name:name1 resourceVersion:51959 uid:e6031dda-20a7-4c31-aef6-05e3697e37a1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 04/17/23 22:13:01.63
    Apr 17 22:13:01.634: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:13:01Z]] name:name2 resourceVersion:52053 uid:3ace6b8c-0da7-4f0d-9333-158c4e055a3a] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 04/17/23 22:13:11.637
    Apr 17 22:13:11.643: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:31Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:12:51Z]] name:name1 resourceVersion:52137 uid:e6031dda-20a7-4c31-aef6-05e3697e37a1] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 04/17/23 22:13:21.644
    Apr 17 22:13:21.651: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-04-17T22:12:41Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-04-17T22:13:01Z]] name:name2 resourceVersion:52225 uid:3ace6b8c-0da7-4f0d-9333-158c4e055a3a] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:13:32.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-8170" for this suite. 04/17/23 22:13:32.165
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:13:32.17
Apr 17 22:13:32.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename discovery 04/17/23 22:13:32.171
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:13:32.185
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:13:32.187
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 04/17/23 22:13:32.189
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Apr 17 22:13:32.311: INFO: Checking APIGroup: apiregistration.k8s.io
Apr 17 22:13:32.312: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Apr 17 22:13:32.312: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Apr 17 22:13:32.312: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Apr 17 22:13:32.312: INFO: Checking APIGroup: apps
Apr 17 22:13:32.312: INFO: PreferredVersion.GroupVersion: apps/v1
Apr 17 22:13:32.312: INFO: Versions found [{apps/v1 v1}]
Apr 17 22:13:32.312: INFO: apps/v1 matches apps/v1
Apr 17 22:13:32.312: INFO: Checking APIGroup: events.k8s.io
Apr 17 22:13:32.313: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Apr 17 22:13:32.313: INFO: Versions found [{events.k8s.io/v1 v1}]
Apr 17 22:13:32.313: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Apr 17 22:13:32.313: INFO: Checking APIGroup: authentication.k8s.io
Apr 17 22:13:32.313: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Apr 17 22:13:32.313: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Apr 17 22:13:32.313: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Apr 17 22:13:32.313: INFO: Checking APIGroup: authorization.k8s.io
Apr 17 22:13:32.314: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Apr 17 22:13:32.314: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Apr 17 22:13:32.314: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Apr 17 22:13:32.314: INFO: Checking APIGroup: autoscaling
Apr 17 22:13:32.315: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Apr 17 22:13:32.315: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Apr 17 22:13:32.315: INFO: autoscaling/v2 matches autoscaling/v2
Apr 17 22:13:32.315: INFO: Checking APIGroup: batch
Apr 17 22:13:32.315: INFO: PreferredVersion.GroupVersion: batch/v1
Apr 17 22:13:32.315: INFO: Versions found [{batch/v1 v1}]
Apr 17 22:13:32.315: INFO: batch/v1 matches batch/v1
Apr 17 22:13:32.315: INFO: Checking APIGroup: certificates.k8s.io
Apr 17 22:13:32.316: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Apr 17 22:13:32.316: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Apr 17 22:13:32.316: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Apr 17 22:13:32.316: INFO: Checking APIGroup: networking.k8s.io
Apr 17 22:13:32.316: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Apr 17 22:13:32.316: INFO: Versions found [{networking.k8s.io/v1 v1}]
Apr 17 22:13:32.316: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Apr 17 22:13:32.316: INFO: Checking APIGroup: policy
Apr 17 22:13:32.317: INFO: PreferredVersion.GroupVersion: policy/v1
Apr 17 22:13:32.317: INFO: Versions found [{policy/v1 v1}]
Apr 17 22:13:32.317: INFO: policy/v1 matches policy/v1
Apr 17 22:13:32.317: INFO: Checking APIGroup: rbac.authorization.k8s.io
Apr 17 22:13:32.317: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Apr 17 22:13:32.317: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Apr 17 22:13:32.317: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Apr 17 22:13:32.317: INFO: Checking APIGroup: storage.k8s.io
Apr 17 22:13:32.318: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Apr 17 22:13:32.318: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Apr 17 22:13:32.318: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Apr 17 22:13:32.318: INFO: Checking APIGroup: admissionregistration.k8s.io
Apr 17 22:13:32.318: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Apr 17 22:13:32.318: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Apr 17 22:13:32.319: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Apr 17 22:13:32.319: INFO: Checking APIGroup: apiextensions.k8s.io
Apr 17 22:13:32.319: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Apr 17 22:13:32.319: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Apr 17 22:13:32.319: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Apr 17 22:13:32.319: INFO: Checking APIGroup: scheduling.k8s.io
Apr 17 22:13:32.320: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Apr 17 22:13:32.320: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Apr 17 22:13:32.320: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Apr 17 22:13:32.320: INFO: Checking APIGroup: coordination.k8s.io
Apr 17 22:13:32.320: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Apr 17 22:13:32.320: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Apr 17 22:13:32.320: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Apr 17 22:13:32.320: INFO: Checking APIGroup: node.k8s.io
Apr 17 22:13:32.321: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Apr 17 22:13:32.321: INFO: Versions found [{node.k8s.io/v1 v1}]
Apr 17 22:13:32.321: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Apr 17 22:13:32.321: INFO: Checking APIGroup: discovery.k8s.io
Apr 17 22:13:32.321: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Apr 17 22:13:32.321: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Apr 17 22:13:32.321: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Apr 17 22:13:32.321: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Apr 17 22:13:32.322: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Apr 17 22:13:32.322: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Apr 17 22:13:32.322: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Apr 17 22:13:32.322: INFO: Checking APIGroup: aadpodidentity.k8s.io
Apr 17 22:13:32.322: INFO: PreferredVersion.GroupVersion: aadpodidentity.k8s.io/v1
Apr 17 22:13:32.322: INFO: Versions found [{aadpodidentity.k8s.io/v1 v1}]
Apr 17 22:13:32.322: INFO: aadpodidentity.k8s.io/v1 matches aadpodidentity.k8s.io/v1
Apr 17 22:13:32.322: INFO: Checking APIGroup: acme.cert-manager.io
Apr 17 22:13:32.323: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Apr 17 22:13:32.323: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Apr 17 22:13:32.323: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Apr 17 22:13:32.323: INFO: Checking APIGroup: cert-manager.io
Apr 17 22:13:32.323: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Apr 17 22:13:32.323: INFO: Versions found [{cert-manager.io/v1 v1}]
Apr 17 22:13:32.323: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Apr 17 22:13:32.323: INFO: Checking APIGroup: crd.projectcalico.org
Apr 17 22:13:32.324: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Apr 17 22:13:32.324: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Apr 17 22:13:32.324: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Apr 17 22:13:32.324: INFO: Checking APIGroup: operator.tigera.io
Apr 17 22:13:32.324: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Apr 17 22:13:32.325: INFO: Versions found [{operator.tigera.io/v1 v1}]
Apr 17 22:13:32.325: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Apr 17 22:13:32.325: INFO: Checking APIGroup: snapshot.storage.k8s.io
Apr 17 22:13:32.325: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Apr 17 22:13:32.325: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Apr 17 22:13:32.325: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Apr 17 22:13:32.325: INFO: Checking APIGroup: infrastructure.cluster.konvoy.d2iq.io
Apr 17 22:13:32.326: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.konvoy.d2iq.io/v1alpha1
Apr 17 22:13:32.326: INFO: Versions found [{infrastructure.cluster.konvoy.d2iq.io/v1alpha1 v1alpha1}]
Apr 17 22:13:32.326: INFO: infrastructure.cluster.konvoy.d2iq.io/v1alpha1 matches infrastructure.cluster.konvoy.d2iq.io/v1alpha1
Apr 17 22:13:32.326: INFO: Checking APIGroup: ipam.cluster.x-k8s.io
Apr 17 22:13:32.326: INFO: PreferredVersion.GroupVersion: ipam.cluster.x-k8s.io/v1alpha1
Apr 17 22:13:32.326: INFO: Versions found [{ipam.cluster.x-k8s.io/v1alpha1 v1alpha1}]
Apr 17 22:13:32.326: INFO: ipam.cluster.x-k8s.io/v1alpha1 matches ipam.cluster.x-k8s.io/v1alpha1
Apr 17 22:13:32.326: INFO: Checking APIGroup: nfd.k8s-sigs.io
Apr 17 22:13:32.327: INFO: PreferredVersion.GroupVersion: nfd.k8s-sigs.io/v1alpha1
Apr 17 22:13:32.327: INFO: Versions found [{nfd.k8s-sigs.io/v1alpha1 v1alpha1}]
Apr 17 22:13:32.327: INFO: nfd.k8s-sigs.io/v1alpha1 matches nfd.k8s-sigs.io/v1alpha1
Apr 17 22:13:32.327: INFO: Checking APIGroup: runtime.cluster.x-k8s.io
Apr 17 22:13:32.327: INFO: PreferredVersion.GroupVersion: runtime.cluster.x-k8s.io/v1alpha1
Apr 17 22:13:32.327: INFO: Versions found [{runtime.cluster.x-k8s.io/v1alpha1 v1alpha1}]
Apr 17 22:13:32.327: INFO: runtime.cluster.x-k8s.io/v1alpha1 matches runtime.cluster.x-k8s.io/v1alpha1
Apr 17 22:13:32.327: INFO: Checking APIGroup: addons.cluster.x-k8s.io
Apr 17 22:13:32.328: INFO: PreferredVersion.GroupVersion: addons.cluster.x-k8s.io/v1beta1
Apr 17 22:13:32.328: INFO: Versions found [{addons.cluster.x-k8s.io/v1beta1 v1beta1} {addons.cluster.x-k8s.io/v1alpha4 v1alpha4} {addons.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Apr 17 22:13:32.328: INFO: addons.cluster.x-k8s.io/v1beta1 matches addons.cluster.x-k8s.io/v1beta1
Apr 17 22:13:32.328: INFO: Checking APIGroup: bootstrap.cluster.x-k8s.io
Apr 17 22:13:32.328: INFO: PreferredVersion.GroupVersion: bootstrap.cluster.x-k8s.io/v1beta2
Apr 17 22:13:32.328: INFO: Versions found [{bootstrap.cluster.x-k8s.io/v1beta2 v1beta2} {bootstrap.cluster.x-k8s.io/v1beta1 v1beta1} {bootstrap.cluster.x-k8s.io/v1alpha4 v1alpha4} {bootstrap.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Apr 17 22:13:32.328: INFO: bootstrap.cluster.x-k8s.io/v1beta2 matches bootstrap.cluster.x-k8s.io/v1beta2
Apr 17 22:13:32.328: INFO: Checking APIGroup: cluster.x-k8s.io
Apr 17 22:13:32.329: INFO: PreferredVersion.GroupVersion: cluster.x-k8s.io/v1beta1
Apr 17 22:13:32.329: INFO: Versions found [{cluster.x-k8s.io/v1beta1 v1beta1} {cluster.x-k8s.io/v1alpha4 v1alpha4} {cluster.x-k8s.io/v1alpha3 v1alpha3}]
Apr 17 22:13:32.329: INFO: cluster.x-k8s.io/v1beta1 matches cluster.x-k8s.io/v1beta1
Apr 17 22:13:32.329: INFO: Checking APIGroup: clusterctl.cluster.x-k8s.io
Apr 17 22:13:32.329: INFO: PreferredVersion.GroupVersion: clusterctl.cluster.x-k8s.io/v1alpha3
Apr 17 22:13:32.329: INFO: Versions found [{clusterctl.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Apr 17 22:13:32.329: INFO: clusterctl.cluster.x-k8s.io/v1alpha3 matches clusterctl.cluster.x-k8s.io/v1alpha3
Apr 17 22:13:32.329: INFO: Checking APIGroup: controlplane.cluster.x-k8s.io
Apr 17 22:13:32.330: INFO: PreferredVersion.GroupVersion: controlplane.cluster.x-k8s.io/v1beta2
Apr 17 22:13:32.330: INFO: Versions found [{controlplane.cluster.x-k8s.io/v1beta2 v1beta2} {controlplane.cluster.x-k8s.io/v1beta1 v1beta1} {controlplane.cluster.x-k8s.io/v1alpha4 v1alpha4} {controlplane.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Apr 17 22:13:32.330: INFO: controlplane.cluster.x-k8s.io/v1beta2 matches controlplane.cluster.x-k8s.io/v1beta2
Apr 17 22:13:32.330: INFO: Checking APIGroup: infrastructure.cluster.x-k8s.io
Apr 17 22:13:32.330: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.x-k8s.io/v1beta2
Apr 17 22:13:32.330: INFO: Versions found [{infrastructure.cluster.x-k8s.io/v1beta2 v1beta2} {infrastructure.cluster.x-k8s.io/v1beta1 v1beta1} {infrastructure.cluster.x-k8s.io/v1alpha4 v1alpha4} {infrastructure.cluster.x-k8s.io/v1alpha3 v1alpha3}]
Apr 17 22:13:32.330: INFO: infrastructure.cluster.x-k8s.io/v1beta2 matches infrastructure.cluster.x-k8s.io/v1beta2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Apr 17 22:13:32.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-1260" for this suite. 04/17/23 22:13:32.335
------------------------------
• [0.170 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:13:32.17
    Apr 17 22:13:32.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename discovery 04/17/23 22:13:32.171
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:13:32.185
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:13:32.187
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 04/17/23 22:13:32.189
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Apr 17 22:13:32.311: INFO: Checking APIGroup: apiregistration.k8s.io
    Apr 17 22:13:32.312: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Apr 17 22:13:32.312: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Apr 17 22:13:32.312: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Apr 17 22:13:32.312: INFO: Checking APIGroup: apps
    Apr 17 22:13:32.312: INFO: PreferredVersion.GroupVersion: apps/v1
    Apr 17 22:13:32.312: INFO: Versions found [{apps/v1 v1}]
    Apr 17 22:13:32.312: INFO: apps/v1 matches apps/v1
    Apr 17 22:13:32.312: INFO: Checking APIGroup: events.k8s.io
    Apr 17 22:13:32.313: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Apr 17 22:13:32.313: INFO: Versions found [{events.k8s.io/v1 v1}]
    Apr 17 22:13:32.313: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Apr 17 22:13:32.313: INFO: Checking APIGroup: authentication.k8s.io
    Apr 17 22:13:32.313: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Apr 17 22:13:32.313: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Apr 17 22:13:32.313: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Apr 17 22:13:32.313: INFO: Checking APIGroup: authorization.k8s.io
    Apr 17 22:13:32.314: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Apr 17 22:13:32.314: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Apr 17 22:13:32.314: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Apr 17 22:13:32.314: INFO: Checking APIGroup: autoscaling
    Apr 17 22:13:32.315: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Apr 17 22:13:32.315: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Apr 17 22:13:32.315: INFO: autoscaling/v2 matches autoscaling/v2
    Apr 17 22:13:32.315: INFO: Checking APIGroup: batch
    Apr 17 22:13:32.315: INFO: PreferredVersion.GroupVersion: batch/v1
    Apr 17 22:13:32.315: INFO: Versions found [{batch/v1 v1}]
    Apr 17 22:13:32.315: INFO: batch/v1 matches batch/v1
    Apr 17 22:13:32.315: INFO: Checking APIGroup: certificates.k8s.io
    Apr 17 22:13:32.316: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Apr 17 22:13:32.316: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Apr 17 22:13:32.316: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Apr 17 22:13:32.316: INFO: Checking APIGroup: networking.k8s.io
    Apr 17 22:13:32.316: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Apr 17 22:13:32.316: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Apr 17 22:13:32.316: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Apr 17 22:13:32.316: INFO: Checking APIGroup: policy
    Apr 17 22:13:32.317: INFO: PreferredVersion.GroupVersion: policy/v1
    Apr 17 22:13:32.317: INFO: Versions found [{policy/v1 v1}]
    Apr 17 22:13:32.317: INFO: policy/v1 matches policy/v1
    Apr 17 22:13:32.317: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Apr 17 22:13:32.317: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Apr 17 22:13:32.317: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Apr 17 22:13:32.317: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Apr 17 22:13:32.317: INFO: Checking APIGroup: storage.k8s.io
    Apr 17 22:13:32.318: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Apr 17 22:13:32.318: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Apr 17 22:13:32.318: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Apr 17 22:13:32.318: INFO: Checking APIGroup: admissionregistration.k8s.io
    Apr 17 22:13:32.318: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Apr 17 22:13:32.318: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Apr 17 22:13:32.319: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Apr 17 22:13:32.319: INFO: Checking APIGroup: apiextensions.k8s.io
    Apr 17 22:13:32.319: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Apr 17 22:13:32.319: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Apr 17 22:13:32.319: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Apr 17 22:13:32.319: INFO: Checking APIGroup: scheduling.k8s.io
    Apr 17 22:13:32.320: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Apr 17 22:13:32.320: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Apr 17 22:13:32.320: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Apr 17 22:13:32.320: INFO: Checking APIGroup: coordination.k8s.io
    Apr 17 22:13:32.320: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Apr 17 22:13:32.320: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Apr 17 22:13:32.320: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Apr 17 22:13:32.320: INFO: Checking APIGroup: node.k8s.io
    Apr 17 22:13:32.321: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Apr 17 22:13:32.321: INFO: Versions found [{node.k8s.io/v1 v1}]
    Apr 17 22:13:32.321: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Apr 17 22:13:32.321: INFO: Checking APIGroup: discovery.k8s.io
    Apr 17 22:13:32.321: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Apr 17 22:13:32.321: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Apr 17 22:13:32.321: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Apr 17 22:13:32.321: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Apr 17 22:13:32.322: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Apr 17 22:13:32.322: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Apr 17 22:13:32.322: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Apr 17 22:13:32.322: INFO: Checking APIGroup: aadpodidentity.k8s.io
    Apr 17 22:13:32.322: INFO: PreferredVersion.GroupVersion: aadpodidentity.k8s.io/v1
    Apr 17 22:13:32.322: INFO: Versions found [{aadpodidentity.k8s.io/v1 v1}]
    Apr 17 22:13:32.322: INFO: aadpodidentity.k8s.io/v1 matches aadpodidentity.k8s.io/v1
    Apr 17 22:13:32.322: INFO: Checking APIGroup: acme.cert-manager.io
    Apr 17 22:13:32.323: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Apr 17 22:13:32.323: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Apr 17 22:13:32.323: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Apr 17 22:13:32.323: INFO: Checking APIGroup: cert-manager.io
    Apr 17 22:13:32.323: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Apr 17 22:13:32.323: INFO: Versions found [{cert-manager.io/v1 v1}]
    Apr 17 22:13:32.323: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Apr 17 22:13:32.323: INFO: Checking APIGroup: crd.projectcalico.org
    Apr 17 22:13:32.324: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Apr 17 22:13:32.324: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Apr 17 22:13:32.324: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Apr 17 22:13:32.324: INFO: Checking APIGroup: operator.tigera.io
    Apr 17 22:13:32.324: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    Apr 17 22:13:32.325: INFO: Versions found [{operator.tigera.io/v1 v1}]
    Apr 17 22:13:32.325: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    Apr 17 22:13:32.325: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Apr 17 22:13:32.325: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Apr 17 22:13:32.325: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Apr 17 22:13:32.325: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Apr 17 22:13:32.325: INFO: Checking APIGroup: infrastructure.cluster.konvoy.d2iq.io
    Apr 17 22:13:32.326: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.konvoy.d2iq.io/v1alpha1
    Apr 17 22:13:32.326: INFO: Versions found [{infrastructure.cluster.konvoy.d2iq.io/v1alpha1 v1alpha1}]
    Apr 17 22:13:32.326: INFO: infrastructure.cluster.konvoy.d2iq.io/v1alpha1 matches infrastructure.cluster.konvoy.d2iq.io/v1alpha1
    Apr 17 22:13:32.326: INFO: Checking APIGroup: ipam.cluster.x-k8s.io
    Apr 17 22:13:32.326: INFO: PreferredVersion.GroupVersion: ipam.cluster.x-k8s.io/v1alpha1
    Apr 17 22:13:32.326: INFO: Versions found [{ipam.cluster.x-k8s.io/v1alpha1 v1alpha1}]
    Apr 17 22:13:32.326: INFO: ipam.cluster.x-k8s.io/v1alpha1 matches ipam.cluster.x-k8s.io/v1alpha1
    Apr 17 22:13:32.326: INFO: Checking APIGroup: nfd.k8s-sigs.io
    Apr 17 22:13:32.327: INFO: PreferredVersion.GroupVersion: nfd.k8s-sigs.io/v1alpha1
    Apr 17 22:13:32.327: INFO: Versions found [{nfd.k8s-sigs.io/v1alpha1 v1alpha1}]
    Apr 17 22:13:32.327: INFO: nfd.k8s-sigs.io/v1alpha1 matches nfd.k8s-sigs.io/v1alpha1
    Apr 17 22:13:32.327: INFO: Checking APIGroup: runtime.cluster.x-k8s.io
    Apr 17 22:13:32.327: INFO: PreferredVersion.GroupVersion: runtime.cluster.x-k8s.io/v1alpha1
    Apr 17 22:13:32.327: INFO: Versions found [{runtime.cluster.x-k8s.io/v1alpha1 v1alpha1}]
    Apr 17 22:13:32.327: INFO: runtime.cluster.x-k8s.io/v1alpha1 matches runtime.cluster.x-k8s.io/v1alpha1
    Apr 17 22:13:32.327: INFO: Checking APIGroup: addons.cluster.x-k8s.io
    Apr 17 22:13:32.328: INFO: PreferredVersion.GroupVersion: addons.cluster.x-k8s.io/v1beta1
    Apr 17 22:13:32.328: INFO: Versions found [{addons.cluster.x-k8s.io/v1beta1 v1beta1} {addons.cluster.x-k8s.io/v1alpha4 v1alpha4} {addons.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Apr 17 22:13:32.328: INFO: addons.cluster.x-k8s.io/v1beta1 matches addons.cluster.x-k8s.io/v1beta1
    Apr 17 22:13:32.328: INFO: Checking APIGroup: bootstrap.cluster.x-k8s.io
    Apr 17 22:13:32.328: INFO: PreferredVersion.GroupVersion: bootstrap.cluster.x-k8s.io/v1beta2
    Apr 17 22:13:32.328: INFO: Versions found [{bootstrap.cluster.x-k8s.io/v1beta2 v1beta2} {bootstrap.cluster.x-k8s.io/v1beta1 v1beta1} {bootstrap.cluster.x-k8s.io/v1alpha4 v1alpha4} {bootstrap.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Apr 17 22:13:32.328: INFO: bootstrap.cluster.x-k8s.io/v1beta2 matches bootstrap.cluster.x-k8s.io/v1beta2
    Apr 17 22:13:32.328: INFO: Checking APIGroup: cluster.x-k8s.io
    Apr 17 22:13:32.329: INFO: PreferredVersion.GroupVersion: cluster.x-k8s.io/v1beta1
    Apr 17 22:13:32.329: INFO: Versions found [{cluster.x-k8s.io/v1beta1 v1beta1} {cluster.x-k8s.io/v1alpha4 v1alpha4} {cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Apr 17 22:13:32.329: INFO: cluster.x-k8s.io/v1beta1 matches cluster.x-k8s.io/v1beta1
    Apr 17 22:13:32.329: INFO: Checking APIGroup: clusterctl.cluster.x-k8s.io
    Apr 17 22:13:32.329: INFO: PreferredVersion.GroupVersion: clusterctl.cluster.x-k8s.io/v1alpha3
    Apr 17 22:13:32.329: INFO: Versions found [{clusterctl.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Apr 17 22:13:32.329: INFO: clusterctl.cluster.x-k8s.io/v1alpha3 matches clusterctl.cluster.x-k8s.io/v1alpha3
    Apr 17 22:13:32.329: INFO: Checking APIGroup: controlplane.cluster.x-k8s.io
    Apr 17 22:13:32.330: INFO: PreferredVersion.GroupVersion: controlplane.cluster.x-k8s.io/v1beta2
    Apr 17 22:13:32.330: INFO: Versions found [{controlplane.cluster.x-k8s.io/v1beta2 v1beta2} {controlplane.cluster.x-k8s.io/v1beta1 v1beta1} {controlplane.cluster.x-k8s.io/v1alpha4 v1alpha4} {controlplane.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Apr 17 22:13:32.330: INFO: controlplane.cluster.x-k8s.io/v1beta2 matches controlplane.cluster.x-k8s.io/v1beta2
    Apr 17 22:13:32.330: INFO: Checking APIGroup: infrastructure.cluster.x-k8s.io
    Apr 17 22:13:32.330: INFO: PreferredVersion.GroupVersion: infrastructure.cluster.x-k8s.io/v1beta2
    Apr 17 22:13:32.330: INFO: Versions found [{infrastructure.cluster.x-k8s.io/v1beta2 v1beta2} {infrastructure.cluster.x-k8s.io/v1beta1 v1beta1} {infrastructure.cluster.x-k8s.io/v1alpha4 v1alpha4} {infrastructure.cluster.x-k8s.io/v1alpha3 v1alpha3}]
    Apr 17 22:13:32.330: INFO: infrastructure.cluster.x-k8s.io/v1beta2 matches infrastructure.cluster.x-k8s.io/v1beta2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:13:32.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-1260" for this suite. 04/17/23 22:13:32.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:13:32.341
Apr 17 22:13:32.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename endpointslice 04/17/23 22:13:32.341
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:13:32.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:13:32.354
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 04/17/23 22:13:32.356
STEP: getting /apis/discovery.k8s.io 04/17/23 22:13:32.357
STEP: getting /apis/discovery.k8s.iov1 04/17/23 22:13:32.358
STEP: creating 04/17/23 22:13:32.358
STEP: getting 04/17/23 22:13:32.371
STEP: listing 04/17/23 22:13:32.373
STEP: watching 04/17/23 22:13:32.375
Apr 17 22:13:32.375: INFO: starting watch
STEP: cluster-wide listing 04/17/23 22:13:32.376
STEP: cluster-wide watching 04/17/23 22:13:32.379
Apr 17 22:13:32.379: INFO: starting watch
STEP: patching 04/17/23 22:13:32.379
STEP: updating 04/17/23 22:13:32.383
Apr 17 22:13:32.390: INFO: waiting for watch events with expected annotations
Apr 17 22:13:32.390: INFO: saw patched and updated annotations
STEP: deleting 04/17/23 22:13:32.39
STEP: deleting a collection 04/17/23 22:13:32.399
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 17 22:13:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2672" for this suite. 04/17/23 22:13:32.414
------------------------------
• [0.078 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:13:32.341
    Apr 17 22:13:32.341: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename endpointslice 04/17/23 22:13:32.341
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:13:32.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:13:32.354
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 04/17/23 22:13:32.356
    STEP: getting /apis/discovery.k8s.io 04/17/23 22:13:32.357
    STEP: getting /apis/discovery.k8s.iov1 04/17/23 22:13:32.358
    STEP: creating 04/17/23 22:13:32.358
    STEP: getting 04/17/23 22:13:32.371
    STEP: listing 04/17/23 22:13:32.373
    STEP: watching 04/17/23 22:13:32.375
    Apr 17 22:13:32.375: INFO: starting watch
    STEP: cluster-wide listing 04/17/23 22:13:32.376
    STEP: cluster-wide watching 04/17/23 22:13:32.379
    Apr 17 22:13:32.379: INFO: starting watch
    STEP: patching 04/17/23 22:13:32.379
    STEP: updating 04/17/23 22:13:32.383
    Apr 17 22:13:32.390: INFO: waiting for watch events with expected annotations
    Apr 17 22:13:32.390: INFO: saw patched and updated annotations
    STEP: deleting 04/17/23 22:13:32.39
    STEP: deleting a collection 04/17/23 22:13:32.399
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:13:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2672" for this suite. 04/17/23 22:13:32.414
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:13:32.419
Apr 17 22:13:32.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename taint-multiple-pods 04/17/23 22:13:32.42
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:13:32.432
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:13:32.434
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Apr 17 22:13:32.435: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 17 22:14:32.482: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Apr 17 22:14:32.485: INFO: Starting informer...
STEP: Starting pods... 04/17/23 22:14:32.485
Apr 17 22:14:32.700: INFO: Pod1 is running on ip-10-0-74-52.us-west-2.compute.internal. Tainting Node
Apr 17 22:14:32.908: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-150" to be "running"
Apr 17 22:14:32.911: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204761ms
Apr 17 22:14:34.914: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005610471s
Apr 17 22:14:34.914: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Apr 17 22:14:34.914: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-150" to be "running"
Apr 17 22:14:34.916: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.358135ms
Apr 17 22:14:34.916: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Apr 17 22:14:34.916: INFO: Pod2 is running on ip-10-0-74-52.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 04/17/23 22:14:34.916
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/17/23 22:14:34.929
STEP: Waiting for Pod1 and Pod2 to be deleted 04/17/23 22:14:34.932
Apr 17 22:14:40.772: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Apr 17 22:15:00.807: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/17/23 22:15:00.824
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:00.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-150" for this suite. 04/17/23 22:15:00.834
------------------------------
• [SLOW TEST] [88.426 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:13:32.419
    Apr 17 22:13:32.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename taint-multiple-pods 04/17/23 22:13:32.42
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:13:32.432
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:13:32.434
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Apr 17 22:13:32.435: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 17 22:14:32.482: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Apr 17 22:14:32.485: INFO: Starting informer...
    STEP: Starting pods... 04/17/23 22:14:32.485
    Apr 17 22:14:32.700: INFO: Pod1 is running on ip-10-0-74-52.us-west-2.compute.internal. Tainting Node
    Apr 17 22:14:32.908: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-150" to be "running"
    Apr 17 22:14:32.911: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204761ms
    Apr 17 22:14:34.914: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005610471s
    Apr 17 22:14:34.914: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Apr 17 22:14:34.914: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-150" to be "running"
    Apr 17 22:14:34.916: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.358135ms
    Apr 17 22:14:34.916: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Apr 17 22:14:34.916: INFO: Pod2 is running on ip-10-0-74-52.us-west-2.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 04/17/23 22:14:34.916
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/17/23 22:14:34.929
    STEP: Waiting for Pod1 and Pod2 to be deleted 04/17/23 22:14:34.932
    Apr 17 22:14:40.772: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Apr 17 22:15:00.807: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/17/23 22:15:00.824
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:00.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-150" for this suite. 04/17/23 22:15:00.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:00.845
Apr 17 22:15:00.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename dns 04/17/23 22:15:00.846
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:00.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:00.867
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 04/17/23 22:15:00.868
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3778.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3778.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 04/17/23 22:15:00.872
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3778.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3778.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 04/17/23 22:15:00.872
STEP: creating a pod to probe DNS 04/17/23 22:15:00.872
STEP: submitting the pod to kubernetes 04/17/23 22:15:00.872
Apr 17 22:15:00.880: INFO: Waiting up to 15m0s for pod "dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9" in namespace "dns-3778" to be "running"
Apr 17 22:15:00.882: INFO: Pod "dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134468ms
Apr 17 22:15:02.885: INFO: Pod "dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005133902s
Apr 17 22:15:02.885: INFO: Pod "dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9" satisfied condition "running"
STEP: retrieving the pod 04/17/23 22:15:02.885
STEP: looking for the results for each expected name from probers 04/17/23 22:15:02.888
Apr 17 22:15:02.898: INFO: DNS probes using dns-3778/dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9 succeeded

STEP: deleting the pod 04/17/23 22:15:02.898
STEP: deleting the test headless service 04/17/23 22:15:02.982
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:02.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3778" for this suite. 04/17/23 22:15:03.003
------------------------------
• [2.163 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:00.845
    Apr 17 22:15:00.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename dns 04/17/23 22:15:00.846
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:00.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:00.867
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 04/17/23 22:15:00.868
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3778.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3778.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     04/17/23 22:15:00.872
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3778.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3778.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     04/17/23 22:15:00.872
    STEP: creating a pod to probe DNS 04/17/23 22:15:00.872
    STEP: submitting the pod to kubernetes 04/17/23 22:15:00.872
    Apr 17 22:15:00.880: INFO: Waiting up to 15m0s for pod "dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9" in namespace "dns-3778" to be "running"
    Apr 17 22:15:00.882: INFO: Pod "dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134468ms
    Apr 17 22:15:02.885: INFO: Pod "dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9": Phase="Running", Reason="", readiness=true. Elapsed: 2.005133902s
    Apr 17 22:15:02.885: INFO: Pod "dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9" satisfied condition "running"
    STEP: retrieving the pod 04/17/23 22:15:02.885
    STEP: looking for the results for each expected name from probers 04/17/23 22:15:02.888
    Apr 17 22:15:02.898: INFO: DNS probes using dns-3778/dns-test-f4fb8654-58dc-4e19-9d13-d73ce15964c9 succeeded

    STEP: deleting the pod 04/17/23 22:15:02.898
    STEP: deleting the test headless service 04/17/23 22:15:02.982
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:02.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3778" for this suite. 04/17/23 22:15:03.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:03.009
Apr 17 22:15:03.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename security-context 04/17/23 22:15:03.01
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:03.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:03.023
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/17/23 22:15:03.025
Apr 17 22:15:03.033: INFO: Waiting up to 5m0s for pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170" in namespace "security-context-4283" to be "Succeeded or Failed"
Apr 17 22:15:03.036: INFO: Pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170": Phase="Pending", Reason="", readiness=false. Elapsed: 3.554206ms
Apr 17 22:15:05.040: INFO: Pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170": Phase="Running", Reason="", readiness=false. Elapsed: 2.007599837s
Apr 17 22:15:07.041: INFO: Pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008487714s
STEP: Saw pod success 04/17/23 22:15:07.041
Apr 17 22:15:07.041: INFO: Pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170" satisfied condition "Succeeded or Failed"
Apr 17 22:15:07.044: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod security-context-f16cff37-0257-42a5-9467-8ec15e9f0170 container test-container: <nil>
STEP: delete the pod 04/17/23 22:15:07.055
Apr 17 22:15:07.066: INFO: Waiting for pod security-context-f16cff37-0257-42a5-9467-8ec15e9f0170 to disappear
Apr 17 22:15:07.068: INFO: Pod security-context-f16cff37-0257-42a5-9467-8ec15e9f0170 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:07.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-4283" for this suite. 04/17/23 22:15:07.073
------------------------------
• [4.069 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:03.009
    Apr 17 22:15:03.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename security-context 04/17/23 22:15:03.01
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:03.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:03.023
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 04/17/23 22:15:03.025
    Apr 17 22:15:03.033: INFO: Waiting up to 5m0s for pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170" in namespace "security-context-4283" to be "Succeeded or Failed"
    Apr 17 22:15:03.036: INFO: Pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170": Phase="Pending", Reason="", readiness=false. Elapsed: 3.554206ms
    Apr 17 22:15:05.040: INFO: Pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170": Phase="Running", Reason="", readiness=false. Elapsed: 2.007599837s
    Apr 17 22:15:07.041: INFO: Pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008487714s
    STEP: Saw pod success 04/17/23 22:15:07.041
    Apr 17 22:15:07.041: INFO: Pod "security-context-f16cff37-0257-42a5-9467-8ec15e9f0170" satisfied condition "Succeeded or Failed"
    Apr 17 22:15:07.044: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod security-context-f16cff37-0257-42a5-9467-8ec15e9f0170 container test-container: <nil>
    STEP: delete the pod 04/17/23 22:15:07.055
    Apr 17 22:15:07.066: INFO: Waiting for pod security-context-f16cff37-0257-42a5-9467-8ec15e9f0170 to disappear
    Apr 17 22:15:07.068: INFO: Pod security-context-f16cff37-0257-42a5-9467-8ec15e9f0170 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:07.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-4283" for this suite. 04/17/23 22:15:07.073
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:07.078
Apr 17 22:15:07.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 22:15:07.078
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:07.09
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:07.091
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-f08f9e59-c3f2-44c5-b2de-338b71e6f759 04/17/23 22:15:07.093
STEP: Creating a pod to test consume secrets 04/17/23 22:15:07.096
Apr 17 22:15:07.103: INFO: Waiting up to 5m0s for pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7" in namespace "secrets-7223" to be "Succeeded or Failed"
Apr 17 22:15:07.105: INFO: Pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.254922ms
Apr 17 22:15:09.109: INFO: Pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006323069s
Apr 17 22:15:11.108: INFO: Pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005388567s
STEP: Saw pod success 04/17/23 22:15:11.109
Apr 17 22:15:11.109: INFO: Pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7" satisfied condition "Succeeded or Failed"
Apr 17 22:15:11.111: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7 container secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:15:11.124
Apr 17 22:15:11.135: INFO: Waiting for pod pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7 to disappear
Apr 17 22:15:11.138: INFO: Pod pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:11.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7223" for this suite. 04/17/23 22:15:11.142
------------------------------
• [4.070 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:07.078
    Apr 17 22:15:07.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 22:15:07.078
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:07.09
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:07.091
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-f08f9e59-c3f2-44c5-b2de-338b71e6f759 04/17/23 22:15:07.093
    STEP: Creating a pod to test consume secrets 04/17/23 22:15:07.096
    Apr 17 22:15:07.103: INFO: Waiting up to 5m0s for pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7" in namespace "secrets-7223" to be "Succeeded or Failed"
    Apr 17 22:15:07.105: INFO: Pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.254922ms
    Apr 17 22:15:09.109: INFO: Pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006323069s
    Apr 17 22:15:11.108: INFO: Pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005388567s
    STEP: Saw pod success 04/17/23 22:15:11.109
    Apr 17 22:15:11.109: INFO: Pod "pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7" satisfied condition "Succeeded or Failed"
    Apr 17 22:15:11.111: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7 container secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:15:11.124
    Apr 17 22:15:11.135: INFO: Waiting for pod pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7 to disappear
    Apr 17 22:15:11.138: INFO: Pod pod-secrets-b2334b0a-dfaf-4dce-a230-3effebcde5f7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:11.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7223" for this suite. 04/17/23 22:15:11.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:11.149
Apr 17 22:15:11.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 22:15:11.149
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:11.161
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:11.163
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-82 04/17/23 22:15:11.164
STEP: creating service affinity-nodeport in namespace services-82 04/17/23 22:15:11.165
STEP: creating replication controller affinity-nodeport in namespace services-82 04/17/23 22:15:11.183
I0417 22:15:11.189392      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-82, replica count: 3
I0417 22:15:14.240254      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 22:15:14.249: INFO: Creating new exec pod
Apr 17 22:15:14.254: INFO: Waiting up to 5m0s for pod "execpod-affinityc8bwz" in namespace "services-82" to be "running"
Apr 17 22:15:14.257: INFO: Pod "execpod-affinityc8bwz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.87668ms
Apr 17 22:15:16.261: INFO: Pod "execpod-affinityc8bwz": Phase="Running", Reason="", readiness=true. Elapsed: 2.007196932s
Apr 17 22:15:16.261: INFO: Pod "execpod-affinityc8bwz" satisfied condition "running"
Apr 17 22:15:17.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Apr 17 22:15:17.419: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Apr 17 22:15:17.419: INFO: stdout: ""
Apr 17 22:15:17.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c nc -v -z -w 2 10.102.150.147 80'
Apr 17 22:15:17.537: INFO: stderr: "+ nc -v -z -w 2 10.102.150.147 80\nConnection to 10.102.150.147 80 port [tcp/http] succeeded!\n"
Apr 17 22:15:17.537: INFO: stdout: ""
Apr 17 22:15:17.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c nc -v -z -w 2 10.0.64.189 31469'
Apr 17 22:15:17.670: INFO: stderr: "+ nc -v -z -w 2 10.0.64.189 31469\nConnection to 10.0.64.189 31469 port [tcp/*] succeeded!\n"
Apr 17 22:15:17.670: INFO: stdout: ""
Apr 17 22:15:17.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c nc -v -z -w 2 10.0.93.18 31469'
Apr 17 22:15:17.813: INFO: stderr: "+ nc -v -z -w 2 10.0.93.18 31469\nConnection to 10.0.93.18 31469 port [tcp/*] succeeded!\n"
Apr 17 22:15:17.813: INFO: stdout: ""
Apr 17 22:15:17.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.106.231:31469/ ; done'
Apr 17 22:15:17.995: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n"
Apr 17 22:15:17.995: INFO: stdout: "\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7"
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
Apr 17 22:15:17.995: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-82, will wait for the garbage collector to delete the pods 04/17/23 22:15:18.008
Apr 17 22:15:18.066: INFO: Deleting ReplicationController affinity-nodeport took: 4.671568ms
Apr 17 22:15:18.167: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.683575ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:20.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-82" for this suite. 04/17/23 22:15:20.094
------------------------------
• [SLOW TEST] [8.950 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:11.149
    Apr 17 22:15:11.149: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 22:15:11.149
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:11.161
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:11.163
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-82 04/17/23 22:15:11.164
    STEP: creating service affinity-nodeport in namespace services-82 04/17/23 22:15:11.165
    STEP: creating replication controller affinity-nodeport in namespace services-82 04/17/23 22:15:11.183
    I0417 22:15:11.189392      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-82, replica count: 3
    I0417 22:15:14.240254      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 17 22:15:14.249: INFO: Creating new exec pod
    Apr 17 22:15:14.254: INFO: Waiting up to 5m0s for pod "execpod-affinityc8bwz" in namespace "services-82" to be "running"
    Apr 17 22:15:14.257: INFO: Pod "execpod-affinityc8bwz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.87668ms
    Apr 17 22:15:16.261: INFO: Pod "execpod-affinityc8bwz": Phase="Running", Reason="", readiness=true. Elapsed: 2.007196932s
    Apr 17 22:15:16.261: INFO: Pod "execpod-affinityc8bwz" satisfied condition "running"
    Apr 17 22:15:17.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Apr 17 22:15:17.419: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Apr 17 22:15:17.419: INFO: stdout: ""
    Apr 17 22:15:17.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c nc -v -z -w 2 10.102.150.147 80'
    Apr 17 22:15:17.537: INFO: stderr: "+ nc -v -z -w 2 10.102.150.147 80\nConnection to 10.102.150.147 80 port [tcp/http] succeeded!\n"
    Apr 17 22:15:17.537: INFO: stdout: ""
    Apr 17 22:15:17.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c nc -v -z -w 2 10.0.64.189 31469'
    Apr 17 22:15:17.670: INFO: stderr: "+ nc -v -z -w 2 10.0.64.189 31469\nConnection to 10.0.64.189 31469 port [tcp/*] succeeded!\n"
    Apr 17 22:15:17.670: INFO: stdout: ""
    Apr 17 22:15:17.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c nc -v -z -w 2 10.0.93.18 31469'
    Apr 17 22:15:17.813: INFO: stderr: "+ nc -v -z -w 2 10.0.93.18 31469\nConnection to 10.0.93.18 31469 port [tcp/*] succeeded!\n"
    Apr 17 22:15:17.813: INFO: stdout: ""
    Apr 17 22:15:17.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-82 exec execpod-affinityc8bwz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.106.231:31469/ ; done'
    Apr 17 22:15:17.995: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:31469/\n"
    Apr 17 22:15:17.995: INFO: stdout: "\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7\naffinity-nodeport-7njq7"
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Received response from host: affinity-nodeport-7njq7
    Apr 17 22:15:17.995: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-82, will wait for the garbage collector to delete the pods 04/17/23 22:15:18.008
    Apr 17 22:15:18.066: INFO: Deleting ReplicationController affinity-nodeport took: 4.671568ms
    Apr 17 22:15:18.167: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.683575ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:20.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-82" for this suite. 04/17/23 22:15:20.094
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:20.099
Apr 17 22:15:20.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-runtime 04/17/23 22:15:20.1
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:20.111
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:20.113
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 04/17/23 22:15:20.115
STEP: wait for the container to reach Succeeded 04/17/23 22:15:20.121
STEP: get the container status 04/17/23 22:15:24.138
STEP: the container should be terminated 04/17/23 22:15:24.14
STEP: the termination message should be set 04/17/23 22:15:24.14
Apr 17 22:15:24.140: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 04/17/23 22:15:24.14
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:24.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-3154" for this suite. 04/17/23 22:15:24.159
------------------------------
• [4.064 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:20.099
    Apr 17 22:15:20.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-runtime 04/17/23 22:15:20.1
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:20.111
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:20.113
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 04/17/23 22:15:20.115
    STEP: wait for the container to reach Succeeded 04/17/23 22:15:20.121
    STEP: get the container status 04/17/23 22:15:24.138
    STEP: the container should be terminated 04/17/23 22:15:24.14
    STEP: the termination message should be set 04/17/23 22:15:24.14
    Apr 17 22:15:24.140: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 04/17/23 22:15:24.14
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:24.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-3154" for this suite. 04/17/23 22:15:24.159
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:24.164
Apr 17 22:15:24.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:15:24.164
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:24.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:24.177
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-e7fa34c2-2e31-4241-8ae6-fcb2674c6cff 04/17/23 22:15:24.179
STEP: Creating a pod to test consume configMaps 04/17/23 22:15:24.182
Apr 17 22:15:24.189: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468" in namespace "projected-4857" to be "Succeeded or Failed"
Apr 17 22:15:24.191: INFO: Pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362863ms
Apr 17 22:15:26.195: INFO: Pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006180076s
Apr 17 22:15:28.194: INFO: Pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005340141s
STEP: Saw pod success 04/17/23 22:15:28.194
Apr 17 22:15:28.194: INFO: Pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468" satisfied condition "Succeeded or Failed"
Apr 17 22:15:28.197: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:15:28.201
Apr 17 22:15:28.212: INFO: Waiting for pod pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468 to disappear
Apr 17 22:15:28.215: INFO: Pod pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:28.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4857" for this suite. 04/17/23 22:15:28.219
------------------------------
• [4.060 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:24.164
    Apr 17 22:15:24.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:15:24.164
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:24.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:24.177
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-e7fa34c2-2e31-4241-8ae6-fcb2674c6cff 04/17/23 22:15:24.179
    STEP: Creating a pod to test consume configMaps 04/17/23 22:15:24.182
    Apr 17 22:15:24.189: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468" in namespace "projected-4857" to be "Succeeded or Failed"
    Apr 17 22:15:24.191: INFO: Pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.362863ms
    Apr 17 22:15:26.195: INFO: Pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006180076s
    Apr 17 22:15:28.194: INFO: Pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005340141s
    STEP: Saw pod success 04/17/23 22:15:28.194
    Apr 17 22:15:28.194: INFO: Pod "pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468" satisfied condition "Succeeded or Failed"
    Apr 17 22:15:28.197: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:15:28.201
    Apr 17 22:15:28.212: INFO: Waiting for pod pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468 to disappear
    Apr 17 22:15:28.215: INFO: Pod pod-projected-configmaps-911bf6cc-43e4-4375-b8fc-efe2d9680468 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:28.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4857" for this suite. 04/17/23 22:15:28.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:28.224
Apr 17 22:15:28.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:15:28.225
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:28.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:28.237
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Apr 17 22:15:28.241: INFO: Got root ca configmap in namespace "svcaccounts-1852"
Apr 17 22:15:28.245: INFO: Deleted root ca configmap in namespace "svcaccounts-1852"
STEP: waiting for a new root ca configmap created 04/17/23 22:15:28.745
Apr 17 22:15:28.748: INFO: Recreated root ca configmap in namespace "svcaccounts-1852"
Apr 17 22:15:28.752: INFO: Updated root ca configmap in namespace "svcaccounts-1852"
STEP: waiting for the root ca configmap reconciled 04/17/23 22:15:29.253
Apr 17 22:15:29.255: INFO: Reconciled root ca configmap in namespace "svcaccounts-1852"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:29.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1852" for this suite. 04/17/23 22:15:29.26
------------------------------
• [1.041 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:28.224
    Apr 17 22:15:28.224: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:15:28.225
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:28.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:28.237
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Apr 17 22:15:28.241: INFO: Got root ca configmap in namespace "svcaccounts-1852"
    Apr 17 22:15:28.245: INFO: Deleted root ca configmap in namespace "svcaccounts-1852"
    STEP: waiting for a new root ca configmap created 04/17/23 22:15:28.745
    Apr 17 22:15:28.748: INFO: Recreated root ca configmap in namespace "svcaccounts-1852"
    Apr 17 22:15:28.752: INFO: Updated root ca configmap in namespace "svcaccounts-1852"
    STEP: waiting for the root ca configmap reconciled 04/17/23 22:15:29.253
    Apr 17 22:15:29.255: INFO: Reconciled root ca configmap in namespace "svcaccounts-1852"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:29.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1852" for this suite. 04/17/23 22:15:29.26
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:29.265
Apr 17 22:15:29.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:15:29.266
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:29.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:29.278
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:15:29.28
Apr 17 22:15:29.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98" in namespace "downward-api-6425" to be "Succeeded or Failed"
Apr 17 22:15:29.292: INFO: Pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98": Phase="Pending", Reason="", readiness=false. Elapsed: 5.10394ms
Apr 17 22:15:31.296: INFO: Pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98": Phase="Running", Reason="", readiness=false. Elapsed: 2.008706714s
Apr 17 22:15:33.296: INFO: Pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008852624s
STEP: Saw pod success 04/17/23 22:15:33.296
Apr 17 22:15:33.296: INFO: Pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98" satisfied condition "Succeeded or Failed"
Apr 17 22:15:33.298: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98 container client-container: <nil>
STEP: delete the pod 04/17/23 22:15:33.303
Apr 17 22:15:33.316: INFO: Waiting for pod downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98 to disappear
Apr 17 22:15:33.318: INFO: Pod downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:33.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6425" for this suite. 04/17/23 22:15:33.323
------------------------------
• [4.064 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:29.265
    Apr 17 22:15:29.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:15:29.266
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:29.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:29.278
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:15:29.28
    Apr 17 22:15:29.287: INFO: Waiting up to 5m0s for pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98" in namespace "downward-api-6425" to be "Succeeded or Failed"
    Apr 17 22:15:29.292: INFO: Pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98": Phase="Pending", Reason="", readiness=false. Elapsed: 5.10394ms
    Apr 17 22:15:31.296: INFO: Pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98": Phase="Running", Reason="", readiness=false. Elapsed: 2.008706714s
    Apr 17 22:15:33.296: INFO: Pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008852624s
    STEP: Saw pod success 04/17/23 22:15:33.296
    Apr 17 22:15:33.296: INFO: Pod "downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98" satisfied condition "Succeeded or Failed"
    Apr 17 22:15:33.298: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98 container client-container: <nil>
    STEP: delete the pod 04/17/23 22:15:33.303
    Apr 17 22:15:33.316: INFO: Waiting for pod downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98 to disappear
    Apr 17 22:15:33.318: INFO: Pod downwardapi-volume-35185f83-954b-4f5e-a2b1-e43464c25a98 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:33.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6425" for this suite. 04/17/23 22:15:33.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:33.329
Apr 17 22:15:33.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubelet-test 04/17/23 22:15:33.33
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:33.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:33.345
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Apr 17 22:15:33.356: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581" in namespace "kubelet-test-7123" to be "running and ready"
Apr 17 22:15:33.358: INFO: Pod "busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238962ms
Apr 17 22:15:33.358: INFO: The phase of Pod busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:15:35.362: INFO: Pod "busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581": Phase="Running", Reason="", readiness=true. Elapsed: 2.006197371s
Apr 17 22:15:35.362: INFO: The phase of Pod busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581 is Running (Ready = true)
Apr 17 22:15:35.362: INFO: Pod "busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:35.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-7123" for this suite. 04/17/23 22:15:35.378
------------------------------
• [2.054 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:33.329
    Apr 17 22:15:33.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubelet-test 04/17/23 22:15:33.33
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:33.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:33.345
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Apr 17 22:15:33.356: INFO: Waiting up to 5m0s for pod "busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581" in namespace "kubelet-test-7123" to be "running and ready"
    Apr 17 22:15:33.358: INFO: Pod "busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238962ms
    Apr 17 22:15:33.358: INFO: The phase of Pod busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:15:35.362: INFO: Pod "busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581": Phase="Running", Reason="", readiness=true. Elapsed: 2.006197371s
    Apr 17 22:15:35.362: INFO: The phase of Pod busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581 is Running (Ready = true)
    Apr 17 22:15:35.362: INFO: Pod "busybox-scheduling-3e52d9e4-dbf5-42e7-b33f-6f0cc5d82581" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:35.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-7123" for this suite. 04/17/23 22:15:35.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:35.384
Apr 17 22:15:35.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 22:15:35.385
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:35.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:35.398
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Apr 17 22:15:35.407: INFO: Waiting up to 5m0s for pod "server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b" in namespace "pods-4376" to be "running and ready"
Apr 17 22:15:35.409: INFO: Pod "server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133011ms
Apr 17 22:15:35.409: INFO: The phase of Pod server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:15:37.413: INFO: Pod "server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006156849s
Apr 17 22:15:37.413: INFO: The phase of Pod server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b is Running (Ready = true)
Apr 17 22:15:37.413: INFO: Pod "server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b" satisfied condition "running and ready"
Apr 17 22:15:37.434: INFO: Waiting up to 5m0s for pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962" in namespace "pods-4376" to be "Succeeded or Failed"
Apr 17 22:15:37.436: INFO: Pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777182ms
Apr 17 22:15:39.440: INFO: Pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006078248s
Apr 17 22:15:41.441: INFO: Pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007163142s
STEP: Saw pod success 04/17/23 22:15:41.441
Apr 17 22:15:41.441: INFO: Pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962" satisfied condition "Succeeded or Failed"
Apr 17 22:15:41.443: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962 container env3cont: <nil>
STEP: delete the pod 04/17/23 22:15:41.449
Apr 17 22:15:41.460: INFO: Waiting for pod client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962 to disappear
Apr 17 22:15:41.463: INFO: Pod client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:41.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4376" for this suite. 04/17/23 22:15:41.467
------------------------------
• [SLOW TEST] [6.088 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:35.384
    Apr 17 22:15:35.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 22:15:35.385
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:35.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:35.398
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Apr 17 22:15:35.407: INFO: Waiting up to 5m0s for pod "server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b" in namespace "pods-4376" to be "running and ready"
    Apr 17 22:15:35.409: INFO: Pod "server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.133011ms
    Apr 17 22:15:35.409: INFO: The phase of Pod server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:15:37.413: INFO: Pod "server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006156849s
    Apr 17 22:15:37.413: INFO: The phase of Pod server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b is Running (Ready = true)
    Apr 17 22:15:37.413: INFO: Pod "server-envvars-490d92f8-2875-4e03-852b-4b861c64ac0b" satisfied condition "running and ready"
    Apr 17 22:15:37.434: INFO: Waiting up to 5m0s for pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962" in namespace "pods-4376" to be "Succeeded or Failed"
    Apr 17 22:15:37.436: INFO: Pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777182ms
    Apr 17 22:15:39.440: INFO: Pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006078248s
    Apr 17 22:15:41.441: INFO: Pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007163142s
    STEP: Saw pod success 04/17/23 22:15:41.441
    Apr 17 22:15:41.441: INFO: Pod "client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962" satisfied condition "Succeeded or Failed"
    Apr 17 22:15:41.443: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962 container env3cont: <nil>
    STEP: delete the pod 04/17/23 22:15:41.449
    Apr 17 22:15:41.460: INFO: Waiting for pod client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962 to disappear
    Apr 17 22:15:41.463: INFO: Pod client-envvars-4f63823c-25e8-4af1-b4ce-50086fb00962 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:41.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4376" for this suite. 04/17/23 22:15:41.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:41.473
Apr 17 22:15:41.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:15:41.473
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:41.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:41.484
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:15:41.497
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:15:41.893
STEP: Deploying the webhook pod 04/17/23 22:15:41.899
STEP: Wait for the deployment to be ready 04/17/23 22:15:41.907
Apr 17 22:15:41.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:15:43.922
STEP: Verifying the service has paired with the endpoint 04/17/23 22:15:43.934
Apr 17 22:15:44.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 04/17/23 22:15:44.937
STEP: create a pod 04/17/23 22:15:44.95
Apr 17 22:15:44.958: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4515" to be "running"
Apr 17 22:15:44.961: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.273248ms
Apr 17 22:15:46.965: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006895177s
Apr 17 22:15:46.965: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 04/17/23 22:15:46.965
Apr 17 22:15:46.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=webhook-4515 attach --namespace=webhook-4515 to-be-attached-pod -i -c=container1'
Apr 17 22:15:47.034: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:47.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4515" for this suite. 04/17/23 22:15:47.082
STEP: Destroying namespace "webhook-4515-markers" for this suite. 04/17/23 22:15:47.09
------------------------------
• [SLOW TEST] [5.623 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:41.473
    Apr 17 22:15:41.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:15:41.473
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:41.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:41.484
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:15:41.497
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:15:41.893
    STEP: Deploying the webhook pod 04/17/23 22:15:41.899
    STEP: Wait for the deployment to be ready 04/17/23 22:15:41.907
    Apr 17 22:15:41.915: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:15:43.922
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:15:43.934
    Apr 17 22:15:44.934: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 04/17/23 22:15:44.937
    STEP: create a pod 04/17/23 22:15:44.95
    Apr 17 22:15:44.958: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-4515" to be "running"
    Apr 17 22:15:44.961: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.273248ms
    Apr 17 22:15:46.965: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006895177s
    Apr 17 22:15:46.965: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 04/17/23 22:15:46.965
    Apr 17 22:15:46.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=webhook-4515 attach --namespace=webhook-4515 to-be-attached-pod -i -c=container1'
    Apr 17 22:15:47.034: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:47.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4515" for this suite. 04/17/23 22:15:47.082
    STEP: Destroying namespace "webhook-4515-markers" for this suite. 04/17/23 22:15:47.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:47.097
Apr 17 22:15:47.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:15:47.098
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:47.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:47.11
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 04/17/23 22:15:47.112
Apr 17 22:15:47.118: INFO: Waiting up to 5m0s for pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4" in namespace "downward-api-4886" to be "Succeeded or Failed"
Apr 17 22:15:47.120: INFO: Pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.251486ms
Apr 17 22:15:49.123: INFO: Pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005824704s
Apr 17 22:15:51.124: INFO: Pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006187258s
STEP: Saw pod success 04/17/23 22:15:51.124
Apr 17 22:15:51.124: INFO: Pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4" satisfied condition "Succeeded or Failed"
Apr 17 22:15:51.126: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4 container dapi-container: <nil>
STEP: delete the pod 04/17/23 22:15:51.132
Apr 17 22:15:51.144: INFO: Waiting for pod downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4 to disappear
Apr 17 22:15:51.146: INFO: Pod downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:51.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4886" for this suite. 04/17/23 22:15:51.15
------------------------------
• [4.057 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:47.097
    Apr 17 22:15:47.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:15:47.098
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:47.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:47.11
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 04/17/23 22:15:47.112
    Apr 17 22:15:47.118: INFO: Waiting up to 5m0s for pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4" in namespace "downward-api-4886" to be "Succeeded or Failed"
    Apr 17 22:15:47.120: INFO: Pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.251486ms
    Apr 17 22:15:49.123: INFO: Pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005824704s
    Apr 17 22:15:51.124: INFO: Pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006187258s
    STEP: Saw pod success 04/17/23 22:15:51.124
    Apr 17 22:15:51.124: INFO: Pod "downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4" satisfied condition "Succeeded or Failed"
    Apr 17 22:15:51.126: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4 container dapi-container: <nil>
    STEP: delete the pod 04/17/23 22:15:51.132
    Apr 17 22:15:51.144: INFO: Waiting for pod downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4 to disappear
    Apr 17 22:15:51.146: INFO: Pod downward-api-8ae7a18a-8660-4a08-8ed1-d26b58fa54f4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:51.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4886" for this suite. 04/17/23 22:15:51.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:51.156
Apr 17 22:15:51.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:15:51.157
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:51.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:51.17
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:15:51.18
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:15:51.806
STEP: Deploying the webhook pod 04/17/23 22:15:51.81
STEP: Wait for the deployment to be ready 04/17/23 22:15:51.819
Apr 17 22:15:51.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:15:53.833
STEP: Verifying the service has paired with the endpoint 04/17/23 22:15:53.847
Apr 17 22:15:54.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Apr 17 22:15:54.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2879-crds.webhook.example.com via the AdmissionRegistration API 04/17/23 22:15:55.359
STEP: Creating a custom resource that should be mutated by the webhook 04/17/23 22:15:55.386
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:57.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4041" for this suite. 04/17/23 22:15:58.012
STEP: Destroying namespace "webhook-4041-markers" for this suite. 04/17/23 22:15:58.019
------------------------------
• [SLOW TEST] [6.878 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:51.156
    Apr 17 22:15:51.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:15:51.157
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:51.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:51.17
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:15:51.18
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:15:51.806
    STEP: Deploying the webhook pod 04/17/23 22:15:51.81
    STEP: Wait for the deployment to be ready 04/17/23 22:15:51.819
    Apr 17 22:15:51.826: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:15:53.833
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:15:53.847
    Apr 17 22:15:54.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Apr 17 22:15:54.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2879-crds.webhook.example.com via the AdmissionRegistration API 04/17/23 22:15:55.359
    STEP: Creating a custom resource that should be mutated by the webhook 04/17/23 22:15:55.386
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:57.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4041" for this suite. 04/17/23 22:15:58.012
    STEP: Destroying namespace "webhook-4041-markers" for this suite. 04/17/23 22:15:58.019
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:58.034
Apr 17 22:15:58.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename events 04/17/23 22:15:58.035
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:58.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:58.054
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 04/17/23 22:15:58.056
STEP: listing all events in all namespaces 04/17/23 22:15:58.06
STEP: patching the test event 04/17/23 22:15:58.065
STEP: fetching the test event 04/17/23 22:15:58.071
STEP: updating the test event 04/17/23 22:15:58.076
STEP: getting the test event 04/17/23 22:15:58.091
STEP: deleting the test event 04/17/23 22:15:58.093
STEP: listing all events in all namespaces 04/17/23 22:15:58.106
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:58.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-546" for this suite. 04/17/23 22:15:58.115
------------------------------
• [0.088 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:58.034
    Apr 17 22:15:58.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename events 04/17/23 22:15:58.035
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:58.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:58.054
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 04/17/23 22:15:58.056
    STEP: listing all events in all namespaces 04/17/23 22:15:58.06
    STEP: patching the test event 04/17/23 22:15:58.065
    STEP: fetching the test event 04/17/23 22:15:58.071
    STEP: updating the test event 04/17/23 22:15:58.076
    STEP: getting the test event 04/17/23 22:15:58.091
    STEP: deleting the test event 04/17/23 22:15:58.093
    STEP: listing all events in all namespaces 04/17/23 22:15:58.106
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:58.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-546" for this suite. 04/17/23 22:15:58.115
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:58.122
Apr 17 22:15:58.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 22:15:58.123
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:58.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:58.139
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Apr 17 22:15:58.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:15:59.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6752" for this suite. 04/17/23 22:15:59.165
------------------------------
• [1.048 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:58.122
    Apr 17 22:15:58.122: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 22:15:58.123
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:58.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:58.139
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Apr 17 22:15:58.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:15:59.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6752" for this suite. 04/17/23 22:15:59.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:15:59.173
Apr 17 22:15:59.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 22:15:59.173
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:59.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:59.188
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-6946 04/17/23 22:15:59.189
STEP: changing the ExternalName service to type=ClusterIP 04/17/23 22:15:59.193
STEP: creating replication controller externalname-service in namespace services-6946 04/17/23 22:15:59.31
I0417 22:15:59.316127      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6946, replica count: 2
I0417 22:16:02.366852      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 22:16:02.366: INFO: Creating new exec pod
Apr 17 22:16:02.372: INFO: Waiting up to 5m0s for pod "execpodqbpx9" in namespace "services-6946" to be "running"
Apr 17 22:16:02.374: INFO: Pod "execpodqbpx9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.621292ms
Apr 17 22:16:04.378: INFO: Pod "execpodqbpx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006277684s
Apr 17 22:16:04.378: INFO: Pod "execpodqbpx9" satisfied condition "running"
Apr 17 22:16:05.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6946 exec execpodqbpx9 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Apr 17 22:16:05.519: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Apr 17 22:16:05.519: INFO: stdout: ""
Apr 17 22:16:05.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6946 exec execpodqbpx9 -- /bin/sh -x -c nc -v -z -w 2 10.105.118.188 80'
Apr 17 22:16:05.650: INFO: stderr: "+ nc -v -z -w 2 10.105.118.188 80\nConnection to 10.105.118.188 80 port [tcp/http] succeeded!\n"
Apr 17 22:16:05.650: INFO: stdout: ""
Apr 17 22:16:05.650: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:05.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6946" for this suite. 04/17/23 22:16:05.675
------------------------------
• [SLOW TEST] [6.508 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:15:59.173
    Apr 17 22:15:59.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 22:15:59.173
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:15:59.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:15:59.188
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-6946 04/17/23 22:15:59.189
    STEP: changing the ExternalName service to type=ClusterIP 04/17/23 22:15:59.193
    STEP: creating replication controller externalname-service in namespace services-6946 04/17/23 22:15:59.31
    I0417 22:15:59.316127      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-6946, replica count: 2
    I0417 22:16:02.366852      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 17 22:16:02.366: INFO: Creating new exec pod
    Apr 17 22:16:02.372: INFO: Waiting up to 5m0s for pod "execpodqbpx9" in namespace "services-6946" to be "running"
    Apr 17 22:16:02.374: INFO: Pod "execpodqbpx9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.621292ms
    Apr 17 22:16:04.378: INFO: Pod "execpodqbpx9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006277684s
    Apr 17 22:16:04.378: INFO: Pod "execpodqbpx9" satisfied condition "running"
    Apr 17 22:16:05.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6946 exec execpodqbpx9 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Apr 17 22:16:05.519: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Apr 17 22:16:05.519: INFO: stdout: ""
    Apr 17 22:16:05.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6946 exec execpodqbpx9 -- /bin/sh -x -c nc -v -z -w 2 10.105.118.188 80'
    Apr 17 22:16:05.650: INFO: stderr: "+ nc -v -z -w 2 10.105.118.188 80\nConnection to 10.105.118.188 80 port [tcp/http] succeeded!\n"
    Apr 17 22:16:05.650: INFO: stdout: ""
    Apr 17 22:16:05.650: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:05.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6946" for this suite. 04/17/23 22:16:05.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:05.681
Apr 17 22:16:05.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:16:05.682
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:05.694
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:05.696
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-a5ad05e9-a609-40ed-b297-25c51be3e425 04/17/23 22:16:05.698
STEP: Creating a pod to test consume secrets 04/17/23 22:16:05.702
Apr 17 22:16:05.710: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8" in namespace "projected-1306" to be "Succeeded or Failed"
Apr 17 22:16:05.714: INFO: Pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.360759ms
Apr 17 22:16:07.717: INFO: Pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006840051s
Apr 17 22:16:09.717: INFO: Pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006351926s
STEP: Saw pod success 04/17/23 22:16:09.717
Apr 17 22:16:09.717: INFO: Pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8" satisfied condition "Succeeded or Failed"
Apr 17 22:16:09.719: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:16:09.724
Apr 17 22:16:09.736: INFO: Waiting for pod pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8 to disappear
Apr 17 22:16:09.738: INFO: Pod pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:09.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1306" for this suite. 04/17/23 22:16:09.742
------------------------------
• [4.066 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:05.681
    Apr 17 22:16:05.681: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:16:05.682
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:05.694
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:05.696
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-a5ad05e9-a609-40ed-b297-25c51be3e425 04/17/23 22:16:05.698
    STEP: Creating a pod to test consume secrets 04/17/23 22:16:05.702
    Apr 17 22:16:05.710: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8" in namespace "projected-1306" to be "Succeeded or Failed"
    Apr 17 22:16:05.714: INFO: Pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.360759ms
    Apr 17 22:16:07.717: INFO: Pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006840051s
    Apr 17 22:16:09.717: INFO: Pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006351926s
    STEP: Saw pod success 04/17/23 22:16:09.717
    Apr 17 22:16:09.717: INFO: Pod "pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8" satisfied condition "Succeeded or Failed"
    Apr 17 22:16:09.719: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:16:09.724
    Apr 17 22:16:09.736: INFO: Waiting for pod pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8 to disappear
    Apr 17 22:16:09.738: INFO: Pod pod-projected-secrets-97a8123e-0710-4f68-b6c9-1bca82a612c8 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:09.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1306" for this suite. 04/17/23 22:16:09.742
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:09.747
Apr 17 22:16:09.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename tables 04/17/23 22:16:09.748
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:09.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:09.763
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:09.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-6985" for this suite. 04/17/23 22:16:09.77
------------------------------
• [0.028 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:09.747
    Apr 17 22:16:09.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename tables 04/17/23 22:16:09.748
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:09.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:09.763
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:09.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-6985" for this suite. 04/17/23 22:16:09.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:09.776
Apr 17 22:16:09.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubelet-test 04/17/23 22:16:09.777
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:09.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:09.792
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:09.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4940" for this suite. 04/17/23 22:16:09.819
------------------------------
• [0.050 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:09.776
    Apr 17 22:16:09.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubelet-test 04/17/23 22:16:09.777
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:09.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:09.792
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:09.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4940" for this suite. 04/17/23 22:16:09.819
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:09.826
Apr 17 22:16:09.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:16:09.827
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:09.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:09.844
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:16:09.846
Apr 17 22:16:09.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e" in namespace "downward-api-5358" to be "Succeeded or Failed"
Apr 17 22:16:09.855: INFO: Pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.16931ms
Apr 17 22:16:11.858: INFO: Pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005767385s
Apr 17 22:16:13.859: INFO: Pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006253949s
STEP: Saw pod success 04/17/23 22:16:13.859
Apr 17 22:16:13.859: INFO: Pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e" satisfied condition "Succeeded or Failed"
Apr 17 22:16:13.861: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e container client-container: <nil>
STEP: delete the pod 04/17/23 22:16:13.866
Apr 17 22:16:13.878: INFO: Waiting for pod downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e to disappear
Apr 17 22:16:13.880: INFO: Pod downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:13.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5358" for this suite. 04/17/23 22:16:13.884
------------------------------
• [4.062 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:09.826
    Apr 17 22:16:09.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:16:09.827
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:09.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:09.844
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:16:09.846
    Apr 17 22:16:09.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e" in namespace "downward-api-5358" to be "Succeeded or Failed"
    Apr 17 22:16:09.855: INFO: Pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.16931ms
    Apr 17 22:16:11.858: INFO: Pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005767385s
    Apr 17 22:16:13.859: INFO: Pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006253949s
    STEP: Saw pod success 04/17/23 22:16:13.859
    Apr 17 22:16:13.859: INFO: Pod "downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e" satisfied condition "Succeeded or Failed"
    Apr 17 22:16:13.861: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e container client-container: <nil>
    STEP: delete the pod 04/17/23 22:16:13.866
    Apr 17 22:16:13.878: INFO: Waiting for pod downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e to disappear
    Apr 17 22:16:13.880: INFO: Pod downwardapi-volume-1d0a314b-76ce-4b3a-b617-8e65610d552e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:13.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5358" for this suite. 04/17/23 22:16:13.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:13.891
Apr 17 22:16:13.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 22:16:13.891
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:13.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:13.903
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-387b4605-d1a5-4849-aa6a-12cda6aa89d8 04/17/23 22:16:13.923
STEP: Creating a pod to test consume secrets 04/17/23 22:16:13.927
Apr 17 22:16:13.936: INFO: Waiting up to 5m0s for pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5" in namespace "secrets-8824" to be "Succeeded or Failed"
Apr 17 22:16:13.939: INFO: Pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.03457ms
Apr 17 22:16:15.943: INFO: Pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006730053s
Apr 17 22:16:17.943: INFO: Pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007359803s
STEP: Saw pod success 04/17/23 22:16:17.943
Apr 17 22:16:17.943: INFO: Pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5" satisfied condition "Succeeded or Failed"
Apr 17 22:16:17.946: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5 container secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:16:17.951
Apr 17 22:16:17.960: INFO: Waiting for pod pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5 to disappear
Apr 17 22:16:17.962: INFO: Pod pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:17.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8824" for this suite. 04/17/23 22:16:17.966
STEP: Destroying namespace "secret-namespace-5317" for this suite. 04/17/23 22:16:17.974
------------------------------
• [4.090 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:13.891
    Apr 17 22:16:13.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 22:16:13.891
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:13.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:13.903
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-387b4605-d1a5-4849-aa6a-12cda6aa89d8 04/17/23 22:16:13.923
    STEP: Creating a pod to test consume secrets 04/17/23 22:16:13.927
    Apr 17 22:16:13.936: INFO: Waiting up to 5m0s for pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5" in namespace "secrets-8824" to be "Succeeded or Failed"
    Apr 17 22:16:13.939: INFO: Pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.03457ms
    Apr 17 22:16:15.943: INFO: Pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006730053s
    Apr 17 22:16:17.943: INFO: Pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007359803s
    STEP: Saw pod success 04/17/23 22:16:17.943
    Apr 17 22:16:17.943: INFO: Pod "pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5" satisfied condition "Succeeded or Failed"
    Apr 17 22:16:17.946: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5 container secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:16:17.951
    Apr 17 22:16:17.960: INFO: Waiting for pod pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5 to disappear
    Apr 17 22:16:17.962: INFO: Pod pod-secrets-ae971bc0-8fb9-43ea-a2b5-f9a179c7b3e5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:17.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8824" for this suite. 04/17/23 22:16:17.966
    STEP: Destroying namespace "secret-namespace-5317" for this suite. 04/17/23 22:16:17.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:17.986
Apr 17 22:16:17.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 22:16:17.986
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:17.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:17.998
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 04/17/23 22:16:17.999
Apr 17 22:16:18.007: INFO: Waiting up to 5m0s for pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79" in namespace "emptydir-4121" to be "Succeeded or Failed"
Apr 17 22:16:18.009: INFO: Pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242312ms
Apr 17 22:16:20.012: INFO: Pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00525862s
Apr 17 22:16:22.013: INFO: Pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005933092s
STEP: Saw pod success 04/17/23 22:16:22.013
Apr 17 22:16:22.013: INFO: Pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79" satisfied condition "Succeeded or Failed"
Apr 17 22:16:22.017: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-b185853c-9227-45bc-acc9-8c895f4d9b79 container test-container: <nil>
STEP: delete the pod 04/17/23 22:16:22.022
Apr 17 22:16:22.033: INFO: Waiting for pod pod-b185853c-9227-45bc-acc9-8c895f4d9b79 to disappear
Apr 17 22:16:22.035: INFO: Pod pod-b185853c-9227-45bc-acc9-8c895f4d9b79 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:22.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4121" for this suite. 04/17/23 22:16:22.039
------------------------------
• [4.059 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:17.986
    Apr 17 22:16:17.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 22:16:17.986
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:17.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:17.998
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 04/17/23 22:16:17.999
    Apr 17 22:16:18.007: INFO: Waiting up to 5m0s for pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79" in namespace "emptydir-4121" to be "Succeeded or Failed"
    Apr 17 22:16:18.009: INFO: Pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242312ms
    Apr 17 22:16:20.012: INFO: Pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00525862s
    Apr 17 22:16:22.013: INFO: Pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005933092s
    STEP: Saw pod success 04/17/23 22:16:22.013
    Apr 17 22:16:22.013: INFO: Pod "pod-b185853c-9227-45bc-acc9-8c895f4d9b79" satisfied condition "Succeeded or Failed"
    Apr 17 22:16:22.017: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-b185853c-9227-45bc-acc9-8c895f4d9b79 container test-container: <nil>
    STEP: delete the pod 04/17/23 22:16:22.022
    Apr 17 22:16:22.033: INFO: Waiting for pod pod-b185853c-9227-45bc-acc9-8c895f4d9b79 to disappear
    Apr 17 22:16:22.035: INFO: Pod pod-b185853c-9227-45bc-acc9-8c895f4d9b79 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:22.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4121" for this suite. 04/17/23 22:16:22.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:22.045
Apr 17 22:16:22.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename deployment 04/17/23 22:16:22.046
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:22.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:22.059
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Apr 17 22:16:22.060: INFO: Creating simple deployment test-new-deployment
Apr 17 22:16:22.072: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 04/17/23 22:16:24.082
STEP: updating a scale subresource 04/17/23 22:16:24.084
STEP: verifying the deployment Spec.Replicas was modified 04/17/23 22:16:24.091
STEP: Patch a scale subresource 04/17/23 22:16:24.095
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 17 22:16:24.118: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-4155  35a15685-e2db-42bf-a4d6-774b2ec488f3 54890 3 2023-04-17 22:16:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-17 22:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:16:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c3bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-17 22:16:23 +0000 UTC,LastTransitionTime:2023-04-17 22:16:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-04-17 22:16:23 +0000 UTC,LastTransitionTime:2023-04-17 22:16:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 17 22:16:24.125: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4155  2475cac7-ed91-4f4e-a324-703c46c65294 54894 2 2023-04-17 22:16:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 35a15685-e2db-42bf-a4d6-774b2ec488f3 0xc00b29bf97 0xc00b29bf98}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:16:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35a15685-e2db-42bf-a4d6-774b2ec488f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:16:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006af2028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:16:24.128: INFO: Pod "test-new-deployment-7f5969cbc7-q7rjx" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-q7rjx test-new-deployment-7f5969cbc7- deployment-4155  daeacacc-61d0-478a-828c-e58745803bcd 54864 0 2023-04-17 22:16:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2587b71e572fd7c745d7fc4ede84f39b01c7531d9fd53a359f8882f07b730317 cni.projectcalico.org/podIP:192.168.213.63/32 cni.projectcalico.org/podIPs:192.168.213.63/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 2475cac7-ed91-4f4e-a324-703c46c65294 0xc006af2417 0xc006af2418}] [] [{calico Update v1 2023-04-17 22:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 22:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2475cac7-ed91-4f4e-a324-703c46c65294\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:16:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-52xcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52xcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.63,StartTime:2023-04-17 22:16:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:16:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2e6e25d3b3ef794ff800f09f236dc5f12a4604621ebf627c613eb337422255d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:16:24.129: INFO: Pod "test-new-deployment-7f5969cbc7-x9n5s" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-x9n5s test-new-deployment-7f5969cbc7- deployment-4155  5b989672-68e0-422c-b8ab-724b33a421b6 54896 0 2023-04-17 22:16:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 2475cac7-ed91-4f4e-a324-703c46c65294 0xc006af2600 0xc006af2601}] [] [{kube-controller-manager Update v1 2023-04-17 22:16:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2475cac7-ed91-4f4e-a324-703c46c65294\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:16:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6tn9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6tn9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:16:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:24.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4155" for this suite. 04/17/23 22:16:24.135
------------------------------
• [2.104 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:22.045
    Apr 17 22:16:22.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename deployment 04/17/23 22:16:22.046
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:22.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:22.059
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Apr 17 22:16:22.060: INFO: Creating simple deployment test-new-deployment
    Apr 17 22:16:22.072: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 04/17/23 22:16:24.082
    STEP: updating a scale subresource 04/17/23 22:16:24.084
    STEP: verifying the deployment Spec.Replicas was modified 04/17/23 22:16:24.091
    STEP: Patch a scale subresource 04/17/23 22:16:24.095
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 17 22:16:24.118: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-4155  35a15685-e2db-42bf-a4d6-774b2ec488f3 54890 3 2023-04-17 22:16:22 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-04-17 22:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:16:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0007c3bb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-17 22:16:23 +0000 UTC,LastTransitionTime:2023-04-17 22:16:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-04-17 22:16:23 +0000 UTC,LastTransitionTime:2023-04-17 22:16:22 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 17 22:16:24.125: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-4155  2475cac7-ed91-4f4e-a324-703c46c65294 54894 2 2023-04-17 22:16:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 35a15685-e2db-42bf-a4d6-774b2ec488f3 0xc00b29bf97 0xc00b29bf98}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:16:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"35a15685-e2db-42bf-a4d6-774b2ec488f3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:16:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006af2028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 22:16:24.128: INFO: Pod "test-new-deployment-7f5969cbc7-q7rjx" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-q7rjx test-new-deployment-7f5969cbc7- deployment-4155  daeacacc-61d0-478a-828c-e58745803bcd 54864 0 2023-04-17 22:16:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2587b71e572fd7c745d7fc4ede84f39b01c7531d9fd53a359f8882f07b730317 cni.projectcalico.org/podIP:192.168.213.63/32 cni.projectcalico.org/podIPs:192.168.213.63/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 2475cac7-ed91-4f4e-a324-703c46c65294 0xc006af2417 0xc006af2418}] [] [{calico Update v1 2023-04-17 22:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 22:16:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2475cac7-ed91-4f4e-a324-703c46c65294\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:16:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-52xcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-52xcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.63,StartTime:2023-04-17 22:16:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:16:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2e6e25d3b3ef794ff800f09f236dc5f12a4604621ebf627c613eb337422255d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:16:24.129: INFO: Pod "test-new-deployment-7f5969cbc7-x9n5s" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-x9n5s test-new-deployment-7f5969cbc7- deployment-4155  5b989672-68e0-422c-b8ab-724b33a421b6 54896 0 2023-04-17 22:16:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 2475cac7-ed91-4f4e-a324-703c46c65294 0xc006af2600 0xc006af2601}] [] [{kube-controller-manager Update v1 2023-04-17 22:16:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2475cac7-ed91-4f4e-a324-703c46c65294\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:16:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6tn9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6tn9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:16:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:16:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:24.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4155" for this suite. 04/17/23 22:16:24.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:24.15
Apr 17 22:16:24.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:16:24.15
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:24.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:24.167
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-9172/configmap-test-2ca6e44a-fc36-4328-8b7a-2eb4c0c651c5 04/17/23 22:16:24.169
STEP: Creating a pod to test consume configMaps 04/17/23 22:16:24.173
Apr 17 22:16:24.181: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a" in namespace "configmap-9172" to be "Succeeded or Failed"
Apr 17 22:16:24.183: INFO: Pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108347ms
Apr 17 22:16:26.187: INFO: Pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a": Phase="Running", Reason="", readiness=false. Elapsed: 2.005596375s
Apr 17 22:16:28.187: INFO: Pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006316172s
STEP: Saw pod success 04/17/23 22:16:28.187
Apr 17 22:16:28.187: INFO: Pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a" satisfied condition "Succeeded or Failed"
Apr 17 22:16:28.189: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a container env-test: <nil>
STEP: delete the pod 04/17/23 22:16:28.194
Apr 17 22:16:28.206: INFO: Waiting for pod pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a to disappear
Apr 17 22:16:28.208: INFO: Pod pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:28.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9172" for this suite. 04/17/23 22:16:28.212
------------------------------
• [4.067 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:24.15
    Apr 17 22:16:24.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:16:24.15
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:24.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:24.167
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-9172/configmap-test-2ca6e44a-fc36-4328-8b7a-2eb4c0c651c5 04/17/23 22:16:24.169
    STEP: Creating a pod to test consume configMaps 04/17/23 22:16:24.173
    Apr 17 22:16:24.181: INFO: Waiting up to 5m0s for pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a" in namespace "configmap-9172" to be "Succeeded or Failed"
    Apr 17 22:16:24.183: INFO: Pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.108347ms
    Apr 17 22:16:26.187: INFO: Pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a": Phase="Running", Reason="", readiness=false. Elapsed: 2.005596375s
    Apr 17 22:16:28.187: INFO: Pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006316172s
    STEP: Saw pod success 04/17/23 22:16:28.187
    Apr 17 22:16:28.187: INFO: Pod "pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a" satisfied condition "Succeeded or Failed"
    Apr 17 22:16:28.189: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a container env-test: <nil>
    STEP: delete the pod 04/17/23 22:16:28.194
    Apr 17 22:16:28.206: INFO: Waiting for pod pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a to disappear
    Apr 17 22:16:28.208: INFO: Pod pod-configmaps-e4db9703-63fe-4eff-8d00-fb49be60285a no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:28.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9172" for this suite. 04/17/23 22:16:28.212
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:28.217
Apr 17 22:16:28.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:16:28.218
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:28.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:28.232
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 04/17/23 22:16:28.233
Apr 17 22:16:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: rename a version 04/17/23 22:16:34.512
STEP: check the new version name is served 04/17/23 22:16:34.525
STEP: check the old version name is removed 04/17/23 22:16:38.466
STEP: check the other version is not changed 04/17/23 22:16:39.364
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:45.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8085" for this suite. 04/17/23 22:16:45.146
------------------------------
• [SLOW TEST] [16.934 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:28.217
    Apr 17 22:16:28.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:16:28.218
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:28.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:28.232
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 04/17/23 22:16:28.233
    Apr 17 22:16:28.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: rename a version 04/17/23 22:16:34.512
    STEP: check the new version name is served 04/17/23 22:16:34.525
    STEP: check the old version name is removed 04/17/23 22:16:38.466
    STEP: check the other version is not changed 04/17/23 22:16:39.364
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:45.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8085" for this suite. 04/17/23 22:16:45.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:45.152
Apr 17 22:16:45.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:16:45.153
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:45.166
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:45.168
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-3b63e6ae-cb5a-4232-9485-e087e4b117a7 04/17/23 22:16:45.17
STEP: Creating a pod to test consume configMaps 04/17/23 22:16:45.173
Apr 17 22:16:45.180: INFO: Waiting up to 5m0s for pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150" in namespace "configmap-5719" to be "Succeeded or Failed"
Apr 17 22:16:45.184: INFO: Pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150": Phase="Pending", Reason="", readiness=false. Elapsed: 3.475385ms
Apr 17 22:16:47.188: INFO: Pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007668146s
Apr 17 22:16:49.187: INFO: Pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006530404s
STEP: Saw pod success 04/17/23 22:16:49.187
Apr 17 22:16:49.187: INFO: Pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150" satisfied condition "Succeeded or Failed"
Apr 17 22:16:49.189: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:16:49.199
Apr 17 22:16:49.207: INFO: Waiting for pod pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150 to disappear
Apr 17 22:16:49.209: INFO: Pod pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:49.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5719" for this suite. 04/17/23 22:16:49.214
------------------------------
• [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:45.152
    Apr 17 22:16:45.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:16:45.153
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:45.166
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:45.168
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-3b63e6ae-cb5a-4232-9485-e087e4b117a7 04/17/23 22:16:45.17
    STEP: Creating a pod to test consume configMaps 04/17/23 22:16:45.173
    Apr 17 22:16:45.180: INFO: Waiting up to 5m0s for pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150" in namespace "configmap-5719" to be "Succeeded or Failed"
    Apr 17 22:16:45.184: INFO: Pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150": Phase="Pending", Reason="", readiness=false. Elapsed: 3.475385ms
    Apr 17 22:16:47.188: INFO: Pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007668146s
    Apr 17 22:16:49.187: INFO: Pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006530404s
    STEP: Saw pod success 04/17/23 22:16:49.187
    Apr 17 22:16:49.187: INFO: Pod "pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150" satisfied condition "Succeeded or Failed"
    Apr 17 22:16:49.189: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:16:49.199
    Apr 17 22:16:49.207: INFO: Waiting for pod pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150 to disappear
    Apr 17 22:16:49.209: INFO: Pod pod-configmaps-75f4d39c-73db-41f0-b198-f9fa6a002150 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:49.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5719" for this suite. 04/17/23 22:16:49.214
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:49.221
Apr 17 22:16:49.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubelet-test 04/17/23 22:16:49.222
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:49.233
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:49.235
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Apr 17 22:16:49.245: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5" in namespace "kubelet-test-4893" to be "running and ready"
Apr 17 22:16:49.248: INFO: Pod "busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139173ms
Apr 17 22:16:49.248: INFO: The phase of Pod busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:16:51.252: INFO: Pod "busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007158474s
Apr 17 22:16:51.252: INFO: The phase of Pod busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5 is Running (Ready = true)
Apr 17 22:16:51.252: INFO: Pod "busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:51.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4893" for this suite. 04/17/23 22:16:51.263
------------------------------
• [2.048 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:49.221
    Apr 17 22:16:49.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubelet-test 04/17/23 22:16:49.222
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:49.233
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:49.235
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Apr 17 22:16:49.245: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5" in namespace "kubelet-test-4893" to be "running and ready"
    Apr 17 22:16:49.248: INFO: Pod "busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.139173ms
    Apr 17 22:16:49.248: INFO: The phase of Pod busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:16:51.252: INFO: Pod "busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007158474s
    Apr 17 22:16:51.252: INFO: The phase of Pod busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5 is Running (Ready = true)
    Apr 17 22:16:51.252: INFO: Pod "busybox-readonly-fs35677ede-2ecb-42d9-8508-2dd9731101e5" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:51.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4893" for this suite. 04/17/23 22:16:51.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:51.27
Apr 17 22:16:51.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-webhook 04/17/23 22:16:51.271
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:51.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:51.284
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 04/17/23 22:16:51.287
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/17/23 22:16:51.561
STEP: Deploying the custom resource conversion webhook pod 04/17/23 22:16:51.568
STEP: Wait for the deployment to be ready 04/17/23 22:16:51.579
Apr 17 22:16:51.585: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 04/17/23 22:16:53.593
STEP: Verifying the service has paired with the endpoint 04/17/23 22:16:53.603
Apr 17 22:16:54.604: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Apr 17 22:16:54.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Creating a v1 custom resource 04/17/23 22:16:57.165
STEP: v2 custom resource should be converted 04/17/23 22:16:57.171
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:16:57.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-3959" for this suite. 04/17/23 22:16:57.726
------------------------------
• [SLOW TEST] [6.462 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:51.27
    Apr 17 22:16:51.270: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-webhook 04/17/23 22:16:51.271
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:51.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:51.284
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 04/17/23 22:16:51.287
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 04/17/23 22:16:51.561
    STEP: Deploying the custom resource conversion webhook pod 04/17/23 22:16:51.568
    STEP: Wait for the deployment to be ready 04/17/23 22:16:51.579
    Apr 17 22:16:51.585: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 04/17/23 22:16:53.593
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:16:53.603
    Apr 17 22:16:54.604: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Apr 17 22:16:54.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Creating a v1 custom resource 04/17/23 22:16:57.165
    STEP: v2 custom resource should be converted 04/17/23 22:16:57.171
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:16:57.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-3959" for this suite. 04/17/23 22:16:57.726
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:16:57.732
Apr 17 22:16:57.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename disruption 04/17/23 22:16:57.733
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:57.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:57.753
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 04/17/23 22:16:57.762
STEP: Waiting for all pods to be running 04/17/23 22:16:59.786
Apr 17 22:16:59.791: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Apr 17 22:17:01.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-5730" for this suite. 04/17/23 22:17:01.801
------------------------------
• [4.074 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:16:57.732
    Apr 17 22:16:57.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename disruption 04/17/23 22:16:57.733
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:16:57.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:16:57.753
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 04/17/23 22:16:57.762
    STEP: Waiting for all pods to be running 04/17/23 22:16:59.786
    Apr 17 22:16:59.791: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:17:01.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-5730" for this suite. 04/17/23 22:17:01.801
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:17:01.806
Apr 17 22:17:01.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename hostport 04/17/23 22:17:01.807
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:17:01.82
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:17:01.823
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 04/17/23 22:17:01.829
Apr 17 22:17:01.836: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5440" to be "running and ready"
Apr 17 22:17:01.839: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.731862ms
Apr 17 22:17:01.839: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:17:03.842: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006085091s
Apr 17 22:17:03.842: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr 17 22:17:03.842: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.106.231 on the node which pod1 resides and expect scheduled 04/17/23 22:17:03.842
Apr 17 22:17:03.847: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5440" to be "running and ready"
Apr 17 22:17:03.850: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198117ms
Apr 17 22:17:03.850: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:17:05.852: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.004961319s
Apr 17 22:17:05.852: INFO: The phase of Pod pod2 is Running (Ready = false)
Apr 17 22:17:07.853: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.005659603s
Apr 17 22:17:07.853: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr 17 22:17:07.853: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.106.231 but use UDP protocol on the node which pod2 resides 04/17/23 22:17:07.853
Apr 17 22:17:07.859: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5440" to be "running and ready"
Apr 17 22:17:07.862: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.163275ms
Apr 17 22:17:07.862: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:17:09.866: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006579083s
Apr 17 22:17:09.866: INFO: The phase of Pod pod3 is Running (Ready = true)
Apr 17 22:17:09.866: INFO: Pod "pod3" satisfied condition "running and ready"
Apr 17 22:17:09.870: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5440" to be "running and ready"
Apr 17 22:17:09.872: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349545ms
Apr 17 22:17:09.872: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:17:11.875: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005382142s
Apr 17 22:17:11.875: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Apr 17 22:17:11.875: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 04/17/23 22:17:11.878
Apr 17 22:17:11.878: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.106.231 http://127.0.0.1:54323/hostname] Namespace:hostport-5440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:17:11.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:17:11.878: INFO: ExecWithOptions: Clientset creation
Apr 17 22:17:11.878: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.106.231+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.106.231, port: 54323 04/17/23 22:17:11.98
Apr 17 22:17:11.980: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.106.231:54323/hostname] Namespace:hostport-5440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:17:11.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:17:11.980: INFO: ExecWithOptions: Clientset creation
Apr 17 22:17:11.980: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.106.231%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.106.231, port: 54323 UDP 04/17/23 22:17:12.067
Apr 17 22:17:12.067: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.106.231 54323] Namespace:hostport-5440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:17:12.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:17:12.067: INFO: ExecWithOptions: Clientset creation
Apr 17 22:17:12.067: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.106.231+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Apr 17 22:17:17.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-5440" for this suite. 04/17/23 22:17:17.113
------------------------------
• [SLOW TEST] [15.310 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:17:01.806
    Apr 17 22:17:01.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename hostport 04/17/23 22:17:01.807
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:17:01.82
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:17:01.823
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 04/17/23 22:17:01.829
    Apr 17 22:17:01.836: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-5440" to be "running and ready"
    Apr 17 22:17:01.839: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.731862ms
    Apr 17 22:17:01.839: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:17:03.842: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006085091s
    Apr 17 22:17:03.842: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr 17 22:17:03.842: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.0.106.231 on the node which pod1 resides and expect scheduled 04/17/23 22:17:03.842
    Apr 17 22:17:03.847: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-5440" to be "running and ready"
    Apr 17 22:17:03.850: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198117ms
    Apr 17 22:17:03.850: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:17:05.852: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.004961319s
    Apr 17 22:17:05.852: INFO: The phase of Pod pod2 is Running (Ready = false)
    Apr 17 22:17:07.853: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.005659603s
    Apr 17 22:17:07.853: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr 17 22:17:07.853: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.0.106.231 but use UDP protocol on the node which pod2 resides 04/17/23 22:17:07.853
    Apr 17 22:17:07.859: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-5440" to be "running and ready"
    Apr 17 22:17:07.862: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.163275ms
    Apr 17 22:17:07.862: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:17:09.866: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006579083s
    Apr 17 22:17:09.866: INFO: The phase of Pod pod3 is Running (Ready = true)
    Apr 17 22:17:09.866: INFO: Pod "pod3" satisfied condition "running and ready"
    Apr 17 22:17:09.870: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-5440" to be "running and ready"
    Apr 17 22:17:09.872: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349545ms
    Apr 17 22:17:09.872: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:17:11.875: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.005382142s
    Apr 17 22:17:11.875: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Apr 17 22:17:11.875: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 04/17/23 22:17:11.878
    Apr 17 22:17:11.878: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.0.106.231 http://127.0.0.1:54323/hostname] Namespace:hostport-5440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:17:11.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:17:11.878: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:17:11.878: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.0.106.231+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.106.231, port: 54323 04/17/23 22:17:11.98
    Apr 17 22:17:11.980: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.0.106.231:54323/hostname] Namespace:hostport-5440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:17:11.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:17:11.980: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:17:11.980: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.0.106.231%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.0.106.231, port: 54323 UDP 04/17/23 22:17:12.067
    Apr 17 22:17:12.067: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.0.106.231 54323] Namespace:hostport-5440 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:17:12.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:17:12.067: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:17:12.067: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5440/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.0.106.231+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:17:17.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-5440" for this suite. 04/17/23 22:17:17.113
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:17:17.118
Apr 17 22:17:17.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename daemonsets 04/17/23 22:17:17.119
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:17:17.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:17:17.135
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Apr 17 22:17:17.153: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 04/17/23 22:17:17.157
Apr 17 22:17:17.159: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:17:17.159: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 04/17/23 22:17:17.159
Apr 17 22:17:17.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:17:17.183: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:17:18.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 17 22:17:18.187: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 04/17/23 22:17:18.189
Apr 17 22:17:18.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 17 22:17:18.207: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Apr 17 22:17:19.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:17:19.211: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 04/17/23 22:17:19.211
Apr 17 22:17:19.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:17:19.219: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:17:20.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:17:20.222: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:17:21.223: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:17:21.223: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:17:22.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 17 22:17:22.222: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/17/23 22:17:22.227
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4750, will wait for the garbage collector to delete the pods 04/17/23 22:17:22.227
Apr 17 22:17:22.284: INFO: Deleting DaemonSet.extensions daemon-set took: 4.806044ms
Apr 17 22:17:22.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.791978ms
Apr 17 22:17:25.288: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:17:25.288: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 17 22:17:25.290: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"55892"},"items":null}

Apr 17 22:17:25.292: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"55892"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:17:25.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4750" for this suite. 04/17/23 22:17:25.323
------------------------------
• [SLOW TEST] [8.210 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:17:17.118
    Apr 17 22:17:17.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename daemonsets 04/17/23 22:17:17.119
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:17:17.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:17:17.135
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Apr 17 22:17:17.153: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 04/17/23 22:17:17.157
    Apr 17 22:17:17.159: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:17:17.159: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 04/17/23 22:17:17.159
    Apr 17 22:17:17.183: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:17:17.183: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:17:18.186: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 17 22:17:18.187: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 04/17/23 22:17:18.189
    Apr 17 22:17:18.207: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 17 22:17:18.207: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Apr 17 22:17:19.211: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:17:19.211: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 04/17/23 22:17:19.211
    Apr 17 22:17:19.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:17:19.219: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:17:20.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:17:20.222: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:17:21.223: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:17:21.223: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:17:22.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 17 22:17:22.222: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/17/23 22:17:22.227
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4750, will wait for the garbage collector to delete the pods 04/17/23 22:17:22.227
    Apr 17 22:17:22.284: INFO: Deleting DaemonSet.extensions daemon-set took: 4.806044ms
    Apr 17 22:17:22.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.791978ms
    Apr 17 22:17:25.288: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:17:25.288: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 17 22:17:25.290: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"55892"},"items":null}

    Apr 17 22:17:25.292: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"55892"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:17:25.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4750" for this suite. 04/17/23 22:17:25.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:17:25.329
Apr 17 22:17:25.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename var-expansion 04/17/23 22:17:25.33
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:17:25.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:17:25.343
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 04/17/23 22:17:25.345
STEP: waiting for pod running 04/17/23 22:17:25.352
Apr 17 22:17:25.352: INFO: Waiting up to 2m0s for pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" in namespace "var-expansion-8316" to be "running"
Apr 17 22:17:25.354: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242316ms
Apr 17 22:17:27.358: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60": Phase="Running", Reason="", readiness=true. Elapsed: 2.005490017s
Apr 17 22:17:27.358: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" satisfied condition "running"
STEP: creating a file in subpath 04/17/23 22:17:27.358
Apr 17 22:17:27.360: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8316 PodName:var-expansion-b6954cde-894f-430e-88f7-3188009e0f60 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:17:27.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:17:27.361: INFO: ExecWithOptions: Clientset creation
Apr 17 22:17:27.361: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8316/pods/var-expansion-b6954cde-894f-430e-88f7-3188009e0f60/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 04/17/23 22:17:27.444
Apr 17 22:17:27.446: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8316 PodName:var-expansion-b6954cde-894f-430e-88f7-3188009e0f60 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:17:27.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:17:27.447: INFO: ExecWithOptions: Clientset creation
Apr 17 22:17:27.447: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8316/pods/var-expansion-b6954cde-894f-430e-88f7-3188009e0f60/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 04/17/23 22:17:27.519
Apr 17 22:17:28.030: INFO: Successfully updated pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60"
STEP: waiting for annotated pod running 04/17/23 22:17:28.03
Apr 17 22:17:28.030: INFO: Waiting up to 2m0s for pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" in namespace "var-expansion-8316" to be "running"
Apr 17 22:17:28.032: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60": Phase="Running", Reason="", readiness=true. Elapsed: 2.449844ms
Apr 17 22:17:28.032: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" satisfied condition "running"
STEP: deleting the pod gracefully 04/17/23 22:17:28.032
Apr 17 22:17:28.032: INFO: Deleting pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" in namespace "var-expansion-8316"
Apr 17 22:17:28.040: INFO: Wait up to 5m0s for pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 17 22:18:02.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8316" for this suite. 04/17/23 22:18:02.051
------------------------------
• [SLOW TEST] [36.727 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:17:25.329
    Apr 17 22:17:25.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename var-expansion 04/17/23 22:17:25.33
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:17:25.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:17:25.343
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 04/17/23 22:17:25.345
    STEP: waiting for pod running 04/17/23 22:17:25.352
    Apr 17 22:17:25.352: INFO: Waiting up to 2m0s for pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" in namespace "var-expansion-8316" to be "running"
    Apr 17 22:17:25.354: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.242316ms
    Apr 17 22:17:27.358: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60": Phase="Running", Reason="", readiness=true. Elapsed: 2.005490017s
    Apr 17 22:17:27.358: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" satisfied condition "running"
    STEP: creating a file in subpath 04/17/23 22:17:27.358
    Apr 17 22:17:27.360: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-8316 PodName:var-expansion-b6954cde-894f-430e-88f7-3188009e0f60 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:17:27.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:17:27.361: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:17:27.361: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8316/pods/var-expansion-b6954cde-894f-430e-88f7-3188009e0f60/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 04/17/23 22:17:27.444
    Apr 17 22:17:27.446: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-8316 PodName:var-expansion-b6954cde-894f-430e-88f7-3188009e0f60 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:17:27.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:17:27.447: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:17:27.447: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-8316/pods/var-expansion-b6954cde-894f-430e-88f7-3188009e0f60/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 04/17/23 22:17:27.519
    Apr 17 22:17:28.030: INFO: Successfully updated pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60"
    STEP: waiting for annotated pod running 04/17/23 22:17:28.03
    Apr 17 22:17:28.030: INFO: Waiting up to 2m0s for pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" in namespace "var-expansion-8316" to be "running"
    Apr 17 22:17:28.032: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60": Phase="Running", Reason="", readiness=true. Elapsed: 2.449844ms
    Apr 17 22:17:28.032: INFO: Pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" satisfied condition "running"
    STEP: deleting the pod gracefully 04/17/23 22:17:28.032
    Apr 17 22:17:28.032: INFO: Deleting pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" in namespace "var-expansion-8316"
    Apr 17 22:17:28.040: INFO: Wait up to 5m0s for pod "var-expansion-b6954cde-894f-430e-88f7-3188009e0f60" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:18:02.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8316" for this suite. 04/17/23 22:18:02.051
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:18:02.056
Apr 17 22:18:02.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename var-expansion 04/17/23 22:18:02.057
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:02.071
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:02.073
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 04/17/23 22:18:02.075
Apr 17 22:18:02.081: INFO: Waiting up to 5m0s for pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8" in namespace "var-expansion-6120" to be "Succeeded or Failed"
Apr 17 22:18:02.084: INFO: Pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281162ms
Apr 17 22:18:04.087: INFO: Pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005739219s
Apr 17 22:18:06.088: INFO: Pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00609313s
STEP: Saw pod success 04/17/23 22:18:06.088
Apr 17 22:18:06.088: INFO: Pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8" satisfied condition "Succeeded or Failed"
Apr 17 22:18:06.090: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8 container dapi-container: <nil>
STEP: delete the pod 04/17/23 22:18:06.095
Apr 17 22:18:06.107: INFO: Waiting for pod var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8 to disappear
Apr 17 22:18:06.109: INFO: Pod var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 17 22:18:06.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-6120" for this suite. 04/17/23 22:18:06.114
------------------------------
• [4.062 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:18:02.056
    Apr 17 22:18:02.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename var-expansion 04/17/23 22:18:02.057
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:02.071
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:02.073
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 04/17/23 22:18:02.075
    Apr 17 22:18:02.081: INFO: Waiting up to 5m0s for pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8" in namespace "var-expansion-6120" to be "Succeeded or Failed"
    Apr 17 22:18:02.084: INFO: Pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.281162ms
    Apr 17 22:18:04.087: INFO: Pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005739219s
    Apr 17 22:18:06.088: INFO: Pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00609313s
    STEP: Saw pod success 04/17/23 22:18:06.088
    Apr 17 22:18:06.088: INFO: Pod "var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8" satisfied condition "Succeeded or Failed"
    Apr 17 22:18:06.090: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8 container dapi-container: <nil>
    STEP: delete the pod 04/17/23 22:18:06.095
    Apr 17 22:18:06.107: INFO: Waiting for pod var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8 to disappear
    Apr 17 22:18:06.109: INFO: Pod var-expansion-2a700ecd-825e-4bd5-9329-fcbd51407cc8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:18:06.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-6120" for this suite. 04/17/23 22:18:06.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:18:06.12
Apr 17 22:18:06.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 22:18:06.121
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:06.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:06.136
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-19114285-3cae-4654-8332-b70172546e9b 04/17/23 22:18:06.138
STEP: Creating a pod to test consume secrets 04/17/23 22:18:06.141
Apr 17 22:18:06.147: INFO: Waiting up to 5m0s for pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d" in namespace "secrets-7832" to be "Succeeded or Failed"
Apr 17 22:18:06.149: INFO: Pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.171099ms
Apr 17 22:18:08.153: INFO: Pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006247399s
Apr 17 22:18:10.152: INFO: Pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005886339s
STEP: Saw pod success 04/17/23 22:18:10.152
Apr 17 22:18:10.152: INFO: Pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d" satisfied condition "Succeeded or Failed"
Apr 17 22:18:10.155: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d container secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:18:10.168
Apr 17 22:18:10.178: INFO: Waiting for pod pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d to disappear
Apr 17 22:18:10.180: INFO: Pod pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 22:18:10.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7832" for this suite. 04/17/23 22:18:10.184
------------------------------
• [4.069 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:18:06.12
    Apr 17 22:18:06.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 22:18:06.121
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:06.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:06.136
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-19114285-3cae-4654-8332-b70172546e9b 04/17/23 22:18:06.138
    STEP: Creating a pod to test consume secrets 04/17/23 22:18:06.141
    Apr 17 22:18:06.147: INFO: Waiting up to 5m0s for pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d" in namespace "secrets-7832" to be "Succeeded or Failed"
    Apr 17 22:18:06.149: INFO: Pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.171099ms
    Apr 17 22:18:08.153: INFO: Pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006247399s
    Apr 17 22:18:10.152: INFO: Pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005886339s
    STEP: Saw pod success 04/17/23 22:18:10.152
    Apr 17 22:18:10.152: INFO: Pod "pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d" satisfied condition "Succeeded or Failed"
    Apr 17 22:18:10.155: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d container secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:18:10.168
    Apr 17 22:18:10.178: INFO: Waiting for pod pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d to disappear
    Apr 17 22:18:10.180: INFO: Pod pod-secrets-3efe3b34-dbe0-4086-83ae-00c023838b2d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:18:10.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7832" for this suite. 04/17/23 22:18:10.184
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:18:10.189
Apr 17 22:18:10.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename watch 04/17/23 22:18:10.19
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:10.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:10.204
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 04/17/23 22:18:10.206
STEP: starting a background goroutine to produce watch events 04/17/23 22:18:10.208
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 04/17/23 22:18:10.208
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Apr 17 22:18:12.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8715" for this suite. 04/17/23 22:18:13.044
------------------------------
• [2.906 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:18:10.189
    Apr 17 22:18:10.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename watch 04/17/23 22:18:10.19
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:10.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:10.204
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 04/17/23 22:18:10.206
    STEP: starting a background goroutine to produce watch events 04/17/23 22:18:10.208
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 04/17/23 22:18:10.208
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:18:12.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8715" for this suite. 04/17/23 22:18:13.044
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:18:13.095
Apr 17 22:18:13.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubelet-test 04/17/23 22:18:13.096
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:13.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:13.112
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:18:17.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-5785" for this suite. 04/17/23 22:18:17.132
------------------------------
• [4.042 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:18:13.095
    Apr 17 22:18:13.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubelet-test 04/17/23 22:18:13.096
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:13.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:13.112
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:18:17.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-5785" for this suite. 04/17/23 22:18:17.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:18:17.138
Apr 17 22:18:17.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:18:17.139
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:17.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:17.165
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:18:17.167
Apr 17 22:18:17.178: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2" in namespace "projected-3827" to be "Succeeded or Failed"
Apr 17 22:18:17.185: INFO: Pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.822212ms
Apr 17 22:18:19.188: INFO: Pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010734381s
Apr 17 22:18:21.188: INFO: Pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010668453s
STEP: Saw pod success 04/17/23 22:18:21.188
Apr 17 22:18:21.189: INFO: Pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2" satisfied condition "Succeeded or Failed"
Apr 17 22:18:21.191: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2 container client-container: <nil>
STEP: delete the pod 04/17/23 22:18:21.204
Apr 17 22:18:21.213: INFO: Waiting for pod downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2 to disappear
Apr 17 22:18:21.216: INFO: Pod downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 22:18:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3827" for this suite. 04/17/23 22:18:21.22
------------------------------
• [4.086 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:18:17.138
    Apr 17 22:18:17.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:18:17.139
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:17.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:17.165
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:18:17.167
    Apr 17 22:18:17.178: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2" in namespace "projected-3827" to be "Succeeded or Failed"
    Apr 17 22:18:17.185: INFO: Pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.822212ms
    Apr 17 22:18:19.188: INFO: Pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010734381s
    Apr 17 22:18:21.188: INFO: Pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010668453s
    STEP: Saw pod success 04/17/23 22:18:21.188
    Apr 17 22:18:21.189: INFO: Pod "downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2" satisfied condition "Succeeded or Failed"
    Apr 17 22:18:21.191: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2 container client-container: <nil>
    STEP: delete the pod 04/17/23 22:18:21.204
    Apr 17 22:18:21.213: INFO: Waiting for pod downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2 to disappear
    Apr 17 22:18:21.216: INFO: Pod downwardapi-volume-a0129bfc-e622-4c0b-9439-89fd811025b2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:18:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3827" for this suite. 04/17/23 22:18:21.22
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:18:21.226
Apr 17 22:18:21.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 22:18:21.227
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:21.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:21.241
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 04/17/23 22:18:21.243
Apr 17 22:18:21.248: INFO: Waiting up to 5m0s for pod "pod-7e317b15-3049-4a96-9d88-263eab30da45" in namespace "emptydir-5050" to be "Succeeded or Failed"
Apr 17 22:18:21.250: INFO: Pod "pod-7e317b15-3049-4a96-9d88-263eab30da45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233388ms
Apr 17 22:18:23.254: INFO: Pod "pod-7e317b15-3049-4a96-9d88-263eab30da45": Phase="Running", Reason="", readiness=false. Elapsed: 2.005391014s
Apr 17 22:18:25.254: INFO: Pod "pod-7e317b15-3049-4a96-9d88-263eab30da45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006051871s
STEP: Saw pod success 04/17/23 22:18:25.254
Apr 17 22:18:25.254: INFO: Pod "pod-7e317b15-3049-4a96-9d88-263eab30da45" satisfied condition "Succeeded or Failed"
Apr 17 22:18:25.257: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-7e317b15-3049-4a96-9d88-263eab30da45 container test-container: <nil>
STEP: delete the pod 04/17/23 22:18:25.27
Apr 17 22:18:25.280: INFO: Waiting for pod pod-7e317b15-3049-4a96-9d88-263eab30da45 to disappear
Apr 17 22:18:25.282: INFO: Pod pod-7e317b15-3049-4a96-9d88-263eab30da45 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:18:25.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5050" for this suite. 04/17/23 22:18:25.287
------------------------------
• [4.065 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:18:21.226
    Apr 17 22:18:21.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 22:18:21.227
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:21.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:21.241
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 04/17/23 22:18:21.243
    Apr 17 22:18:21.248: INFO: Waiting up to 5m0s for pod "pod-7e317b15-3049-4a96-9d88-263eab30da45" in namespace "emptydir-5050" to be "Succeeded or Failed"
    Apr 17 22:18:21.250: INFO: Pod "pod-7e317b15-3049-4a96-9d88-263eab30da45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233388ms
    Apr 17 22:18:23.254: INFO: Pod "pod-7e317b15-3049-4a96-9d88-263eab30da45": Phase="Running", Reason="", readiness=false. Elapsed: 2.005391014s
    Apr 17 22:18:25.254: INFO: Pod "pod-7e317b15-3049-4a96-9d88-263eab30da45": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006051871s
    STEP: Saw pod success 04/17/23 22:18:25.254
    Apr 17 22:18:25.254: INFO: Pod "pod-7e317b15-3049-4a96-9d88-263eab30da45" satisfied condition "Succeeded or Failed"
    Apr 17 22:18:25.257: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-7e317b15-3049-4a96-9d88-263eab30da45 container test-container: <nil>
    STEP: delete the pod 04/17/23 22:18:25.27
    Apr 17 22:18:25.280: INFO: Waiting for pod pod-7e317b15-3049-4a96-9d88-263eab30da45 to disappear
    Apr 17 22:18:25.282: INFO: Pod pod-7e317b15-3049-4a96-9d88-263eab30da45 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:18:25.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5050" for this suite. 04/17/23 22:18:25.287
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:18:25.291
Apr 17 22:18:25.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:18:25.292
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:25.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:25.307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:18:25.318
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:18:25.681
STEP: Deploying the webhook pod 04/17/23 22:18:25.687
STEP: Wait for the deployment to be ready 04/17/23 22:18:25.697
Apr 17 22:18:25.701: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
Apr 17 22:18:27.709: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 18, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 18, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 18, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 18, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 04/17/23 22:18:29.712
STEP: Verifying the service has paired with the endpoint 04/17/23 22:18:29.725
Apr 17 22:18:30.726: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 04/17/23 22:18:30.728
STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 22:18:30.741
STEP: Updating a validating webhook configuration's rules to not include the create operation 04/17/23 22:18:30.747
STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 22:18:30.755
STEP: Patching a validating webhook configuration's rules to include the create operation 04/17/23 22:18:30.762
STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 22:18:30.769
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:18:30.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8370" for this suite. 04/17/23 22:18:30.88
STEP: Destroying namespace "webhook-8370-markers" for this suite. 04/17/23 22:18:30.885
------------------------------
• [SLOW TEST] [5.599 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:18:25.291
    Apr 17 22:18:25.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:18:25.292
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:25.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:25.307
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:18:25.318
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:18:25.681
    STEP: Deploying the webhook pod 04/17/23 22:18:25.687
    STEP: Wait for the deployment to be ready 04/17/23 22:18:25.697
    Apr 17 22:18:25.701: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    Apr 17 22:18:27.709: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 18, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 18, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 18, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 18, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 04/17/23 22:18:29.712
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:18:29.725
    Apr 17 22:18:30.726: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 04/17/23 22:18:30.728
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 22:18:30.741
    STEP: Updating a validating webhook configuration's rules to not include the create operation 04/17/23 22:18:30.747
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 22:18:30.755
    STEP: Patching a validating webhook configuration's rules to include the create operation 04/17/23 22:18:30.762
    STEP: Creating a configMap that does not comply to the validation webhook rules 04/17/23 22:18:30.769
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:18:30.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8370" for this suite. 04/17/23 22:18:30.88
    STEP: Destroying namespace "webhook-8370-markers" for this suite. 04/17/23 22:18:30.885
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:18:30.891
Apr 17 22:18:30.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename statefulset 04/17/23 22:18:30.892
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:30.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:30.908
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1673 04/17/23 22:18:30.91
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 04/17/23 22:18:30.914
STEP: Creating stateful set ss in namespace statefulset-1673 04/17/23 22:18:30.917
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1673 04/17/23 22:18:30.922
Apr 17 22:18:30.925: INFO: Found 0 stateful pods, waiting for 1
Apr 17 22:18:40.929: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 04/17/23 22:18:40.929
Apr 17 22:18:40.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:18:41.065: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:18:41.065: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:18:41.065: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:18:41.068: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr 17 22:18:51.072: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:18:51.072: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:18:51.083: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999839s
Apr 17 22:18:52.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997515994s
Apr 17 22:18:53.089: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994333656s
Apr 17 22:18:54.092: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991282867s
Apr 17 22:18:55.095: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988186414s
Apr 17 22:18:56.099: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985183674s
Apr 17 22:18:57.101: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.982119742s
Apr 17 22:18:58.105: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.978331191s
Apr 17 22:18:59.109: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.974706727s
Apr 17 22:19:00.112: INFO: Verifying statefulset ss doesn't scale past 1 for another 971.751261ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1673 04/17/23 22:19:01.112
Apr 17 22:19:01.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:19:01.246: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:19:01.246: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:19:01.246: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:19:01.248: INFO: Found 1 stateful pods, waiting for 3
Apr 17 22:19:11.254: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:19:11.254: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:19:11.254: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 04/17/23 22:19:11.255
STEP: Scale down will halt with unhealthy stateful pod 04/17/23 22:19:11.255
Apr 17 22:19:11.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:19:11.399: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:19:11.399: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:19:11.399: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:19:11.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:19:11.531: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:19:11.531: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:19:11.531: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:19:11.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:19:11.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:19:11.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:19:11.668: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:19:11.668: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:19:11.670: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Apr 17 22:19:21.676: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:19:21.676: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:19:21.676: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr 17 22:19:21.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999828s
Apr 17 22:19:22.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997423961s
Apr 17 22:19:23.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994347135s
Apr 17 22:19:24.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990833235s
Apr 17 22:19:25.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987609014s
Apr 17 22:19:26.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984475897s
Apr 17 22:19:27.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981533478s
Apr 17 22:19:28.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977167947s
Apr 17 22:19:29.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974297799s
Apr 17 22:19:30.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.37443ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1673 04/17/23 22:19:31.716
Apr 17 22:19:31.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:19:31.872: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:19:31.872: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:19:31.872: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:19:31.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:19:32.010: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:19:32.010: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:19:32.010: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:19:32.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:19:32.136: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:19:32.136: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:19:32.136: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Apr 17 22:19:32.136: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 04/17/23 22:19:42.152
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 17 22:19:42.152: INFO: Deleting all statefulset in ns statefulset-1673
Apr 17 22:19:42.154: INFO: Scaling statefulset ss to 0
Apr 17 22:19:42.162: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:19:42.164: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:19:42.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1673" for this suite. 04/17/23 22:19:42.176
------------------------------
• [SLOW TEST] [71.290 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:18:30.891
    Apr 17 22:18:30.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename statefulset 04/17/23 22:18:30.892
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:18:30.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:18:30.908
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1673 04/17/23 22:18:30.91
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 04/17/23 22:18:30.914
    STEP: Creating stateful set ss in namespace statefulset-1673 04/17/23 22:18:30.917
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1673 04/17/23 22:18:30.922
    Apr 17 22:18:30.925: INFO: Found 0 stateful pods, waiting for 1
    Apr 17 22:18:40.929: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 04/17/23 22:18:40.929
    Apr 17 22:18:40.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 22:18:41.065: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 22:18:41.065: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 22:18:41.065: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 17 22:18:41.068: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Apr 17 22:18:51.072: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 17 22:18:51.072: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 22:18:51.083: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999839s
    Apr 17 22:18:52.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997515994s
    Apr 17 22:18:53.089: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994333656s
    Apr 17 22:18:54.092: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991282867s
    Apr 17 22:18:55.095: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988186414s
    Apr 17 22:18:56.099: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985183674s
    Apr 17 22:18:57.101: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.982119742s
    Apr 17 22:18:58.105: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.978331191s
    Apr 17 22:18:59.109: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.974706727s
    Apr 17 22:19:00.112: INFO: Verifying statefulset ss doesn't scale past 1 for another 971.751261ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1673 04/17/23 22:19:01.112
    Apr 17 22:19:01.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 17 22:19:01.246: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 17 22:19:01.246: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 17 22:19:01.246: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 17 22:19:01.248: INFO: Found 1 stateful pods, waiting for 3
    Apr 17 22:19:11.254: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:19:11.254: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:19:11.254: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 04/17/23 22:19:11.255
    STEP: Scale down will halt with unhealthy stateful pod 04/17/23 22:19:11.255
    Apr 17 22:19:11.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 22:19:11.399: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 22:19:11.399: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 22:19:11.399: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 17 22:19:11.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 22:19:11.531: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 22:19:11.531: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 22:19:11.531: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 17 22:19:11.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 22:19:11.668: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 22:19:11.668: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 22:19:11.668: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 17 22:19:11.668: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 22:19:11.670: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Apr 17 22:19:21.676: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Apr 17 22:19:21.676: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Apr 17 22:19:21.676: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Apr 17 22:19:21.687: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999828s
    Apr 17 22:19:22.690: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997423961s
    Apr 17 22:19:23.693: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994347135s
    Apr 17 22:19:24.697: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990833235s
    Apr 17 22:19:25.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987609014s
    Apr 17 22:19:26.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984475897s
    Apr 17 22:19:27.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981533478s
    Apr 17 22:19:28.710: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977167947s
    Apr 17 22:19:29.713: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974297799s
    Apr 17 22:19:30.716: INFO: Verifying statefulset ss doesn't scale past 3 for another 971.37443ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1673 04/17/23 22:19:31.716
    Apr 17 22:19:31.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 17 22:19:31.872: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 17 22:19:31.872: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 17 22:19:31.872: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 17 22:19:31.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 17 22:19:32.010: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 17 22:19:32.010: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 17 22:19:32.010: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 17 22:19:32.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1673 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 17 22:19:32.136: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 17 22:19:32.136: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 17 22:19:32.136: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Apr 17 22:19:32.136: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 04/17/23 22:19:42.152
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 17 22:19:42.152: INFO: Deleting all statefulset in ns statefulset-1673
    Apr 17 22:19:42.154: INFO: Scaling statefulset ss to 0
    Apr 17 22:19:42.162: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 22:19:42.164: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:19:42.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1673" for this suite. 04/17/23 22:19:42.176
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:19:42.182
Apr 17 22:19:42.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 22:19:42.182
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:19:42.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:19:42.198
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-9482 04/17/23 22:19:42.2
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[] 04/17/23 22:19:42.306
Apr 17 22:19:42.311: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Apr 17 22:19:43.318: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-9482 04/17/23 22:19:43.318
Apr 17 22:19:43.324: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9482" to be "running and ready"
Apr 17 22:19:43.327: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.311987ms
Apr 17 22:19:43.327: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:19:45.331: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006234533s
Apr 17 22:19:45.331: INFO: The phase of Pod pod1 is Running (Ready = true)
Apr 17 22:19:45.331: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[pod1:[100]] 04/17/23 22:19:45.333
Apr 17 22:19:45.340: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-9482 04/17/23 22:19:45.34
Apr 17 22:19:45.344: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9482" to be "running and ready"
Apr 17 22:19:45.346: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.301307ms
Apr 17 22:19:45.346: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:19:47.350: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006106661s
Apr 17 22:19:47.350: INFO: The phase of Pod pod2 is Running (Ready = true)
Apr 17 22:19:47.350: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[pod1:[100] pod2:[101]] 04/17/23 22:19:47.353
Apr 17 22:19:47.362: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 04/17/23 22:19:47.362
Apr 17 22:19:47.362: INFO: Creating new exec pod
Apr 17 22:19:47.366: INFO: Waiting up to 5m0s for pod "execpodwc877" in namespace "services-9482" to be "running"
Apr 17 22:19:47.368: INFO: Pod "execpodwc877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340368ms
Apr 17 22:19:49.372: INFO: Pod "execpodwc877": Phase="Running", Reason="", readiness=true. Elapsed: 2.006188683s
Apr 17 22:19:49.372: INFO: Pod "execpodwc877" satisfied condition "running"
Apr 17 22:19:50.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9482 exec execpodwc877 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Apr 17 22:19:50.499: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Apr 17 22:19:50.499: INFO: stdout: ""
Apr 17 22:19:50.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9482 exec execpodwc877 -- /bin/sh -x -c nc -v -z -w 2 10.110.148.216 80'
Apr 17 22:19:50.645: INFO: stderr: "+ nc -v -z -w 2 10.110.148.216 80\nConnection to 10.110.148.216 80 port [tcp/http] succeeded!\n"
Apr 17 22:19:50.645: INFO: stdout: ""
Apr 17 22:19:50.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9482 exec execpodwc877 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Apr 17 22:19:50.749: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Apr 17 22:19:50.749: INFO: stdout: ""
Apr 17 22:19:50.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9482 exec execpodwc877 -- /bin/sh -x -c nc -v -z -w 2 10.110.148.216 81'
Apr 17 22:19:50.874: INFO: stderr: "+ nc -v -z -w 2 10.110.148.216 81\nConnection to 10.110.148.216 81 port [tcp/*] succeeded!\n"
Apr 17 22:19:50.874: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-9482 04/17/23 22:19:50.874
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[pod2:[101]] 04/17/23 22:19:50.89
Apr 17 22:19:50.899: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-9482 04/17/23 22:19:50.899
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[] 04/17/23 22:19:50.913
Apr 17 22:19:50.919: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 22:19:50.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9482" for this suite. 04/17/23 22:19:50.942
------------------------------
• [SLOW TEST] [8.765 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:19:42.182
    Apr 17 22:19:42.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 22:19:42.182
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:19:42.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:19:42.198
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-9482 04/17/23 22:19:42.2
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[] 04/17/23 22:19:42.306
    Apr 17 22:19:42.311: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Apr 17 22:19:43.318: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-9482 04/17/23 22:19:43.318
    Apr 17 22:19:43.324: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-9482" to be "running and ready"
    Apr 17 22:19:43.327: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.311987ms
    Apr 17 22:19:43.327: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:19:45.331: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006234533s
    Apr 17 22:19:45.331: INFO: The phase of Pod pod1 is Running (Ready = true)
    Apr 17 22:19:45.331: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[pod1:[100]] 04/17/23 22:19:45.333
    Apr 17 22:19:45.340: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-9482 04/17/23 22:19:45.34
    Apr 17 22:19:45.344: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-9482" to be "running and ready"
    Apr 17 22:19:45.346: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.301307ms
    Apr 17 22:19:45.346: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:19:47.350: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.006106661s
    Apr 17 22:19:47.350: INFO: The phase of Pod pod2 is Running (Ready = true)
    Apr 17 22:19:47.350: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[pod1:[100] pod2:[101]] 04/17/23 22:19:47.353
    Apr 17 22:19:47.362: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 04/17/23 22:19:47.362
    Apr 17 22:19:47.362: INFO: Creating new exec pod
    Apr 17 22:19:47.366: INFO: Waiting up to 5m0s for pod "execpodwc877" in namespace "services-9482" to be "running"
    Apr 17 22:19:47.368: INFO: Pod "execpodwc877": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340368ms
    Apr 17 22:19:49.372: INFO: Pod "execpodwc877": Phase="Running", Reason="", readiness=true. Elapsed: 2.006188683s
    Apr 17 22:19:49.372: INFO: Pod "execpodwc877" satisfied condition "running"
    Apr 17 22:19:50.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9482 exec execpodwc877 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Apr 17 22:19:50.499: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Apr 17 22:19:50.499: INFO: stdout: ""
    Apr 17 22:19:50.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9482 exec execpodwc877 -- /bin/sh -x -c nc -v -z -w 2 10.110.148.216 80'
    Apr 17 22:19:50.645: INFO: stderr: "+ nc -v -z -w 2 10.110.148.216 80\nConnection to 10.110.148.216 80 port [tcp/http] succeeded!\n"
    Apr 17 22:19:50.645: INFO: stdout: ""
    Apr 17 22:19:50.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9482 exec execpodwc877 -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Apr 17 22:19:50.749: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Apr 17 22:19:50.749: INFO: stdout: ""
    Apr 17 22:19:50.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-9482 exec execpodwc877 -- /bin/sh -x -c nc -v -z -w 2 10.110.148.216 81'
    Apr 17 22:19:50.874: INFO: stderr: "+ nc -v -z -w 2 10.110.148.216 81\nConnection to 10.110.148.216 81 port [tcp/*] succeeded!\n"
    Apr 17 22:19:50.874: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-9482 04/17/23 22:19:50.874
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[pod2:[101]] 04/17/23 22:19:50.89
    Apr 17 22:19:50.899: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-9482 04/17/23 22:19:50.899
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9482 to expose endpoints map[] 04/17/23 22:19:50.913
    Apr 17 22:19:50.919: INFO: successfully validated that service multi-endpoint-test in namespace services-9482 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:19:50.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9482" for this suite. 04/17/23 22:19:50.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:19:50.948
Apr 17 22:19:50.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename containers 04/17/23 22:19:50.948
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:19:50.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:19:50.965
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 04/17/23 22:19:50.967
Apr 17 22:19:50.974: INFO: Waiting up to 5m0s for pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f" in namespace "containers-4991" to be "Succeeded or Failed"
Apr 17 22:19:50.977: INFO: Pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.824161ms
Apr 17 22:19:52.981: INFO: Pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006166746s
Apr 17 22:19:54.982: INFO: Pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00737747s
STEP: Saw pod success 04/17/23 22:19:54.982
Apr 17 22:19:54.982: INFO: Pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f" satisfied condition "Succeeded or Failed"
Apr 17 22:19:54.984: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod client-containers-9451728e-086b-4897-be8d-2df6ce50031f container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:19:54.996
Apr 17 22:19:55.007: INFO: Waiting for pod client-containers-9451728e-086b-4897-be8d-2df6ce50031f to disappear
Apr 17 22:19:55.009: INFO: Pod client-containers-9451728e-086b-4897-be8d-2df6ce50031f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Apr 17 22:19:55.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-4991" for this suite. 04/17/23 22:19:55.013
------------------------------
• [4.072 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:19:50.948
    Apr 17 22:19:50.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename containers 04/17/23 22:19:50.948
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:19:50.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:19:50.965
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 04/17/23 22:19:50.967
    Apr 17 22:19:50.974: INFO: Waiting up to 5m0s for pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f" in namespace "containers-4991" to be "Succeeded or Failed"
    Apr 17 22:19:50.977: INFO: Pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.824161ms
    Apr 17 22:19:52.981: INFO: Pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006166746s
    Apr 17 22:19:54.982: INFO: Pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00737747s
    STEP: Saw pod success 04/17/23 22:19:54.982
    Apr 17 22:19:54.982: INFO: Pod "client-containers-9451728e-086b-4897-be8d-2df6ce50031f" satisfied condition "Succeeded or Failed"
    Apr 17 22:19:54.984: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod client-containers-9451728e-086b-4897-be8d-2df6ce50031f container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:19:54.996
    Apr 17 22:19:55.007: INFO: Waiting for pod client-containers-9451728e-086b-4897-be8d-2df6ce50031f to disappear
    Apr 17 22:19:55.009: INFO: Pod client-containers-9451728e-086b-4897-be8d-2df6ce50031f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:19:55.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-4991" for this suite. 04/17/23 22:19:55.013
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:19:55.019
Apr 17 22:19:55.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename subpath 04/17/23 22:19:55.02
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:19:55.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:19:55.034
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 04/17/23 22:19:55.036
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-x8p2 04/17/23 22:19:55.043
STEP: Creating a pod to test atomic-volume-subpath 04/17/23 22:19:55.043
Apr 17 22:19:55.049: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-x8p2" in namespace "subpath-4081" to be "Succeeded or Failed"
Apr 17 22:19:55.051: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127001ms
Apr 17 22:19:57.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005139262s
Apr 17 22:19:59.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 4.005511839s
Apr 17 22:20:01.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 6.005331892s
Apr 17 22:20:03.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 8.006009036s
Apr 17 22:20:05.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 10.005831932s
Apr 17 22:20:07.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 12.006691875s
Apr 17 22:20:09.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 14.00597115s
Apr 17 22:20:11.056: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 16.00696034s
Apr 17 22:20:13.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 18.005676262s
Apr 17 22:20:15.056: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 20.006945164s
Apr 17 22:20:17.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=false. Elapsed: 22.006690683s
Apr 17 22:20:19.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005650938s
STEP: Saw pod success 04/17/23 22:20:19.054
Apr 17 22:20:19.054: INFO: Pod "pod-subpath-test-projected-x8p2" satisfied condition "Succeeded or Failed"
Apr 17 22:20:19.057: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-subpath-test-projected-x8p2 container test-container-subpath-projected-x8p2: <nil>
STEP: delete the pod 04/17/23 22:20:19.064
Apr 17 22:20:19.074: INFO: Waiting for pod pod-subpath-test-projected-x8p2 to disappear
Apr 17 22:20:19.077: INFO: Pod pod-subpath-test-projected-x8p2 no longer exists
STEP: Deleting pod pod-subpath-test-projected-x8p2 04/17/23 22:20:19.077
Apr 17 22:20:19.077: INFO: Deleting pod "pod-subpath-test-projected-x8p2" in namespace "subpath-4081"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Apr 17 22:20:19.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-4081" for this suite. 04/17/23 22:20:19.083
------------------------------
• [SLOW TEST] [24.068 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:19:55.019
    Apr 17 22:19:55.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename subpath 04/17/23 22:19:55.02
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:19:55.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:19:55.034
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 04/17/23 22:19:55.036
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-x8p2 04/17/23 22:19:55.043
    STEP: Creating a pod to test atomic-volume-subpath 04/17/23 22:19:55.043
    Apr 17 22:19:55.049: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-x8p2" in namespace "subpath-4081" to be "Succeeded or Failed"
    Apr 17 22:19:55.051: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.127001ms
    Apr 17 22:19:57.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005139262s
    Apr 17 22:19:59.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 4.005511839s
    Apr 17 22:20:01.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 6.005331892s
    Apr 17 22:20:03.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 8.006009036s
    Apr 17 22:20:05.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 10.005831932s
    Apr 17 22:20:07.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 12.006691875s
    Apr 17 22:20:09.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 14.00597115s
    Apr 17 22:20:11.056: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 16.00696034s
    Apr 17 22:20:13.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 18.005676262s
    Apr 17 22:20:15.056: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=true. Elapsed: 20.006945164s
    Apr 17 22:20:17.055: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Running", Reason="", readiness=false. Elapsed: 22.006690683s
    Apr 17 22:20:19.054: INFO: Pod "pod-subpath-test-projected-x8p2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.005650938s
    STEP: Saw pod success 04/17/23 22:20:19.054
    Apr 17 22:20:19.054: INFO: Pod "pod-subpath-test-projected-x8p2" satisfied condition "Succeeded or Failed"
    Apr 17 22:20:19.057: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-subpath-test-projected-x8p2 container test-container-subpath-projected-x8p2: <nil>
    STEP: delete the pod 04/17/23 22:20:19.064
    Apr 17 22:20:19.074: INFO: Waiting for pod pod-subpath-test-projected-x8p2 to disappear
    Apr 17 22:20:19.077: INFO: Pod pod-subpath-test-projected-x8p2 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-x8p2 04/17/23 22:20:19.077
    Apr 17 22:20:19.077: INFO: Deleting pod "pod-subpath-test-projected-x8p2" in namespace "subpath-4081"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:20:19.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-4081" for this suite. 04/17/23 22:20:19.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:20:19.089
Apr 17 22:20:19.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename podtemplate 04/17/23 22:20:19.09
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:20:19.104
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:20:19.107
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 04/17/23 22:20:19.109
Apr 17 22:20:19.112: INFO: created test-podtemplate-1
Apr 17 22:20:19.118: INFO: created test-podtemplate-2
Apr 17 22:20:19.121: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 04/17/23 22:20:19.121
STEP: delete collection of pod templates 04/17/23 22:20:19.124
Apr 17 22:20:19.124: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 04/17/23 22:20:19.136
Apr 17 22:20:19.136: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Apr 17 22:20:19.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7124" for this suite. 04/17/23 22:20:19.142
------------------------------
• [0.058 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:20:19.089
    Apr 17 22:20:19.089: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename podtemplate 04/17/23 22:20:19.09
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:20:19.104
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:20:19.107
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 04/17/23 22:20:19.109
    Apr 17 22:20:19.112: INFO: created test-podtemplate-1
    Apr 17 22:20:19.118: INFO: created test-podtemplate-2
    Apr 17 22:20:19.121: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 04/17/23 22:20:19.121
    STEP: delete collection of pod templates 04/17/23 22:20:19.124
    Apr 17 22:20:19.124: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 04/17/23 22:20:19.136
    Apr 17 22:20:19.136: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:20:19.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7124" for this suite. 04/17/23 22:20:19.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:20:19.147
Apr 17 22:20:19.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename gc 04/17/23 22:20:19.148
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:20:19.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:20:19.161
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 04/17/23 22:20:19.163
STEP: Wait for the Deployment to create new ReplicaSet 04/17/23 22:20:19.167
STEP: delete the deployment 04/17/23 22:20:19.675
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 04/17/23 22:20:19.679
STEP: Gathering metrics 04/17/23 22:20:20.194
Apr 17 22:20:20.214: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
Apr 17 22:20:20.217: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.68939ms
Apr 17 22:20:20.217: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
Apr 17 22:20:20.217: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
Apr 17 22:20:20.278: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 17 22:20:20.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-6876" for this suite. 04/17/23 22:20:20.282
------------------------------
• [1.140 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:20:19.147
    Apr 17 22:20:19.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename gc 04/17/23 22:20:19.148
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:20:19.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:20:19.161
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 04/17/23 22:20:19.163
    STEP: Wait for the Deployment to create new ReplicaSet 04/17/23 22:20:19.167
    STEP: delete the deployment 04/17/23 22:20:19.675
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 04/17/23 22:20:19.679
    STEP: Gathering metrics 04/17/23 22:20:20.194
    Apr 17 22:20:20.214: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Apr 17 22:20:20.217: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.68939ms
    Apr 17 22:20:20.217: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
    Apr 17 22:20:20.217: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
    Apr 17 22:20:20.278: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:20:20.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-6876" for this suite. 04/17/23 22:20:20.282
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:20:20.289
Apr 17 22:20:20.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename podtemplate 04/17/23 22:20:20.289
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:20:20.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:20:20.304
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Apr 17 22:20:20.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-5299" for this suite. 04/17/23 22:20:20.332
------------------------------
• [0.047 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:20:20.289
    Apr 17 22:20:20.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename podtemplate 04/17/23 22:20:20.289
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:20:20.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:20:20.304
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:20:20.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-5299" for this suite. 04/17/23 22:20:20.332
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:20:20.336
Apr 17 22:20:20.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-probe 04/17/23 22:20:20.337
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:20:20.349
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:20:20.352
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5 in namespace container-probe-3381 04/17/23 22:20:20.354
Apr 17 22:20:20.359: INFO: Waiting up to 5m0s for pod "busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5" in namespace "container-probe-3381" to be "not pending"
Apr 17 22:20:20.361: INFO: Pod "busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140319ms
Apr 17 22:20:22.365: INFO: Pod "busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005782074s
Apr 17 22:20:22.365: INFO: Pod "busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5" satisfied condition "not pending"
Apr 17 22:20:22.365: INFO: Started pod busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5 in namespace container-probe-3381
STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 22:20:22.365
Apr 17 22:20:22.367: INFO: Initial restart count of pod busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5 is 0
Apr 17 22:21:12.460: INFO: Restart count of pod container-probe-3381/busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5 is now 1 (50.093115692s elapsed)
STEP: deleting the pod 04/17/23 22:21:12.46
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 17 22:21:12.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-3381" for this suite. 04/17/23 22:21:12.477
------------------------------
• [SLOW TEST] [52.145 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:20:20.336
    Apr 17 22:20:20.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-probe 04/17/23 22:20:20.337
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:20:20.349
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:20:20.352
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5 in namespace container-probe-3381 04/17/23 22:20:20.354
    Apr 17 22:20:20.359: INFO: Waiting up to 5m0s for pod "busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5" in namespace "container-probe-3381" to be "not pending"
    Apr 17 22:20:20.361: INFO: Pod "busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.140319ms
    Apr 17 22:20:22.365: INFO: Pod "busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.005782074s
    Apr 17 22:20:22.365: INFO: Pod "busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5" satisfied condition "not pending"
    Apr 17 22:20:22.365: INFO: Started pod busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5 in namespace container-probe-3381
    STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 22:20:22.365
    Apr 17 22:20:22.367: INFO: Initial restart count of pod busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5 is 0
    Apr 17 22:21:12.460: INFO: Restart count of pod container-probe-3381/busybox-1352888a-40a0-49aa-a6b0-472e69f98bc5 is now 1 (50.093115692s elapsed)
    STEP: deleting the pod 04/17/23 22:21:12.46
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:21:12.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-3381" for this suite. 04/17/23 22:21:12.477
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:21:12.482
Apr 17 22:21:12.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:21:12.483
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:12.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:12.497
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:21:12.509
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:21:12.768
STEP: Deploying the webhook pod 04/17/23 22:21:12.774
STEP: Wait for the deployment to be ready 04/17/23 22:21:12.785
Apr 17 22:21:12.791: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:21:14.8
STEP: Verifying the service has paired with the endpoint 04/17/23 22:21:14.811
Apr 17 22:21:15.811: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 04/17/23 22:21:15.814
STEP: create a pod that should be denied by the webhook 04/17/23 22:21:15.827
STEP: create a pod that causes the webhook to hang 04/17/23 22:21:15.837
STEP: create a configmap that should be denied by the webhook 04/17/23 22:21:25.842
STEP: create a configmap that should be admitted by the webhook 04/17/23 22:21:25.859
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 04/17/23 22:21:25.866
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 04/17/23 22:21:25.872
STEP: create a namespace that bypass the webhook 04/17/23 22:21:25.876
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 04/17/23 22:21:25.881
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:21:25.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7756" for this suite. 04/17/23 22:21:25.942
STEP: Destroying namespace "webhook-7756-markers" for this suite. 04/17/23 22:21:25.947
------------------------------
• [SLOW TEST] [13.473 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:21:12.482
    Apr 17 22:21:12.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:21:12.483
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:12.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:12.497
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:21:12.509
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:21:12.768
    STEP: Deploying the webhook pod 04/17/23 22:21:12.774
    STEP: Wait for the deployment to be ready 04/17/23 22:21:12.785
    Apr 17 22:21:12.791: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:21:14.8
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:21:14.811
    Apr 17 22:21:15.811: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 04/17/23 22:21:15.814
    STEP: create a pod that should be denied by the webhook 04/17/23 22:21:15.827
    STEP: create a pod that causes the webhook to hang 04/17/23 22:21:15.837
    STEP: create a configmap that should be denied by the webhook 04/17/23 22:21:25.842
    STEP: create a configmap that should be admitted by the webhook 04/17/23 22:21:25.859
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 04/17/23 22:21:25.866
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 04/17/23 22:21:25.872
    STEP: create a namespace that bypass the webhook 04/17/23 22:21:25.876
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 04/17/23 22:21:25.881
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:21:25.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7756" for this suite. 04/17/23 22:21:25.942
    STEP: Destroying namespace "webhook-7756-markers" for this suite. 04/17/23 22:21:25.947
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:21:25.956
Apr 17 22:21:25.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 22:21:25.956
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:25.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:25.974
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-92bf0643-2a68-48c0-b266-4096f014778c 04/17/23 22:21:25.976
STEP: Creating a pod to test consume secrets 04/17/23 22:21:25.979
Apr 17 22:21:25.987: INFO: Waiting up to 5m0s for pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d" in namespace "secrets-1595" to be "Succeeded or Failed"
Apr 17 22:21:25.989: INFO: Pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248743ms
Apr 17 22:21:27.992: INFO: Pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d": Phase="Running", Reason="", readiness=false. Elapsed: 2.005252058s
Apr 17 22:21:29.993: INFO: Pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005689903s
STEP: Saw pod success 04/17/23 22:21:29.993
Apr 17 22:21:29.993: INFO: Pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d" satisfied condition "Succeeded or Failed"
Apr 17 22:21:29.995: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d container secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:21:30
Apr 17 22:21:30.008: INFO: Waiting for pod pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d to disappear
Apr 17 22:21:30.011: INFO: Pod pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 22:21:30.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1595" for this suite. 04/17/23 22:21:30.015
------------------------------
• [4.064 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:21:25.956
    Apr 17 22:21:25.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 22:21:25.956
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:25.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:25.974
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-92bf0643-2a68-48c0-b266-4096f014778c 04/17/23 22:21:25.976
    STEP: Creating a pod to test consume secrets 04/17/23 22:21:25.979
    Apr 17 22:21:25.987: INFO: Waiting up to 5m0s for pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d" in namespace "secrets-1595" to be "Succeeded or Failed"
    Apr 17 22:21:25.989: INFO: Pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.248743ms
    Apr 17 22:21:27.992: INFO: Pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d": Phase="Running", Reason="", readiness=false. Elapsed: 2.005252058s
    Apr 17 22:21:29.993: INFO: Pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005689903s
    STEP: Saw pod success 04/17/23 22:21:29.993
    Apr 17 22:21:29.993: INFO: Pod "pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d" satisfied condition "Succeeded or Failed"
    Apr 17 22:21:29.995: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d container secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:21:30
    Apr 17 22:21:30.008: INFO: Waiting for pod pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d to disappear
    Apr 17 22:21:30.011: INFO: Pod pod-secrets-afd57dbc-d8bf-4302-9e87-796f607cb55d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:21:30.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1595" for this suite. 04/17/23 22:21:30.015
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:21:30.02
Apr 17 22:21:30.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:21:30.021
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:30.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:30.037
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 04/17/23 22:21:30.039
Apr 17 22:21:30.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 create -f -'
Apr 17 22:21:32.208: INFO: stderr: ""
Apr 17 22:21:32.209: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 04/17/23 22:21:32.209
Apr 17 22:21:32.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 17 22:21:32.268: INFO: stderr: ""
Apr 17 22:21:32.268: INFO: stdout: "update-demo-nautilus-g9s26 update-demo-nautilus-xl9kl "
Apr 17 22:21:32.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-g9s26 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:21:32.326: INFO: stderr: ""
Apr 17 22:21:32.326: INFO: stdout: ""
Apr 17 22:21:32.326: INFO: update-demo-nautilus-g9s26 is created but not running
Apr 17 22:21:37.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Apr 17 22:21:37.386: INFO: stderr: ""
Apr 17 22:21:37.386: INFO: stdout: "update-demo-nautilus-g9s26 update-demo-nautilus-xl9kl "
Apr 17 22:21:37.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-g9s26 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:21:37.442: INFO: stderr: ""
Apr 17 22:21:37.442: INFO: stdout: "true"
Apr 17 22:21:37.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-g9s26 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 17 22:21:37.499: INFO: stderr: ""
Apr 17 22:21:37.499: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 17 22:21:37.499: INFO: validating pod update-demo-nautilus-g9s26
Apr 17 22:21:37.503: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 22:21:37.503: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 22:21:37.503: INFO: update-demo-nautilus-g9s26 is verified up and running
Apr 17 22:21:37.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-xl9kl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Apr 17 22:21:37.560: INFO: stderr: ""
Apr 17 22:21:37.560: INFO: stdout: "true"
Apr 17 22:21:37.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-xl9kl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Apr 17 22:21:37.615: INFO: stderr: ""
Apr 17 22:21:37.615: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Apr 17 22:21:37.615: INFO: validating pod update-demo-nautilus-xl9kl
Apr 17 22:21:37.619: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr 17 22:21:37.619: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr 17 22:21:37.619: INFO: update-demo-nautilus-xl9kl is verified up and running
STEP: using delete to clean up resources 04/17/23 22:21:37.619
Apr 17 22:21:37.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 delete --grace-period=0 --force -f -'
Apr 17 22:21:37.678: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr 17 22:21:37.678: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr 17 22:21:37.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get rc,svc -l name=update-demo --no-headers'
Apr 17 22:21:37.742: INFO: stderr: "No resources found in kubectl-4548 namespace.\n"
Apr 17 22:21:37.742: INFO: stdout: ""
Apr 17 22:21:37.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr 17 22:21:37.803: INFO: stderr: ""
Apr 17 22:21:37.803: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:21:37.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4548" for this suite. 04/17/23 22:21:37.808
------------------------------
• [SLOW TEST] [7.793 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:21:30.02
    Apr 17 22:21:30.020: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:21:30.021
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:30.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:30.037
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 04/17/23 22:21:30.039
    Apr 17 22:21:30.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 create -f -'
    Apr 17 22:21:32.208: INFO: stderr: ""
    Apr 17 22:21:32.209: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 04/17/23 22:21:32.209
    Apr 17 22:21:32.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 17 22:21:32.268: INFO: stderr: ""
    Apr 17 22:21:32.268: INFO: stdout: "update-demo-nautilus-g9s26 update-demo-nautilus-xl9kl "
    Apr 17 22:21:32.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-g9s26 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:21:32.326: INFO: stderr: ""
    Apr 17 22:21:32.326: INFO: stdout: ""
    Apr 17 22:21:32.326: INFO: update-demo-nautilus-g9s26 is created but not running
    Apr 17 22:21:37.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Apr 17 22:21:37.386: INFO: stderr: ""
    Apr 17 22:21:37.386: INFO: stdout: "update-demo-nautilus-g9s26 update-demo-nautilus-xl9kl "
    Apr 17 22:21:37.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-g9s26 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:21:37.442: INFO: stderr: ""
    Apr 17 22:21:37.442: INFO: stdout: "true"
    Apr 17 22:21:37.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-g9s26 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 17 22:21:37.499: INFO: stderr: ""
    Apr 17 22:21:37.499: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 17 22:21:37.499: INFO: validating pod update-demo-nautilus-g9s26
    Apr 17 22:21:37.503: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 17 22:21:37.503: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 17 22:21:37.503: INFO: update-demo-nautilus-g9s26 is verified up and running
    Apr 17 22:21:37.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-xl9kl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Apr 17 22:21:37.560: INFO: stderr: ""
    Apr 17 22:21:37.560: INFO: stdout: "true"
    Apr 17 22:21:37.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods update-demo-nautilus-xl9kl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Apr 17 22:21:37.615: INFO: stderr: ""
    Apr 17 22:21:37.615: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Apr 17 22:21:37.615: INFO: validating pod update-demo-nautilus-xl9kl
    Apr 17 22:21:37.619: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Apr 17 22:21:37.619: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Apr 17 22:21:37.619: INFO: update-demo-nautilus-xl9kl is verified up and running
    STEP: using delete to clean up resources 04/17/23 22:21:37.619
    Apr 17 22:21:37.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 delete --grace-period=0 --force -f -'
    Apr 17 22:21:37.678: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Apr 17 22:21:37.678: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Apr 17 22:21:37.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get rc,svc -l name=update-demo --no-headers'
    Apr 17 22:21:37.742: INFO: stderr: "No resources found in kubectl-4548 namespace.\n"
    Apr 17 22:21:37.742: INFO: stdout: ""
    Apr 17 22:21:37.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4548 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Apr 17 22:21:37.803: INFO: stderr: ""
    Apr 17 22:21:37.803: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:21:37.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4548" for this suite. 04/17/23 22:21:37.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:21:37.814
Apr 17 22:21:37.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename limitrange 04/17/23 22:21:37.814
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:37.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:37.829
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 04/17/23 22:21:37.831
STEP: Setting up watch 04/17/23 22:21:37.831
STEP: Submitting a LimitRange 04/17/23 22:21:37.935
STEP: Verifying LimitRange creation was observed 04/17/23 22:21:37.94
STEP: Fetching the LimitRange to ensure it has proper values 04/17/23 22:21:37.94
Apr 17 22:21:37.942: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 17 22:21:37.942: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 04/17/23 22:21:37.942
STEP: Ensuring Pod has resource requirements applied from LimitRange 04/17/23 22:21:37.946
Apr 17 22:21:37.948: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Apr 17 22:21:37.948: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 04/17/23 22:21:37.948
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 04/17/23 22:21:37.955
Apr 17 22:21:37.957: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Apr 17 22:21:37.957: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 04/17/23 22:21:37.957
STEP: Failing to create a Pod with more than max resources 04/17/23 22:21:37.959
STEP: Updating a LimitRange 04/17/23 22:21:37.96
STEP: Verifying LimitRange updating is effective 04/17/23 22:21:37.965
STEP: Creating a Pod with less than former min resources 04/17/23 22:21:39.969
STEP: Failing to create a Pod with more than max resources 04/17/23 22:21:39.974
STEP: Deleting a LimitRange 04/17/23 22:21:39.977
STEP: Verifying the LimitRange was deleted 04/17/23 22:21:39.982
Apr 17 22:21:44.987: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 04/17/23 22:21:44.987
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Apr 17 22:21:44.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-6380" for this suite. 04/17/23 22:21:44.997
------------------------------
• [SLOW TEST] [7.189 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:21:37.814
    Apr 17 22:21:37.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename limitrange 04/17/23 22:21:37.814
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:37.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:37.829
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 04/17/23 22:21:37.831
    STEP: Setting up watch 04/17/23 22:21:37.831
    STEP: Submitting a LimitRange 04/17/23 22:21:37.935
    STEP: Verifying LimitRange creation was observed 04/17/23 22:21:37.94
    STEP: Fetching the LimitRange to ensure it has proper values 04/17/23 22:21:37.94
    Apr 17 22:21:37.942: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Apr 17 22:21:37.942: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 04/17/23 22:21:37.942
    STEP: Ensuring Pod has resource requirements applied from LimitRange 04/17/23 22:21:37.946
    Apr 17 22:21:37.948: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Apr 17 22:21:37.948: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 04/17/23 22:21:37.948
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 04/17/23 22:21:37.955
    Apr 17 22:21:37.957: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Apr 17 22:21:37.957: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 04/17/23 22:21:37.957
    STEP: Failing to create a Pod with more than max resources 04/17/23 22:21:37.959
    STEP: Updating a LimitRange 04/17/23 22:21:37.96
    STEP: Verifying LimitRange updating is effective 04/17/23 22:21:37.965
    STEP: Creating a Pod with less than former min resources 04/17/23 22:21:39.969
    STEP: Failing to create a Pod with more than max resources 04/17/23 22:21:39.974
    STEP: Deleting a LimitRange 04/17/23 22:21:39.977
    STEP: Verifying the LimitRange was deleted 04/17/23 22:21:39.982
    Apr 17 22:21:44.987: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 04/17/23 22:21:44.987
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:21:44.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-6380" for this suite. 04/17/23 22:21:44.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:21:45.003
Apr 17 22:21:45.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 22:21:45.004
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:45.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:45.089
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 04/17/23 22:21:45.091
STEP: Creating a ResourceQuota 04/17/23 22:21:50.095
STEP: Ensuring resource quota status is calculated 04/17/23 22:21:50.099
STEP: Creating a ReplicaSet 04/17/23 22:21:52.102
STEP: Ensuring resource quota status captures replicaset creation 04/17/23 22:21:52.112
STEP: Deleting a ReplicaSet 04/17/23 22:21:54.116
STEP: Ensuring resource quota status released usage 04/17/23 22:21:54.12
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 22:21:56.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-70" for this suite. 04/17/23 22:21:56.127
------------------------------
• [SLOW TEST] [11.129 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:21:45.003
    Apr 17 22:21:45.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 22:21:45.004
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:45.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:45.089
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 04/17/23 22:21:45.091
    STEP: Creating a ResourceQuota 04/17/23 22:21:50.095
    STEP: Ensuring resource quota status is calculated 04/17/23 22:21:50.099
    STEP: Creating a ReplicaSet 04/17/23 22:21:52.102
    STEP: Ensuring resource quota status captures replicaset creation 04/17/23 22:21:52.112
    STEP: Deleting a ReplicaSet 04/17/23 22:21:54.116
    STEP: Ensuring resource quota status released usage 04/17/23 22:21:54.12
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:21:56.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-70" for this suite. 04/17/23 22:21:56.127
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:21:56.132
Apr 17 22:21:56.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir-wrapper 04/17/23 22:21:56.133
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:56.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:56.148
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Apr 17 22:21:56.164: INFO: Waiting up to 5m0s for pod "pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f" in namespace "emptydir-wrapper-1967" to be "running and ready"
Apr 17 22:21:56.166: INFO: Pod "pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144738ms
Apr 17 22:21:56.166: INFO: The phase of Pod pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:21:58.169: INFO: Pod "pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005020762s
Apr 17 22:21:58.169: INFO: The phase of Pod pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f is Running (Ready = true)
Apr 17 22:21:58.169: INFO: Pod "pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f" satisfied condition "running and ready"
STEP: Cleaning up the secret 04/17/23 22:21:58.172
STEP: Cleaning up the configmap 04/17/23 22:21:58.178
STEP: Cleaning up the pod 04/17/23 22:21:58.182
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:21:58.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1967" for this suite. 04/17/23 22:21:58.196
------------------------------
• [2.068 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:21:56.132
    Apr 17 22:21:56.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir-wrapper 04/17/23 22:21:56.133
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:56.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:56.148
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Apr 17 22:21:56.164: INFO: Waiting up to 5m0s for pod "pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f" in namespace "emptydir-wrapper-1967" to be "running and ready"
    Apr 17 22:21:56.166: INFO: Pod "pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.144738ms
    Apr 17 22:21:56.166: INFO: The phase of Pod pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:21:58.169: INFO: Pod "pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005020762s
    Apr 17 22:21:58.169: INFO: The phase of Pod pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f is Running (Ready = true)
    Apr 17 22:21:58.169: INFO: Pod "pod-secrets-058f6b49-bba1-4cad-81c6-1a4ca14ff60f" satisfied condition "running and ready"
    STEP: Cleaning up the secret 04/17/23 22:21:58.172
    STEP: Cleaning up the configmap 04/17/23 22:21:58.178
    STEP: Cleaning up the pod 04/17/23 22:21:58.182
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:21:58.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1967" for this suite. 04/17/23 22:21:58.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:21:58.201
Apr 17 22:21:58.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:21:58.202
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:58.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:58.216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 04/17/23 22:21:58.218
Apr 17 22:21:58.224: INFO: Waiting up to 5m0s for pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8" in namespace "downward-api-798" to be "Succeeded or Failed"
Apr 17 22:21:58.227: INFO: Pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.458716ms
Apr 17 22:22:00.230: INFO: Pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005701554s
Apr 17 22:22:02.231: INFO: Pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006788536s
STEP: Saw pod success 04/17/23 22:22:02.231
Apr 17 22:22:02.231: INFO: Pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8" satisfied condition "Succeeded or Failed"
Apr 17 22:22:02.233: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downward-api-244211a3-23f6-4210-83f8-733a103bb1f8 container dapi-container: <nil>
STEP: delete the pod 04/17/23 22:22:02.239
Apr 17 22:22:02.250: INFO: Waiting for pod downward-api-244211a3-23f6-4210-83f8-733a103bb1f8 to disappear
Apr 17 22:22:02.252: INFO: Pod downward-api-244211a3-23f6-4210-83f8-733a103bb1f8 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:02.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-798" for this suite. 04/17/23 22:22:02.256
------------------------------
• [4.060 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:21:58.201
    Apr 17 22:21:58.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:21:58.202
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:21:58.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:21:58.216
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 04/17/23 22:21:58.218
    Apr 17 22:21:58.224: INFO: Waiting up to 5m0s for pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8" in namespace "downward-api-798" to be "Succeeded or Failed"
    Apr 17 22:21:58.227: INFO: Pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.458716ms
    Apr 17 22:22:00.230: INFO: Pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005701554s
    Apr 17 22:22:02.231: INFO: Pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006788536s
    STEP: Saw pod success 04/17/23 22:22:02.231
    Apr 17 22:22:02.231: INFO: Pod "downward-api-244211a3-23f6-4210-83f8-733a103bb1f8" satisfied condition "Succeeded or Failed"
    Apr 17 22:22:02.233: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downward-api-244211a3-23f6-4210-83f8-733a103bb1f8 container dapi-container: <nil>
    STEP: delete the pod 04/17/23 22:22:02.239
    Apr 17 22:22:02.250: INFO: Waiting for pod downward-api-244211a3-23f6-4210-83f8-733a103bb1f8 to disappear
    Apr 17 22:22:02.252: INFO: Pod downward-api-244211a3-23f6-4210-83f8-733a103bb1f8 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:02.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-798" for this suite. 04/17/23 22:22:02.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:02.262
Apr 17 22:22:02.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 22:22:02.262
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:02.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:02.278
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-5334/secret-test-51e37cc8-0a65-4455-a459-57f45bc51cb6 04/17/23 22:22:02.28
STEP: Creating a pod to test consume secrets 04/17/23 22:22:02.284
Apr 17 22:22:02.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba" in namespace "secrets-5334" to be "Succeeded or Failed"
Apr 17 22:22:02.291: INFO: Pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.266247ms
Apr 17 22:22:04.295: INFO: Pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005418815s
Apr 17 22:22:06.295: INFO: Pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005372919s
STEP: Saw pod success 04/17/23 22:22:06.295
Apr 17 22:22:06.295: INFO: Pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba" satisfied condition "Succeeded or Failed"
Apr 17 22:22:06.297: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba container env-test: <nil>
STEP: delete the pod 04/17/23 22:22:06.307
Apr 17 22:22:06.318: INFO: Waiting for pod pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba to disappear
Apr 17 22:22:06.320: INFO: Pod pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:06.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5334" for this suite. 04/17/23 22:22:06.324
------------------------------
• [4.067 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:02.262
    Apr 17 22:22:02.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 22:22:02.262
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:02.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:02.278
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-5334/secret-test-51e37cc8-0a65-4455-a459-57f45bc51cb6 04/17/23 22:22:02.28
    STEP: Creating a pod to test consume secrets 04/17/23 22:22:02.284
    Apr 17 22:22:02.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba" in namespace "secrets-5334" to be "Succeeded or Failed"
    Apr 17 22:22:02.291: INFO: Pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.266247ms
    Apr 17 22:22:04.295: INFO: Pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005418815s
    Apr 17 22:22:06.295: INFO: Pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005372919s
    STEP: Saw pod success 04/17/23 22:22:06.295
    Apr 17 22:22:06.295: INFO: Pod "pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba" satisfied condition "Succeeded or Failed"
    Apr 17 22:22:06.297: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba container env-test: <nil>
    STEP: delete the pod 04/17/23 22:22:06.307
    Apr 17 22:22:06.318: INFO: Waiting for pod pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba to disappear
    Apr 17 22:22:06.320: INFO: Pod pod-configmaps-5bbb726b-782b-4058-ae15-7bc6057de8ba no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:06.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5334" for this suite. 04/17/23 22:22:06.324
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:06.329
Apr 17 22:22:06.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replication-controller 04/17/23 22:22:06.33
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:06.345
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:06.347
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Apr 17 22:22:06.349: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 04/17/23 22:22:07.357
STEP: Checking rc "condition-test" has the desired failure condition set 04/17/23 22:22:07.364
STEP: Scaling down rc "condition-test" to satisfy pod quota 04/17/23 22:22:08.37
Apr 17 22:22:08.378: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 04/17/23 22:22:08.378
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:09.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-651" for this suite. 04/17/23 22:22:09.388
------------------------------
• [3.063 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:06.329
    Apr 17 22:22:06.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replication-controller 04/17/23 22:22:06.33
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:06.345
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:06.347
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Apr 17 22:22:06.349: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 04/17/23 22:22:07.357
    STEP: Checking rc "condition-test" has the desired failure condition set 04/17/23 22:22:07.364
    STEP: Scaling down rc "condition-test" to satisfy pod quota 04/17/23 22:22:08.37
    Apr 17 22:22:08.378: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 04/17/23 22:22:08.378
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:09.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-651" for this suite. 04/17/23 22:22:09.388
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:09.393
Apr 17 22:22:09.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:22:09.394
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:09.407
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:09.409
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:22:09.42
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:22:10.022
STEP: Deploying the webhook pod 04/17/23 22:22:10.03
STEP: Wait for the deployment to be ready 04/17/23 22:22:10.041
Apr 17 22:22:10.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:22:12.164
STEP: Verifying the service has paired with the endpoint 04/17/23 22:22:12.175
Apr 17 22:22:13.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Apr 17 22:22:13.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4175-crds.webhook.example.com via the AdmissionRegistration API 04/17/23 22:22:13.688
STEP: Creating a custom resource while v1 is storage version 04/17/23 22:22:13.7
STEP: Patching Custom Resource Definition to set v2 as storage 04/17/23 22:22:15.755
STEP: Patching the custom resource while v2 is storage version 04/17/23 22:22:15.77
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:16.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9665" for this suite. 04/17/23 22:22:16.372
STEP: Destroying namespace "webhook-9665-markers" for this suite. 04/17/23 22:22:16.376
------------------------------
• [SLOW TEST] [6.991 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:09.393
    Apr 17 22:22:09.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:22:09.394
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:09.407
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:09.409
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:22:09.42
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:22:10.022
    STEP: Deploying the webhook pod 04/17/23 22:22:10.03
    STEP: Wait for the deployment to be ready 04/17/23 22:22:10.041
    Apr 17 22:22:10.156: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:22:12.164
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:22:12.175
    Apr 17 22:22:13.176: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Apr 17 22:22:13.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4175-crds.webhook.example.com via the AdmissionRegistration API 04/17/23 22:22:13.688
    STEP: Creating a custom resource while v1 is storage version 04/17/23 22:22:13.7
    STEP: Patching Custom Resource Definition to set v2 as storage 04/17/23 22:22:15.755
    STEP: Patching the custom resource while v2 is storage version 04/17/23 22:22:15.77
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:16.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9665" for this suite. 04/17/23 22:22:16.372
    STEP: Destroying namespace "webhook-9665-markers" for this suite. 04/17/23 22:22:16.376
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:16.385
Apr 17 22:22:16.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 22:22:16.386
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:16.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:16.41
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 04/17/23 22:22:16.412
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 04/17/23 22:22:16.413
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 04/17/23 22:22:16.413
STEP: fetching the /apis/apiextensions.k8s.io discovery document 04/17/23 22:22:16.413
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 04/17/23 22:22:16.414
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 04/17/23 22:22:16.414
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 04/17/23 22:22:16.415
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:16.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8950" for this suite. 04/17/23 22:22:16.42
------------------------------
• [0.041 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:16.385
    Apr 17 22:22:16.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 22:22:16.386
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:16.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:16.41
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 04/17/23 22:22:16.412
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 04/17/23 22:22:16.413
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 04/17/23 22:22:16.413
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 04/17/23 22:22:16.413
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 04/17/23 22:22:16.414
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 04/17/23 22:22:16.414
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 04/17/23 22:22:16.415
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:16.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8950" for this suite. 04/17/23 22:22:16.42
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:16.426
Apr 17 22:22:16.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename svc-latency 04/17/23 22:22:16.427
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:16.441
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:16.443
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Apr 17 22:22:16.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: creating replication controller svc-latency-rc in namespace svc-latency-2426 04/17/23 22:22:16.446
I0417 22:22:16.451109      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2426, replica count: 1
I0417 22:22:17.502607      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0417 22:22:18.503524      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 22:22:18.758: INFO: Created: latency-svc-t6v5n
Apr 17 22:22:18.762: INFO: Got endpoints: latency-svc-t6v5n [158.300904ms]
Apr 17 22:22:18.779: INFO: Created: latency-svc-bll6d
Apr 17 22:22:18.785: INFO: Got endpoints: latency-svc-bll6d [22.976132ms]
Apr 17 22:22:18.790: INFO: Created: latency-svc-c7d2l
Apr 17 22:22:18.797: INFO: Got endpoints: latency-svc-c7d2l [34.875317ms]
Apr 17 22:22:18.910: INFO: Created: latency-svc-gh472
Apr 17 22:22:18.914: INFO: Got endpoints: latency-svc-gh472 [151.938052ms]
Apr 17 22:22:18.916: INFO: Created: latency-svc-xc522
Apr 17 22:22:18.923: INFO: Got endpoints: latency-svc-xc522 [160.317884ms]
Apr 17 22:22:18.927: INFO: Created: latency-svc-2fc5x
Apr 17 22:22:18.933: INFO: Got endpoints: latency-svc-2fc5x [170.413765ms]
Apr 17 22:22:18.938: INFO: Created: latency-svc-jn25f
Apr 17 22:22:18.944: INFO: Got endpoints: latency-svc-jn25f [181.810272ms]
Apr 17 22:22:18.948: INFO: Created: latency-svc-vrhcg
Apr 17 22:22:19.044: INFO: Got endpoints: latency-svc-vrhcg [281.479191ms]
Apr 17 22:22:19.052: INFO: Created: latency-svc-lnmz8
Apr 17 22:22:19.059: INFO: Got endpoints: latency-svc-lnmz8 [296.244507ms]
Apr 17 22:22:19.062: INFO: Created: latency-svc-g2nxw
Apr 17 22:22:19.065: INFO: Got endpoints: latency-svc-g2nxw [302.452421ms]
Apr 17 22:22:19.073: INFO: Created: latency-svc-8vz44
Apr 17 22:22:19.076: INFO: Got endpoints: latency-svc-8vz44 [313.435261ms]
Apr 17 22:22:19.163: INFO: Created: latency-svc-j2cjv
Apr 17 22:22:19.169: INFO: Got endpoints: latency-svc-j2cjv [406.700574ms]
Apr 17 22:22:19.174: INFO: Created: latency-svc-w4r72
Apr 17 22:22:19.184: INFO: Got endpoints: latency-svc-w4r72 [421.201379ms]
Apr 17 22:22:19.192: INFO: Created: latency-svc-2b8zb
Apr 17 22:22:19.200: INFO: Got endpoints: latency-svc-2b8zb [437.107106ms]
Apr 17 22:22:19.205: INFO: Created: latency-svc-nsfxr
Apr 17 22:22:19.319: INFO: Got endpoints: latency-svc-nsfxr [556.176944ms]
Apr 17 22:22:19.321: INFO: Created: latency-svc-672rf
Apr 17 22:22:19.328: INFO: Got endpoints: latency-svc-672rf [565.420558ms]
Apr 17 22:22:19.332: INFO: Created: latency-svc-vrf8w
Apr 17 22:22:19.338: INFO: Got endpoints: latency-svc-vrf8w [552.874188ms]
Apr 17 22:22:19.343: INFO: Created: latency-svc-x5zlz
Apr 17 22:22:19.347: INFO: Got endpoints: latency-svc-x5zlz [549.574889ms]
Apr 17 22:22:19.355: INFO: Created: latency-svc-z2mhj
Apr 17 22:22:19.445: INFO: Got endpoints: latency-svc-z2mhj [531.032837ms]
Apr 17 22:22:19.449: INFO: Created: latency-svc-sghhn
Apr 17 22:22:19.454: INFO: Got endpoints: latency-svc-sghhn [530.802467ms]
Apr 17 22:22:19.461: INFO: Created: latency-svc-sj7m2
Apr 17 22:22:19.468: INFO: Got endpoints: latency-svc-sj7m2 [535.423925ms]
Apr 17 22:22:19.474: INFO: Created: latency-svc-g7pm6
Apr 17 22:22:19.481: INFO: Got endpoints: latency-svc-g7pm6 [536.81345ms]
Apr 17 22:22:19.484: INFO: Created: latency-svc-lzzgd
Apr 17 22:22:19.566: INFO: Got endpoints: latency-svc-lzzgd [522.136749ms]
Apr 17 22:22:19.571: INFO: Created: latency-svc-dk6l4
Apr 17 22:22:19.579: INFO: Got endpoints: latency-svc-dk6l4 [520.358255ms]
Apr 17 22:22:19.580: INFO: Created: latency-svc-xcrnj
Apr 17 22:22:19.587: INFO: Got endpoints: latency-svc-xcrnj [521.459171ms]
Apr 17 22:22:19.590: INFO: Created: latency-svc-7kjrn
Apr 17 22:22:19.595: INFO: Got endpoints: latency-svc-7kjrn [519.06103ms]
Apr 17 22:22:19.601: INFO: Created: latency-svc-q5rk6
Apr 17 22:22:19.606: INFO: Got endpoints: latency-svc-q5rk6 [436.608509ms]
Apr 17 22:22:19.701: INFO: Created: latency-svc-w7h5v
Apr 17 22:22:19.705: INFO: Got endpoints: latency-svc-w7h5v [521.549583ms]
Apr 17 22:22:19.713: INFO: Created: latency-svc-bbhc5
Apr 17 22:22:19.720: INFO: Got endpoints: latency-svc-bbhc5 [520.339619ms]
Apr 17 22:22:19.724: INFO: Created: latency-svc-chld9
Apr 17 22:22:19.735: INFO: Got endpoints: latency-svc-chld9 [416.595881ms]
Apr 17 22:22:19.737: INFO: Created: latency-svc-7x8fg
Apr 17 22:22:19.740: INFO: Got endpoints: latency-svc-7x8fg [412.283776ms]
Apr 17 22:22:19.874: INFO: Created: latency-svc-v9mb4
Apr 17 22:22:19.878: INFO: Got endpoints: latency-svc-v9mb4 [539.883926ms]
Apr 17 22:22:19.885: INFO: Created: latency-svc-m4wx9
Apr 17 22:22:19.891: INFO: Got endpoints: latency-svc-m4wx9 [544.217975ms]
Apr 17 22:22:19.899: INFO: Created: latency-svc-6glr7
Apr 17 22:22:19.905: INFO: Got endpoints: latency-svc-6glr7 [459.234259ms]
Apr 17 22:22:19.998: INFO: Created: latency-svc-94bqf
Apr 17 22:22:20.002: INFO: Created: latency-svc-q5pg7
Apr 17 22:22:20.003: INFO: Got endpoints: latency-svc-94bqf [548.947878ms]
Apr 17 22:22:20.006: INFO: Got endpoints: latency-svc-q5pg7 [537.653981ms]
Apr 17 22:22:20.012: INFO: Created: latency-svc-w5z26
Apr 17 22:22:20.018: INFO: Got endpoints: latency-svc-w5z26 [536.515081ms]
Apr 17 22:22:20.022: INFO: Created: latency-svc-mzw7n
Apr 17 22:22:20.026: INFO: Got endpoints: latency-svc-mzw7n [459.861756ms]
Apr 17 22:22:20.044: INFO: Created: latency-svc-kpr6x
Apr 17 22:22:20.131: INFO: Created: latency-svc-qdrdw
Apr 17 22:22:20.157: INFO: Got endpoints: latency-svc-kpr6x [578.065264ms]
Apr 17 22:22:20.157: INFO: Got endpoints: latency-svc-qdrdw [570.787358ms]
Apr 17 22:22:20.158: INFO: Created: latency-svc-2l7n7
Apr 17 22:22:20.161: INFO: Got endpoints: latency-svc-2l7n7 [565.774516ms]
Apr 17 22:22:20.170: INFO: Created: latency-svc-vtsz9
Apr 17 22:22:20.174: INFO: Got endpoints: latency-svc-vtsz9 [568.591872ms]
Apr 17 22:22:20.184: INFO: Created: latency-svc-hr2fq
Apr 17 22:22:20.303: INFO: Got endpoints: latency-svc-hr2fq [597.823214ms]
Apr 17 22:22:20.307: INFO: Created: latency-svc-648rd
Apr 17 22:22:20.317: INFO: Created: latency-svc-qtnvw
Apr 17 22:22:20.326: INFO: Created: latency-svc-6zvc5
Apr 17 22:22:20.336: INFO: Created: latency-svc-tnfvq
Apr 17 22:22:20.427: INFO: Got endpoints: latency-svc-qtnvw [691.791151ms]
Apr 17 22:22:20.427: INFO: Got endpoints: latency-svc-648rd [707.189976ms]
Apr 17 22:22:20.427: INFO: Got endpoints: latency-svc-tnfvq [549.111524ms]
Apr 17 22:22:20.427: INFO: Got endpoints: latency-svc-6zvc5 [686.914581ms]
Apr 17 22:22:20.428: INFO: Created: latency-svc-rkv27
Apr 17 22:22:20.437: INFO: Got endpoints: latency-svc-rkv27 [545.335773ms]
Apr 17 22:22:20.444: INFO: Created: latency-svc-gdgvd
Apr 17 22:22:20.450: INFO: Got endpoints: latency-svc-gdgvd [545.615457ms]
Apr 17 22:22:20.452: INFO: Created: latency-svc-pzkh7
Apr 17 22:22:20.545: INFO: Created: latency-svc-vrn65
Apr 17 22:22:20.554: INFO: Created: latency-svc-6mdtw
Apr 17 22:22:20.562: INFO: Created: latency-svc-6nkm9
Apr 17 22:22:20.571: INFO: Created: latency-svc-xqmbs
Apr 17 22:22:20.777: INFO: Got endpoints: latency-svc-vrn65 [771.286355ms]
Apr 17 22:22:20.777: INFO: Got endpoints: latency-svc-pzkh7 [774.80623ms]
Apr 17 22:22:20.778: INFO: Got endpoints: latency-svc-6nkm9 [751.424918ms]
Apr 17 22:22:20.778: INFO: Got endpoints: latency-svc-6mdtw [760.061086ms]
Apr 17 22:22:20.778: INFO: Got endpoints: latency-svc-xqmbs [620.408306ms]
Apr 17 22:22:20.782: INFO: Created: latency-svc-qmw4m
Apr 17 22:22:20.791: INFO: Created: latency-svc-m7t8q
Apr 17 22:22:20.804: INFO: Created: latency-svc-7kj4g
Apr 17 22:22:20.817: INFO: Created: latency-svc-x7v8f
Apr 17 22:22:20.918: INFO: Got endpoints: latency-svc-qmw4m [760.752172ms]
Apr 17 22:22:20.918: INFO: Got endpoints: latency-svc-m7t8q [757.229398ms]
Apr 17 22:22:20.918: INFO: Got endpoints: latency-svc-7kj4g [743.804467ms]
Apr 17 22:22:20.921: INFO: Got endpoints: latency-svc-x7v8f [618.039351ms]
Apr 17 22:22:20.921: INFO: Created: latency-svc-hf99h
Apr 17 22:22:20.933: INFO: Got endpoints: latency-svc-hf99h [506.10656ms]
Apr 17 22:22:20.943: INFO: Created: latency-svc-d84dl
Apr 17 22:22:20.954: INFO: Created: latency-svc-9wqv8
Apr 17 22:22:20.957: INFO: Got endpoints: latency-svc-d84dl [530.104266ms]
Apr 17 22:22:21.128: INFO: Got endpoints: latency-svc-9wqv8 [701.120411ms]
Apr 17 22:22:21.132: INFO: Created: latency-svc-bhqhx
Apr 17 22:22:21.134: INFO: Created: latency-svc-dbt6j
Apr 17 22:22:21.150: INFO: Created: latency-svc-qpmjs
Apr 17 22:22:21.160: INFO: Created: latency-svc-p8tlw
Apr 17 22:22:21.166: INFO: Got endpoints: latency-svc-bhqhx [738.987323ms]
Apr 17 22:22:21.169: INFO: Created: latency-svc-nh98j
Apr 17 22:22:21.173: INFO: Got endpoints: latency-svc-qpmjs [722.113265ms]
Apr 17 22:22:21.173: INFO: Got endpoints: latency-svc-p8tlw [394.782797ms]
Apr 17 22:22:21.173: INFO: Got endpoints: latency-svc-dbt6j [736.088043ms]
Apr 17 22:22:21.277: INFO: Got endpoints: latency-svc-nh98j [499.679651ms]
Apr 17 22:22:21.279: INFO: Created: latency-svc-mtcxk
Apr 17 22:22:21.289: INFO: Created: latency-svc-6l4v8
Apr 17 22:22:21.300: INFO: Created: latency-svc-4chvk
Apr 17 22:22:21.305: INFO: Created: latency-svc-8hrk2
Apr 17 22:22:21.396: INFO: Got endpoints: latency-svc-6l4v8 [618.282348ms]
Apr 17 22:22:21.396: INFO: Got endpoints: latency-svc-mtcxk [618.520979ms]
Apr 17 22:22:21.397: INFO: Created: latency-svc-z2r42
Apr 17 22:22:21.406: INFO: Got endpoints: latency-svc-8hrk2 [487.647282ms]
Apr 17 22:22:21.406: INFO: Got endpoints: latency-svc-4chvk [628.370712ms]
Apr 17 22:22:21.408: INFO: Created: latency-svc-4k6jv
Apr 17 22:22:21.421: INFO: Got endpoints: latency-svc-z2r42 [502.347723ms]
Apr 17 22:22:21.424: INFO: Got endpoints: latency-svc-4k6jv [506.283111ms]
Apr 17 22:22:21.427: INFO: Created: latency-svc-rrbzq
Apr 17 22:22:21.438: INFO: Created: latency-svc-kxspz
Apr 17 22:22:21.439: INFO: Got endpoints: latency-svc-rrbzq [517.401305ms]
Apr 17 22:22:21.512: INFO: Got endpoints: latency-svc-kxspz [578.926073ms]
Apr 17 22:22:21.514: INFO: Created: latency-svc-c4xt2
Apr 17 22:22:21.526: INFO: Created: latency-svc-bcc8b
Apr 17 22:22:21.535: INFO: Created: latency-svc-6fhgl
Apr 17 22:22:21.545: INFO: Created: latency-svc-k2mz7
Apr 17 22:22:21.667: INFO: Got endpoints: latency-svc-6fhgl [501.020889ms]
Apr 17 22:22:21.667: INFO: Got endpoints: latency-svc-k2mz7 [494.804596ms]
Apr 17 22:22:21.668: INFO: Got endpoints: latency-svc-c4xt2 [710.338621ms]
Apr 17 22:22:21.668: INFO: Got endpoints: latency-svc-bcc8b [539.364897ms]
Apr 17 22:22:21.671: INFO: Created: latency-svc-m2phk
Apr 17 22:22:21.681: INFO: Created: latency-svc-74np2
Apr 17 22:22:21.688: INFO: Created: latency-svc-zhjr8
Apr 17 22:22:21.698: INFO: Created: latency-svc-rdx2s
Apr 17 22:22:21.787: INFO: Got endpoints: latency-svc-m2phk [614.724603ms]
Apr 17 22:22:21.788: INFO: Got endpoints: latency-svc-zhjr8 [510.010015ms]
Apr 17 22:22:21.788: INFO: Got endpoints: latency-svc-74np2 [615.008347ms]
Apr 17 22:22:21.791: INFO: Got endpoints: latency-svc-rdx2s [394.648545ms]
Apr 17 22:22:21.793: INFO: Created: latency-svc-pgvvn
Apr 17 22:22:21.802: INFO: Got endpoints: latency-svc-pgvvn [405.564283ms]
Apr 17 22:22:21.805: INFO: Created: latency-svc-z8wlx
Apr 17 22:22:21.813: INFO: Got endpoints: latency-svc-z8wlx [406.921565ms]
Apr 17 22:22:21.816: INFO: Created: latency-svc-9c2tv
Apr 17 22:22:21.887: INFO: Got endpoints: latency-svc-9c2tv [480.883892ms]
Apr 17 22:22:21.890: INFO: Created: latency-svc-hrln2
Apr 17 22:22:21.897: INFO: Created: latency-svc-zxhvg
Apr 17 22:22:21.908: INFO: Created: latency-svc-5vn2g
Apr 17 22:22:21.927: INFO: Created: latency-svc-2vs6q
Apr 17 22:22:22.003: INFO: Created: latency-svc-wt4x5
Apr 17 22:22:22.040: INFO: Got endpoints: latency-svc-hrln2 [619.047415ms]
Apr 17 22:22:22.042: INFO: Created: latency-svc-5lqlf
Apr 17 22:22:22.043: INFO: Got endpoints: latency-svc-5vn2g [604.546754ms]
Apr 17 22:22:22.043: INFO: Got endpoints: latency-svc-zxhvg [618.932842ms]
Apr 17 22:22:22.043: INFO: Got endpoints: latency-svc-2vs6q [531.045314ms]
Apr 17 22:22:22.043: INFO: Got endpoints: latency-svc-wt4x5 [375.591427ms]
Apr 17 22:22:22.048: INFO: Got endpoints: latency-svc-5lqlf [380.712569ms]
Apr 17 22:22:22.055: INFO: Created: latency-svc-wx4qf
Apr 17 22:22:22.063: INFO: Got endpoints: latency-svc-wx4qf [394.851096ms]
Apr 17 22:22:22.068: INFO: Created: latency-svc-6qk44
Apr 17 22:22:22.072: INFO: Got endpoints: latency-svc-6qk44 [404.542925ms]
Apr 17 22:22:22.177: INFO: Created: latency-svc-kx4bk
Apr 17 22:22:22.187: INFO: Created: latency-svc-mrwf5
Apr 17 22:22:22.196: INFO: Got endpoints: latency-svc-mrwf5 [408.839161ms]
Apr 17 22:22:22.196: INFO: Got endpoints: latency-svc-kx4bk [409.077257ms]
Apr 17 22:22:22.199: INFO: Created: latency-svc-pnnn7
Apr 17 22:22:22.204: INFO: Got endpoints: latency-svc-pnnn7 [416.205818ms]
Apr 17 22:22:22.288: INFO: Created: latency-svc-df422
Apr 17 22:22:22.291: INFO: Created: latency-svc-l8n7v
Apr 17 22:22:22.293: INFO: Got endpoints: latency-svc-df422 [502.578364ms]
Apr 17 22:22:22.304: INFO: Created: latency-svc-mz69s
Apr 17 22:22:22.308: INFO: Created: latency-svc-fqdlb
Apr 17 22:22:22.316: INFO: Created: latency-svc-fb25b
Apr 17 22:22:22.419: INFO: Got endpoints: latency-svc-mz69s [606.3042ms]
Apr 17 22:22:22.419: INFO: Got endpoints: latency-svc-l8n7v [617.550151ms]
Apr 17 22:22:22.420: INFO: Got endpoints: latency-svc-fqdlb [532.821567ms]
Apr 17 22:22:22.420: INFO: Got endpoints: latency-svc-fb25b [379.893953ms]
Apr 17 22:22:22.423: INFO: Created: latency-svc-h6sf7
Apr 17 22:22:22.430: INFO: Got endpoints: latency-svc-h6sf7 [386.278701ms]
Apr 17 22:22:22.435: INFO: Created: latency-svc-pbrpv
Apr 17 22:22:22.440: INFO: Got endpoints: latency-svc-pbrpv [396.723489ms]
Apr 17 22:22:22.445: INFO: Created: latency-svc-g8znr
Apr 17 22:22:22.454: INFO: Created: latency-svc-cwvfk
Apr 17 22:22:22.566: INFO: Got endpoints: latency-svc-cwvfk [522.140458ms]
Apr 17 22:22:22.566: INFO: Got endpoints: latency-svc-g8znr [522.176024ms]
Apr 17 22:22:22.572: INFO: Created: latency-svc-rm488
Apr 17 22:22:22.581: INFO: Created: latency-svc-4l84l
Apr 17 22:22:22.590: INFO: Created: latency-svc-8wk8c
Apr 17 22:22:22.606: INFO: Created: latency-svc-jmbfn
Apr 17 22:22:22.692: INFO: Got endpoints: latency-svc-4l84l [629.166431ms]
Apr 17 22:22:22.692: INFO: Got endpoints: latency-svc-rm488 [643.623582ms]
Apr 17 22:22:22.692: INFO: Got endpoints: latency-svc-8wk8c [619.811872ms]
Apr 17 22:22:22.694: INFO: Created: latency-svc-lp66w
Apr 17 22:22:22.710: INFO: Created: latency-svc-zsm9r
Apr 17 22:22:22.715: INFO: Got endpoints: latency-svc-jmbfn [518.864046ms]
Apr 17 22:22:22.722: INFO: Created: latency-svc-g58cq
Apr 17 22:22:22.728: INFO: Created: latency-svc-8wpfm
Apr 17 22:22:22.833: INFO: Got endpoints: latency-svc-zsm9r [629.546ms]
Apr 17 22:22:22.833: INFO: Got endpoints: latency-svc-lp66w [636.819216ms]
Apr 17 22:22:22.836: INFO: Created: latency-svc-bvsgp
Apr 17 22:22:22.844: INFO: Created: latency-svc-qx7wx
Apr 17 22:22:22.854: INFO: Created: latency-svc-cg6tz
Apr 17 22:22:22.863: INFO: Created: latency-svc-7j247
Apr 17 22:22:22.958: INFO: Got endpoints: latency-svc-g58cq [664.825986ms]
Apr 17 22:22:22.961: INFO: Got endpoints: latency-svc-8wpfm [541.667832ms]
Apr 17 22:22:22.963: INFO: Created: latency-svc-vzrgg
Apr 17 22:22:22.965: INFO: Got endpoints: latency-svc-bvsgp [545.94315ms]
Apr 17 22:22:22.973: INFO: Created: latency-svc-2fl4m
Apr 17 22:22:22.981: INFO: Created: latency-svc-p8kqq
Apr 17 22:22:22.993: INFO: Created: latency-svc-mblf7
Apr 17 22:22:23.065: INFO: Got endpoints: latency-svc-qx7wx [644.864778ms]
Apr 17 22:22:23.066: INFO: Got endpoints: latency-svc-cg6tz [646.861849ms]
Apr 17 22:22:23.069: INFO: Created: latency-svc-z7wxh
Apr 17 22:22:23.076: INFO: Created: latency-svc-pkrtd
Apr 17 22:22:23.086: INFO: Created: latency-svc-6dsbh
Apr 17 22:22:23.095: INFO: Created: latency-svc-jnvj8
Apr 17 22:22:23.203: INFO: Got endpoints: latency-svc-vzrgg [762.566589ms]
Apr 17 22:22:23.203: INFO: Got endpoints: latency-svc-7j247 [773.169912ms]
Apr 17 22:22:23.206: INFO: Created: latency-svc-hnskk
Apr 17 22:22:23.213: INFO: Got endpoints: latency-svc-2fl4m [647.10548ms]
Apr 17 22:22:23.216: INFO: Created: latency-svc-jnqqx
Apr 17 22:22:23.225: INFO: Created: latency-svc-xdwfk
Apr 17 22:22:23.312: INFO: Got endpoints: latency-svc-p8kqq [746.029608ms]
Apr 17 22:22:23.315: INFO: Got endpoints: latency-svc-mblf7 [622.814299ms]
Apr 17 22:22:23.319: INFO: Created: latency-svc-64qz8
Apr 17 22:22:23.329: INFO: Created: latency-svc-m7vnd
Apr 17 22:22:23.339: INFO: Created: latency-svc-7jhhl
Apr 17 22:22:23.352: INFO: Created: latency-svc-x9gfj
Apr 17 22:22:23.472: INFO: Got endpoints: latency-svc-z7wxh [779.91819ms]
Apr 17 22:22:23.472: INFO: Got endpoints: latency-svc-pkrtd [780.32645ms]
Apr 17 22:22:23.472: INFO: Got endpoints: latency-svc-6dsbh [756.993718ms]
Apr 17 22:22:23.477: INFO: Created: latency-svc-2qjql
Apr 17 22:22:23.484: INFO: Created: latency-svc-w6646
Apr 17 22:22:23.494: INFO: Created: latency-svc-msvrj
Apr 17 22:22:23.582: INFO: Created: latency-svc-6dx6b
Apr 17 22:22:23.584: INFO: Got endpoints: latency-svc-jnvj8 [750.327743ms]
Apr 17 22:22:23.584: INFO: Got endpoints: latency-svc-hnskk [750.311947ms]
Apr 17 22:22:23.587: INFO: Created: latency-svc-stmc9
Apr 17 22:22:23.596: INFO: Created: latency-svc-2jssr
Apr 17 22:22:23.608: INFO: Created: latency-svc-llnzh
Apr 17 22:22:23.617: INFO: Got endpoints: latency-svc-jnqqx [658.629731ms]
Apr 17 22:22:23.619: INFO: Created: latency-svc-vgx4c
Apr 17 22:22:23.693: INFO: Got endpoints: latency-svc-xdwfk [732.266082ms]
Apr 17 22:22:23.696: INFO: Created: latency-svc-kvj2w
Apr 17 22:22:23.707: INFO: Created: latency-svc-j722t
Apr 17 22:22:23.721: INFO: Got endpoints: latency-svc-64qz8 [756.146064ms]
Apr 17 22:22:23.724: INFO: Created: latency-svc-kn8dk
Apr 17 22:22:23.735: INFO: Created: latency-svc-mvmhl
Apr 17 22:22:23.764: INFO: Got endpoints: latency-svc-m7vnd [699.324399ms]
Apr 17 22:22:23.914: INFO: Got endpoints: latency-svc-x9gfj [711.102239ms]
Apr 17 22:22:23.914: INFO: Got endpoints: latency-svc-7jhhl [847.48899ms]
Apr 17 22:22:23.917: INFO: Created: latency-svc-2bbg9
Apr 17 22:22:23.917: INFO: Got endpoints: latency-svc-2qjql [714.618046ms]
Apr 17 22:22:23.927: INFO: Created: latency-svc-dbh9f
Apr 17 22:22:23.937: INFO: Created: latency-svc-25fl5
Apr 17 22:22:23.947: INFO: Created: latency-svc-5h9ht
Apr 17 22:22:23.962: INFO: Got endpoints: latency-svc-w6646 [749.259473ms]
Apr 17 22:22:24.045: INFO: Got endpoints: latency-svc-msvrj [733.35982ms]
Apr 17 22:22:24.046: INFO: Created: latency-svc-jksfw
Apr 17 22:22:24.062: INFO: Created: latency-svc-gxs2b
Apr 17 22:22:24.064: INFO: Got endpoints: latency-svc-6dx6b [748.984154ms]
Apr 17 22:22:24.079: INFO: Created: latency-svc-h4mwb
Apr 17 22:22:24.113: INFO: Got endpoints: latency-svc-stmc9 [641.134583ms]
Apr 17 22:22:24.133: INFO: Created: latency-svc-fjtx8
Apr 17 22:22:24.165: INFO: Got endpoints: latency-svc-2jssr [693.079597ms]
Apr 17 22:22:24.178: INFO: Created: latency-svc-8xgsz
Apr 17 22:22:24.214: INFO: Got endpoints: latency-svc-llnzh [741.366831ms]
Apr 17 22:22:24.227: INFO: Created: latency-svc-hldws
Apr 17 22:22:24.264: INFO: Got endpoints: latency-svc-vgx4c [680.16928ms]
Apr 17 22:22:24.278: INFO: Created: latency-svc-ws67k
Apr 17 22:22:24.312: INFO: Got endpoints: latency-svc-kvj2w [728.608161ms]
Apr 17 22:22:24.325: INFO: Created: latency-svc-q2qgc
Apr 17 22:22:24.362: INFO: Got endpoints: latency-svc-j722t [745.539643ms]
Apr 17 22:22:24.457: INFO: Got endpoints: latency-svc-kn8dk [763.71572ms]
Apr 17 22:22:24.461: INFO: Created: latency-svc-28nn6
Apr 17 22:22:24.464: INFO: Got endpoints: latency-svc-mvmhl [743.096917ms]
Apr 17 22:22:24.480: INFO: Created: latency-svc-bxpmb
Apr 17 22:22:24.487: INFO: Created: latency-svc-5m9st
Apr 17 22:22:24.513: INFO: Got endpoints: latency-svc-2bbg9 [749.364179ms]
Apr 17 22:22:24.525: INFO: Created: latency-svc-b9lpm
Apr 17 22:22:24.564: INFO: Got endpoints: latency-svc-dbh9f [650.452844ms]
Apr 17 22:22:24.663: INFO: Created: latency-svc-xwtpw
Apr 17 22:22:24.705: INFO: Got endpoints: latency-svc-5h9ht [787.89711ms]
Apr 17 22:22:24.705: INFO: Got endpoints: latency-svc-25fl5 [791.356536ms]
Apr 17 22:22:24.714: INFO: Got endpoints: latency-svc-jksfw [751.926796ms]
Apr 17 22:22:24.723: INFO: Created: latency-svc-rrwsb
Apr 17 22:22:24.732: INFO: Created: latency-svc-lf5nl
Apr 17 22:22:24.739: INFO: Created: latency-svc-gqfb6
Apr 17 22:22:24.765: INFO: Got endpoints: latency-svc-gxs2b [719.887738ms]
Apr 17 22:22:24.851: INFO: Created: latency-svc-mm6r2
Apr 17 22:22:24.866: INFO: Got endpoints: latency-svc-h4mwb [801.839686ms]
Apr 17 22:22:24.866: INFO: Got endpoints: latency-svc-fjtx8 [752.786619ms]
Apr 17 22:22:24.883: INFO: Created: latency-svc-hchgw
Apr 17 22:22:24.892: INFO: Created: latency-svc-ldlcd
Apr 17 22:22:24.913: INFO: Got endpoints: latency-svc-8xgsz [748.105742ms]
Apr 17 22:22:24.955: INFO: Created: latency-svc-mkzsj
Apr 17 22:22:24.964: INFO: Got endpoints: latency-svc-hldws [750.614566ms]
Apr 17 22:22:25.053: INFO: Got endpoints: latency-svc-ws67k [788.631532ms]
Apr 17 22:22:25.058: INFO: Created: latency-svc-fwvlv
Apr 17 22:22:25.100: INFO: Got endpoints: latency-svc-q2qgc [787.840865ms]
Apr 17 22:22:25.111: INFO: Created: latency-svc-lmwz5
Apr 17 22:22:25.116: INFO: Got endpoints: latency-svc-28nn6 [753.615115ms]
Apr 17 22:22:25.119: INFO: Created: latency-svc-rkdb5
Apr 17 22:22:25.130: INFO: Created: latency-svc-lhr7k
Apr 17 22:22:25.164: INFO: Got endpoints: latency-svc-bxpmb [707.281731ms]
Apr 17 22:22:25.275: INFO: Got endpoints: latency-svc-b9lpm [762.019172ms]
Apr 17 22:22:25.275: INFO: Got endpoints: latency-svc-5m9st [810.990221ms]
Apr 17 22:22:25.279: INFO: Created: latency-svc-gwp55
Apr 17 22:22:25.289: INFO: Created: latency-svc-k7stf
Apr 17 22:22:25.296: INFO: Created: latency-svc-fpsjf
Apr 17 22:22:25.315: INFO: Got endpoints: latency-svc-xwtpw [750.7345ms]
Apr 17 22:22:25.328: INFO: Created: latency-svc-88sf5
Apr 17 22:22:25.363: INFO: Got endpoints: latency-svc-rrwsb [657.433521ms]
Apr 17 22:22:25.474: INFO: Created: latency-svc-s625b
Apr 17 22:22:25.585: INFO: Got endpoints: latency-svc-lf5nl [879.665118ms]
Apr 17 22:22:25.585: INFO: Got endpoints: latency-svc-mm6r2 [820.186709ms]
Apr 17 22:22:25.585: INFO: Got endpoints: latency-svc-gqfb6 [871.3108ms]
Apr 17 22:22:25.585: INFO: Got endpoints: latency-svc-hchgw [719.607738ms]
Apr 17 22:22:25.600: INFO: Created: latency-svc-vpgsf
Apr 17 22:22:25.610: INFO: Created: latency-svc-dpp2k
Apr 17 22:22:25.613: INFO: Got endpoints: latency-svc-ldlcd [747.605616ms]
Apr 17 22:22:25.621: INFO: Created: latency-svc-2prr2
Apr 17 22:22:25.787: INFO: Got endpoints: latency-svc-mkzsj [873.56469ms]
Apr 17 22:22:25.787: INFO: Got endpoints: latency-svc-lmwz5 [734.533649ms]
Apr 17 22:22:25.787: INFO: Got endpoints: latency-svc-fwvlv [822.654402ms]
Apr 17 22:22:25.792: INFO: Created: latency-svc-xxkcr
Apr 17 22:22:25.800: INFO: Created: latency-svc-8j8n4
Apr 17 22:22:25.825: INFO: Got endpoints: latency-svc-rkdb5 [724.400096ms]
Apr 17 22:22:25.825: INFO: Created: latency-svc-4hb8z
Apr 17 22:22:25.839: INFO: Created: latency-svc-6cnfs
Apr 17 22:22:25.971: INFO: Got endpoints: latency-svc-lhr7k [855.059541ms]
Apr 17 22:22:25.971: INFO: Got endpoints: latency-svc-gwp55 [807.082102ms]
Apr 17 22:22:25.971: INFO: Got endpoints: latency-svc-k7stf [695.81054ms]
Apr 17 22:22:25.976: INFO: Created: latency-svc-bqzfl
Apr 17 22:22:25.984: INFO: Created: latency-svc-225xw
Apr 17 22:22:25.992: INFO: Created: latency-svc-j9m8t
Apr 17 22:22:26.000: INFO: Created: latency-svc-7lggh
Apr 17 22:22:26.195: INFO: Got endpoints: latency-svc-fpsjf [919.683141ms]
Apr 17 22:22:26.197: INFO: Got endpoints: latency-svc-s625b [834.163709ms]
Apr 17 22:22:26.197: INFO: Got endpoints: latency-svc-88sf5 [881.827341ms]
Apr 17 22:22:26.197: INFO: Got endpoints: latency-svc-vpgsf [611.744954ms]
Apr 17 22:22:26.200: INFO: Created: latency-svc-8mb4t
Apr 17 22:22:26.211: INFO: Created: latency-svc-wk7d9
Apr 17 22:22:26.215: INFO: Got endpoints: latency-svc-dpp2k [629.534528ms]
Apr 17 22:22:26.219: INFO: Created: latency-svc-knxb2
Apr 17 22:22:26.228: INFO: Created: latency-svc-5h5q7
Apr 17 22:22:26.381: INFO: Got endpoints: latency-svc-xxkcr [795.382797ms]
Apr 17 22:22:26.381: INFO: Got endpoints: latency-svc-2prr2 [795.656351ms]
Apr 17 22:22:26.381: INFO: Got endpoints: latency-svc-8j8n4 [767.675494ms]
Apr 17 22:22:26.386: INFO: Created: latency-svc-gd2hz
Apr 17 22:22:26.392: INFO: Created: latency-svc-dtx85
Apr 17 22:22:26.402: INFO: Created: latency-svc-tvc2q
Apr 17 22:22:26.559: INFO: Got endpoints: latency-svc-6cnfs [771.552891ms]
Apr 17 22:22:26.559: INFO: Got endpoints: latency-svc-4hb8z [771.605717ms]
Apr 17 22:22:26.559: INFO: Got endpoints: latency-svc-bqzfl [771.619662ms]
Apr 17 22:22:26.564: INFO: Created: latency-svc-dt59d
Apr 17 22:22:26.566: INFO: Got endpoints: latency-svc-225xw [741.556932ms]
Apr 17 22:22:26.575: INFO: Created: latency-svc-8txdn
Apr 17 22:22:26.583: INFO: Created: latency-svc-6g9rh
Apr 17 22:22:26.591: INFO: Created: latency-svc-7mmgf
Apr 17 22:22:26.777: INFO: Got endpoints: latency-svc-j9m8t [805.714109ms]
Apr 17 22:22:26.777: INFO: Got endpoints: latency-svc-7lggh [806.065721ms]
Apr 17 22:22:26.778: INFO: Got endpoints: latency-svc-8mb4t [807.01718ms]
Apr 17 22:22:26.778: INFO: Got endpoints: latency-svc-wk7d9 [583.237011ms]
Apr 17 22:22:26.781: INFO: Created: latency-svc-82mss
Apr 17 22:22:26.792: INFO: Created: latency-svc-ngkbw
Apr 17 22:22:26.814: INFO: Got endpoints: latency-svc-knxb2 [616.72919ms]
Apr 17 22:22:26.866: INFO: Got endpoints: latency-svc-dtx85 [650.601025ms]
Apr 17 22:22:26.977: INFO: Got endpoints: latency-svc-5h5q7 [779.878036ms]
Apr 17 22:22:26.977: INFO: Got endpoints: latency-svc-gd2hz [779.964281ms]
Apr 17 22:22:27.014: INFO: Got endpoints: latency-svc-tvc2q [633.097515ms]
Apr 17 22:22:27.063: INFO: Got endpoints: latency-svc-dt59d [682.53715ms]
Apr 17 22:22:27.113: INFO: Got endpoints: latency-svc-8txdn [732.332319ms]
Apr 17 22:22:27.163: INFO: Got endpoints: latency-svc-6g9rh [604.58639ms]
Apr 17 22:22:27.214: INFO: Got endpoints: latency-svc-7mmgf [655.501542ms]
Apr 17 22:22:27.262: INFO: Got endpoints: latency-svc-82mss [703.201941ms]
Apr 17 22:22:27.314: INFO: Got endpoints: latency-svc-ngkbw [747.964378ms]
Apr 17 22:22:27.314: INFO: Latencies: [22.976132ms 34.875317ms 151.938052ms 160.317884ms 170.413765ms 181.810272ms 281.479191ms 296.244507ms 302.452421ms 313.435261ms 375.591427ms 379.893953ms 380.712569ms 386.278701ms 394.648545ms 394.782797ms 394.851096ms 396.723489ms 404.542925ms 405.564283ms 406.700574ms 406.921565ms 408.839161ms 409.077257ms 412.283776ms 416.205818ms 416.595881ms 421.201379ms 436.608509ms 437.107106ms 459.234259ms 459.861756ms 480.883892ms 487.647282ms 494.804596ms 499.679651ms 501.020889ms 502.347723ms 502.578364ms 506.10656ms 506.283111ms 510.010015ms 517.401305ms 518.864046ms 519.06103ms 520.339619ms 520.358255ms 521.459171ms 521.549583ms 522.136749ms 522.140458ms 522.176024ms 530.104266ms 530.802467ms 531.032837ms 531.045314ms 532.821567ms 535.423925ms 536.515081ms 536.81345ms 537.653981ms 539.364897ms 539.883926ms 541.667832ms 544.217975ms 545.335773ms 545.615457ms 545.94315ms 548.947878ms 549.111524ms 549.574889ms 552.874188ms 556.176944ms 565.420558ms 565.774516ms 568.591872ms 570.787358ms 578.065264ms 578.926073ms 583.237011ms 597.823214ms 604.546754ms 604.58639ms 606.3042ms 611.744954ms 614.724603ms 615.008347ms 616.72919ms 617.550151ms 618.039351ms 618.282348ms 618.520979ms 618.932842ms 619.047415ms 619.811872ms 620.408306ms 622.814299ms 628.370712ms 629.166431ms 629.534528ms 629.546ms 633.097515ms 636.819216ms 641.134583ms 643.623582ms 644.864778ms 646.861849ms 647.10548ms 650.452844ms 650.601025ms 655.501542ms 657.433521ms 658.629731ms 664.825986ms 680.16928ms 682.53715ms 686.914581ms 691.791151ms 693.079597ms 695.81054ms 699.324399ms 701.120411ms 703.201941ms 707.189976ms 707.281731ms 710.338621ms 711.102239ms 714.618046ms 719.607738ms 719.887738ms 722.113265ms 724.400096ms 728.608161ms 732.266082ms 732.332319ms 733.35982ms 734.533649ms 736.088043ms 738.987323ms 741.366831ms 741.556932ms 743.096917ms 743.804467ms 745.539643ms 746.029608ms 747.605616ms 747.964378ms 748.105742ms 748.984154ms 749.259473ms 749.364179ms 750.311947ms 750.327743ms 750.614566ms 750.7345ms 751.424918ms 751.926796ms 752.786619ms 753.615115ms 756.146064ms 756.993718ms 757.229398ms 760.061086ms 760.752172ms 762.019172ms 762.566589ms 763.71572ms 767.675494ms 771.286355ms 771.552891ms 771.605717ms 771.619662ms 773.169912ms 774.80623ms 779.878036ms 779.91819ms 779.964281ms 780.32645ms 787.840865ms 787.89711ms 788.631532ms 791.356536ms 795.382797ms 795.656351ms 801.839686ms 805.714109ms 806.065721ms 807.01718ms 807.082102ms 810.990221ms 820.186709ms 822.654402ms 834.163709ms 847.48899ms 855.059541ms 871.3108ms 873.56469ms 879.665118ms 881.827341ms 919.683141ms]
Apr 17 22:22:27.314: INFO: 50 %ile: 629.546ms
Apr 17 22:22:27.314: INFO: 90 %ile: 788.631532ms
Apr 17 22:22:27.314: INFO: 99 %ile: 881.827341ms
Apr 17 22:22:27.314: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:27.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-2426" for this suite. 04/17/23 22:22:27.323
------------------------------
• [SLOW TEST] [10.901 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:16.426
    Apr 17 22:22:16.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename svc-latency 04/17/23 22:22:16.427
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:16.441
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:16.443
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Apr 17 22:22:16.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-2426 04/17/23 22:22:16.446
    I0417 22:22:16.451109      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-2426, replica count: 1
    I0417 22:22:17.502607      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0417 22:22:18.503524      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 17 22:22:18.758: INFO: Created: latency-svc-t6v5n
    Apr 17 22:22:18.762: INFO: Got endpoints: latency-svc-t6v5n [158.300904ms]
    Apr 17 22:22:18.779: INFO: Created: latency-svc-bll6d
    Apr 17 22:22:18.785: INFO: Got endpoints: latency-svc-bll6d [22.976132ms]
    Apr 17 22:22:18.790: INFO: Created: latency-svc-c7d2l
    Apr 17 22:22:18.797: INFO: Got endpoints: latency-svc-c7d2l [34.875317ms]
    Apr 17 22:22:18.910: INFO: Created: latency-svc-gh472
    Apr 17 22:22:18.914: INFO: Got endpoints: latency-svc-gh472 [151.938052ms]
    Apr 17 22:22:18.916: INFO: Created: latency-svc-xc522
    Apr 17 22:22:18.923: INFO: Got endpoints: latency-svc-xc522 [160.317884ms]
    Apr 17 22:22:18.927: INFO: Created: latency-svc-2fc5x
    Apr 17 22:22:18.933: INFO: Got endpoints: latency-svc-2fc5x [170.413765ms]
    Apr 17 22:22:18.938: INFO: Created: latency-svc-jn25f
    Apr 17 22:22:18.944: INFO: Got endpoints: latency-svc-jn25f [181.810272ms]
    Apr 17 22:22:18.948: INFO: Created: latency-svc-vrhcg
    Apr 17 22:22:19.044: INFO: Got endpoints: latency-svc-vrhcg [281.479191ms]
    Apr 17 22:22:19.052: INFO: Created: latency-svc-lnmz8
    Apr 17 22:22:19.059: INFO: Got endpoints: latency-svc-lnmz8 [296.244507ms]
    Apr 17 22:22:19.062: INFO: Created: latency-svc-g2nxw
    Apr 17 22:22:19.065: INFO: Got endpoints: latency-svc-g2nxw [302.452421ms]
    Apr 17 22:22:19.073: INFO: Created: latency-svc-8vz44
    Apr 17 22:22:19.076: INFO: Got endpoints: latency-svc-8vz44 [313.435261ms]
    Apr 17 22:22:19.163: INFO: Created: latency-svc-j2cjv
    Apr 17 22:22:19.169: INFO: Got endpoints: latency-svc-j2cjv [406.700574ms]
    Apr 17 22:22:19.174: INFO: Created: latency-svc-w4r72
    Apr 17 22:22:19.184: INFO: Got endpoints: latency-svc-w4r72 [421.201379ms]
    Apr 17 22:22:19.192: INFO: Created: latency-svc-2b8zb
    Apr 17 22:22:19.200: INFO: Got endpoints: latency-svc-2b8zb [437.107106ms]
    Apr 17 22:22:19.205: INFO: Created: latency-svc-nsfxr
    Apr 17 22:22:19.319: INFO: Got endpoints: latency-svc-nsfxr [556.176944ms]
    Apr 17 22:22:19.321: INFO: Created: latency-svc-672rf
    Apr 17 22:22:19.328: INFO: Got endpoints: latency-svc-672rf [565.420558ms]
    Apr 17 22:22:19.332: INFO: Created: latency-svc-vrf8w
    Apr 17 22:22:19.338: INFO: Got endpoints: latency-svc-vrf8w [552.874188ms]
    Apr 17 22:22:19.343: INFO: Created: latency-svc-x5zlz
    Apr 17 22:22:19.347: INFO: Got endpoints: latency-svc-x5zlz [549.574889ms]
    Apr 17 22:22:19.355: INFO: Created: latency-svc-z2mhj
    Apr 17 22:22:19.445: INFO: Got endpoints: latency-svc-z2mhj [531.032837ms]
    Apr 17 22:22:19.449: INFO: Created: latency-svc-sghhn
    Apr 17 22:22:19.454: INFO: Got endpoints: latency-svc-sghhn [530.802467ms]
    Apr 17 22:22:19.461: INFO: Created: latency-svc-sj7m2
    Apr 17 22:22:19.468: INFO: Got endpoints: latency-svc-sj7m2 [535.423925ms]
    Apr 17 22:22:19.474: INFO: Created: latency-svc-g7pm6
    Apr 17 22:22:19.481: INFO: Got endpoints: latency-svc-g7pm6 [536.81345ms]
    Apr 17 22:22:19.484: INFO: Created: latency-svc-lzzgd
    Apr 17 22:22:19.566: INFO: Got endpoints: latency-svc-lzzgd [522.136749ms]
    Apr 17 22:22:19.571: INFO: Created: latency-svc-dk6l4
    Apr 17 22:22:19.579: INFO: Got endpoints: latency-svc-dk6l4 [520.358255ms]
    Apr 17 22:22:19.580: INFO: Created: latency-svc-xcrnj
    Apr 17 22:22:19.587: INFO: Got endpoints: latency-svc-xcrnj [521.459171ms]
    Apr 17 22:22:19.590: INFO: Created: latency-svc-7kjrn
    Apr 17 22:22:19.595: INFO: Got endpoints: latency-svc-7kjrn [519.06103ms]
    Apr 17 22:22:19.601: INFO: Created: latency-svc-q5rk6
    Apr 17 22:22:19.606: INFO: Got endpoints: latency-svc-q5rk6 [436.608509ms]
    Apr 17 22:22:19.701: INFO: Created: latency-svc-w7h5v
    Apr 17 22:22:19.705: INFO: Got endpoints: latency-svc-w7h5v [521.549583ms]
    Apr 17 22:22:19.713: INFO: Created: latency-svc-bbhc5
    Apr 17 22:22:19.720: INFO: Got endpoints: latency-svc-bbhc5 [520.339619ms]
    Apr 17 22:22:19.724: INFO: Created: latency-svc-chld9
    Apr 17 22:22:19.735: INFO: Got endpoints: latency-svc-chld9 [416.595881ms]
    Apr 17 22:22:19.737: INFO: Created: latency-svc-7x8fg
    Apr 17 22:22:19.740: INFO: Got endpoints: latency-svc-7x8fg [412.283776ms]
    Apr 17 22:22:19.874: INFO: Created: latency-svc-v9mb4
    Apr 17 22:22:19.878: INFO: Got endpoints: latency-svc-v9mb4 [539.883926ms]
    Apr 17 22:22:19.885: INFO: Created: latency-svc-m4wx9
    Apr 17 22:22:19.891: INFO: Got endpoints: latency-svc-m4wx9 [544.217975ms]
    Apr 17 22:22:19.899: INFO: Created: latency-svc-6glr7
    Apr 17 22:22:19.905: INFO: Got endpoints: latency-svc-6glr7 [459.234259ms]
    Apr 17 22:22:19.998: INFO: Created: latency-svc-94bqf
    Apr 17 22:22:20.002: INFO: Created: latency-svc-q5pg7
    Apr 17 22:22:20.003: INFO: Got endpoints: latency-svc-94bqf [548.947878ms]
    Apr 17 22:22:20.006: INFO: Got endpoints: latency-svc-q5pg7 [537.653981ms]
    Apr 17 22:22:20.012: INFO: Created: latency-svc-w5z26
    Apr 17 22:22:20.018: INFO: Got endpoints: latency-svc-w5z26 [536.515081ms]
    Apr 17 22:22:20.022: INFO: Created: latency-svc-mzw7n
    Apr 17 22:22:20.026: INFO: Got endpoints: latency-svc-mzw7n [459.861756ms]
    Apr 17 22:22:20.044: INFO: Created: latency-svc-kpr6x
    Apr 17 22:22:20.131: INFO: Created: latency-svc-qdrdw
    Apr 17 22:22:20.157: INFO: Got endpoints: latency-svc-kpr6x [578.065264ms]
    Apr 17 22:22:20.157: INFO: Got endpoints: latency-svc-qdrdw [570.787358ms]
    Apr 17 22:22:20.158: INFO: Created: latency-svc-2l7n7
    Apr 17 22:22:20.161: INFO: Got endpoints: latency-svc-2l7n7 [565.774516ms]
    Apr 17 22:22:20.170: INFO: Created: latency-svc-vtsz9
    Apr 17 22:22:20.174: INFO: Got endpoints: latency-svc-vtsz9 [568.591872ms]
    Apr 17 22:22:20.184: INFO: Created: latency-svc-hr2fq
    Apr 17 22:22:20.303: INFO: Got endpoints: latency-svc-hr2fq [597.823214ms]
    Apr 17 22:22:20.307: INFO: Created: latency-svc-648rd
    Apr 17 22:22:20.317: INFO: Created: latency-svc-qtnvw
    Apr 17 22:22:20.326: INFO: Created: latency-svc-6zvc5
    Apr 17 22:22:20.336: INFO: Created: latency-svc-tnfvq
    Apr 17 22:22:20.427: INFO: Got endpoints: latency-svc-qtnvw [691.791151ms]
    Apr 17 22:22:20.427: INFO: Got endpoints: latency-svc-648rd [707.189976ms]
    Apr 17 22:22:20.427: INFO: Got endpoints: latency-svc-tnfvq [549.111524ms]
    Apr 17 22:22:20.427: INFO: Got endpoints: latency-svc-6zvc5 [686.914581ms]
    Apr 17 22:22:20.428: INFO: Created: latency-svc-rkv27
    Apr 17 22:22:20.437: INFO: Got endpoints: latency-svc-rkv27 [545.335773ms]
    Apr 17 22:22:20.444: INFO: Created: latency-svc-gdgvd
    Apr 17 22:22:20.450: INFO: Got endpoints: latency-svc-gdgvd [545.615457ms]
    Apr 17 22:22:20.452: INFO: Created: latency-svc-pzkh7
    Apr 17 22:22:20.545: INFO: Created: latency-svc-vrn65
    Apr 17 22:22:20.554: INFO: Created: latency-svc-6mdtw
    Apr 17 22:22:20.562: INFO: Created: latency-svc-6nkm9
    Apr 17 22:22:20.571: INFO: Created: latency-svc-xqmbs
    Apr 17 22:22:20.777: INFO: Got endpoints: latency-svc-vrn65 [771.286355ms]
    Apr 17 22:22:20.777: INFO: Got endpoints: latency-svc-pzkh7 [774.80623ms]
    Apr 17 22:22:20.778: INFO: Got endpoints: latency-svc-6nkm9 [751.424918ms]
    Apr 17 22:22:20.778: INFO: Got endpoints: latency-svc-6mdtw [760.061086ms]
    Apr 17 22:22:20.778: INFO: Got endpoints: latency-svc-xqmbs [620.408306ms]
    Apr 17 22:22:20.782: INFO: Created: latency-svc-qmw4m
    Apr 17 22:22:20.791: INFO: Created: latency-svc-m7t8q
    Apr 17 22:22:20.804: INFO: Created: latency-svc-7kj4g
    Apr 17 22:22:20.817: INFO: Created: latency-svc-x7v8f
    Apr 17 22:22:20.918: INFO: Got endpoints: latency-svc-qmw4m [760.752172ms]
    Apr 17 22:22:20.918: INFO: Got endpoints: latency-svc-m7t8q [757.229398ms]
    Apr 17 22:22:20.918: INFO: Got endpoints: latency-svc-7kj4g [743.804467ms]
    Apr 17 22:22:20.921: INFO: Got endpoints: latency-svc-x7v8f [618.039351ms]
    Apr 17 22:22:20.921: INFO: Created: latency-svc-hf99h
    Apr 17 22:22:20.933: INFO: Got endpoints: latency-svc-hf99h [506.10656ms]
    Apr 17 22:22:20.943: INFO: Created: latency-svc-d84dl
    Apr 17 22:22:20.954: INFO: Created: latency-svc-9wqv8
    Apr 17 22:22:20.957: INFO: Got endpoints: latency-svc-d84dl [530.104266ms]
    Apr 17 22:22:21.128: INFO: Got endpoints: latency-svc-9wqv8 [701.120411ms]
    Apr 17 22:22:21.132: INFO: Created: latency-svc-bhqhx
    Apr 17 22:22:21.134: INFO: Created: latency-svc-dbt6j
    Apr 17 22:22:21.150: INFO: Created: latency-svc-qpmjs
    Apr 17 22:22:21.160: INFO: Created: latency-svc-p8tlw
    Apr 17 22:22:21.166: INFO: Got endpoints: latency-svc-bhqhx [738.987323ms]
    Apr 17 22:22:21.169: INFO: Created: latency-svc-nh98j
    Apr 17 22:22:21.173: INFO: Got endpoints: latency-svc-qpmjs [722.113265ms]
    Apr 17 22:22:21.173: INFO: Got endpoints: latency-svc-p8tlw [394.782797ms]
    Apr 17 22:22:21.173: INFO: Got endpoints: latency-svc-dbt6j [736.088043ms]
    Apr 17 22:22:21.277: INFO: Got endpoints: latency-svc-nh98j [499.679651ms]
    Apr 17 22:22:21.279: INFO: Created: latency-svc-mtcxk
    Apr 17 22:22:21.289: INFO: Created: latency-svc-6l4v8
    Apr 17 22:22:21.300: INFO: Created: latency-svc-4chvk
    Apr 17 22:22:21.305: INFO: Created: latency-svc-8hrk2
    Apr 17 22:22:21.396: INFO: Got endpoints: latency-svc-6l4v8 [618.282348ms]
    Apr 17 22:22:21.396: INFO: Got endpoints: latency-svc-mtcxk [618.520979ms]
    Apr 17 22:22:21.397: INFO: Created: latency-svc-z2r42
    Apr 17 22:22:21.406: INFO: Got endpoints: latency-svc-8hrk2 [487.647282ms]
    Apr 17 22:22:21.406: INFO: Got endpoints: latency-svc-4chvk [628.370712ms]
    Apr 17 22:22:21.408: INFO: Created: latency-svc-4k6jv
    Apr 17 22:22:21.421: INFO: Got endpoints: latency-svc-z2r42 [502.347723ms]
    Apr 17 22:22:21.424: INFO: Got endpoints: latency-svc-4k6jv [506.283111ms]
    Apr 17 22:22:21.427: INFO: Created: latency-svc-rrbzq
    Apr 17 22:22:21.438: INFO: Created: latency-svc-kxspz
    Apr 17 22:22:21.439: INFO: Got endpoints: latency-svc-rrbzq [517.401305ms]
    Apr 17 22:22:21.512: INFO: Got endpoints: latency-svc-kxspz [578.926073ms]
    Apr 17 22:22:21.514: INFO: Created: latency-svc-c4xt2
    Apr 17 22:22:21.526: INFO: Created: latency-svc-bcc8b
    Apr 17 22:22:21.535: INFO: Created: latency-svc-6fhgl
    Apr 17 22:22:21.545: INFO: Created: latency-svc-k2mz7
    Apr 17 22:22:21.667: INFO: Got endpoints: latency-svc-6fhgl [501.020889ms]
    Apr 17 22:22:21.667: INFO: Got endpoints: latency-svc-k2mz7 [494.804596ms]
    Apr 17 22:22:21.668: INFO: Got endpoints: latency-svc-c4xt2 [710.338621ms]
    Apr 17 22:22:21.668: INFO: Got endpoints: latency-svc-bcc8b [539.364897ms]
    Apr 17 22:22:21.671: INFO: Created: latency-svc-m2phk
    Apr 17 22:22:21.681: INFO: Created: latency-svc-74np2
    Apr 17 22:22:21.688: INFO: Created: latency-svc-zhjr8
    Apr 17 22:22:21.698: INFO: Created: latency-svc-rdx2s
    Apr 17 22:22:21.787: INFO: Got endpoints: latency-svc-m2phk [614.724603ms]
    Apr 17 22:22:21.788: INFO: Got endpoints: latency-svc-zhjr8 [510.010015ms]
    Apr 17 22:22:21.788: INFO: Got endpoints: latency-svc-74np2 [615.008347ms]
    Apr 17 22:22:21.791: INFO: Got endpoints: latency-svc-rdx2s [394.648545ms]
    Apr 17 22:22:21.793: INFO: Created: latency-svc-pgvvn
    Apr 17 22:22:21.802: INFO: Got endpoints: latency-svc-pgvvn [405.564283ms]
    Apr 17 22:22:21.805: INFO: Created: latency-svc-z8wlx
    Apr 17 22:22:21.813: INFO: Got endpoints: latency-svc-z8wlx [406.921565ms]
    Apr 17 22:22:21.816: INFO: Created: latency-svc-9c2tv
    Apr 17 22:22:21.887: INFO: Got endpoints: latency-svc-9c2tv [480.883892ms]
    Apr 17 22:22:21.890: INFO: Created: latency-svc-hrln2
    Apr 17 22:22:21.897: INFO: Created: latency-svc-zxhvg
    Apr 17 22:22:21.908: INFO: Created: latency-svc-5vn2g
    Apr 17 22:22:21.927: INFO: Created: latency-svc-2vs6q
    Apr 17 22:22:22.003: INFO: Created: latency-svc-wt4x5
    Apr 17 22:22:22.040: INFO: Got endpoints: latency-svc-hrln2 [619.047415ms]
    Apr 17 22:22:22.042: INFO: Created: latency-svc-5lqlf
    Apr 17 22:22:22.043: INFO: Got endpoints: latency-svc-5vn2g [604.546754ms]
    Apr 17 22:22:22.043: INFO: Got endpoints: latency-svc-zxhvg [618.932842ms]
    Apr 17 22:22:22.043: INFO: Got endpoints: latency-svc-2vs6q [531.045314ms]
    Apr 17 22:22:22.043: INFO: Got endpoints: latency-svc-wt4x5 [375.591427ms]
    Apr 17 22:22:22.048: INFO: Got endpoints: latency-svc-5lqlf [380.712569ms]
    Apr 17 22:22:22.055: INFO: Created: latency-svc-wx4qf
    Apr 17 22:22:22.063: INFO: Got endpoints: latency-svc-wx4qf [394.851096ms]
    Apr 17 22:22:22.068: INFO: Created: latency-svc-6qk44
    Apr 17 22:22:22.072: INFO: Got endpoints: latency-svc-6qk44 [404.542925ms]
    Apr 17 22:22:22.177: INFO: Created: latency-svc-kx4bk
    Apr 17 22:22:22.187: INFO: Created: latency-svc-mrwf5
    Apr 17 22:22:22.196: INFO: Got endpoints: latency-svc-mrwf5 [408.839161ms]
    Apr 17 22:22:22.196: INFO: Got endpoints: latency-svc-kx4bk [409.077257ms]
    Apr 17 22:22:22.199: INFO: Created: latency-svc-pnnn7
    Apr 17 22:22:22.204: INFO: Got endpoints: latency-svc-pnnn7 [416.205818ms]
    Apr 17 22:22:22.288: INFO: Created: latency-svc-df422
    Apr 17 22:22:22.291: INFO: Created: latency-svc-l8n7v
    Apr 17 22:22:22.293: INFO: Got endpoints: latency-svc-df422 [502.578364ms]
    Apr 17 22:22:22.304: INFO: Created: latency-svc-mz69s
    Apr 17 22:22:22.308: INFO: Created: latency-svc-fqdlb
    Apr 17 22:22:22.316: INFO: Created: latency-svc-fb25b
    Apr 17 22:22:22.419: INFO: Got endpoints: latency-svc-mz69s [606.3042ms]
    Apr 17 22:22:22.419: INFO: Got endpoints: latency-svc-l8n7v [617.550151ms]
    Apr 17 22:22:22.420: INFO: Got endpoints: latency-svc-fqdlb [532.821567ms]
    Apr 17 22:22:22.420: INFO: Got endpoints: latency-svc-fb25b [379.893953ms]
    Apr 17 22:22:22.423: INFO: Created: latency-svc-h6sf7
    Apr 17 22:22:22.430: INFO: Got endpoints: latency-svc-h6sf7 [386.278701ms]
    Apr 17 22:22:22.435: INFO: Created: latency-svc-pbrpv
    Apr 17 22:22:22.440: INFO: Got endpoints: latency-svc-pbrpv [396.723489ms]
    Apr 17 22:22:22.445: INFO: Created: latency-svc-g8znr
    Apr 17 22:22:22.454: INFO: Created: latency-svc-cwvfk
    Apr 17 22:22:22.566: INFO: Got endpoints: latency-svc-cwvfk [522.140458ms]
    Apr 17 22:22:22.566: INFO: Got endpoints: latency-svc-g8znr [522.176024ms]
    Apr 17 22:22:22.572: INFO: Created: latency-svc-rm488
    Apr 17 22:22:22.581: INFO: Created: latency-svc-4l84l
    Apr 17 22:22:22.590: INFO: Created: latency-svc-8wk8c
    Apr 17 22:22:22.606: INFO: Created: latency-svc-jmbfn
    Apr 17 22:22:22.692: INFO: Got endpoints: latency-svc-4l84l [629.166431ms]
    Apr 17 22:22:22.692: INFO: Got endpoints: latency-svc-rm488 [643.623582ms]
    Apr 17 22:22:22.692: INFO: Got endpoints: latency-svc-8wk8c [619.811872ms]
    Apr 17 22:22:22.694: INFO: Created: latency-svc-lp66w
    Apr 17 22:22:22.710: INFO: Created: latency-svc-zsm9r
    Apr 17 22:22:22.715: INFO: Got endpoints: latency-svc-jmbfn [518.864046ms]
    Apr 17 22:22:22.722: INFO: Created: latency-svc-g58cq
    Apr 17 22:22:22.728: INFO: Created: latency-svc-8wpfm
    Apr 17 22:22:22.833: INFO: Got endpoints: latency-svc-zsm9r [629.546ms]
    Apr 17 22:22:22.833: INFO: Got endpoints: latency-svc-lp66w [636.819216ms]
    Apr 17 22:22:22.836: INFO: Created: latency-svc-bvsgp
    Apr 17 22:22:22.844: INFO: Created: latency-svc-qx7wx
    Apr 17 22:22:22.854: INFO: Created: latency-svc-cg6tz
    Apr 17 22:22:22.863: INFO: Created: latency-svc-7j247
    Apr 17 22:22:22.958: INFO: Got endpoints: latency-svc-g58cq [664.825986ms]
    Apr 17 22:22:22.961: INFO: Got endpoints: latency-svc-8wpfm [541.667832ms]
    Apr 17 22:22:22.963: INFO: Created: latency-svc-vzrgg
    Apr 17 22:22:22.965: INFO: Got endpoints: latency-svc-bvsgp [545.94315ms]
    Apr 17 22:22:22.973: INFO: Created: latency-svc-2fl4m
    Apr 17 22:22:22.981: INFO: Created: latency-svc-p8kqq
    Apr 17 22:22:22.993: INFO: Created: latency-svc-mblf7
    Apr 17 22:22:23.065: INFO: Got endpoints: latency-svc-qx7wx [644.864778ms]
    Apr 17 22:22:23.066: INFO: Got endpoints: latency-svc-cg6tz [646.861849ms]
    Apr 17 22:22:23.069: INFO: Created: latency-svc-z7wxh
    Apr 17 22:22:23.076: INFO: Created: latency-svc-pkrtd
    Apr 17 22:22:23.086: INFO: Created: latency-svc-6dsbh
    Apr 17 22:22:23.095: INFO: Created: latency-svc-jnvj8
    Apr 17 22:22:23.203: INFO: Got endpoints: latency-svc-vzrgg [762.566589ms]
    Apr 17 22:22:23.203: INFO: Got endpoints: latency-svc-7j247 [773.169912ms]
    Apr 17 22:22:23.206: INFO: Created: latency-svc-hnskk
    Apr 17 22:22:23.213: INFO: Got endpoints: latency-svc-2fl4m [647.10548ms]
    Apr 17 22:22:23.216: INFO: Created: latency-svc-jnqqx
    Apr 17 22:22:23.225: INFO: Created: latency-svc-xdwfk
    Apr 17 22:22:23.312: INFO: Got endpoints: latency-svc-p8kqq [746.029608ms]
    Apr 17 22:22:23.315: INFO: Got endpoints: latency-svc-mblf7 [622.814299ms]
    Apr 17 22:22:23.319: INFO: Created: latency-svc-64qz8
    Apr 17 22:22:23.329: INFO: Created: latency-svc-m7vnd
    Apr 17 22:22:23.339: INFO: Created: latency-svc-7jhhl
    Apr 17 22:22:23.352: INFO: Created: latency-svc-x9gfj
    Apr 17 22:22:23.472: INFO: Got endpoints: latency-svc-z7wxh [779.91819ms]
    Apr 17 22:22:23.472: INFO: Got endpoints: latency-svc-pkrtd [780.32645ms]
    Apr 17 22:22:23.472: INFO: Got endpoints: latency-svc-6dsbh [756.993718ms]
    Apr 17 22:22:23.477: INFO: Created: latency-svc-2qjql
    Apr 17 22:22:23.484: INFO: Created: latency-svc-w6646
    Apr 17 22:22:23.494: INFO: Created: latency-svc-msvrj
    Apr 17 22:22:23.582: INFO: Created: latency-svc-6dx6b
    Apr 17 22:22:23.584: INFO: Got endpoints: latency-svc-jnvj8 [750.327743ms]
    Apr 17 22:22:23.584: INFO: Got endpoints: latency-svc-hnskk [750.311947ms]
    Apr 17 22:22:23.587: INFO: Created: latency-svc-stmc9
    Apr 17 22:22:23.596: INFO: Created: latency-svc-2jssr
    Apr 17 22:22:23.608: INFO: Created: latency-svc-llnzh
    Apr 17 22:22:23.617: INFO: Got endpoints: latency-svc-jnqqx [658.629731ms]
    Apr 17 22:22:23.619: INFO: Created: latency-svc-vgx4c
    Apr 17 22:22:23.693: INFO: Got endpoints: latency-svc-xdwfk [732.266082ms]
    Apr 17 22:22:23.696: INFO: Created: latency-svc-kvj2w
    Apr 17 22:22:23.707: INFO: Created: latency-svc-j722t
    Apr 17 22:22:23.721: INFO: Got endpoints: latency-svc-64qz8 [756.146064ms]
    Apr 17 22:22:23.724: INFO: Created: latency-svc-kn8dk
    Apr 17 22:22:23.735: INFO: Created: latency-svc-mvmhl
    Apr 17 22:22:23.764: INFO: Got endpoints: latency-svc-m7vnd [699.324399ms]
    Apr 17 22:22:23.914: INFO: Got endpoints: latency-svc-x9gfj [711.102239ms]
    Apr 17 22:22:23.914: INFO: Got endpoints: latency-svc-7jhhl [847.48899ms]
    Apr 17 22:22:23.917: INFO: Created: latency-svc-2bbg9
    Apr 17 22:22:23.917: INFO: Got endpoints: latency-svc-2qjql [714.618046ms]
    Apr 17 22:22:23.927: INFO: Created: latency-svc-dbh9f
    Apr 17 22:22:23.937: INFO: Created: latency-svc-25fl5
    Apr 17 22:22:23.947: INFO: Created: latency-svc-5h9ht
    Apr 17 22:22:23.962: INFO: Got endpoints: latency-svc-w6646 [749.259473ms]
    Apr 17 22:22:24.045: INFO: Got endpoints: latency-svc-msvrj [733.35982ms]
    Apr 17 22:22:24.046: INFO: Created: latency-svc-jksfw
    Apr 17 22:22:24.062: INFO: Created: latency-svc-gxs2b
    Apr 17 22:22:24.064: INFO: Got endpoints: latency-svc-6dx6b [748.984154ms]
    Apr 17 22:22:24.079: INFO: Created: latency-svc-h4mwb
    Apr 17 22:22:24.113: INFO: Got endpoints: latency-svc-stmc9 [641.134583ms]
    Apr 17 22:22:24.133: INFO: Created: latency-svc-fjtx8
    Apr 17 22:22:24.165: INFO: Got endpoints: latency-svc-2jssr [693.079597ms]
    Apr 17 22:22:24.178: INFO: Created: latency-svc-8xgsz
    Apr 17 22:22:24.214: INFO: Got endpoints: latency-svc-llnzh [741.366831ms]
    Apr 17 22:22:24.227: INFO: Created: latency-svc-hldws
    Apr 17 22:22:24.264: INFO: Got endpoints: latency-svc-vgx4c [680.16928ms]
    Apr 17 22:22:24.278: INFO: Created: latency-svc-ws67k
    Apr 17 22:22:24.312: INFO: Got endpoints: latency-svc-kvj2w [728.608161ms]
    Apr 17 22:22:24.325: INFO: Created: latency-svc-q2qgc
    Apr 17 22:22:24.362: INFO: Got endpoints: latency-svc-j722t [745.539643ms]
    Apr 17 22:22:24.457: INFO: Got endpoints: latency-svc-kn8dk [763.71572ms]
    Apr 17 22:22:24.461: INFO: Created: latency-svc-28nn6
    Apr 17 22:22:24.464: INFO: Got endpoints: latency-svc-mvmhl [743.096917ms]
    Apr 17 22:22:24.480: INFO: Created: latency-svc-bxpmb
    Apr 17 22:22:24.487: INFO: Created: latency-svc-5m9st
    Apr 17 22:22:24.513: INFO: Got endpoints: latency-svc-2bbg9 [749.364179ms]
    Apr 17 22:22:24.525: INFO: Created: latency-svc-b9lpm
    Apr 17 22:22:24.564: INFO: Got endpoints: latency-svc-dbh9f [650.452844ms]
    Apr 17 22:22:24.663: INFO: Created: latency-svc-xwtpw
    Apr 17 22:22:24.705: INFO: Got endpoints: latency-svc-5h9ht [787.89711ms]
    Apr 17 22:22:24.705: INFO: Got endpoints: latency-svc-25fl5 [791.356536ms]
    Apr 17 22:22:24.714: INFO: Got endpoints: latency-svc-jksfw [751.926796ms]
    Apr 17 22:22:24.723: INFO: Created: latency-svc-rrwsb
    Apr 17 22:22:24.732: INFO: Created: latency-svc-lf5nl
    Apr 17 22:22:24.739: INFO: Created: latency-svc-gqfb6
    Apr 17 22:22:24.765: INFO: Got endpoints: latency-svc-gxs2b [719.887738ms]
    Apr 17 22:22:24.851: INFO: Created: latency-svc-mm6r2
    Apr 17 22:22:24.866: INFO: Got endpoints: latency-svc-h4mwb [801.839686ms]
    Apr 17 22:22:24.866: INFO: Got endpoints: latency-svc-fjtx8 [752.786619ms]
    Apr 17 22:22:24.883: INFO: Created: latency-svc-hchgw
    Apr 17 22:22:24.892: INFO: Created: latency-svc-ldlcd
    Apr 17 22:22:24.913: INFO: Got endpoints: latency-svc-8xgsz [748.105742ms]
    Apr 17 22:22:24.955: INFO: Created: latency-svc-mkzsj
    Apr 17 22:22:24.964: INFO: Got endpoints: latency-svc-hldws [750.614566ms]
    Apr 17 22:22:25.053: INFO: Got endpoints: latency-svc-ws67k [788.631532ms]
    Apr 17 22:22:25.058: INFO: Created: latency-svc-fwvlv
    Apr 17 22:22:25.100: INFO: Got endpoints: latency-svc-q2qgc [787.840865ms]
    Apr 17 22:22:25.111: INFO: Created: latency-svc-lmwz5
    Apr 17 22:22:25.116: INFO: Got endpoints: latency-svc-28nn6 [753.615115ms]
    Apr 17 22:22:25.119: INFO: Created: latency-svc-rkdb5
    Apr 17 22:22:25.130: INFO: Created: latency-svc-lhr7k
    Apr 17 22:22:25.164: INFO: Got endpoints: latency-svc-bxpmb [707.281731ms]
    Apr 17 22:22:25.275: INFO: Got endpoints: latency-svc-b9lpm [762.019172ms]
    Apr 17 22:22:25.275: INFO: Got endpoints: latency-svc-5m9st [810.990221ms]
    Apr 17 22:22:25.279: INFO: Created: latency-svc-gwp55
    Apr 17 22:22:25.289: INFO: Created: latency-svc-k7stf
    Apr 17 22:22:25.296: INFO: Created: latency-svc-fpsjf
    Apr 17 22:22:25.315: INFO: Got endpoints: latency-svc-xwtpw [750.7345ms]
    Apr 17 22:22:25.328: INFO: Created: latency-svc-88sf5
    Apr 17 22:22:25.363: INFO: Got endpoints: latency-svc-rrwsb [657.433521ms]
    Apr 17 22:22:25.474: INFO: Created: latency-svc-s625b
    Apr 17 22:22:25.585: INFO: Got endpoints: latency-svc-lf5nl [879.665118ms]
    Apr 17 22:22:25.585: INFO: Got endpoints: latency-svc-mm6r2 [820.186709ms]
    Apr 17 22:22:25.585: INFO: Got endpoints: latency-svc-gqfb6 [871.3108ms]
    Apr 17 22:22:25.585: INFO: Got endpoints: latency-svc-hchgw [719.607738ms]
    Apr 17 22:22:25.600: INFO: Created: latency-svc-vpgsf
    Apr 17 22:22:25.610: INFO: Created: latency-svc-dpp2k
    Apr 17 22:22:25.613: INFO: Got endpoints: latency-svc-ldlcd [747.605616ms]
    Apr 17 22:22:25.621: INFO: Created: latency-svc-2prr2
    Apr 17 22:22:25.787: INFO: Got endpoints: latency-svc-mkzsj [873.56469ms]
    Apr 17 22:22:25.787: INFO: Got endpoints: latency-svc-lmwz5 [734.533649ms]
    Apr 17 22:22:25.787: INFO: Got endpoints: latency-svc-fwvlv [822.654402ms]
    Apr 17 22:22:25.792: INFO: Created: latency-svc-xxkcr
    Apr 17 22:22:25.800: INFO: Created: latency-svc-8j8n4
    Apr 17 22:22:25.825: INFO: Got endpoints: latency-svc-rkdb5 [724.400096ms]
    Apr 17 22:22:25.825: INFO: Created: latency-svc-4hb8z
    Apr 17 22:22:25.839: INFO: Created: latency-svc-6cnfs
    Apr 17 22:22:25.971: INFO: Got endpoints: latency-svc-lhr7k [855.059541ms]
    Apr 17 22:22:25.971: INFO: Got endpoints: latency-svc-gwp55 [807.082102ms]
    Apr 17 22:22:25.971: INFO: Got endpoints: latency-svc-k7stf [695.81054ms]
    Apr 17 22:22:25.976: INFO: Created: latency-svc-bqzfl
    Apr 17 22:22:25.984: INFO: Created: latency-svc-225xw
    Apr 17 22:22:25.992: INFO: Created: latency-svc-j9m8t
    Apr 17 22:22:26.000: INFO: Created: latency-svc-7lggh
    Apr 17 22:22:26.195: INFO: Got endpoints: latency-svc-fpsjf [919.683141ms]
    Apr 17 22:22:26.197: INFO: Got endpoints: latency-svc-s625b [834.163709ms]
    Apr 17 22:22:26.197: INFO: Got endpoints: latency-svc-88sf5 [881.827341ms]
    Apr 17 22:22:26.197: INFO: Got endpoints: latency-svc-vpgsf [611.744954ms]
    Apr 17 22:22:26.200: INFO: Created: latency-svc-8mb4t
    Apr 17 22:22:26.211: INFO: Created: latency-svc-wk7d9
    Apr 17 22:22:26.215: INFO: Got endpoints: latency-svc-dpp2k [629.534528ms]
    Apr 17 22:22:26.219: INFO: Created: latency-svc-knxb2
    Apr 17 22:22:26.228: INFO: Created: latency-svc-5h5q7
    Apr 17 22:22:26.381: INFO: Got endpoints: latency-svc-xxkcr [795.382797ms]
    Apr 17 22:22:26.381: INFO: Got endpoints: latency-svc-2prr2 [795.656351ms]
    Apr 17 22:22:26.381: INFO: Got endpoints: latency-svc-8j8n4 [767.675494ms]
    Apr 17 22:22:26.386: INFO: Created: latency-svc-gd2hz
    Apr 17 22:22:26.392: INFO: Created: latency-svc-dtx85
    Apr 17 22:22:26.402: INFO: Created: latency-svc-tvc2q
    Apr 17 22:22:26.559: INFO: Got endpoints: latency-svc-6cnfs [771.552891ms]
    Apr 17 22:22:26.559: INFO: Got endpoints: latency-svc-4hb8z [771.605717ms]
    Apr 17 22:22:26.559: INFO: Got endpoints: latency-svc-bqzfl [771.619662ms]
    Apr 17 22:22:26.564: INFO: Created: latency-svc-dt59d
    Apr 17 22:22:26.566: INFO: Got endpoints: latency-svc-225xw [741.556932ms]
    Apr 17 22:22:26.575: INFO: Created: latency-svc-8txdn
    Apr 17 22:22:26.583: INFO: Created: latency-svc-6g9rh
    Apr 17 22:22:26.591: INFO: Created: latency-svc-7mmgf
    Apr 17 22:22:26.777: INFO: Got endpoints: latency-svc-j9m8t [805.714109ms]
    Apr 17 22:22:26.777: INFO: Got endpoints: latency-svc-7lggh [806.065721ms]
    Apr 17 22:22:26.778: INFO: Got endpoints: latency-svc-8mb4t [807.01718ms]
    Apr 17 22:22:26.778: INFO: Got endpoints: latency-svc-wk7d9 [583.237011ms]
    Apr 17 22:22:26.781: INFO: Created: latency-svc-82mss
    Apr 17 22:22:26.792: INFO: Created: latency-svc-ngkbw
    Apr 17 22:22:26.814: INFO: Got endpoints: latency-svc-knxb2 [616.72919ms]
    Apr 17 22:22:26.866: INFO: Got endpoints: latency-svc-dtx85 [650.601025ms]
    Apr 17 22:22:26.977: INFO: Got endpoints: latency-svc-5h5q7 [779.878036ms]
    Apr 17 22:22:26.977: INFO: Got endpoints: latency-svc-gd2hz [779.964281ms]
    Apr 17 22:22:27.014: INFO: Got endpoints: latency-svc-tvc2q [633.097515ms]
    Apr 17 22:22:27.063: INFO: Got endpoints: latency-svc-dt59d [682.53715ms]
    Apr 17 22:22:27.113: INFO: Got endpoints: latency-svc-8txdn [732.332319ms]
    Apr 17 22:22:27.163: INFO: Got endpoints: latency-svc-6g9rh [604.58639ms]
    Apr 17 22:22:27.214: INFO: Got endpoints: latency-svc-7mmgf [655.501542ms]
    Apr 17 22:22:27.262: INFO: Got endpoints: latency-svc-82mss [703.201941ms]
    Apr 17 22:22:27.314: INFO: Got endpoints: latency-svc-ngkbw [747.964378ms]
    Apr 17 22:22:27.314: INFO: Latencies: [22.976132ms 34.875317ms 151.938052ms 160.317884ms 170.413765ms 181.810272ms 281.479191ms 296.244507ms 302.452421ms 313.435261ms 375.591427ms 379.893953ms 380.712569ms 386.278701ms 394.648545ms 394.782797ms 394.851096ms 396.723489ms 404.542925ms 405.564283ms 406.700574ms 406.921565ms 408.839161ms 409.077257ms 412.283776ms 416.205818ms 416.595881ms 421.201379ms 436.608509ms 437.107106ms 459.234259ms 459.861756ms 480.883892ms 487.647282ms 494.804596ms 499.679651ms 501.020889ms 502.347723ms 502.578364ms 506.10656ms 506.283111ms 510.010015ms 517.401305ms 518.864046ms 519.06103ms 520.339619ms 520.358255ms 521.459171ms 521.549583ms 522.136749ms 522.140458ms 522.176024ms 530.104266ms 530.802467ms 531.032837ms 531.045314ms 532.821567ms 535.423925ms 536.515081ms 536.81345ms 537.653981ms 539.364897ms 539.883926ms 541.667832ms 544.217975ms 545.335773ms 545.615457ms 545.94315ms 548.947878ms 549.111524ms 549.574889ms 552.874188ms 556.176944ms 565.420558ms 565.774516ms 568.591872ms 570.787358ms 578.065264ms 578.926073ms 583.237011ms 597.823214ms 604.546754ms 604.58639ms 606.3042ms 611.744954ms 614.724603ms 615.008347ms 616.72919ms 617.550151ms 618.039351ms 618.282348ms 618.520979ms 618.932842ms 619.047415ms 619.811872ms 620.408306ms 622.814299ms 628.370712ms 629.166431ms 629.534528ms 629.546ms 633.097515ms 636.819216ms 641.134583ms 643.623582ms 644.864778ms 646.861849ms 647.10548ms 650.452844ms 650.601025ms 655.501542ms 657.433521ms 658.629731ms 664.825986ms 680.16928ms 682.53715ms 686.914581ms 691.791151ms 693.079597ms 695.81054ms 699.324399ms 701.120411ms 703.201941ms 707.189976ms 707.281731ms 710.338621ms 711.102239ms 714.618046ms 719.607738ms 719.887738ms 722.113265ms 724.400096ms 728.608161ms 732.266082ms 732.332319ms 733.35982ms 734.533649ms 736.088043ms 738.987323ms 741.366831ms 741.556932ms 743.096917ms 743.804467ms 745.539643ms 746.029608ms 747.605616ms 747.964378ms 748.105742ms 748.984154ms 749.259473ms 749.364179ms 750.311947ms 750.327743ms 750.614566ms 750.7345ms 751.424918ms 751.926796ms 752.786619ms 753.615115ms 756.146064ms 756.993718ms 757.229398ms 760.061086ms 760.752172ms 762.019172ms 762.566589ms 763.71572ms 767.675494ms 771.286355ms 771.552891ms 771.605717ms 771.619662ms 773.169912ms 774.80623ms 779.878036ms 779.91819ms 779.964281ms 780.32645ms 787.840865ms 787.89711ms 788.631532ms 791.356536ms 795.382797ms 795.656351ms 801.839686ms 805.714109ms 806.065721ms 807.01718ms 807.082102ms 810.990221ms 820.186709ms 822.654402ms 834.163709ms 847.48899ms 855.059541ms 871.3108ms 873.56469ms 879.665118ms 881.827341ms 919.683141ms]
    Apr 17 22:22:27.314: INFO: 50 %ile: 629.546ms
    Apr 17 22:22:27.314: INFO: 90 %ile: 788.631532ms
    Apr 17 22:22:27.314: INFO: 99 %ile: 881.827341ms
    Apr 17 22:22:27.314: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:27.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-2426" for this suite. 04/17/23 22:22:27.323
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:27.328
Apr 17 22:22:27.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-lifecycle-hook 04/17/23 22:22:27.329
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:27.341
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:27.343
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/17/23 22:22:27.351
Apr 17 22:22:27.358: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9059" to be "running and ready"
Apr 17 22:22:27.360: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.322458ms
Apr 17 22:22:27.360: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:22:29.369: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011362388s
Apr 17 22:22:29.369: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 17 22:22:29.369: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 04/17/23 22:22:29.38
Apr 17 22:22:29.391: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9059" to be "running and ready"
Apr 17 22:22:29.404: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.464317ms
Apr 17 22:22:29.404: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:22:31.408: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016432601s
Apr 17 22:22:31.408: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Apr 17 22:22:31.408: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 04/17/23 22:22:31.41
STEP: delete the pod with lifecycle hook 04/17/23 22:22:31.415
Apr 17 22:22:31.421: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 17 22:22:31.423: INFO: Pod pod-with-poststart-http-hook still exists
Apr 17 22:22:33.424: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 17 22:22:33.429: INFO: Pod pod-with-poststart-http-hook still exists
Apr 17 22:22:35.425: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr 17 22:22:35.455: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:35.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9059" for this suite. 04/17/23 22:22:35.462
------------------------------
• [SLOW TEST] [8.141 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:27.328
    Apr 17 22:22:27.328: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/17/23 22:22:27.329
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:27.341
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:27.343
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/17/23 22:22:27.351
    Apr 17 22:22:27.358: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9059" to be "running and ready"
    Apr 17 22:22:27.360: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.322458ms
    Apr 17 22:22:27.360: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:22:29.369: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011362388s
    Apr 17 22:22:29.369: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 17 22:22:29.369: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 04/17/23 22:22:29.38
    Apr 17 22:22:29.391: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9059" to be "running and ready"
    Apr 17 22:22:29.404: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 12.464317ms
    Apr 17 22:22:29.404: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:22:31.408: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.016432601s
    Apr 17 22:22:31.408: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Apr 17 22:22:31.408: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 04/17/23 22:22:31.41
    STEP: delete the pod with lifecycle hook 04/17/23 22:22:31.415
    Apr 17 22:22:31.421: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr 17 22:22:31.423: INFO: Pod pod-with-poststart-http-hook still exists
    Apr 17 22:22:33.424: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr 17 22:22:33.429: INFO: Pod pod-with-poststart-http-hook still exists
    Apr 17 22:22:35.425: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Apr 17 22:22:35.455: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:35.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9059" for this suite. 04/17/23 22:22:35.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:35.47
Apr 17 22:22:35.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename security-context-test 04/17/23 22:22:35.471
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:35.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:35.494
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Apr 17 22:22:35.645: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c" in namespace "security-context-test-517" to be "Succeeded or Failed"
Apr 17 22:22:35.650: INFO: Pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.833448ms
Apr 17 22:22:37.689: INFO: Pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c": Phase="Running", Reason="", readiness=true. Elapsed: 2.043822929s
Apr 17 22:22:39.724: INFO: Pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078803514s
Apr 17 22:22:39.724: INFO: Pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:39.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-517" for this suite. 04/17/23 22:22:39.808
------------------------------
• [4.343 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:35.47
    Apr 17 22:22:35.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename security-context-test 04/17/23 22:22:35.471
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:35.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:35.494
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Apr 17 22:22:35.645: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c" in namespace "security-context-test-517" to be "Succeeded or Failed"
    Apr 17 22:22:35.650: INFO: Pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.833448ms
    Apr 17 22:22:37.689: INFO: Pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c": Phase="Running", Reason="", readiness=true. Elapsed: 2.043822929s
    Apr 17 22:22:39.724: INFO: Pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.078803514s
    Apr 17 22:22:39.724: INFO: Pod "busybox-readonly-false-79d7dd05-2022-4d93-bf4b-72a81ad2063c" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:39.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-517" for this suite. 04/17/23 22:22:39.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:39.814
Apr 17 22:22:39.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 22:22:39.815
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:39.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:39.966
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 04/17/23 22:22:39.979
STEP: watching for Pod to be ready 04/17/23 22:22:39.986
Apr 17 22:22:39.988: INFO: observed Pod pod-test in namespace pods-4015 in phase Pending with labels: map[test-pod-static:true] & conditions []
Apr 17 22:22:39.991: INFO: observed Pod pod-test in namespace pods-4015 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  }]
Apr 17 22:22:40.003: INFO: observed Pod pod-test in namespace pods-4015 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  }]
Apr 17 22:22:40.710: INFO: observed Pod pod-test in namespace pods-4015 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  }]
Apr 17 22:22:41.643: INFO: Found Pod pod-test in namespace pods-4015 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 04/17/23 22:22:41.646
STEP: getting the Pod and ensuring that it's patched 04/17/23 22:22:41.655
STEP: replacing the Pod's status Ready condition to False 04/17/23 22:22:41.66
STEP: check the Pod again to ensure its Ready conditions are False 04/17/23 22:22:41.669
STEP: deleting the Pod via a Collection with a LabelSelector 04/17/23 22:22:41.669
STEP: watching for the Pod to be deleted 04/17/23 22:22:41.677
Apr 17 22:22:41.679: INFO: observed event type MODIFIED
Apr 17 22:22:43.619: INFO: observed event type MODIFIED
Apr 17 22:22:44.017: INFO: observed event type MODIFIED
Apr 17 22:22:44.621: INFO: observed event type MODIFIED
Apr 17 22:22:44.627: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:44.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4015" for this suite. 04/17/23 22:22:44.636
------------------------------
• [4.826 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:39.814
    Apr 17 22:22:39.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 22:22:39.815
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:39.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:39.966
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 04/17/23 22:22:39.979
    STEP: watching for Pod to be ready 04/17/23 22:22:39.986
    Apr 17 22:22:39.988: INFO: observed Pod pod-test in namespace pods-4015 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Apr 17 22:22:39.991: INFO: observed Pod pod-test in namespace pods-4015 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  }]
    Apr 17 22:22:40.003: INFO: observed Pod pod-test in namespace pods-4015 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  }]
    Apr 17 22:22:40.710: INFO: observed Pod pod-test in namespace pods-4015 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  }]
    Apr 17 22:22:41.643: INFO: Found Pod pod-test in namespace pods-4015 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:41 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:41 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:22:39 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 04/17/23 22:22:41.646
    STEP: getting the Pod and ensuring that it's patched 04/17/23 22:22:41.655
    STEP: replacing the Pod's status Ready condition to False 04/17/23 22:22:41.66
    STEP: check the Pod again to ensure its Ready conditions are False 04/17/23 22:22:41.669
    STEP: deleting the Pod via a Collection with a LabelSelector 04/17/23 22:22:41.669
    STEP: watching for the Pod to be deleted 04/17/23 22:22:41.677
    Apr 17 22:22:41.679: INFO: observed event type MODIFIED
    Apr 17 22:22:43.619: INFO: observed event type MODIFIED
    Apr 17 22:22:44.017: INFO: observed event type MODIFIED
    Apr 17 22:22:44.621: INFO: observed event type MODIFIED
    Apr 17 22:22:44.627: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:44.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4015" for this suite. 04/17/23 22:22:44.636
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:44.641
Apr 17 22:22:44.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:22:44.642
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:44.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:44.656
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 04/17/23 22:22:44.658
STEP: fetching the ConfigMap 04/17/23 22:22:44.661
STEP: patching the ConfigMap 04/17/23 22:22:44.664
STEP: listing all ConfigMaps in all namespaces with a label selector 04/17/23 22:22:44.667
STEP: deleting the ConfigMap by collection with a label selector 04/17/23 22:22:44.675
STEP: listing all ConfigMaps in test namespace 04/17/23 22:22:44.68
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:44.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8698" for this suite. 04/17/23 22:22:44.686
------------------------------
• [0.050 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:44.641
    Apr 17 22:22:44.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:22:44.642
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:44.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:44.656
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 04/17/23 22:22:44.658
    STEP: fetching the ConfigMap 04/17/23 22:22:44.661
    STEP: patching the ConfigMap 04/17/23 22:22:44.664
    STEP: listing all ConfigMaps in all namespaces with a label selector 04/17/23 22:22:44.667
    STEP: deleting the ConfigMap by collection with a label selector 04/17/23 22:22:44.675
    STEP: listing all ConfigMaps in test namespace 04/17/23 22:22:44.68
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:44.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8698" for this suite. 04/17/23 22:22:44.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:44.692
Apr 17 22:22:44.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replicaset 04/17/23 22:22:44.693
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:44.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:44.705
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 04/17/23 22:22:44.707
STEP: Verify that the required pods have come up 04/17/23 22:22:44.712
Apr 17 22:22:44.714: INFO: Pod name sample-pod: Found 0 pods out of 3
Apr 17 22:22:49.719: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 04/17/23 22:22:49.719
Apr 17 22:22:49.722: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 04/17/23 22:22:49.722
STEP: DeleteCollection of the ReplicaSets 04/17/23 22:22:49.726
STEP: After DeleteCollection verify that ReplicaSets have been deleted 04/17/23 22:22:49.733
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:49.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-1435" for this suite. 04/17/23 22:22:49.743
------------------------------
• [SLOW TEST] [5.057 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:44.692
    Apr 17 22:22:44.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replicaset 04/17/23 22:22:44.693
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:44.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:44.705
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 04/17/23 22:22:44.707
    STEP: Verify that the required pods have come up 04/17/23 22:22:44.712
    Apr 17 22:22:44.714: INFO: Pod name sample-pod: Found 0 pods out of 3
    Apr 17 22:22:49.719: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 04/17/23 22:22:49.719
    Apr 17 22:22:49.722: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 04/17/23 22:22:49.722
    STEP: DeleteCollection of the ReplicaSets 04/17/23 22:22:49.726
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 04/17/23 22:22:49.733
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:49.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-1435" for this suite. 04/17/23 22:22:49.743
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:49.75
Apr 17 22:22:49.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:22:49.75
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:49.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:49.787
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-833599a1-e279-4297-9e67-aeca0fa22a14 04/17/23 22:22:49.789
STEP: Creating a pod to test consume configMaps 04/17/23 22:22:49.793
Apr 17 22:22:49.801: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659" in namespace "configmap-5381" to be "Succeeded or Failed"
Apr 17 22:22:49.804: INFO: Pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552131ms
Apr 17 22:22:51.808: INFO: Pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659": Phase="Running", Reason="", readiness=false. Elapsed: 2.006175028s
Apr 17 22:22:53.807: INFO: Pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005542337s
STEP: Saw pod success 04/17/23 22:22:53.807
Apr 17 22:22:53.807: INFO: Pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659" satisfied condition "Succeeded or Failed"
Apr 17 22:22:53.809: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:22:53.814
Apr 17 22:22:53.822: INFO: Waiting for pod pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659 to disappear
Apr 17 22:22:53.825: INFO: Pod pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:53.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5381" for this suite. 04/17/23 22:22:53.829
------------------------------
• [4.084 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:49.75
    Apr 17 22:22:49.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:22:49.75
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:49.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:49.787
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-833599a1-e279-4297-9e67-aeca0fa22a14 04/17/23 22:22:49.789
    STEP: Creating a pod to test consume configMaps 04/17/23 22:22:49.793
    Apr 17 22:22:49.801: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659" in namespace "configmap-5381" to be "Succeeded or Failed"
    Apr 17 22:22:49.804: INFO: Pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659": Phase="Pending", Reason="", readiness=false. Elapsed: 2.552131ms
    Apr 17 22:22:51.808: INFO: Pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659": Phase="Running", Reason="", readiness=false. Elapsed: 2.006175028s
    Apr 17 22:22:53.807: INFO: Pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005542337s
    STEP: Saw pod success 04/17/23 22:22:53.807
    Apr 17 22:22:53.807: INFO: Pod "pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659" satisfied condition "Succeeded or Failed"
    Apr 17 22:22:53.809: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:22:53.814
    Apr 17 22:22:53.822: INFO: Waiting for pod pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659 to disappear
    Apr 17 22:22:53.825: INFO: Pod pod-configmaps-aa9cf34f-2d76-45a1-a6fe-7c7350b9c659 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:53.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5381" for this suite. 04/17/23 22:22:53.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:53.834
Apr 17 22:22:53.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename csiinlinevolumes 04/17/23 22:22:53.835
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:53.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:53.849
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 04/17/23 22:22:53.851
STEP: getting 04/17/23 22:22:53.862
STEP: listing 04/17/23 22:22:53.866
STEP: deleting 04/17/23 22:22:53.868
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:22:53.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-8230" for this suite. 04/17/23 22:22:53.884
------------------------------
• [0.055 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:53.834
    Apr 17 22:22:53.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename csiinlinevolumes 04/17/23 22:22:53.835
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:53.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:53.849
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 04/17/23 22:22:53.851
    STEP: getting 04/17/23 22:22:53.862
    STEP: listing 04/17/23 22:22:53.866
    STEP: deleting 04/17/23 22:22:53.868
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:22:53.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-8230" for this suite. 04/17/23 22:22:53.884
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:22:53.89
Apr 17 22:22:53.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-lifecycle-hook 04/17/23 22:22:53.89
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:53.903
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:53.905
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 04/17/23 22:22:53.912
Apr 17 22:22:53.918: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-32" to be "running and ready"
Apr 17 22:22:53.921: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.424267ms
Apr 17 22:22:53.921: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:22:55.924: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005820339s
Apr 17 22:22:55.924: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Apr 17 22:22:55.924: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 04/17/23 22:22:55.927
Apr 17 22:22:55.932: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-32" to be "running and ready"
Apr 17 22:22:55.936: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595121ms
Apr 17 22:22:55.936: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:22:57.940: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.0076094s
Apr 17 22:22:57.940: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Apr 17 22:22:57.940: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 04/17/23 22:22:57.942
Apr 17 22:22:57.947: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 17 22:22:57.950: INFO: Pod pod-with-prestop-http-hook still exists
Apr 17 22:22:59.951: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 17 22:22:59.954: INFO: Pod pod-with-prestop-http-hook still exists
Apr 17 22:23:01.951: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr 17 22:23:01.954: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 04/17/23 22:23:01.954
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:01.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-32" for this suite. 04/17/23 22:23:01.963
------------------------------
• [SLOW TEST] [8.079 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:22:53.89
    Apr 17 22:22:53.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-lifecycle-hook 04/17/23 22:22:53.89
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:22:53.903
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:22:53.905
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 04/17/23 22:22:53.912
    Apr 17 22:22:53.918: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-32" to be "running and ready"
    Apr 17 22:22:53.921: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.424267ms
    Apr 17 22:22:53.921: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:22:55.924: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.005820339s
    Apr 17 22:22:55.924: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Apr 17 22:22:55.924: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 04/17/23 22:22:55.927
    Apr 17 22:22:55.932: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-32" to be "running and ready"
    Apr 17 22:22:55.936: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595121ms
    Apr 17 22:22:55.936: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:22:57.940: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.0076094s
    Apr 17 22:22:57.940: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Apr 17 22:22:57.940: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 04/17/23 22:22:57.942
    Apr 17 22:22:57.947: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr 17 22:22:57.950: INFO: Pod pod-with-prestop-http-hook still exists
    Apr 17 22:22:59.951: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr 17 22:22:59.954: INFO: Pod pod-with-prestop-http-hook still exists
    Apr 17 22:23:01.951: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Apr 17 22:23:01.954: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 04/17/23 22:23:01.954
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:01.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-32" for this suite. 04/17/23 22:23:01.963
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:01.969
Apr 17 22:23:01.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:23:01.97
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:01.982
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:01.985
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-11fb36f2-90a3-4a9e-b243-de3e45d2d633 04/17/23 22:23:01.987
STEP: Creating a pod to test consume configMaps 04/17/23 22:23:01.99
Apr 17 22:23:01.997: INFO: Waiting up to 5m0s for pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6" in namespace "configmap-130" to be "Succeeded or Failed"
Apr 17 22:23:01.999: INFO: Pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.209014ms
Apr 17 22:23:04.003: INFO: Pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005717151s
Apr 17 22:23:06.003: INFO: Pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005984507s
STEP: Saw pod success 04/17/23 22:23:06.003
Apr 17 22:23:06.003: INFO: Pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6" satisfied condition "Succeeded or Failed"
Apr 17 22:23:06.005: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:23:06.015
Apr 17 22:23:06.025: INFO: Waiting for pod pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6 to disappear
Apr 17 22:23:06.027: INFO: Pod pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:06.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-130" for this suite. 04/17/23 22:23:06.031
------------------------------
• [4.066 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:01.969
    Apr 17 22:23:01.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:23:01.97
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:01.982
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:01.985
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-11fb36f2-90a3-4a9e-b243-de3e45d2d633 04/17/23 22:23:01.987
    STEP: Creating a pod to test consume configMaps 04/17/23 22:23:01.99
    Apr 17 22:23:01.997: INFO: Waiting up to 5m0s for pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6" in namespace "configmap-130" to be "Succeeded or Failed"
    Apr 17 22:23:01.999: INFO: Pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.209014ms
    Apr 17 22:23:04.003: INFO: Pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005717151s
    Apr 17 22:23:06.003: INFO: Pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005984507s
    STEP: Saw pod success 04/17/23 22:23:06.003
    Apr 17 22:23:06.003: INFO: Pod "pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6" satisfied condition "Succeeded or Failed"
    Apr 17 22:23:06.005: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:23:06.015
    Apr 17 22:23:06.025: INFO: Waiting for pod pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6 to disappear
    Apr 17 22:23:06.027: INFO: Pod pod-configmaps-40460cd0-a06b-4d00-ac55-70e9615fc8f6 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:06.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-130" for this suite. 04/17/23 22:23:06.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:06.036
Apr 17 22:23:06.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:23:06.037
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:06.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:06.05
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:06.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1985" for this suite. 04/17/23 22:23:06.082
------------------------------
• [0.050 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:06.036
    Apr 17 22:23:06.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:23:06.037
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:06.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:06.05
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:06.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1985" for this suite. 04/17/23 22:23:06.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:06.087
Apr 17 22:23:06.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:23:06.087
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:06.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:06.101
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-7858818b-c3f9-4a94-bcdc-09670aad2844 04/17/23 22:23:06.103
STEP: Creating a pod to test consume configMaps 04/17/23 22:23:06.107
Apr 17 22:23:06.114: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02" in namespace "projected-3409" to be "Succeeded or Failed"
Apr 17 22:23:06.118: INFO: Pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.605928ms
Apr 17 22:23:08.121: INFO: Pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007579728s
Apr 17 22:23:10.122: INFO: Pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008565372s
STEP: Saw pod success 04/17/23 22:23:10.123
Apr 17 22:23:10.123: INFO: Pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02" satisfied condition "Succeeded or Failed"
Apr 17 22:23:10.125: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02 container projected-configmap-volume-test: <nil>
STEP: delete the pod 04/17/23 22:23:10.129
Apr 17 22:23:10.140: INFO: Waiting for pod pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02 to disappear
Apr 17 22:23:10.142: INFO: Pod pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:10.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3409" for this suite. 04/17/23 22:23:10.146
------------------------------
• [4.064 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:06.087
    Apr 17 22:23:06.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:23:06.087
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:06.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:06.101
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-7858818b-c3f9-4a94-bcdc-09670aad2844 04/17/23 22:23:06.103
    STEP: Creating a pod to test consume configMaps 04/17/23 22:23:06.107
    Apr 17 22:23:06.114: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02" in namespace "projected-3409" to be "Succeeded or Failed"
    Apr 17 22:23:06.118: INFO: Pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.605928ms
    Apr 17 22:23:08.121: INFO: Pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007579728s
    Apr 17 22:23:10.122: INFO: Pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008565372s
    STEP: Saw pod success 04/17/23 22:23:10.123
    Apr 17 22:23:10.123: INFO: Pod "pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02" satisfied condition "Succeeded or Failed"
    Apr 17 22:23:10.125: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:23:10.129
    Apr 17 22:23:10.140: INFO: Waiting for pod pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02 to disappear
    Apr 17 22:23:10.142: INFO: Pod pod-projected-configmaps-73fb2255-fe7a-44d8-8d58-c9bfc85ddc02 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:10.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3409" for this suite. 04/17/23 22:23:10.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:10.151
Apr 17 22:23:10.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:23:10.152
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:10.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:10.169
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 04/17/23 22:23:10.171
Apr 17 22:23:10.179: INFO: Waiting up to 5m0s for pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53" in namespace "projected-4458" to be "running and ready"
Apr 17 22:23:10.182: INFO: Pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.636199ms
Apr 17 22:23:10.182: INFO: The phase of Pod labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:23:12.185: INFO: Pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53": Phase="Running", Reason="", readiness=true. Elapsed: 2.005610786s
Apr 17 22:23:12.185: INFO: The phase of Pod labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53 is Running (Ready = true)
Apr 17 22:23:12.185: INFO: Pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53" satisfied condition "running and ready"
Apr 17 22:23:12.709: INFO: Successfully updated pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:16.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4458" for this suite. 04/17/23 22:23:16.73
------------------------------
• [SLOW TEST] [6.585 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:10.151
    Apr 17 22:23:10.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:23:10.152
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:10.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:10.169
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 04/17/23 22:23:10.171
    Apr 17 22:23:10.179: INFO: Waiting up to 5m0s for pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53" in namespace "projected-4458" to be "running and ready"
    Apr 17 22:23:10.182: INFO: Pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.636199ms
    Apr 17 22:23:10.182: INFO: The phase of Pod labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:23:12.185: INFO: Pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53": Phase="Running", Reason="", readiness=true. Elapsed: 2.005610786s
    Apr 17 22:23:12.185: INFO: The phase of Pod labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53 is Running (Ready = true)
    Apr 17 22:23:12.185: INFO: Pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53" satisfied condition "running and ready"
    Apr 17 22:23:12.709: INFO: Successfully updated pod "labelsupdatef73498db-aaac-4d15-a2e2-df3ecdab8a53"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:16.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4458" for this suite. 04/17/23 22:23:16.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:16.737
Apr 17 22:23:16.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename job 04/17/23 22:23:16.738
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:16.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:16.752
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 04/17/23 22:23:16.754
STEP: Ensuring active pods == parallelism 04/17/23 22:23:16.76
STEP: Orphaning one of the Job's Pods 04/17/23 22:23:18.764
Apr 17 22:23:19.277: INFO: Successfully updated pod "adopt-release-8hjfg"
STEP: Checking that the Job readopts the Pod 04/17/23 22:23:19.277
Apr 17 22:23:19.277: INFO: Waiting up to 15m0s for pod "adopt-release-8hjfg" in namespace "job-806" to be "adopted"
Apr 17 22:23:19.279: INFO: Pod "adopt-release-8hjfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.540522ms
Apr 17 22:23:21.283: INFO: Pod "adopt-release-8hjfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.006337823s
Apr 17 22:23:21.283: INFO: Pod "adopt-release-8hjfg" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 04/17/23 22:23:21.283
Apr 17 22:23:21.794: INFO: Successfully updated pod "adopt-release-8hjfg"
STEP: Checking that the Job releases the Pod 04/17/23 22:23:21.794
Apr 17 22:23:21.794: INFO: Waiting up to 15m0s for pod "adopt-release-8hjfg" in namespace "job-806" to be "released"
Apr 17 22:23:21.796: INFO: Pod "adopt-release-8hjfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.526104ms
Apr 17 22:23:23.799: INFO: Pod "adopt-release-8hjfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.005569357s
Apr 17 22:23:23.799: INFO: Pod "adopt-release-8hjfg" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:23.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-806" for this suite. 04/17/23 22:23:23.804
------------------------------
• [SLOW TEST] [7.073 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:16.737
    Apr 17 22:23:16.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename job 04/17/23 22:23:16.738
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:16.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:16.752
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 04/17/23 22:23:16.754
    STEP: Ensuring active pods == parallelism 04/17/23 22:23:16.76
    STEP: Orphaning one of the Job's Pods 04/17/23 22:23:18.764
    Apr 17 22:23:19.277: INFO: Successfully updated pod "adopt-release-8hjfg"
    STEP: Checking that the Job readopts the Pod 04/17/23 22:23:19.277
    Apr 17 22:23:19.277: INFO: Waiting up to 15m0s for pod "adopt-release-8hjfg" in namespace "job-806" to be "adopted"
    Apr 17 22:23:19.279: INFO: Pod "adopt-release-8hjfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.540522ms
    Apr 17 22:23:21.283: INFO: Pod "adopt-release-8hjfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.006337823s
    Apr 17 22:23:21.283: INFO: Pod "adopt-release-8hjfg" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 04/17/23 22:23:21.283
    Apr 17 22:23:21.794: INFO: Successfully updated pod "adopt-release-8hjfg"
    STEP: Checking that the Job releases the Pod 04/17/23 22:23:21.794
    Apr 17 22:23:21.794: INFO: Waiting up to 15m0s for pod "adopt-release-8hjfg" in namespace "job-806" to be "released"
    Apr 17 22:23:21.796: INFO: Pod "adopt-release-8hjfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.526104ms
    Apr 17 22:23:23.799: INFO: Pod "adopt-release-8hjfg": Phase="Running", Reason="", readiness=true. Elapsed: 2.005569357s
    Apr 17 22:23:23.799: INFO: Pod "adopt-release-8hjfg" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:23.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-806" for this suite. 04/17/23 22:23:23.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:23.811
Apr 17 22:23:23.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:23:23.812
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:23.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:23.827
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:23:23.837
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:23:24.268
STEP: Deploying the webhook pod 04/17/23 22:23:24.274
STEP: Wait for the deployment to be ready 04/17/23 22:23:24.284
Apr 17 22:23:24.289: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:23:26.297
STEP: Verifying the service has paired with the endpoint 04/17/23 22:23:26.308
Apr 17 22:23:27.309: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 04/17/23 22:23:27.312
STEP: Registering slow webhook via the AdmissionRegistration API 04/17/23 22:23:27.312
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 04/17/23 22:23:27.324
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 04/17/23 22:23:28.331
STEP: Registering slow webhook via the AdmissionRegistration API 04/17/23 22:23:28.331
STEP: Having no error when timeout is longer than webhook latency 04/17/23 22:23:29.351
STEP: Registering slow webhook via the AdmissionRegistration API 04/17/23 22:23:29.351
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 04/17/23 22:23:34.377
STEP: Registering slow webhook via the AdmissionRegistration API 04/17/23 22:23:34.377
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:39.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3901" for this suite. 04/17/23 22:23:39.436
STEP: Destroying namespace "webhook-3901-markers" for this suite. 04/17/23 22:23:39.444
------------------------------
• [SLOW TEST] [15.642 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:23.811
    Apr 17 22:23:23.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:23:23.812
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:23.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:23.827
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:23:23.837
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:23:24.268
    STEP: Deploying the webhook pod 04/17/23 22:23:24.274
    STEP: Wait for the deployment to be ready 04/17/23 22:23:24.284
    Apr 17 22:23:24.289: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:23:26.297
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:23:26.308
    Apr 17 22:23:27.309: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 04/17/23 22:23:27.312
    STEP: Registering slow webhook via the AdmissionRegistration API 04/17/23 22:23:27.312
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 04/17/23 22:23:27.324
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 04/17/23 22:23:28.331
    STEP: Registering slow webhook via the AdmissionRegistration API 04/17/23 22:23:28.331
    STEP: Having no error when timeout is longer than webhook latency 04/17/23 22:23:29.351
    STEP: Registering slow webhook via the AdmissionRegistration API 04/17/23 22:23:29.351
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 04/17/23 22:23:34.377
    STEP: Registering slow webhook via the AdmissionRegistration API 04/17/23 22:23:34.377
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:39.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3901" for this suite. 04/17/23 22:23:39.436
    STEP: Destroying namespace "webhook-3901-markers" for this suite. 04/17/23 22:23:39.444
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:39.454
Apr 17 22:23:39.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename events 04/17/23 22:23:39.454
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:39.475
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:39.477
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 04/17/23 22:23:39.48
STEP: listing events in all namespaces 04/17/23 22:23:39.486
STEP: listing events in test namespace 04/17/23 22:23:39.489
STEP: listing events with field selection filtering on source 04/17/23 22:23:39.491
STEP: listing events with field selection filtering on reportingController 04/17/23 22:23:39.494
STEP: getting the test event 04/17/23 22:23:39.496
STEP: patching the test event 04/17/23 22:23:39.498
STEP: getting the test event 04/17/23 22:23:39.503
STEP: updating the test event 04/17/23 22:23:39.505
STEP: getting the test event 04/17/23 22:23:39.51
STEP: deleting the test event 04/17/23 22:23:39.514
STEP: listing events in all namespaces 04/17/23 22:23:39.519
STEP: listing events in test namespace 04/17/23 22:23:39.522
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:39.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5181" for this suite. 04/17/23 22:23:39.529
------------------------------
• [0.081 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:39.454
    Apr 17 22:23:39.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename events 04/17/23 22:23:39.454
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:39.475
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:39.477
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 04/17/23 22:23:39.48
    STEP: listing events in all namespaces 04/17/23 22:23:39.486
    STEP: listing events in test namespace 04/17/23 22:23:39.489
    STEP: listing events with field selection filtering on source 04/17/23 22:23:39.491
    STEP: listing events with field selection filtering on reportingController 04/17/23 22:23:39.494
    STEP: getting the test event 04/17/23 22:23:39.496
    STEP: patching the test event 04/17/23 22:23:39.498
    STEP: getting the test event 04/17/23 22:23:39.503
    STEP: updating the test event 04/17/23 22:23:39.505
    STEP: getting the test event 04/17/23 22:23:39.51
    STEP: deleting the test event 04/17/23 22:23:39.514
    STEP: listing events in all namespaces 04/17/23 22:23:39.519
    STEP: listing events in test namespace 04/17/23 22:23:39.522
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:39.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5181" for this suite. 04/17/23 22:23:39.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:39.535
Apr 17 22:23:39.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:23:39.536
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:39.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:39.55
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 04/17/23 22:23:39.552
Apr 17 22:23:39.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8279 create -f -'
Apr 17 22:23:42.177: INFO: stderr: ""
Apr 17 22:23:42.177: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 04/17/23 22:23:42.177
Apr 17 22:23:42.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8279 diff -f -'
Apr 17 22:23:42.826: INFO: rc: 1
Apr 17 22:23:42.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8279 delete -f -'
Apr 17 22:23:42.885: INFO: stderr: ""
Apr 17 22:23:42.885: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:42.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8279" for this suite. 04/17/23 22:23:42.889
------------------------------
• [3.359 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:39.535
    Apr 17 22:23:39.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:23:39.536
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:39.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:39.55
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 04/17/23 22:23:39.552
    Apr 17 22:23:39.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8279 create -f -'
    Apr 17 22:23:42.177: INFO: stderr: ""
    Apr 17 22:23:42.177: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 04/17/23 22:23:42.177
    Apr 17 22:23:42.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8279 diff -f -'
    Apr 17 22:23:42.826: INFO: rc: 1
    Apr 17 22:23:42.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-8279 delete -f -'
    Apr 17 22:23:42.885: INFO: stderr: ""
    Apr 17 22:23:42.885: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:42.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8279" for this suite. 04/17/23 22:23:42.889
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:42.895
Apr 17 22:23:42.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:23:42.895
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:42.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:42.913
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-cb8a26f4-2758-4b91-a6f5-ce70f21f1dc2 04/17/23 22:23:42.92
STEP: Creating the pod 04/17/23 22:23:42.923
Apr 17 22:23:42.930: INFO: Waiting up to 5m0s for pod "pod-configmaps-71813bf2-2062-45b1-9ee5-4ad445373665" in namespace "configmap-8357" to be "running"
Apr 17 22:23:42.933: INFO: Pod "pod-configmaps-71813bf2-2062-45b1-9ee5-4ad445373665": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236816ms
Apr 17 22:23:44.937: INFO: Pod "pod-configmaps-71813bf2-2062-45b1-9ee5-4ad445373665": Phase="Running", Reason="", readiness=false. Elapsed: 2.006958071s
Apr 17 22:23:44.937: INFO: Pod "pod-configmaps-71813bf2-2062-45b1-9ee5-4ad445373665" satisfied condition "running"
STEP: Waiting for pod with text data 04/17/23 22:23:44.937
STEP: Waiting for pod with binary data 04/17/23 22:23:44.945
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:44.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8357" for this suite. 04/17/23 22:23:44.953
------------------------------
• [2.064 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:42.895
    Apr 17 22:23:42.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:23:42.895
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:42.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:42.913
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-cb8a26f4-2758-4b91-a6f5-ce70f21f1dc2 04/17/23 22:23:42.92
    STEP: Creating the pod 04/17/23 22:23:42.923
    Apr 17 22:23:42.930: INFO: Waiting up to 5m0s for pod "pod-configmaps-71813bf2-2062-45b1-9ee5-4ad445373665" in namespace "configmap-8357" to be "running"
    Apr 17 22:23:42.933: INFO: Pod "pod-configmaps-71813bf2-2062-45b1-9ee5-4ad445373665": Phase="Pending", Reason="", readiness=false. Elapsed: 2.236816ms
    Apr 17 22:23:44.937: INFO: Pod "pod-configmaps-71813bf2-2062-45b1-9ee5-4ad445373665": Phase="Running", Reason="", readiness=false. Elapsed: 2.006958071s
    Apr 17 22:23:44.937: INFO: Pod "pod-configmaps-71813bf2-2062-45b1-9ee5-4ad445373665" satisfied condition "running"
    STEP: Waiting for pod with text data 04/17/23 22:23:44.937
    STEP: Waiting for pod with binary data 04/17/23 22:23:44.945
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:44.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8357" for this suite. 04/17/23 22:23:44.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:44.961
Apr 17 22:23:44.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename job 04/17/23 22:23:44.962
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:44.976
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:44.979
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 04/17/23 22:23:44.985
STEP: Patching the Job 04/17/23 22:23:44.99
STEP: Watching for Job to be patched 04/17/23 22:23:45.007
Apr 17 22:23:45.009: INFO: Event ADDED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking:]
Apr 17 22:23:45.009: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking:]
Apr 17 22:23:45.009: INFO: Event MODIFIED found for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 04/17/23 22:23:45.009
STEP: Watching for Job to be updated 04/17/23 22:23:45.017
Apr 17 22:23:45.018: INFO: Event MODIFIED found for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 17 22:23:45.018: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 04/17/23 22:23:45.018
Apr 17 22:23:45.020: INFO: Job: e2e-6mr69 as labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69]
STEP: Waiting for job to complete 04/17/23 22:23:45.02
STEP: Delete a job collection with a labelselector 04/17/23 22:23:53.024
STEP: Watching for Job to be deleted 04/17/23 22:23:53.03
Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Apr 17 22:23:53.032: INFO: Event DELETED found for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 04/17/23 22:23:53.032
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 17 22:23:53.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-2662" for this suite. 04/17/23 22:23:53.038
------------------------------
• [SLOW TEST] [8.083 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:44.961
    Apr 17 22:23:44.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename job 04/17/23 22:23:44.962
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:44.976
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:44.979
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 04/17/23 22:23:44.985
    STEP: Patching the Job 04/17/23 22:23:44.99
    STEP: Watching for Job to be patched 04/17/23 22:23:45.007
    Apr 17 22:23:45.009: INFO: Event ADDED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking:]
    Apr 17 22:23:45.009: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking:]
    Apr 17 22:23:45.009: INFO: Event MODIFIED found for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 04/17/23 22:23:45.009
    STEP: Watching for Job to be updated 04/17/23 22:23:45.017
    Apr 17 22:23:45.018: INFO: Event MODIFIED found for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 17 22:23:45.018: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 04/17/23 22:23:45.018
    Apr 17 22:23:45.020: INFO: Job: e2e-6mr69 as labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69]
    STEP: Waiting for job to complete 04/17/23 22:23:45.02
    STEP: Delete a job collection with a labelselector 04/17/23 22:23:53.024
    STEP: Watching for Job to be deleted 04/17/23 22:23:53.03
    Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 17 22:23:53.031: INFO: Event MODIFIED observed for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Apr 17 22:23:53.032: INFO: Event DELETED found for Job e2e-6mr69 in namespace job-2662 with labels: map[e2e-6mr69:patched e2e-job-label:e2e-6mr69] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 04/17/23 22:23:53.032
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:23:53.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-2662" for this suite. 04/17/23 22:23:53.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:23:53.044
Apr 17 22:23:53.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 22:23:53.045
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:53.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:53.066
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-4087 04/17/23 22:23:53.068
STEP: creating service affinity-clusterip-transition in namespace services-4087 04/17/23 22:23:53.068
STEP: creating replication controller affinity-clusterip-transition in namespace services-4087 04/17/23 22:23:53.084
I0417 22:23:53.089187      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4087, replica count: 3
I0417 22:23:56.141066      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 22:23:56.146: INFO: Creating new exec pod
Apr 17 22:23:56.154: INFO: Waiting up to 5m0s for pod "execpod-affinity8z6ln" in namespace "services-4087" to be "running"
Apr 17 22:23:56.156: INFO: Pod "execpod-affinity8z6ln": Phase="Pending", Reason="", readiness=false. Elapsed: 2.78452ms
Apr 17 22:23:58.160: INFO: Pod "execpod-affinity8z6ln": Phase="Running", Reason="", readiness=true. Elapsed: 2.005955986s
Apr 17 22:23:58.160: INFO: Pod "execpod-affinity8z6ln" satisfied condition "running"
Apr 17 22:23:59.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4087 exec execpod-affinity8z6ln -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Apr 17 22:23:59.294: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Apr 17 22:23:59.294: INFO: stdout: ""
Apr 17 22:23:59.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4087 exec execpod-affinity8z6ln -- /bin/sh -x -c nc -v -z -w 2 10.99.232.180 80'
Apr 17 22:23:59.421: INFO: stderr: "+ nc -v -z -w 2 10.99.232.180 80\nConnection to 10.99.232.180 80 port [tcp/http] succeeded!\n"
Apr 17 22:23:59.421: INFO: stdout: ""
Apr 17 22:23:59.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4087 exec execpod-affinity8z6ln -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.232.180:80/ ; done'
Apr 17 22:23:59.621: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n"
Apr 17 22:23:59.621: INFO: stdout: "\naffinity-clusterip-transition-slc2q\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-slc2q\naffinity-clusterip-transition-k88td\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-k88td\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-k88td\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-k88td\naffinity-clusterip-transition-slc2q\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-k88td"
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-slc2q
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-slc2q
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-slc2q
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
Apr 17 22:23:59.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4087 exec execpod-affinity8z6ln -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.232.180:80/ ; done'
Apr 17 22:23:59.812: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n"
Apr 17 22:23:59.812: INFO: stdout: "\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg"
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
Apr 17 22:23:59.813: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4087, will wait for the garbage collector to delete the pods 04/17/23 22:23:59.825
Apr 17 22:23:59.886: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.010422ms
Apr 17 22:23:59.986: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.640346ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 22:24:02.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4087" for this suite. 04/17/23 22:24:02.208
------------------------------
• [SLOW TEST] [9.168 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:23:53.044
    Apr 17 22:23:53.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 22:23:53.045
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:23:53.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:23:53.066
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-4087 04/17/23 22:23:53.068
    STEP: creating service affinity-clusterip-transition in namespace services-4087 04/17/23 22:23:53.068
    STEP: creating replication controller affinity-clusterip-transition in namespace services-4087 04/17/23 22:23:53.084
    I0417 22:23:53.089187      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-4087, replica count: 3
    I0417 22:23:56.141066      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 17 22:23:56.146: INFO: Creating new exec pod
    Apr 17 22:23:56.154: INFO: Waiting up to 5m0s for pod "execpod-affinity8z6ln" in namespace "services-4087" to be "running"
    Apr 17 22:23:56.156: INFO: Pod "execpod-affinity8z6ln": Phase="Pending", Reason="", readiness=false. Elapsed: 2.78452ms
    Apr 17 22:23:58.160: INFO: Pod "execpod-affinity8z6ln": Phase="Running", Reason="", readiness=true. Elapsed: 2.005955986s
    Apr 17 22:23:58.160: INFO: Pod "execpod-affinity8z6ln" satisfied condition "running"
    Apr 17 22:23:59.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4087 exec execpod-affinity8z6ln -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Apr 17 22:23:59.294: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Apr 17 22:23:59.294: INFO: stdout: ""
    Apr 17 22:23:59.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4087 exec execpod-affinity8z6ln -- /bin/sh -x -c nc -v -z -w 2 10.99.232.180 80'
    Apr 17 22:23:59.421: INFO: stderr: "+ nc -v -z -w 2 10.99.232.180 80\nConnection to 10.99.232.180 80 port [tcp/http] succeeded!\n"
    Apr 17 22:23:59.421: INFO: stdout: ""
    Apr 17 22:23:59.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4087 exec execpod-affinity8z6ln -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.232.180:80/ ; done'
    Apr 17 22:23:59.621: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n"
    Apr 17 22:23:59.621: INFO: stdout: "\naffinity-clusterip-transition-slc2q\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-slc2q\naffinity-clusterip-transition-k88td\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-k88td\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-k88td\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-k88td\naffinity-clusterip-transition-slc2q\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-k88td"
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-slc2q
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-slc2q
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-slc2q
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.621: INFO: Received response from host: affinity-clusterip-transition-k88td
    Apr 17 22:23:59.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-4087 exec execpod-affinity8z6ln -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.232.180:80/ ; done'
    Apr 17 22:23:59.812: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.232.180:80/\n"
    Apr 17 22:23:59.812: INFO: stdout: "\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg\naffinity-clusterip-transition-mmvdg"
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Received response from host: affinity-clusterip-transition-mmvdg
    Apr 17 22:23:59.813: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4087, will wait for the garbage collector to delete the pods 04/17/23 22:23:59.825
    Apr 17 22:23:59.886: INFO: Deleting ReplicationController affinity-clusterip-transition took: 5.010422ms
    Apr 17 22:23:59.986: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.640346ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:24:02.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4087" for this suite. 04/17/23 22:24:02.208
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:24:02.213
Apr 17 22:24:02.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:24:02.213
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:02.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:02.228
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Apr 17 22:24:02.302: INFO: Waiting up to 5m0s for pod "pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b" in namespace "svcaccounts-4053" to be "running"
Apr 17 22:24:02.305: INFO: Pod "pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.553782ms
Apr 17 22:24:04.309: INFO: Pod "pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006619775s
Apr 17 22:24:04.309: INFO: Pod "pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b" satisfied condition "running"
STEP: reading a file in the container 04/17/23 22:24:04.309
Apr 17 22:24:04.309: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4053 pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 04/17/23 22:24:04.457
Apr 17 22:24:04.457: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4053 pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 04/17/23 22:24:04.593
Apr 17 22:24:04.593: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4053 pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Apr 17 22:24:04.724: INFO: Got root ca configmap in namespace "svcaccounts-4053"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 17 22:24:04.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4053" for this suite. 04/17/23 22:24:04.73
------------------------------
• [2.522 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:24:02.213
    Apr 17 22:24:02.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:24:02.213
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:02.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:02.228
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Apr 17 22:24:02.302: INFO: Waiting up to 5m0s for pod "pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b" in namespace "svcaccounts-4053" to be "running"
    Apr 17 22:24:02.305: INFO: Pod "pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.553782ms
    Apr 17 22:24:04.309: INFO: Pod "pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b": Phase="Running", Reason="", readiness=true. Elapsed: 2.006619775s
    Apr 17 22:24:04.309: INFO: Pod "pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b" satisfied condition "running"
    STEP: reading a file in the container 04/17/23 22:24:04.309
    Apr 17 22:24:04.309: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4053 pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 04/17/23 22:24:04.457
    Apr 17 22:24:04.457: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4053 pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 04/17/23 22:24:04.593
    Apr 17 22:24:04.593: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4053 pod-service-account-5e073e87-01e4-42ee-af2c-efa2e46ee59b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Apr 17 22:24:04.724: INFO: Got root ca configmap in namespace "svcaccounts-4053"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:24:04.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4053" for this suite. 04/17/23 22:24:04.73
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:24:04.735
Apr 17 22:24:04.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename namespaces 04/17/23 22:24:04.736
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:04.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:04.752
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 04/17/23 22:24:04.754
STEP: patching the Namespace 04/17/23 22:24:04.766
STEP: get the Namespace and ensuring it has the label 04/17/23 22:24:04.771
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:24:04.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5591" for this suite. 04/17/23 22:24:04.778
STEP: Destroying namespace "nspatchtest-af076e30-afa3-484a-8722-cd114b2449a0-5150" for this suite. 04/17/23 22:24:04.782
------------------------------
• [0.052 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:24:04.735
    Apr 17 22:24:04.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename namespaces 04/17/23 22:24:04.736
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:04.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:04.752
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 04/17/23 22:24:04.754
    STEP: patching the Namespace 04/17/23 22:24:04.766
    STEP: get the Namespace and ensuring it has the label 04/17/23 22:24:04.771
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:24:04.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5591" for this suite. 04/17/23 22:24:04.778
    STEP: Destroying namespace "nspatchtest-af076e30-afa3-484a-8722-cd114b2449a0-5150" for this suite. 04/17/23 22:24:04.782
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:24:04.787
Apr 17 22:24:04.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename security-context-test 04/17/23 22:24:04.788
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:04.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:04.802
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Apr 17 22:24:04.810: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1" in namespace "security-context-test-7499" to be "Succeeded or Failed"
Apr 17 22:24:04.812: INFO: Pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095879ms
Apr 17 22:24:06.816: INFO: Pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005662437s
Apr 17 22:24:08.816: INFO: Pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005638689s
Apr 17 22:24:08.816: INFO: Pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1" satisfied condition "Succeeded or Failed"
Apr 17 22:24:08.820: INFO: Got logs for pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Apr 17 22:24:08.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7499" for this suite. 04/17/23 22:24:08.825
------------------------------
• [4.043 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:24:04.787
    Apr 17 22:24:04.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename security-context-test 04/17/23 22:24:04.788
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:04.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:04.802
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Apr 17 22:24:04.810: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1" in namespace "security-context-test-7499" to be "Succeeded or Failed"
    Apr 17 22:24:04.812: INFO: Pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095879ms
    Apr 17 22:24:06.816: INFO: Pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005662437s
    Apr 17 22:24:08.816: INFO: Pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005638689s
    Apr 17 22:24:08.816: INFO: Pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1" satisfied condition "Succeeded or Failed"
    Apr 17 22:24:08.820: INFO: Got logs for pod "busybox-privileged-false-8f2cc812-58ac-4402-98f7-dc57afa2f4b1": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:24:08.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7499" for this suite. 04/17/23 22:24:08.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:24:08.83
Apr 17 22:24:08.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:24:08.831
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:08.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:08.847
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Apr 17 22:24:08.863: INFO: created pod pod-service-account-defaultsa
Apr 17 22:24:08.863: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr 17 22:24:08.867: INFO: created pod pod-service-account-mountsa
Apr 17 22:24:08.867: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr 17 22:24:08.874: INFO: created pod pod-service-account-nomountsa
Apr 17 22:24:08.874: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr 17 22:24:08.881: INFO: created pod pod-service-account-defaultsa-mountspec
Apr 17 22:24:08.881: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr 17 22:24:08.889: INFO: created pod pod-service-account-mountsa-mountspec
Apr 17 22:24:08.889: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr 17 22:24:08.894: INFO: created pod pod-service-account-nomountsa-mountspec
Apr 17 22:24:08.894: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr 17 22:24:08.905: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr 17 22:24:08.905: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr 17 22:24:08.918: INFO: created pod pod-service-account-mountsa-nomountspec
Apr 17 22:24:08.918: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr 17 22:24:08.928: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr 17 22:24:08.928: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 17 22:24:08.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5776" for this suite. 04/17/23 22:24:08.936
------------------------------
• [0.116 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:24:08.83
    Apr 17 22:24:08.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:24:08.831
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:08.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:08.847
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Apr 17 22:24:08.863: INFO: created pod pod-service-account-defaultsa
    Apr 17 22:24:08.863: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Apr 17 22:24:08.867: INFO: created pod pod-service-account-mountsa
    Apr 17 22:24:08.867: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Apr 17 22:24:08.874: INFO: created pod pod-service-account-nomountsa
    Apr 17 22:24:08.874: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Apr 17 22:24:08.881: INFO: created pod pod-service-account-defaultsa-mountspec
    Apr 17 22:24:08.881: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Apr 17 22:24:08.889: INFO: created pod pod-service-account-mountsa-mountspec
    Apr 17 22:24:08.889: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Apr 17 22:24:08.894: INFO: created pod pod-service-account-nomountsa-mountspec
    Apr 17 22:24:08.894: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Apr 17 22:24:08.905: INFO: created pod pod-service-account-defaultsa-nomountspec
    Apr 17 22:24:08.905: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Apr 17 22:24:08.918: INFO: created pod pod-service-account-mountsa-nomountspec
    Apr 17 22:24:08.918: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Apr 17 22:24:08.928: INFO: created pod pod-service-account-nomountsa-nomountspec
    Apr 17 22:24:08.928: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:24:08.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5776" for this suite. 04/17/23 22:24:08.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:24:08.947
Apr 17 22:24:08.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename cronjob 04/17/23 22:24:08.948
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:08.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:08.963
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 04/17/23 22:24:08.965
STEP: Ensuring more than one job is running at a time 04/17/23 22:24:08.972
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 04/17/23 22:26:00.975
STEP: Removing cronjob 04/17/23 22:26:00.978
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 17 22:26:00.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4465" for this suite. 04/17/23 22:26:00.986
------------------------------
• [SLOW TEST] [112.044 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:24:08.947
    Apr 17 22:24:08.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename cronjob 04/17/23 22:24:08.948
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:24:08.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:24:08.963
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 04/17/23 22:24:08.965
    STEP: Ensuring more than one job is running at a time 04/17/23 22:24:08.972
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 04/17/23 22:26:00.975
    STEP: Removing cronjob 04/17/23 22:26:00.978
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:26:00.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4465" for this suite. 04/17/23 22:26:00.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:26:00.993
Apr 17 22:26:00.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename gc 04/17/23 22:26:00.994
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:26:01.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:26:01.008
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 04/17/23 22:26:01.016
STEP: delete the rc 04/17/23 22:26:06.028
STEP: wait for the rc to be deleted 04/17/23 22:26:06.038
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 04/17/23 22:26:11.041
STEP: Gathering metrics 04/17/23 22:26:41.053
Apr 17 22:26:41.083: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
Apr 17 22:26:41.085: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.344778ms
Apr 17 22:26:41.085: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
Apr 17 22:26:41.085: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
Apr 17 22:26:41.143: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Apr 17 22:26:41.143: INFO: Deleting pod "simpletest.rc-26l6s" in namespace "gc-4611"
Apr 17 22:26:41.154: INFO: Deleting pod "simpletest.rc-2jbsv" in namespace "gc-4611"
Apr 17 22:26:41.163: INFO: Deleting pod "simpletest.rc-2kwxs" in namespace "gc-4611"
Apr 17 22:26:41.172: INFO: Deleting pod "simpletest.rc-2pfwg" in namespace "gc-4611"
Apr 17 22:26:41.183: INFO: Deleting pod "simpletest.rc-2vzsz" in namespace "gc-4611"
Apr 17 22:26:41.191: INFO: Deleting pod "simpletest.rc-47hzj" in namespace "gc-4611"
Apr 17 22:26:41.203: INFO: Deleting pod "simpletest.rc-4fl4g" in namespace "gc-4611"
Apr 17 22:26:41.213: INFO: Deleting pod "simpletest.rc-4pxkd" in namespace "gc-4611"
Apr 17 22:26:41.224: INFO: Deleting pod "simpletest.rc-4t5vt" in namespace "gc-4611"
Apr 17 22:26:41.231: INFO: Deleting pod "simpletest.rc-4tsxj" in namespace "gc-4611"
Apr 17 22:26:41.243: INFO: Deleting pod "simpletest.rc-4vsgg" in namespace "gc-4611"
Apr 17 22:26:41.252: INFO: Deleting pod "simpletest.rc-4w7gj" in namespace "gc-4611"
Apr 17 22:26:41.262: INFO: Deleting pod "simpletest.rc-4wcqm" in namespace "gc-4611"
Apr 17 22:26:41.270: INFO: Deleting pod "simpletest.rc-54n4f" in namespace "gc-4611"
Apr 17 22:26:41.284: INFO: Deleting pod "simpletest.rc-5f9pg" in namespace "gc-4611"
Apr 17 22:26:41.296: INFO: Deleting pod "simpletest.rc-5j5ch" in namespace "gc-4611"
Apr 17 22:26:41.307: INFO: Deleting pod "simpletest.rc-626dq" in namespace "gc-4611"
Apr 17 22:26:41.319: INFO: Deleting pod "simpletest.rc-6bxft" in namespace "gc-4611"
Apr 17 22:26:41.329: INFO: Deleting pod "simpletest.rc-6jtnh" in namespace "gc-4611"
Apr 17 22:26:41.338: INFO: Deleting pod "simpletest.rc-6nnnn" in namespace "gc-4611"
Apr 17 22:26:41.348: INFO: Deleting pod "simpletest.rc-6nvqp" in namespace "gc-4611"
Apr 17 22:26:41.358: INFO: Deleting pod "simpletest.rc-6x6q8" in namespace "gc-4611"
Apr 17 22:26:41.371: INFO: Deleting pod "simpletest.rc-72xpp" in namespace "gc-4611"
Apr 17 22:26:41.395: INFO: Deleting pod "simpletest.rc-79m5n" in namespace "gc-4611"
Apr 17 22:26:41.412: INFO: Deleting pod "simpletest.rc-7sr9x" in namespace "gc-4611"
Apr 17 22:26:41.424: INFO: Deleting pod "simpletest.rc-7zqzn" in namespace "gc-4611"
Apr 17 22:26:41.434: INFO: Deleting pod "simpletest.rc-8dv7x" in namespace "gc-4611"
Apr 17 22:26:41.446: INFO: Deleting pod "simpletest.rc-8fvsd" in namespace "gc-4611"
Apr 17 22:26:41.460: INFO: Deleting pod "simpletest.rc-8l5wv" in namespace "gc-4611"
Apr 17 22:26:41.495: INFO: Deleting pod "simpletest.rc-8qfvc" in namespace "gc-4611"
Apr 17 22:26:41.528: INFO: Deleting pod "simpletest.rc-94fs7" in namespace "gc-4611"
Apr 17 22:26:41.541: INFO: Deleting pod "simpletest.rc-99s98" in namespace "gc-4611"
Apr 17 22:26:41.562: INFO: Deleting pod "simpletest.rc-9n7zd" in namespace "gc-4611"
Apr 17 22:26:41.573: INFO: Deleting pod "simpletest.rc-9r25p" in namespace "gc-4611"
Apr 17 22:26:41.584: INFO: Deleting pod "simpletest.rc-9vcrv" in namespace "gc-4611"
Apr 17 22:26:41.596: INFO: Deleting pod "simpletest.rc-9zrrj" in namespace "gc-4611"
Apr 17 22:26:41.608: INFO: Deleting pod "simpletest.rc-bv692" in namespace "gc-4611"
Apr 17 22:26:41.619: INFO: Deleting pod "simpletest.rc-cdhrz" in namespace "gc-4611"
Apr 17 22:26:41.632: INFO: Deleting pod "simpletest.rc-cmwlt" in namespace "gc-4611"
Apr 17 22:26:41.642: INFO: Deleting pod "simpletest.rc-dbnlq" in namespace "gc-4611"
Apr 17 22:26:41.652: INFO: Deleting pod "simpletest.rc-ddhx9" in namespace "gc-4611"
Apr 17 22:26:41.662: INFO: Deleting pod "simpletest.rc-dgqqg" in namespace "gc-4611"
Apr 17 22:26:41.673: INFO: Deleting pod "simpletest.rc-dkfq6" in namespace "gc-4611"
Apr 17 22:26:41.692: INFO: Deleting pod "simpletest.rc-dxk5k" in namespace "gc-4611"
Apr 17 22:26:41.711: INFO: Deleting pod "simpletest.rc-dxl2b" in namespace "gc-4611"
Apr 17 22:26:41.724: INFO: Deleting pod "simpletest.rc-fbd7m" in namespace "gc-4611"
Apr 17 22:26:41.742: INFO: Deleting pod "simpletest.rc-flx7m" in namespace "gc-4611"
Apr 17 22:26:41.764: INFO: Deleting pod "simpletest.rc-frz9m" in namespace "gc-4611"
Apr 17 22:26:41.779: INFO: Deleting pod "simpletest.rc-g4pbc" in namespace "gc-4611"
Apr 17 22:26:41.793: INFO: Deleting pod "simpletest.rc-g5284" in namespace "gc-4611"
Apr 17 22:26:41.806: INFO: Deleting pod "simpletest.rc-g58vp" in namespace "gc-4611"
Apr 17 22:26:41.817: INFO: Deleting pod "simpletest.rc-g98ks" in namespace "gc-4611"
Apr 17 22:26:41.829: INFO: Deleting pod "simpletest.rc-gn8tg" in namespace "gc-4611"
Apr 17 22:26:41.840: INFO: Deleting pod "simpletest.rc-gssp9" in namespace "gc-4611"
Apr 17 22:26:41.857: INFO: Deleting pod "simpletest.rc-gxcnz" in namespace "gc-4611"
Apr 17 22:26:41.869: INFO: Deleting pod "simpletest.rc-hmbgv" in namespace "gc-4611"
Apr 17 22:26:41.887: INFO: Deleting pod "simpletest.rc-j8nz2" in namespace "gc-4611"
Apr 17 22:26:41.907: INFO: Deleting pod "simpletest.rc-jskzt" in namespace "gc-4611"
Apr 17 22:26:41.917: INFO: Deleting pod "simpletest.rc-jxrpl" in namespace "gc-4611"
Apr 17 22:26:41.927: INFO: Deleting pod "simpletest.rc-k2vwh" in namespace "gc-4611"
Apr 17 22:26:41.939: INFO: Deleting pod "simpletest.rc-kfdtr" in namespace "gc-4611"
Apr 17 22:26:41.952: INFO: Deleting pod "simpletest.rc-khhdn" in namespace "gc-4611"
Apr 17 22:26:41.968: INFO: Deleting pod "simpletest.rc-l485w" in namespace "gc-4611"
Apr 17 22:26:41.982: INFO: Deleting pod "simpletest.rc-ljnnp" in namespace "gc-4611"
Apr 17 22:26:41.993: INFO: Deleting pod "simpletest.rc-m5zgc" in namespace "gc-4611"
Apr 17 22:26:42.006: INFO: Deleting pod "simpletest.rc-mdvps" in namespace "gc-4611"
Apr 17 22:26:42.055: INFO: Deleting pod "simpletest.rc-mwvnh" in namespace "gc-4611"
Apr 17 22:26:42.113: INFO: Deleting pod "simpletest.rc-n9s8c" in namespace "gc-4611"
Apr 17 22:26:42.286: INFO: Deleting pod "simpletest.rc-nbb2p" in namespace "gc-4611"
Apr 17 22:26:42.300: INFO: Deleting pod "simpletest.rc-nn4mg" in namespace "gc-4611"
Apr 17 22:26:42.320: INFO: Deleting pod "simpletest.rc-p4vwv" in namespace "gc-4611"
Apr 17 22:26:42.336: INFO: Deleting pod "simpletest.rc-pxcfp" in namespace "gc-4611"
Apr 17 22:26:42.359: INFO: Deleting pod "simpletest.rc-qcw2t" in namespace "gc-4611"
Apr 17 22:26:42.409: INFO: Deleting pod "simpletest.rc-r54dz" in namespace "gc-4611"
Apr 17 22:26:42.456: INFO: Deleting pod "simpletest.rc-rkm5p" in namespace "gc-4611"
Apr 17 22:26:42.509: INFO: Deleting pod "simpletest.rc-rxxqn" in namespace "gc-4611"
Apr 17 22:26:42.559: INFO: Deleting pod "simpletest.rc-sdbqg" in namespace "gc-4611"
Apr 17 22:26:42.610: INFO: Deleting pod "simpletest.rc-sr57k" in namespace "gc-4611"
Apr 17 22:26:42.664: INFO: Deleting pod "simpletest.rc-sthrt" in namespace "gc-4611"
Apr 17 22:26:42.707: INFO: Deleting pod "simpletest.rc-t7n4w" in namespace "gc-4611"
Apr 17 22:26:42.754: INFO: Deleting pod "simpletest.rc-tc5qs" in namespace "gc-4611"
Apr 17 22:26:42.811: INFO: Deleting pod "simpletest.rc-tl9nj" in namespace "gc-4611"
Apr 17 22:26:42.856: INFO: Deleting pod "simpletest.rc-tqwk9" in namespace "gc-4611"
Apr 17 22:26:42.908: INFO: Deleting pod "simpletest.rc-v22cs" in namespace "gc-4611"
Apr 17 22:26:42.962: INFO: Deleting pod "simpletest.rc-v27lk" in namespace "gc-4611"
Apr 17 22:26:43.012: INFO: Deleting pod "simpletest.rc-vhj76" in namespace "gc-4611"
Apr 17 22:26:43.063: INFO: Deleting pod "simpletest.rc-vhpkj" in namespace "gc-4611"
Apr 17 22:26:43.185: INFO: Deleting pod "simpletest.rc-vslgj" in namespace "gc-4611"
Apr 17 22:26:43.202: INFO: Deleting pod "simpletest.rc-w6nrf" in namespace "gc-4611"
Apr 17 22:26:43.215: INFO: Deleting pod "simpletest.rc-wkc4q" in namespace "gc-4611"
Apr 17 22:26:43.254: INFO: Deleting pod "simpletest.rc-wn668" in namespace "gc-4611"
Apr 17 22:26:43.313: INFO: Deleting pod "simpletest.rc-wrmmh" in namespace "gc-4611"
Apr 17 22:26:43.358: INFO: Deleting pod "simpletest.rc-wvk45" in namespace "gc-4611"
Apr 17 22:26:43.406: INFO: Deleting pod "simpletest.rc-xcd46" in namespace "gc-4611"
Apr 17 22:26:43.460: INFO: Deleting pod "simpletest.rc-xh77c" in namespace "gc-4611"
Apr 17 22:26:43.514: INFO: Deleting pod "simpletest.rc-xsjg5" in namespace "gc-4611"
Apr 17 22:26:43.565: INFO: Deleting pod "simpletest.rc-xsk6n" in namespace "gc-4611"
Apr 17 22:26:43.608: INFO: Deleting pod "simpletest.rc-xvb5m" in namespace "gc-4611"
Apr 17 22:26:43.674: INFO: Deleting pod "simpletest.rc-zrrgz" in namespace "gc-4611"
Apr 17 22:26:43.707: INFO: Deleting pod "simpletest.rc-zvm2d" in namespace "gc-4611"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 17 22:26:43.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-4611" for this suite. 04/17/23 22:26:43.799
------------------------------
• [SLOW TEST] [42.860 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:26:00.993
    Apr 17 22:26:00.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename gc 04/17/23 22:26:00.994
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:26:01.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:26:01.008
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 04/17/23 22:26:01.016
    STEP: delete the rc 04/17/23 22:26:06.028
    STEP: wait for the rc to be deleted 04/17/23 22:26:06.038
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 04/17/23 22:26:11.041
    STEP: Gathering metrics 04/17/23 22:26:41.053
    Apr 17 22:26:41.083: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Apr 17 22:26:41.085: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.344778ms
    Apr 17 22:26:41.085: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
    Apr 17 22:26:41.085: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
    Apr 17 22:26:41.143: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Apr 17 22:26:41.143: INFO: Deleting pod "simpletest.rc-26l6s" in namespace "gc-4611"
    Apr 17 22:26:41.154: INFO: Deleting pod "simpletest.rc-2jbsv" in namespace "gc-4611"
    Apr 17 22:26:41.163: INFO: Deleting pod "simpletest.rc-2kwxs" in namespace "gc-4611"
    Apr 17 22:26:41.172: INFO: Deleting pod "simpletest.rc-2pfwg" in namespace "gc-4611"
    Apr 17 22:26:41.183: INFO: Deleting pod "simpletest.rc-2vzsz" in namespace "gc-4611"
    Apr 17 22:26:41.191: INFO: Deleting pod "simpletest.rc-47hzj" in namespace "gc-4611"
    Apr 17 22:26:41.203: INFO: Deleting pod "simpletest.rc-4fl4g" in namespace "gc-4611"
    Apr 17 22:26:41.213: INFO: Deleting pod "simpletest.rc-4pxkd" in namespace "gc-4611"
    Apr 17 22:26:41.224: INFO: Deleting pod "simpletest.rc-4t5vt" in namespace "gc-4611"
    Apr 17 22:26:41.231: INFO: Deleting pod "simpletest.rc-4tsxj" in namespace "gc-4611"
    Apr 17 22:26:41.243: INFO: Deleting pod "simpletest.rc-4vsgg" in namespace "gc-4611"
    Apr 17 22:26:41.252: INFO: Deleting pod "simpletest.rc-4w7gj" in namespace "gc-4611"
    Apr 17 22:26:41.262: INFO: Deleting pod "simpletest.rc-4wcqm" in namespace "gc-4611"
    Apr 17 22:26:41.270: INFO: Deleting pod "simpletest.rc-54n4f" in namespace "gc-4611"
    Apr 17 22:26:41.284: INFO: Deleting pod "simpletest.rc-5f9pg" in namespace "gc-4611"
    Apr 17 22:26:41.296: INFO: Deleting pod "simpletest.rc-5j5ch" in namespace "gc-4611"
    Apr 17 22:26:41.307: INFO: Deleting pod "simpletest.rc-626dq" in namespace "gc-4611"
    Apr 17 22:26:41.319: INFO: Deleting pod "simpletest.rc-6bxft" in namespace "gc-4611"
    Apr 17 22:26:41.329: INFO: Deleting pod "simpletest.rc-6jtnh" in namespace "gc-4611"
    Apr 17 22:26:41.338: INFO: Deleting pod "simpletest.rc-6nnnn" in namespace "gc-4611"
    Apr 17 22:26:41.348: INFO: Deleting pod "simpletest.rc-6nvqp" in namespace "gc-4611"
    Apr 17 22:26:41.358: INFO: Deleting pod "simpletest.rc-6x6q8" in namespace "gc-4611"
    Apr 17 22:26:41.371: INFO: Deleting pod "simpletest.rc-72xpp" in namespace "gc-4611"
    Apr 17 22:26:41.395: INFO: Deleting pod "simpletest.rc-79m5n" in namespace "gc-4611"
    Apr 17 22:26:41.412: INFO: Deleting pod "simpletest.rc-7sr9x" in namespace "gc-4611"
    Apr 17 22:26:41.424: INFO: Deleting pod "simpletest.rc-7zqzn" in namespace "gc-4611"
    Apr 17 22:26:41.434: INFO: Deleting pod "simpletest.rc-8dv7x" in namespace "gc-4611"
    Apr 17 22:26:41.446: INFO: Deleting pod "simpletest.rc-8fvsd" in namespace "gc-4611"
    Apr 17 22:26:41.460: INFO: Deleting pod "simpletest.rc-8l5wv" in namespace "gc-4611"
    Apr 17 22:26:41.495: INFO: Deleting pod "simpletest.rc-8qfvc" in namespace "gc-4611"
    Apr 17 22:26:41.528: INFO: Deleting pod "simpletest.rc-94fs7" in namespace "gc-4611"
    Apr 17 22:26:41.541: INFO: Deleting pod "simpletest.rc-99s98" in namespace "gc-4611"
    Apr 17 22:26:41.562: INFO: Deleting pod "simpletest.rc-9n7zd" in namespace "gc-4611"
    Apr 17 22:26:41.573: INFO: Deleting pod "simpletest.rc-9r25p" in namespace "gc-4611"
    Apr 17 22:26:41.584: INFO: Deleting pod "simpletest.rc-9vcrv" in namespace "gc-4611"
    Apr 17 22:26:41.596: INFO: Deleting pod "simpletest.rc-9zrrj" in namespace "gc-4611"
    Apr 17 22:26:41.608: INFO: Deleting pod "simpletest.rc-bv692" in namespace "gc-4611"
    Apr 17 22:26:41.619: INFO: Deleting pod "simpletest.rc-cdhrz" in namespace "gc-4611"
    Apr 17 22:26:41.632: INFO: Deleting pod "simpletest.rc-cmwlt" in namespace "gc-4611"
    Apr 17 22:26:41.642: INFO: Deleting pod "simpletest.rc-dbnlq" in namespace "gc-4611"
    Apr 17 22:26:41.652: INFO: Deleting pod "simpletest.rc-ddhx9" in namespace "gc-4611"
    Apr 17 22:26:41.662: INFO: Deleting pod "simpletest.rc-dgqqg" in namespace "gc-4611"
    Apr 17 22:26:41.673: INFO: Deleting pod "simpletest.rc-dkfq6" in namespace "gc-4611"
    Apr 17 22:26:41.692: INFO: Deleting pod "simpletest.rc-dxk5k" in namespace "gc-4611"
    Apr 17 22:26:41.711: INFO: Deleting pod "simpletest.rc-dxl2b" in namespace "gc-4611"
    Apr 17 22:26:41.724: INFO: Deleting pod "simpletest.rc-fbd7m" in namespace "gc-4611"
    Apr 17 22:26:41.742: INFO: Deleting pod "simpletest.rc-flx7m" in namespace "gc-4611"
    Apr 17 22:26:41.764: INFO: Deleting pod "simpletest.rc-frz9m" in namespace "gc-4611"
    Apr 17 22:26:41.779: INFO: Deleting pod "simpletest.rc-g4pbc" in namespace "gc-4611"
    Apr 17 22:26:41.793: INFO: Deleting pod "simpletest.rc-g5284" in namespace "gc-4611"
    Apr 17 22:26:41.806: INFO: Deleting pod "simpletest.rc-g58vp" in namespace "gc-4611"
    Apr 17 22:26:41.817: INFO: Deleting pod "simpletest.rc-g98ks" in namespace "gc-4611"
    Apr 17 22:26:41.829: INFO: Deleting pod "simpletest.rc-gn8tg" in namespace "gc-4611"
    Apr 17 22:26:41.840: INFO: Deleting pod "simpletest.rc-gssp9" in namespace "gc-4611"
    Apr 17 22:26:41.857: INFO: Deleting pod "simpletest.rc-gxcnz" in namespace "gc-4611"
    Apr 17 22:26:41.869: INFO: Deleting pod "simpletest.rc-hmbgv" in namespace "gc-4611"
    Apr 17 22:26:41.887: INFO: Deleting pod "simpletest.rc-j8nz2" in namespace "gc-4611"
    Apr 17 22:26:41.907: INFO: Deleting pod "simpletest.rc-jskzt" in namespace "gc-4611"
    Apr 17 22:26:41.917: INFO: Deleting pod "simpletest.rc-jxrpl" in namespace "gc-4611"
    Apr 17 22:26:41.927: INFO: Deleting pod "simpletest.rc-k2vwh" in namespace "gc-4611"
    Apr 17 22:26:41.939: INFO: Deleting pod "simpletest.rc-kfdtr" in namespace "gc-4611"
    Apr 17 22:26:41.952: INFO: Deleting pod "simpletest.rc-khhdn" in namespace "gc-4611"
    Apr 17 22:26:41.968: INFO: Deleting pod "simpletest.rc-l485w" in namespace "gc-4611"
    Apr 17 22:26:41.982: INFO: Deleting pod "simpletest.rc-ljnnp" in namespace "gc-4611"
    Apr 17 22:26:41.993: INFO: Deleting pod "simpletest.rc-m5zgc" in namespace "gc-4611"
    Apr 17 22:26:42.006: INFO: Deleting pod "simpletest.rc-mdvps" in namespace "gc-4611"
    Apr 17 22:26:42.055: INFO: Deleting pod "simpletest.rc-mwvnh" in namespace "gc-4611"
    Apr 17 22:26:42.113: INFO: Deleting pod "simpletest.rc-n9s8c" in namespace "gc-4611"
    Apr 17 22:26:42.286: INFO: Deleting pod "simpletest.rc-nbb2p" in namespace "gc-4611"
    Apr 17 22:26:42.300: INFO: Deleting pod "simpletest.rc-nn4mg" in namespace "gc-4611"
    Apr 17 22:26:42.320: INFO: Deleting pod "simpletest.rc-p4vwv" in namespace "gc-4611"
    Apr 17 22:26:42.336: INFO: Deleting pod "simpletest.rc-pxcfp" in namespace "gc-4611"
    Apr 17 22:26:42.359: INFO: Deleting pod "simpletest.rc-qcw2t" in namespace "gc-4611"
    Apr 17 22:26:42.409: INFO: Deleting pod "simpletest.rc-r54dz" in namespace "gc-4611"
    Apr 17 22:26:42.456: INFO: Deleting pod "simpletest.rc-rkm5p" in namespace "gc-4611"
    Apr 17 22:26:42.509: INFO: Deleting pod "simpletest.rc-rxxqn" in namespace "gc-4611"
    Apr 17 22:26:42.559: INFO: Deleting pod "simpletest.rc-sdbqg" in namespace "gc-4611"
    Apr 17 22:26:42.610: INFO: Deleting pod "simpletest.rc-sr57k" in namespace "gc-4611"
    Apr 17 22:26:42.664: INFO: Deleting pod "simpletest.rc-sthrt" in namespace "gc-4611"
    Apr 17 22:26:42.707: INFO: Deleting pod "simpletest.rc-t7n4w" in namespace "gc-4611"
    Apr 17 22:26:42.754: INFO: Deleting pod "simpletest.rc-tc5qs" in namespace "gc-4611"
    Apr 17 22:26:42.811: INFO: Deleting pod "simpletest.rc-tl9nj" in namespace "gc-4611"
    Apr 17 22:26:42.856: INFO: Deleting pod "simpletest.rc-tqwk9" in namespace "gc-4611"
    Apr 17 22:26:42.908: INFO: Deleting pod "simpletest.rc-v22cs" in namespace "gc-4611"
    Apr 17 22:26:42.962: INFO: Deleting pod "simpletest.rc-v27lk" in namespace "gc-4611"
    Apr 17 22:26:43.012: INFO: Deleting pod "simpletest.rc-vhj76" in namespace "gc-4611"
    Apr 17 22:26:43.063: INFO: Deleting pod "simpletest.rc-vhpkj" in namespace "gc-4611"
    Apr 17 22:26:43.185: INFO: Deleting pod "simpletest.rc-vslgj" in namespace "gc-4611"
    Apr 17 22:26:43.202: INFO: Deleting pod "simpletest.rc-w6nrf" in namespace "gc-4611"
    Apr 17 22:26:43.215: INFO: Deleting pod "simpletest.rc-wkc4q" in namespace "gc-4611"
    Apr 17 22:26:43.254: INFO: Deleting pod "simpletest.rc-wn668" in namespace "gc-4611"
    Apr 17 22:26:43.313: INFO: Deleting pod "simpletest.rc-wrmmh" in namespace "gc-4611"
    Apr 17 22:26:43.358: INFO: Deleting pod "simpletest.rc-wvk45" in namespace "gc-4611"
    Apr 17 22:26:43.406: INFO: Deleting pod "simpletest.rc-xcd46" in namespace "gc-4611"
    Apr 17 22:26:43.460: INFO: Deleting pod "simpletest.rc-xh77c" in namespace "gc-4611"
    Apr 17 22:26:43.514: INFO: Deleting pod "simpletest.rc-xsjg5" in namespace "gc-4611"
    Apr 17 22:26:43.565: INFO: Deleting pod "simpletest.rc-xsk6n" in namespace "gc-4611"
    Apr 17 22:26:43.608: INFO: Deleting pod "simpletest.rc-xvb5m" in namespace "gc-4611"
    Apr 17 22:26:43.674: INFO: Deleting pod "simpletest.rc-zrrgz" in namespace "gc-4611"
    Apr 17 22:26:43.707: INFO: Deleting pod "simpletest.rc-zvm2d" in namespace "gc-4611"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:26:43.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-4611" for this suite. 04/17/23 22:26:43.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:26:43.859
Apr 17 22:26:43.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 22:26:43.859
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:26:43.878
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:26:43.88
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 04/17/23 22:26:43.883
STEP: Creating a ResourceQuota 04/17/23 22:26:48.885
STEP: Ensuring resource quota status is calculated 04/17/23 22:26:48.889
STEP: Creating a Service 04/17/23 22:26:50.893
STEP: Creating a NodePort Service 04/17/23 22:26:50.909
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 04/17/23 22:26:50.933
STEP: Ensuring resource quota status captures service creation 04/17/23 22:26:51.065
STEP: Deleting Services 04/17/23 22:26:53.069
STEP: Ensuring resource quota status released usage 04/17/23 22:26:53.108
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 22:26:55.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8561" for this suite. 04/17/23 22:26:55.116
------------------------------
• [SLOW TEST] [11.262 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:26:43.859
    Apr 17 22:26:43.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 22:26:43.859
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:26:43.878
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:26:43.88
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 04/17/23 22:26:43.883
    STEP: Creating a ResourceQuota 04/17/23 22:26:48.885
    STEP: Ensuring resource quota status is calculated 04/17/23 22:26:48.889
    STEP: Creating a Service 04/17/23 22:26:50.893
    STEP: Creating a NodePort Service 04/17/23 22:26:50.909
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 04/17/23 22:26:50.933
    STEP: Ensuring resource quota status captures service creation 04/17/23 22:26:51.065
    STEP: Deleting Services 04/17/23 22:26:53.069
    STEP: Ensuring resource quota status released usage 04/17/23 22:26:53.108
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:26:55.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8561" for this suite. 04/17/23 22:26:55.116
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:26:55.121
Apr 17 22:26:55.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename init-container 04/17/23 22:26:55.121
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:26:55.136
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:26:55.139
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 04/17/23 22:26:55.141
Apr 17 22:26:55.141: INFO: PodSpec: initContainers in spec.initContainers
Apr 17 22:27:33.258: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-040d6233-22be-413f-a8ad-81edc14239d2", GenerateName:"", Namespace:"init-container-7883", SelfLink:"", UID:"63c55428-4725-4eb5-a075-cc0890974f80", ResourceVersion:"67828", Generation:0, CreationTimestamp:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"141254030"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"ab672862d30ed17c2444dbf9865825351de0a3cd107492f489fa067c3cdf1bbc", "cni.projectcalico.org/podIP":"192.168.213.49/32", "cni.projectcalico.org/podIPs":"192.168.213.49/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0047a8e28), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0047a8e58), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 17, 22, 27, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0047a8e88), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-6vn86", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc009a112a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6vn86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6vn86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6vn86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004707778), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-93-18.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004e27650), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0047077f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004707810)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004707818), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00470781c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc006b772d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.93.18", PodIP:"192.168.213.49", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.213.49"}}, StartTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0047a8ed0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004e27730)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://9ee61b751601e25bfb0ef3f4bb8be070e181eeab8e44afebb644af9d5d9c0547", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009a11320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009a11300), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0047078af)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:27:33.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7883" for this suite. 04/17/23 22:27:33.263
------------------------------
• [SLOW TEST] [38.147 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:26:55.121
    Apr 17 22:26:55.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename init-container 04/17/23 22:26:55.121
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:26:55.136
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:26:55.139
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 04/17/23 22:26:55.141
    Apr 17 22:26:55.141: INFO: PodSpec: initContainers in spec.initContainers
    Apr 17 22:27:33.258: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-040d6233-22be-413f-a8ad-81edc14239d2", GenerateName:"", Namespace:"init-container-7883", SelfLink:"", UID:"63c55428-4725-4eb5-a075-cc0890974f80", ResourceVersion:"67828", Generation:0, CreationTimestamp:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"141254030"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"ab672862d30ed17c2444dbf9865825351de0a3cd107492f489fa067c3cdf1bbc", "cni.projectcalico.org/podIP":"192.168.213.49/32", "cni.projectcalico.org/podIPs":"192.168.213.49/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0047a8e28), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0047a8e58), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.April, 17, 22, 27, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0047a8e88), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-6vn86", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc009a112a0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6vn86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6vn86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-6vn86", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004707778), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-93-18.us-west-2.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004e27650), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0047077f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004707810)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004707818), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00470781c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc006b772d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.93.18", PodIP:"192.168.213.49", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.213.49"}}, StartTime:time.Date(2023, time.April, 17, 22, 26, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0047a8ed0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004e27730)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://9ee61b751601e25bfb0ef3f4bb8be070e181eeab8e44afebb644af9d5d9c0547", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009a11320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc009a11300), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0047078af)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:27:33.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7883" for this suite. 04/17/23 22:27:33.263
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:27:33.268
Apr 17 22:27:33.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename job 04/17/23 22:27:33.269
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:27:33.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:27:33.284
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 04/17/23 22:27:33.286
STEP: Ensuring job reaches completions 04/17/23 22:27:33.291
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Apr 17 22:27:45.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8179" for this suite. 04/17/23 22:27:45.299
------------------------------
• [SLOW TEST] [12.036 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:27:33.268
    Apr 17 22:27:33.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename job 04/17/23 22:27:33.269
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:27:33.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:27:33.284
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 04/17/23 22:27:33.286
    STEP: Ensuring job reaches completions 04/17/23 22:27:33.291
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:27:45.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8179" for this suite. 04/17/23 22:27:45.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:27:45.304
Apr 17 22:27:45.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename statefulset 04/17/23 22:27:45.305
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:27:45.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:27:45.32
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1670 04/17/23 22:27:45.322
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 04/17/23 22:27:45.325
Apr 17 22:27:45.335: INFO: Found 0 stateful pods, waiting for 3
Apr 17 22:27:55.339: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:27:55.339: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:27:55.339: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:27:55.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1670 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:27:55.481: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:27:55.481: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:27:55.481: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/17/23 22:28:05.492
Apr 17 22:28:05.509: INFO: Updating stateful set ss2
STEP: Creating a new revision 04/17/23 22:28:05.509
STEP: Updating Pods in reverse ordinal order 04/17/23 22:28:15.522
Apr 17 22:28:15.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1670 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:28:15.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:28:15.654: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:28:15.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 04/17/23 22:28:35.671
Apr 17 22:28:35.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1670 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Apr 17 22:28:35.830: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Apr 17 22:28:35.830: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Apr 17 22:28:35.830: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Apr 17 22:28:45.861: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 04/17/23 22:28:55.874
Apr 17 22:28:55.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1670 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Apr 17 22:28:56.006: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Apr 17 22:28:56.006: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Apr 17 22:28:56.006: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 17 22:29:06.024: INFO: Deleting all statefulset in ns statefulset-1670
Apr 17 22:29:06.028: INFO: Scaling statefulset ss2 to 0
Apr 17 22:29:16.039: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:29:16.041: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:29:16.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1670" for this suite. 04/17/23 22:29:16.057
------------------------------
• [SLOW TEST] [90.758 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:27:45.304
    Apr 17 22:27:45.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename statefulset 04/17/23 22:27:45.305
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:27:45.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:27:45.32
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1670 04/17/23 22:27:45.322
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 04/17/23 22:27:45.325
    Apr 17 22:27:45.335: INFO: Found 0 stateful pods, waiting for 3
    Apr 17 22:27:55.339: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:27:55.339: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:27:55.339: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:27:55.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1670 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 22:27:55.481: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 22:27:55.481: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 22:27:55.481: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/17/23 22:28:05.492
    Apr 17 22:28:05.509: INFO: Updating stateful set ss2
    STEP: Creating a new revision 04/17/23 22:28:05.509
    STEP: Updating Pods in reverse ordinal order 04/17/23 22:28:15.522
    Apr 17 22:28:15.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1670 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 17 22:28:15.654: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 17 22:28:15.654: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 17 22:28:15.654: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 04/17/23 22:28:35.671
    Apr 17 22:28:35.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1670 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Apr 17 22:28:35.830: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Apr 17 22:28:35.830: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Apr 17 22:28:35.830: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Apr 17 22:28:45.861: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 04/17/23 22:28:55.874
    Apr 17 22:28:55.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=statefulset-1670 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Apr 17 22:28:56.006: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Apr 17 22:28:56.006: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Apr 17 22:28:56.006: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 17 22:29:06.024: INFO: Deleting all statefulset in ns statefulset-1670
    Apr 17 22:29:06.028: INFO: Scaling statefulset ss2 to 0
    Apr 17 22:29:16.039: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 22:29:16.041: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:29:16.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1670" for this suite. 04/17/23 22:29:16.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:29:16.063
Apr 17 22:29:16.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:29:16.063
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:16.079
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:16.082
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:29:16.084
Apr 17 22:29:16.091: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1" in namespace "downward-api-4621" to be "Succeeded or Failed"
Apr 17 22:29:16.093: INFO: Pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27639ms
Apr 17 22:29:18.098: INFO: Pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007013168s
Apr 17 22:29:20.097: INFO: Pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006956795s
STEP: Saw pod success 04/17/23 22:29:20.098
Apr 17 22:29:20.098: INFO: Pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1" satisfied condition "Succeeded or Failed"
Apr 17 22:29:20.100: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1 container client-container: <nil>
STEP: delete the pod 04/17/23 22:29:20.112
Apr 17 22:29:20.124: INFO: Waiting for pod downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1 to disappear
Apr 17 22:29:20.126: INFO: Pod downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 22:29:20.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-4621" for this suite. 04/17/23 22:29:20.13
------------------------------
• [4.071 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:29:16.063
    Apr 17 22:29:16.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:29:16.063
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:16.079
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:16.082
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:29:16.084
    Apr 17 22:29:16.091: INFO: Waiting up to 5m0s for pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1" in namespace "downward-api-4621" to be "Succeeded or Failed"
    Apr 17 22:29:16.093: INFO: Pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.27639ms
    Apr 17 22:29:18.098: INFO: Pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007013168s
    Apr 17 22:29:20.097: INFO: Pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006956795s
    STEP: Saw pod success 04/17/23 22:29:20.098
    Apr 17 22:29:20.098: INFO: Pod "downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1" satisfied condition "Succeeded or Failed"
    Apr 17 22:29:20.100: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1 container client-container: <nil>
    STEP: delete the pod 04/17/23 22:29:20.112
    Apr 17 22:29:20.124: INFO: Waiting for pod downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1 to disappear
    Apr 17 22:29:20.126: INFO: Pod downwardapi-volume-90b14f02-b82d-4f79-bf2d-c979af9e70c1 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:29:20.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-4621" for this suite. 04/17/23 22:29:20.13
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:29:20.134
Apr 17 22:29:20.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename daemonsets 04/17/23 22:29:20.135
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:20.146
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:20.148
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 04/17/23 22:29:20.17
STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 22:29:20.174
Apr 17 22:29:20.178: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:20.178: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:20.178: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:20.180: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:29:20.180: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:29:21.186: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:21.186: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:21.186: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:21.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:29:21.189: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:29:22.186: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:22.186: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:22.186: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:22.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr 17 22:29:22.188: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Getting /status 04/17/23 22:29:22.19
Apr 17 22:29:22.193: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 04/17/23 22:29:22.193
Apr 17 22:29:22.201: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 04/17/23 22:29:22.202
Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: ADDED
Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.203: INFO: Found daemon set daemon-set in namespace daemonsets-5586 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 17 22:29:22.203: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 04/17/23 22:29:22.203
STEP: watching for the daemon set status to be patched 04/17/23 22:29:22.208
Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: ADDED
Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.210: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.210: INFO: Observed daemon set daemon-set in namespace daemonsets-5586 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Apr 17 22:29:22.210: INFO: Observed &DaemonSet event: MODIFIED
Apr 17 22:29:22.210: INFO: Found daemon set daemon-set in namespace daemonsets-5586 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Apr 17 22:29:22.210: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/17/23 22:29:22.214
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5586, will wait for the garbage collector to delete the pods 04/17/23 22:29:22.214
Apr 17 22:29:22.272: INFO: Deleting DaemonSet.extensions daemon-set took: 5.092074ms
Apr 17 22:29:22.372: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.39876ms
Apr 17 22:29:24.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:29:24.975: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 17 22:29:24.977: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"69526"},"items":null}

Apr 17 22:29:24.978: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"69526"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:29:24.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5586" for this suite. 04/17/23 22:29:24.997
------------------------------
• [4.867 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:29:20.134
    Apr 17 22:29:20.134: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename daemonsets 04/17/23 22:29:20.135
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:20.146
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:20.148
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 04/17/23 22:29:20.17
    STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 22:29:20.174
    Apr 17 22:29:20.178: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:20.178: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:20.178: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:20.180: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:29:20.180: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:29:21.186: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:21.186: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:21.186: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:21.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:29:21.189: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:29:22.186: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:22.186: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:22.186: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:22.188: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr 17 22:29:22.188: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Getting /status 04/17/23 22:29:22.19
    Apr 17 22:29:22.193: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 04/17/23 22:29:22.193
    Apr 17 22:29:22.201: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 04/17/23 22:29:22.202
    Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: ADDED
    Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.203: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.203: INFO: Found daemon set daemon-set in namespace daemonsets-5586 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 17 22:29:22.203: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 04/17/23 22:29:22.203
    STEP: watching for the daemon set status to be patched 04/17/23 22:29:22.208
    Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: ADDED
    Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.209: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.210: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.210: INFO: Observed daemon set daemon-set in namespace daemonsets-5586 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Apr 17 22:29:22.210: INFO: Observed &DaemonSet event: MODIFIED
    Apr 17 22:29:22.210: INFO: Found daemon set daemon-set in namespace daemonsets-5586 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Apr 17 22:29:22.210: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/17/23 22:29:22.214
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5586, will wait for the garbage collector to delete the pods 04/17/23 22:29:22.214
    Apr 17 22:29:22.272: INFO: Deleting DaemonSet.extensions daemon-set took: 5.092074ms
    Apr 17 22:29:22.372: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.39876ms
    Apr 17 22:29:24.974: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:29:24.975: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 17 22:29:24.977: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"69526"},"items":null}

    Apr 17 22:29:24.978: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"69526"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:29:24.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5586" for this suite. 04/17/23 22:29:24.997
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:29:25.002
Apr 17 22:29:25.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename daemonsets 04/17/23 22:29:25.003
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:25.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:25.017
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 04/17/23 22:29:25.04
STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 22:29:25.043
Apr 17 22:29:25.048: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:25.048: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:25.048: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:25.050: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:29:25.050: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:29:26.055: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:26.055: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:26.055: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:26.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:29:26.058: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:29:27.055: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:27.055: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:27.055: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:29:27.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr 17 22:29:27.058: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: listing all DeamonSets 04/17/23 22:29:27.06
STEP: DeleteCollection of the DaemonSets 04/17/23 22:29:27.063
STEP: Verify that ReplicaSets have been deleted 04/17/23 22:29:27.068
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Apr 17 22:29:27.076: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"69617"},"items":null}

Apr 17 22:29:27.082: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"69618"},"items":[{"metadata":{"name":"daemon-set-2stz9","generateName":"daemon-set-","namespace":"daemonsets-9551","uid":"35a22b85-bb91-4b51-8610-9869eb865cec","resourceVersion":"69614","creationTimestamp":"2023-04-17T22:29:25Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3047ab56e74b8ba008e74d533cca15c47e704c9bc7478dbf7477f7ce82ac438a","cni.projectcalico.org/podIP":"192.168.163.252/32","cni.projectcalico.org/podIPs":"192.168.163.252/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a431f87-0063-4c70-94b9-a4fd21af1d0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a431f87-0063-4c70-94b9-a4fd21af1d0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-p2m7q","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-p2m7q","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-106-231.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-106-231.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"}],"hostIP":"10.0.106.231","podIP":"192.168.163.252","podIPs":[{"ip":"192.168.163.252"}],"startTime":"2023-04-17T22:29:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-17T22:29:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b46cc8da9f7887199c34e560fc873f5431bd27afecc123fa3f4254c1afd067ac","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-44srs","generateName":"daemon-set-","namespace":"daemonsets-9551","uid":"48f6e03d-c336-4765-9c60-090cd5f6c428","resourceVersion":"69600","creationTimestamp":"2023-04-17T22:29:25Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b8c715bd97e855d1fdb8b77264fd958e56a429ceecec512741bcb7dffc8f31f5","cni.projectcalico.org/podIP":"192.168.208.183/32","cni.projectcalico.org/podIPs":"192.168.208.183/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a431f87-0063-4c70-94b9-a4fd21af1d0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a431f87-0063-4c70-94b9-a4fd21af1d0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vdj2p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vdj2p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-74-52.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-74-52.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"}],"hostIP":"10.0.74.52","podIP":"192.168.208.183","podIPs":[{"ip":"192.168.208.183"}],"startTime":"2023-04-17T22:29:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-17T22:29:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1f115a42b77bd5457852000dbcc1b9611520cc810ebf852a196feb834fc918f3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qd57b","generateName":"daemon-set-","namespace":"daemonsets-9551","uid":"ae8e8f50-a4dc-4001-8be7-6eb3523a4ae5","resourceVersion":"69604","creationTimestamp":"2023-04-17T22:29:25Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"130252ecb31ddedc46d562351a388d8c1f4ecf1a6ded374bf2ba922934d927c9","cni.projectcalico.org/podIP":"192.168.213.54/32","cni.projectcalico.org/podIPs":"192.168.213.54/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a431f87-0063-4c70-94b9-a4fd21af1d0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a431f87-0063-4c70-94b9-a4fd21af1d0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vxmfr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vxmfr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-93-18.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-93-18.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"}],"hostIP":"10.0.93.18","podIP":"192.168.213.54","podIPs":[{"ip":"192.168.213.54"}],"startTime":"2023-04-17T22:29:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-17T22:29:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://541c75d364a9d6476498058f8246db7a210dc5ac2de9234c146911a105b2e580","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vtlqz","generateName":"daemon-set-","namespace":"daemonsets-9551","uid":"0fca5107-8343-49b1-8dfb-845c4ced4ead","resourceVersion":"69607","creationTimestamp":"2023-04-17T22:29:25Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"53d817680c10153421ecf5537e63a5565623cedb06bc1e415553e790c7e36c35","cni.projectcalico.org/podIP":"192.168.200.159/32","cni.projectcalico.org/podIPs":"192.168.200.159/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a431f87-0063-4c70-94b9-a4fd21af1d0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a431f87-0063-4c70-94b9-a4fd21af1d0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.200.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kvnz9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kvnz9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-64-189.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-64-189.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"}],"hostIP":"10.0.64.189","podIP":"192.168.200.159","podIPs":[{"ip":"192.168.200.159"}],"startTime":"2023-04-17T22:29:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-17T22:29:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a1d7c04bd1771da4eb4554059349e7efb0c5c74e710953c1705b1227f8539b4a","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:29:27.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9551" for this suite. 04/17/23 22:29:27.104
------------------------------
• [2.108 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:29:25.002
    Apr 17 22:29:25.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename daemonsets 04/17/23 22:29:25.003
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:25.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:25.017
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 04/17/23 22:29:25.04
    STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 22:29:25.043
    Apr 17 22:29:25.048: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:25.048: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:25.048: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:25.050: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:29:25.050: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:29:26.055: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:26.055: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:26.055: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:26.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:29:26.058: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:29:27.055: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:27.055: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:27.055: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:29:27.058: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr 17 22:29:27.058: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: listing all DeamonSets 04/17/23 22:29:27.06
    STEP: DeleteCollection of the DaemonSets 04/17/23 22:29:27.063
    STEP: Verify that ReplicaSets have been deleted 04/17/23 22:29:27.068
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Apr 17 22:29:27.076: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"69617"},"items":null}

    Apr 17 22:29:27.082: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"69618"},"items":[{"metadata":{"name":"daemon-set-2stz9","generateName":"daemon-set-","namespace":"daemonsets-9551","uid":"35a22b85-bb91-4b51-8610-9869eb865cec","resourceVersion":"69614","creationTimestamp":"2023-04-17T22:29:25Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3047ab56e74b8ba008e74d533cca15c47e704c9bc7478dbf7477f7ce82ac438a","cni.projectcalico.org/podIP":"192.168.163.252/32","cni.projectcalico.org/podIPs":"192.168.163.252/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a431f87-0063-4c70-94b9-a4fd21af1d0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a431f87-0063-4c70-94b9-a4fd21af1d0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-p2m7q","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-p2m7q","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-106-231.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-106-231.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"}],"hostIP":"10.0.106.231","podIP":"192.168.163.252","podIPs":[{"ip":"192.168.163.252"}],"startTime":"2023-04-17T22:29:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-17T22:29:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://b46cc8da9f7887199c34e560fc873f5431bd27afecc123fa3f4254c1afd067ac","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-44srs","generateName":"daemon-set-","namespace":"daemonsets-9551","uid":"48f6e03d-c336-4765-9c60-090cd5f6c428","resourceVersion":"69600","creationTimestamp":"2023-04-17T22:29:25Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"b8c715bd97e855d1fdb8b77264fd958e56a429ceecec512741bcb7dffc8f31f5","cni.projectcalico.org/podIP":"192.168.208.183/32","cni.projectcalico.org/podIPs":"192.168.208.183/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a431f87-0063-4c70-94b9-a4fd21af1d0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a431f87-0063-4c70-94b9-a4fd21af1d0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vdj2p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vdj2p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-74-52.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-74-52.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"}],"hostIP":"10.0.74.52","podIP":"192.168.208.183","podIPs":[{"ip":"192.168.208.183"}],"startTime":"2023-04-17T22:29:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-17T22:29:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://1f115a42b77bd5457852000dbcc1b9611520cc810ebf852a196feb834fc918f3","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qd57b","generateName":"daemon-set-","namespace":"daemonsets-9551","uid":"ae8e8f50-a4dc-4001-8be7-6eb3523a4ae5","resourceVersion":"69604","creationTimestamp":"2023-04-17T22:29:25Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"130252ecb31ddedc46d562351a388d8c1f4ecf1a6ded374bf2ba922934d927c9","cni.projectcalico.org/podIP":"192.168.213.54/32","cni.projectcalico.org/podIPs":"192.168.213.54/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a431f87-0063-4c70-94b9-a4fd21af1d0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a431f87-0063-4c70-94b9-a4fd21af1d0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.54\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vxmfr","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vxmfr","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-93-18.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-93-18.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"}],"hostIP":"10.0.93.18","podIP":"192.168.213.54","podIPs":[{"ip":"192.168.213.54"}],"startTime":"2023-04-17T22:29:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-17T22:29:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://541c75d364a9d6476498058f8246db7a210dc5ac2de9234c146911a105b2e580","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-vtlqz","generateName":"daemon-set-","namespace":"daemonsets-9551","uid":"0fca5107-8343-49b1-8dfb-845c4ced4ead","resourceVersion":"69607","creationTimestamp":"2023-04-17T22:29:25Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"53d817680c10153421ecf5537e63a5565623cedb06bc1e415553e790c7e36c35","cni.projectcalico.org/podIP":"192.168.200.159/32","cni.projectcalico.org/podIPs":"192.168.200.159/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"4a431f87-0063-4c70-94b9-a4fd21af1d0d","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a431f87-0063-4c70-94b9-a4fd21af1d0d\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-04-17T22:29:26Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.200.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kvnz9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kvnz9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ip-10-0-64-189.us-west-2.compute.internal","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ip-10-0-64-189.us-west-2.compute.internal"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:26Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-04-17T22:29:25Z"}],"hostIP":"10.0.64.189","podIP":"192.168.200.159","podIPs":[{"ip":"192.168.200.159"}],"startTime":"2023-04-17T22:29:25Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-04-17T22:29:25Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://a1d7c04bd1771da4eb4554059349e7efb0c5c74e710953c1705b1227f8539b4a","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:29:27.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9551" for this suite. 04/17/23 22:29:27.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:29:27.111
Apr 17 22:29:27.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename namespaces 04/17/23 22:29:27.112
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:27.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:27.125
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 04/17/23 22:29:27.127
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:27.137
STEP: Creating a pod in the namespace 04/17/23 22:29:27.14
STEP: Waiting for the pod to have running status 04/17/23 22:29:27.147
Apr 17 22:29:27.147: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9926" to be "running"
Apr 17 22:29:27.149: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309964ms
Apr 17 22:29:29.152: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005253694s
Apr 17 22:29:29.152: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 04/17/23 22:29:29.152
STEP: Waiting for the namespace to be removed. 04/17/23 22:29:29.157
STEP: Recreating the namespace 04/17/23 22:29:40.16
STEP: Verifying there are no pods in the namespace 04/17/23 22:29:40.175
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:29:40.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6633" for this suite. 04/17/23 22:29:40.182
STEP: Destroying namespace "nsdeletetest-9926" for this suite. 04/17/23 22:29:40.186
Apr 17 22:29:40.188: INFO: Namespace nsdeletetest-9926 was already deleted
STEP: Destroying namespace "nsdeletetest-1599" for this suite. 04/17/23 22:29:40.188
------------------------------
• [SLOW TEST] [13.082 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:29:27.111
    Apr 17 22:29:27.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename namespaces 04/17/23 22:29:27.112
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:27.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:27.125
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 04/17/23 22:29:27.127
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:27.137
    STEP: Creating a pod in the namespace 04/17/23 22:29:27.14
    STEP: Waiting for the pod to have running status 04/17/23 22:29:27.147
    Apr 17 22:29:27.147: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9926" to be "running"
    Apr 17 22:29:27.149: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309964ms
    Apr 17 22:29:29.152: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005253694s
    Apr 17 22:29:29.152: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 04/17/23 22:29:29.152
    STEP: Waiting for the namespace to be removed. 04/17/23 22:29:29.157
    STEP: Recreating the namespace 04/17/23 22:29:40.16
    STEP: Verifying there are no pods in the namespace 04/17/23 22:29:40.175
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:29:40.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6633" for this suite. 04/17/23 22:29:40.182
    STEP: Destroying namespace "nsdeletetest-9926" for this suite. 04/17/23 22:29:40.186
    Apr 17 22:29:40.188: INFO: Namespace nsdeletetest-9926 was already deleted
    STEP: Destroying namespace "nsdeletetest-1599" for this suite. 04/17/23 22:29:40.188
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:29:40.193
Apr 17 22:29:40.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 22:29:40.193
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:40.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:40.208
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 04/17/23 22:29:40.21
Apr 17 22:29:40.218: INFO: Waiting up to 5m0s for pod "pod-l5p7r" in namespace "pods-9243" to be "running"
Apr 17 22:29:40.220: INFO: Pod "pod-l5p7r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123825ms
Apr 17 22:29:42.224: INFO: Pod "pod-l5p7r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005588197s
Apr 17 22:29:42.224: INFO: Pod "pod-l5p7r" satisfied condition "running"
STEP: patching /status 04/17/23 22:29:42.224
Apr 17 22:29:42.231: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 22:29:42.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9243" for this suite. 04/17/23 22:29:42.236
------------------------------
• [2.049 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:29:40.193
    Apr 17 22:29:40.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 22:29:40.193
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:40.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:40.208
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 04/17/23 22:29:40.21
    Apr 17 22:29:40.218: INFO: Waiting up to 5m0s for pod "pod-l5p7r" in namespace "pods-9243" to be "running"
    Apr 17 22:29:40.220: INFO: Pod "pod-l5p7r": Phase="Pending", Reason="", readiness=false. Elapsed: 2.123825ms
    Apr 17 22:29:42.224: INFO: Pod "pod-l5p7r": Phase="Running", Reason="", readiness=true. Elapsed: 2.005588197s
    Apr 17 22:29:42.224: INFO: Pod "pod-l5p7r" satisfied condition "running"
    STEP: patching /status 04/17/23 22:29:42.224
    Apr 17 22:29:42.231: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:29:42.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9243" for this suite. 04/17/23 22:29:42.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:29:42.242
Apr 17 22:29:42.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 22:29:42.243
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:42.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:42.256
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 04/17/23 22:29:42.258
STEP: submitting the pod to kubernetes 04/17/23 22:29:42.258
STEP: verifying QOS class is set on the pod 04/17/23 22:29:42.265
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Apr 17 22:29:42.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5387" for this suite. 04/17/23 22:29:42.272
------------------------------
• [0.036 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:29:42.242
    Apr 17 22:29:42.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 22:29:42.243
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:42.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:42.256
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 04/17/23 22:29:42.258
    STEP: submitting the pod to kubernetes 04/17/23 22:29:42.258
    STEP: verifying QOS class is set on the pod 04/17/23 22:29:42.265
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:29:42.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5387" for this suite. 04/17/23 22:29:42.272
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:29:42.278
Apr 17 22:29:42.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 22:29:42.279
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:42.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:42.293
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 04/17/23 22:29:42.295
Apr 17 22:29:42.303: INFO: Waiting up to 5m0s for pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75" in namespace "emptydir-183" to be "Succeeded or Failed"
Apr 17 22:29:42.305: INFO: Pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.177788ms
Apr 17 22:29:44.309: INFO: Pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006430264s
Apr 17 22:29:46.309: INFO: Pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006194669s
STEP: Saw pod success 04/17/23 22:29:46.309
Apr 17 22:29:46.309: INFO: Pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75" satisfied condition "Succeeded or Failed"
Apr 17 22:29:46.312: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75 container test-container: <nil>
STEP: delete the pod 04/17/23 22:29:46.325
Apr 17 22:29:46.336: INFO: Waiting for pod pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75 to disappear
Apr 17 22:29:46.339: INFO: Pod pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:29:46.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-183" for this suite. 04/17/23 22:29:46.344
------------------------------
• [4.070 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:29:42.278
    Apr 17 22:29:42.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 22:29:42.279
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:42.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:42.293
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 04/17/23 22:29:42.295
    Apr 17 22:29:42.303: INFO: Waiting up to 5m0s for pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75" in namespace "emptydir-183" to be "Succeeded or Failed"
    Apr 17 22:29:42.305: INFO: Pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.177788ms
    Apr 17 22:29:44.309: INFO: Pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006430264s
    Apr 17 22:29:46.309: INFO: Pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006194669s
    STEP: Saw pod success 04/17/23 22:29:46.309
    Apr 17 22:29:46.309: INFO: Pod "pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75" satisfied condition "Succeeded or Failed"
    Apr 17 22:29:46.312: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75 container test-container: <nil>
    STEP: delete the pod 04/17/23 22:29:46.325
    Apr 17 22:29:46.336: INFO: Waiting for pod pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75 to disappear
    Apr 17 22:29:46.339: INFO: Pod pod-d0dd16be-a2f2-4901-a046-4ff8e0804e75 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:29:46.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-183" for this suite. 04/17/23 22:29:46.344
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:29:46.349
Apr 17 22:29:46.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename statefulset 04/17/23 22:29:46.35
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:46.362
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:46.364
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5509 04/17/23 22:29:46.367
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Apr 17 22:29:46.380: INFO: Found 0 stateful pods, waiting for 1
Apr 17 22:29:56.383: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 04/17/23 22:29:56.388
W0417 22:29:56.394675      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Apr 17 22:29:56.399: INFO: Found 1 stateful pods, waiting for 2
Apr 17 22:30:06.405: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:30:06.405: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 04/17/23 22:30:06.41
STEP: Delete all of the StatefulSets 04/17/23 22:30:06.412
STEP: Verify that StatefulSets have been deleted 04/17/23 22:30:06.418
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 17 22:30:06.420: INFO: Deleting all statefulset in ns statefulset-5509
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:06.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5509" for this suite. 04/17/23 22:30:06.432
------------------------------
• [SLOW TEST] [20.088 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:29:46.349
    Apr 17 22:29:46.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename statefulset 04/17/23 22:29:46.35
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:29:46.362
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:29:46.364
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5509 04/17/23 22:29:46.367
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Apr 17 22:29:46.380: INFO: Found 0 stateful pods, waiting for 1
    Apr 17 22:29:56.383: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 04/17/23 22:29:56.388
    W0417 22:29:56.394675      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Apr 17 22:29:56.399: INFO: Found 1 stateful pods, waiting for 2
    Apr 17 22:30:06.405: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:30:06.405: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 04/17/23 22:30:06.41
    STEP: Delete all of the StatefulSets 04/17/23 22:30:06.412
    STEP: Verify that StatefulSets have been deleted 04/17/23 22:30:06.418
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 17 22:30:06.420: INFO: Deleting all statefulset in ns statefulset-5509
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:06.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5509" for this suite. 04/17/23 22:30:06.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:06.438
Apr 17 22:30:06.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename endpointslice 04/17/23 22:30:06.439
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:06.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:06.457
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Apr 17 22:30:06.470: INFO: Endpoints addresses: [10.0.132.180 10.0.198.248 10.0.86.140] , ports: [6443]
Apr 17 22:30:06.470: INFO: EndpointSlices addresses: [10.0.132.180 10.0.198.248 10.0.86.140] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:06.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-410" for this suite. 04/17/23 22:30:06.475
------------------------------
• [0.041 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:06.438
    Apr 17 22:30:06.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename endpointslice 04/17/23 22:30:06.439
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:06.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:06.457
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Apr 17 22:30:06.470: INFO: Endpoints addresses: [10.0.132.180 10.0.198.248 10.0.86.140] , ports: [6443]
    Apr 17 22:30:06.470: INFO: EndpointSlices addresses: [10.0.132.180 10.0.198.248 10.0.86.140] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:06.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-410" for this suite. 04/17/23 22:30:06.475
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:06.479
Apr 17 22:30:06.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 22:30:06.48
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:06.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:06.496
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 04/17/23 22:30:06.498
STEP: Getting a ResourceQuota 04/17/23 22:30:06.502
STEP: Updating a ResourceQuota 04/17/23 22:30:06.504
STEP: Verifying a ResourceQuota was modified 04/17/23 22:30:06.512
STEP: Deleting a ResourceQuota 04/17/23 22:30:06.514
STEP: Verifying the deleted ResourceQuota 04/17/23 22:30:06.528
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:06.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9315" for this suite. 04/17/23 22:30:06.535
------------------------------
• [0.061 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:06.479
    Apr 17 22:30:06.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 22:30:06.48
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:06.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:06.496
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 04/17/23 22:30:06.498
    STEP: Getting a ResourceQuota 04/17/23 22:30:06.502
    STEP: Updating a ResourceQuota 04/17/23 22:30:06.504
    STEP: Verifying a ResourceQuota was modified 04/17/23 22:30:06.512
    STEP: Deleting a ResourceQuota 04/17/23 22:30:06.514
    STEP: Verifying the deleted ResourceQuota 04/17/23 22:30:06.528
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:06.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9315" for this suite. 04/17/23 22:30:06.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:06.542
Apr 17 22:30:06.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replication-controller 04/17/23 22:30:06.543
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:06.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:06.557
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 04/17/23 22:30:06.559
Apr 17 22:30:06.566: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2309" to be "running and ready"
Apr 17 22:30:06.568: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243002ms
Apr 17 22:30:06.568: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:30:08.571: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.0052179s
Apr 17 22:30:08.571: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Apr 17 22:30:08.571: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 04/17/23 22:30:08.573
STEP: Then the orphan pod is adopted 04/17/23 22:30:08.577
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:09.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-2309" for this suite. 04/17/23 22:30:09.587
------------------------------
• [3.049 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:06.542
    Apr 17 22:30:06.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replication-controller 04/17/23 22:30:06.543
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:06.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:06.557
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 04/17/23 22:30:06.559
    Apr 17 22:30:06.566: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-2309" to be "running and ready"
    Apr 17 22:30:06.568: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 2.243002ms
    Apr 17 22:30:06.568: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:30:08.571: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.0052179s
    Apr 17 22:30:08.571: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Apr 17 22:30:08.571: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 04/17/23 22:30:08.573
    STEP: Then the orphan pod is adopted 04/17/23 22:30:08.577
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:09.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-2309" for this suite. 04/17/23 22:30:09.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:09.592
Apr 17 22:30:09.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:30:09.592
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:09.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:09.607
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-7c1b45d6-39f8-4936-9cba-6f6072dd5bf6 04/17/23 22:30:09.61
STEP: Creating a pod to test consume configMaps 04/17/23 22:30:09.613
Apr 17 22:30:09.619: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a" in namespace "projected-7921" to be "Succeeded or Failed"
Apr 17 22:30:09.622: INFO: Pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.350256ms
Apr 17 22:30:11.626: INFO: Pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00662598s
Apr 17 22:30:13.625: INFO: Pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005693733s
STEP: Saw pod success 04/17/23 22:30:13.625
Apr 17 22:30:13.625: INFO: Pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a" satisfied condition "Succeeded or Failed"
Apr 17 22:30:13.627: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:30:13.639
Apr 17 22:30:13.653: INFO: Waiting for pod pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a to disappear
Apr 17 22:30:13.656: INFO: Pod pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:13.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7921" for this suite. 04/17/23 22:30:13.66
------------------------------
• [4.076 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:09.592
    Apr 17 22:30:09.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:30:09.592
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:09.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:09.607
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-7c1b45d6-39f8-4936-9cba-6f6072dd5bf6 04/17/23 22:30:09.61
    STEP: Creating a pod to test consume configMaps 04/17/23 22:30:09.613
    Apr 17 22:30:09.619: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a" in namespace "projected-7921" to be "Succeeded or Failed"
    Apr 17 22:30:09.622: INFO: Pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.350256ms
    Apr 17 22:30:11.626: INFO: Pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00662598s
    Apr 17 22:30:13.625: INFO: Pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005693733s
    STEP: Saw pod success 04/17/23 22:30:13.625
    Apr 17 22:30:13.625: INFO: Pod "pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a" satisfied condition "Succeeded or Failed"
    Apr 17 22:30:13.627: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:30:13.639
    Apr 17 22:30:13.653: INFO: Waiting for pod pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a to disappear
    Apr 17 22:30:13.656: INFO: Pod pod-projected-configmaps-0de9f928-a748-4b08-b893-450ba0bfc64a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:13.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7921" for this suite. 04/17/23 22:30:13.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:13.669
Apr 17 22:30:13.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 22:30:13.669
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:13.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:13.686
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-34525b3a-3ac3-428a-93d2-7589afe5f0d4 04/17/23 22:30:13.689
STEP: Creating a pod to test consume secrets 04/17/23 22:30:13.692
Apr 17 22:30:13.699: INFO: Waiting up to 5m0s for pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882" in namespace "secrets-6539" to be "Succeeded or Failed"
Apr 17 22:30:13.704: INFO: Pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882": Phase="Pending", Reason="", readiness=false. Elapsed: 5.122359ms
Apr 17 22:30:15.708: INFO: Pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882": Phase="Running", Reason="", readiness=false. Elapsed: 2.008947273s
Apr 17 22:30:17.708: INFO: Pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008846814s
STEP: Saw pod success 04/17/23 22:30:17.708
Apr 17 22:30:17.708: INFO: Pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882" satisfied condition "Succeeded or Failed"
Apr 17 22:30:17.711: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882 container secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:30:17.716
Apr 17 22:30:17.729: INFO: Waiting for pod pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882 to disappear
Apr 17 22:30:17.731: INFO: Pod pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:17.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6539" for this suite. 04/17/23 22:30:17.736
------------------------------
• [4.074 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:13.669
    Apr 17 22:30:13.669: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 22:30:13.669
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:13.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:13.686
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-34525b3a-3ac3-428a-93d2-7589afe5f0d4 04/17/23 22:30:13.689
    STEP: Creating a pod to test consume secrets 04/17/23 22:30:13.692
    Apr 17 22:30:13.699: INFO: Waiting up to 5m0s for pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882" in namespace "secrets-6539" to be "Succeeded or Failed"
    Apr 17 22:30:13.704: INFO: Pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882": Phase="Pending", Reason="", readiness=false. Elapsed: 5.122359ms
    Apr 17 22:30:15.708: INFO: Pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882": Phase="Running", Reason="", readiness=false. Elapsed: 2.008947273s
    Apr 17 22:30:17.708: INFO: Pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008846814s
    STEP: Saw pod success 04/17/23 22:30:17.708
    Apr 17 22:30:17.708: INFO: Pod "pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882" satisfied condition "Succeeded or Failed"
    Apr 17 22:30:17.711: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882 container secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:30:17.716
    Apr 17 22:30:17.729: INFO: Waiting for pod pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882 to disappear
    Apr 17 22:30:17.731: INFO: Pod pod-secrets-0d654494-ce90-4a25-bf71-36a0192df882 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:17.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6539" for this suite. 04/17/23 22:30:17.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:17.745
Apr 17 22:30:17.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:30:17.746
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:17.758
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:17.76
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-996c9798-1e7f-4932-b49c-95cff21748c4 04/17/23 22:30:17.766
STEP: Creating the pod 04/17/23 22:30:17.771
Apr 17 22:30:17.776: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993" in namespace "configmap-4880" to be "running and ready"
Apr 17 22:30:17.779: INFO: Pod "pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32127ms
Apr 17 22:30:17.779: INFO: The phase of Pod pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:30:19.782: INFO: Pod "pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993": Phase="Running", Reason="", readiness=true. Elapsed: 2.005196567s
Apr 17 22:30:19.782: INFO: The phase of Pod pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993 is Running (Ready = true)
Apr 17 22:30:19.782: INFO: Pod "pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-996c9798-1e7f-4932-b49c-95cff21748c4 04/17/23 22:30:19.788
STEP: waiting to observe update in volume 04/17/23 22:30:19.792
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:21.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4880" for this suite. 04/17/23 22:30:21.806
------------------------------
• [4.065 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:17.745
    Apr 17 22:30:17.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:30:17.746
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:17.758
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:17.76
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-996c9798-1e7f-4932-b49c-95cff21748c4 04/17/23 22:30:17.766
    STEP: Creating the pod 04/17/23 22:30:17.771
    Apr 17 22:30:17.776: INFO: Waiting up to 5m0s for pod "pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993" in namespace "configmap-4880" to be "running and ready"
    Apr 17 22:30:17.779: INFO: Pod "pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993": Phase="Pending", Reason="", readiness=false. Elapsed: 2.32127ms
    Apr 17 22:30:17.779: INFO: The phase of Pod pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:30:19.782: INFO: Pod "pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993": Phase="Running", Reason="", readiness=true. Elapsed: 2.005196567s
    Apr 17 22:30:19.782: INFO: The phase of Pod pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993 is Running (Ready = true)
    Apr 17 22:30:19.782: INFO: Pod "pod-configmaps-fe7dc443-925a-4adf-9265-8dd51c32b993" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-996c9798-1e7f-4932-b49c-95cff21748c4 04/17/23 22:30:19.788
    STEP: waiting to observe update in volume 04/17/23 22:30:19.792
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:21.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4880" for this suite. 04/17/23 22:30:21.806
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:21.811
Apr 17 22:30:21.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:30:21.811
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:21.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:21.826
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:30:21.836
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:30:22.064
STEP: Deploying the webhook pod 04/17/23 22:30:22.074
STEP: Wait for the deployment to be ready 04/17/23 22:30:22.084
Apr 17 22:30:22.089: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:30:24.097
STEP: Verifying the service has paired with the endpoint 04/17/23 22:30:24.114
Apr 17 22:30:25.115: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 04/17/23 22:30:25.117
STEP: create a configmap that should be updated by the webhook 04/17/23 22:30:25.13
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:25.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2335" for this suite. 04/17/23 22:30:25.183
STEP: Destroying namespace "webhook-2335-markers" for this suite. 04/17/23 22:30:25.189
------------------------------
• [3.384 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:21.811
    Apr 17 22:30:21.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:30:21.811
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:21.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:21.826
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:30:21.836
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:30:22.064
    STEP: Deploying the webhook pod 04/17/23 22:30:22.074
    STEP: Wait for the deployment to be ready 04/17/23 22:30:22.084
    Apr 17 22:30:22.089: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:30:24.097
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:30:24.114
    Apr 17 22:30:25.115: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 04/17/23 22:30:25.117
    STEP: create a configmap that should be updated by the webhook 04/17/23 22:30:25.13
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:25.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2335" for this suite. 04/17/23 22:30:25.183
    STEP: Destroying namespace "webhook-2335-markers" for this suite. 04/17/23 22:30:25.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:25.195
Apr 17 22:30:25.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:30:25.196
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:25.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:25.215
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 04/17/23 22:30:25.218
Apr 17 22:30:25.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2464 cluster-info'
Apr 17 22:30:25.274: INFO: stderr: ""
Apr 17 22:30:25.274: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:25.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2464" for this suite. 04/17/23 22:30:25.279
------------------------------
• [0.089 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:25.195
    Apr 17 22:30:25.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:30:25.196
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:25.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:25.215
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 04/17/23 22:30:25.218
    Apr 17 22:30:25.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-2464 cluster-info'
    Apr 17 22:30:25.274: INFO: stderr: ""
    Apr 17 22:30:25.274: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:25.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2464" for this suite. 04/17/23 22:30:25.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:25.284
Apr 17 22:30:25.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:30:25.285
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:25.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:25.313
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 04/17/23 22:30:25.315
Apr 17 22:30:25.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4783 api-versions'
Apr 17 22:30:25.382: INFO: stderr: ""
Apr 17 22:30:25.382: INFO: stdout: "aadpodidentity.k8s.io/v1\nacme.cert-manager.io/v1\naddons.cluster.x-k8s.io/v1alpha3\naddons.cluster.x-k8s.io/v1alpha4\naddons.cluster.x-k8s.io/v1beta1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\nbootstrap.cluster.x-k8s.io/v1alpha3\nbootstrap.cluster.x-k8s.io/v1alpha4\nbootstrap.cluster.x-k8s.io/v1beta1\nbootstrap.cluster.x-k8s.io/v1beta2\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncluster.x-k8s.io/v1alpha3\ncluster.x-k8s.io/v1alpha4\ncluster.x-k8s.io/v1beta1\nclusterctl.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha4\ncontrolplane.cluster.x-k8s.io/v1beta1\ncontrolplane.cluster.x-k8s.io/v1beta2\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninfrastructure.cluster.konvoy.d2iq.io/v1alpha1\ninfrastructure.cluster.x-k8s.io/v1alpha3\ninfrastructure.cluster.x-k8s.io/v1alpha4\ninfrastructure.cluster.x-k8s.io/v1beta1\ninfrastructure.cluster.x-k8s.io/v1beta2\nipam.cluster.x-k8s.io/v1alpha1\nnetworking.k8s.io/v1\nnfd.k8s-sigs.io/v1alpha1\nnode.k8s.io/v1\noperator.tigera.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nruntime.cluster.x-k8s.io/v1alpha1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:25.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-4783" for this suite. 04/17/23 22:30:25.387
------------------------------
• [0.108 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:25.284
    Apr 17 22:30:25.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:30:25.285
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:25.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:25.313
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 04/17/23 22:30:25.315
    Apr 17 22:30:25.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-4783 api-versions'
    Apr 17 22:30:25.382: INFO: stderr: ""
    Apr 17 22:30:25.382: INFO: stdout: "aadpodidentity.k8s.io/v1\nacme.cert-manager.io/v1\naddons.cluster.x-k8s.io/v1alpha3\naddons.cluster.x-k8s.io/v1alpha4\naddons.cluster.x-k8s.io/v1beta1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\nbootstrap.cluster.x-k8s.io/v1alpha3\nbootstrap.cluster.x-k8s.io/v1alpha4\nbootstrap.cluster.x-k8s.io/v1beta1\nbootstrap.cluster.x-k8s.io/v1beta2\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncluster.x-k8s.io/v1alpha3\ncluster.x-k8s.io/v1alpha4\ncluster.x-k8s.io/v1beta1\nclusterctl.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha3\ncontrolplane.cluster.x-k8s.io/v1alpha4\ncontrolplane.cluster.x-k8s.io/v1beta1\ncontrolplane.cluster.x-k8s.io/v1beta2\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\ninfrastructure.cluster.konvoy.d2iq.io/v1alpha1\ninfrastructure.cluster.x-k8s.io/v1alpha3\ninfrastructure.cluster.x-k8s.io/v1alpha4\ninfrastructure.cluster.x-k8s.io/v1beta1\ninfrastructure.cluster.x-k8s.io/v1beta2\nipam.cluster.x-k8s.io/v1alpha1\nnetworking.k8s.io/v1\nnfd.k8s-sigs.io/v1alpha1\nnode.k8s.io/v1\noperator.tigera.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nruntime.cluster.x-k8s.io/v1alpha1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:25.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-4783" for this suite. 04/17/23 22:30:25.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:25.393
Apr 17 22:30:25.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 22:30:25.393
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:25.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:25.427
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-d4vk6" 04/17/23 22:30:25.431
Apr 17 22:30:25.440: INFO: Resource quota "e2e-rq-status-d4vk6" reports spec: hard cpu limit of 500m
Apr 17 22:30:25.440: INFO: Resource quota "e2e-rq-status-d4vk6" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-d4vk6" /status 04/17/23 22:30:25.44
STEP: Confirm /status for "e2e-rq-status-d4vk6" resourceQuota via watch 04/17/23 22:30:25.448
Apr 17 22:30:25.449: INFO: observed resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList(nil)
Apr 17 22:30:25.449: INFO: Found resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Apr 17 22:30:25.449: INFO: ResourceQuota "e2e-rq-status-d4vk6" /status was updated
STEP: Patching hard spec values for cpu & memory 04/17/23 22:30:25.452
Apr 17 22:30:25.456: INFO: Resource quota "e2e-rq-status-d4vk6" reports spec: hard cpu limit of 1
Apr 17 22:30:25.456: INFO: Resource quota "e2e-rq-status-d4vk6" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-d4vk6" /status 04/17/23 22:30:25.456
STEP: Confirm /status for "e2e-rq-status-d4vk6" resourceQuota via watch 04/17/23 22:30:25.46
Apr 17 22:30:25.461: INFO: observed resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Apr 17 22:30:25.461: INFO: Found resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Apr 17 22:30:25.461: INFO: ResourceQuota "e2e-rq-status-d4vk6" /status was patched
STEP: Get "e2e-rq-status-d4vk6" /status 04/17/23 22:30:25.461
Apr 17 22:30:25.464: INFO: Resourcequota "e2e-rq-status-d4vk6" reports status: hard cpu of 1
Apr 17 22:30:25.464: INFO: Resourcequota "e2e-rq-status-d4vk6" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-d4vk6" /status before checking Spec is unchanged 04/17/23 22:30:25.466
Apr 17 22:30:25.471: INFO: Resourcequota "e2e-rq-status-d4vk6" reports status: hard cpu of 2
Apr 17 22:30:25.471: INFO: Resourcequota "e2e-rq-status-d4vk6" reports status: hard memory of 2Gi
Apr 17 22:30:25.472: INFO: Found resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Apr 17 22:30:30.478: INFO: ResourceQuota "e2e-rq-status-d4vk6" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:30.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-875" for this suite. 04/17/23 22:30:30.483
------------------------------
• [SLOW TEST] [5.095 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:25.393
    Apr 17 22:30:25.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 22:30:25.393
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:25.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:25.427
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-d4vk6" 04/17/23 22:30:25.431
    Apr 17 22:30:25.440: INFO: Resource quota "e2e-rq-status-d4vk6" reports spec: hard cpu limit of 500m
    Apr 17 22:30:25.440: INFO: Resource quota "e2e-rq-status-d4vk6" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-d4vk6" /status 04/17/23 22:30:25.44
    STEP: Confirm /status for "e2e-rq-status-d4vk6" resourceQuota via watch 04/17/23 22:30:25.448
    Apr 17 22:30:25.449: INFO: observed resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList(nil)
    Apr 17 22:30:25.449: INFO: Found resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Apr 17 22:30:25.449: INFO: ResourceQuota "e2e-rq-status-d4vk6" /status was updated
    STEP: Patching hard spec values for cpu & memory 04/17/23 22:30:25.452
    Apr 17 22:30:25.456: INFO: Resource quota "e2e-rq-status-d4vk6" reports spec: hard cpu limit of 1
    Apr 17 22:30:25.456: INFO: Resource quota "e2e-rq-status-d4vk6" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-d4vk6" /status 04/17/23 22:30:25.456
    STEP: Confirm /status for "e2e-rq-status-d4vk6" resourceQuota via watch 04/17/23 22:30:25.46
    Apr 17 22:30:25.461: INFO: observed resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Apr 17 22:30:25.461: INFO: Found resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Apr 17 22:30:25.461: INFO: ResourceQuota "e2e-rq-status-d4vk6" /status was patched
    STEP: Get "e2e-rq-status-d4vk6" /status 04/17/23 22:30:25.461
    Apr 17 22:30:25.464: INFO: Resourcequota "e2e-rq-status-d4vk6" reports status: hard cpu of 1
    Apr 17 22:30:25.464: INFO: Resourcequota "e2e-rq-status-d4vk6" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-d4vk6" /status before checking Spec is unchanged 04/17/23 22:30:25.466
    Apr 17 22:30:25.471: INFO: Resourcequota "e2e-rq-status-d4vk6" reports status: hard cpu of 2
    Apr 17 22:30:25.471: INFO: Resourcequota "e2e-rq-status-d4vk6" reports status: hard memory of 2Gi
    Apr 17 22:30:25.472: INFO: Found resourceQuota "e2e-rq-status-d4vk6" in namespace "resourcequota-875" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Apr 17 22:30:30.478: INFO: ResourceQuota "e2e-rq-status-d4vk6" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:30.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-875" for this suite. 04/17/23 22:30:30.483
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:30.488
Apr 17 22:30:30.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:30:30.489
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:30.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:30.504
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-624a709b-c06c-49c4-be1e-3daa5f5fa1b3 04/17/23 22:30:30.51
STEP: Creating secret with name s-test-opt-upd-cfef3cc0-7fcb-4f51-b4b6-502d9f1afc82 04/17/23 22:30:30.514
STEP: Creating the pod 04/17/23 22:30:30.519
Apr 17 22:30:30.528: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e" in namespace "projected-1353" to be "running and ready"
Apr 17 22:30:30.533: INFO: Pod "pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.78465ms
Apr 17 22:30:30.533: INFO: The phase of Pod pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:30:32.536: INFO: Pod "pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008730435s
Apr 17 22:30:32.536: INFO: The phase of Pod pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e is Running (Ready = true)
Apr 17 22:30:32.536: INFO: Pod "pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-624a709b-c06c-49c4-be1e-3daa5f5fa1b3 04/17/23 22:30:32.551
STEP: Updating secret s-test-opt-upd-cfef3cc0-7fcb-4f51-b4b6-502d9f1afc82 04/17/23 22:30:32.555
STEP: Creating secret with name s-test-opt-create-05a82727-8b7d-431b-80c9-ca6bcf01a3e9 04/17/23 22:30:32.559
STEP: waiting to observe update in volume 04/17/23 22:30:32.562
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:36.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1353" for this suite. 04/17/23 22:30:36.591
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:30.488
    Apr 17 22:30:30.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:30:30.489
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:30.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:30.504
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-624a709b-c06c-49c4-be1e-3daa5f5fa1b3 04/17/23 22:30:30.51
    STEP: Creating secret with name s-test-opt-upd-cfef3cc0-7fcb-4f51-b4b6-502d9f1afc82 04/17/23 22:30:30.514
    STEP: Creating the pod 04/17/23 22:30:30.519
    Apr 17 22:30:30.528: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e" in namespace "projected-1353" to be "running and ready"
    Apr 17 22:30:30.533: INFO: Pod "pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.78465ms
    Apr 17 22:30:30.533: INFO: The phase of Pod pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:30:32.536: INFO: Pod "pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e": Phase="Running", Reason="", readiness=true. Elapsed: 2.008730435s
    Apr 17 22:30:32.536: INFO: The phase of Pod pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e is Running (Ready = true)
    Apr 17 22:30:32.536: INFO: Pod "pod-projected-secrets-2048ba3b-3e43-41e7-a7b9-677f1701d20e" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-624a709b-c06c-49c4-be1e-3daa5f5fa1b3 04/17/23 22:30:32.551
    STEP: Updating secret s-test-opt-upd-cfef3cc0-7fcb-4f51-b4b6-502d9f1afc82 04/17/23 22:30:32.555
    STEP: Creating secret with name s-test-opt-create-05a82727-8b7d-431b-80c9-ca6bcf01a3e9 04/17/23 22:30:32.559
    STEP: waiting to observe update in volume 04/17/23 22:30:32.562
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:36.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1353" for this suite. 04/17/23 22:30:36.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:36.595
Apr 17 22:30:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename var-expansion 04/17/23 22:30:36.596
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:36.607
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:36.609
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 04/17/23 22:30:36.612
Apr 17 22:30:36.617: INFO: Waiting up to 5m0s for pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380" in namespace "var-expansion-1316" to be "Succeeded or Failed"
Apr 17 22:30:36.619: INFO: Pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380": Phase="Pending", Reason="", readiness=false. Elapsed: 2.237393ms
Apr 17 22:30:38.623: INFO: Pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005909637s
Apr 17 22:30:40.623: INFO: Pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006000675s
STEP: Saw pod success 04/17/23 22:30:40.623
Apr 17 22:30:40.623: INFO: Pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380" satisfied condition "Succeeded or Failed"
Apr 17 22:30:40.626: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380 container dapi-container: <nil>
STEP: delete the pod 04/17/23 22:30:40.63
Apr 17 22:30:40.640: INFO: Waiting for pod var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380 to disappear
Apr 17 22:30:40.642: INFO: Pod var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:40.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1316" for this suite. 04/17/23 22:30:40.646
------------------------------
• [4.055 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:36.595
    Apr 17 22:30:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename var-expansion 04/17/23 22:30:36.596
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:36.607
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:36.609
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 04/17/23 22:30:36.612
    Apr 17 22:30:36.617: INFO: Waiting up to 5m0s for pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380" in namespace "var-expansion-1316" to be "Succeeded or Failed"
    Apr 17 22:30:36.619: INFO: Pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380": Phase="Pending", Reason="", readiness=false. Elapsed: 2.237393ms
    Apr 17 22:30:38.623: INFO: Pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005909637s
    Apr 17 22:30:40.623: INFO: Pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006000675s
    STEP: Saw pod success 04/17/23 22:30:40.623
    Apr 17 22:30:40.623: INFO: Pod "var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380" satisfied condition "Succeeded or Failed"
    Apr 17 22:30:40.626: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380 container dapi-container: <nil>
    STEP: delete the pod 04/17/23 22:30:40.63
    Apr 17 22:30:40.640: INFO: Waiting for pod var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380 to disappear
    Apr 17 22:30:40.642: INFO: Pod var-expansion-e3a43712-1666-4e33-a4ef-e74988c94380 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:40.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1316" for this suite. 04/17/23 22:30:40.646
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:40.651
Apr 17 22:30:40.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 22:30:40.652
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:40.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:40.668
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Apr 17 22:30:40.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:43.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-8182" for this suite. 04/17/23 22:30:43.793
------------------------------
• [3.148 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:40.651
    Apr 17 22:30:40.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename custom-resource-definition 04/17/23 22:30:40.652
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:40.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:40.668
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Apr 17 22:30:40.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:43.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-8182" for this suite. 04/17/23 22:30:43.793
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:43.8
Apr 17 22:30:43.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename gc 04/17/23 22:30:43.801
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:43.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:43.815
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 04/17/23 22:30:43.817
STEP: delete the rc 04/17/23 22:30:48.826
STEP: wait for all pods to be garbage collected 04/17/23 22:30:48.831
STEP: Gathering metrics 04/17/23 22:30:53.837
Apr 17 22:30:53.855: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
Apr 17 22:30:53.858: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.397108ms
Apr 17 22:30:53.858: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
Apr 17 22:30:53.858: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
Apr 17 22:30:53.907: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 17 22:30:53.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9956" for this suite. 04/17/23 22:30:53.911
------------------------------
• [SLOW TEST] [10.116 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:43.8
    Apr 17 22:30:43.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename gc 04/17/23 22:30:43.801
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:43.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:43.815
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 04/17/23 22:30:43.817
    STEP: delete the rc 04/17/23 22:30:48.826
    STEP: wait for all pods to be garbage collected 04/17/23 22:30:48.831
    STEP: Gathering metrics 04/17/23 22:30:53.837
    Apr 17 22:30:53.855: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Apr 17 22:30:53.858: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.397108ms
    Apr 17 22:30:53.858: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
    Apr 17 22:30:53.858: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
    Apr 17 22:30:53.907: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:30:53.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9956" for this suite. 04/17/23 22:30:53.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:30:53.917
Apr 17 22:30:53.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename taint-single-pod 04/17/23 22:30:53.918
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:53.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:53.932
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Apr 17 22:30:53.934: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 17 22:31:53.981: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Apr 17 22:31:53.984: INFO: Starting informer...
STEP: Starting pod... 04/17/23 22:31:53.984
Apr 17 22:31:54.195: INFO: Pod is running on ip-10-0-64-189.us-west-2.compute.internal. Tainting Node
STEP: Trying to apply a taint on the Node 04/17/23 22:31:54.195
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/17/23 22:31:54.21
STEP: Waiting short time to make sure Pod is queued for deletion 04/17/23 22:31:54.213
Apr 17 22:31:54.214: INFO: Pod wasn't evicted. Proceeding
Apr 17 22:31:54.214: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/17/23 22:31:54.233
STEP: Waiting some time to make sure that toleration time passed. 04/17/23 22:31:54.321
Apr 17 22:33:09.322: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:33:09.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-3596" for this suite. 04/17/23 22:33:09.327
------------------------------
• [SLOW TEST] [135.415 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:30:53.917
    Apr 17 22:30:53.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename taint-single-pod 04/17/23 22:30:53.918
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:30:53.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:30:53.932
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Apr 17 22:30:53.934: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 17 22:31:53.981: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Apr 17 22:31:53.984: INFO: Starting informer...
    STEP: Starting pod... 04/17/23 22:31:53.984
    Apr 17 22:31:54.195: INFO: Pod is running on ip-10-0-64-189.us-west-2.compute.internal. Tainting Node
    STEP: Trying to apply a taint on the Node 04/17/23 22:31:54.195
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/17/23 22:31:54.21
    STEP: Waiting short time to make sure Pod is queued for deletion 04/17/23 22:31:54.213
    Apr 17 22:31:54.214: INFO: Pod wasn't evicted. Proceeding
    Apr 17 22:31:54.214: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 04/17/23 22:31:54.233
    STEP: Waiting some time to make sure that toleration time passed. 04/17/23 22:31:54.321
    Apr 17 22:33:09.322: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:33:09.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-3596" for this suite. 04/17/23 22:33:09.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:33:09.333
Apr 17 22:33:09.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename replication-controller 04/17/23 22:33:09.334
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:09.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:09.35
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-zqlrj" 04/17/23 22:33:09.352
Apr 17 22:33:09.356: INFO: Get Replication Controller "e2e-rc-zqlrj" to confirm replicas
Apr 17 22:33:10.358: INFO: Get Replication Controller "e2e-rc-zqlrj" to confirm replicas
Apr 17 22:33:10.361: INFO: Found 1 replicas for "e2e-rc-zqlrj" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-zqlrj" 04/17/23 22:33:10.361
STEP: Updating a scale subresource 04/17/23 22:33:10.363
STEP: Verifying replicas where modified for replication controller "e2e-rc-zqlrj" 04/17/23 22:33:10.368
Apr 17 22:33:10.368: INFO: Get Replication Controller "e2e-rc-zqlrj" to confirm replicas
Apr 17 22:33:11.370: INFO: Get Replication Controller "e2e-rc-zqlrj" to confirm replicas
Apr 17 22:33:11.373: INFO: Found 2 replicas for "e2e-rc-zqlrj" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Apr 17 22:33:11.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9661" for this suite. 04/17/23 22:33:11.377
------------------------------
• [2.051 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:33:09.333
    Apr 17 22:33:09.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename replication-controller 04/17/23 22:33:09.334
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:09.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:09.35
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-zqlrj" 04/17/23 22:33:09.352
    Apr 17 22:33:09.356: INFO: Get Replication Controller "e2e-rc-zqlrj" to confirm replicas
    Apr 17 22:33:10.358: INFO: Get Replication Controller "e2e-rc-zqlrj" to confirm replicas
    Apr 17 22:33:10.361: INFO: Found 1 replicas for "e2e-rc-zqlrj" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-zqlrj" 04/17/23 22:33:10.361
    STEP: Updating a scale subresource 04/17/23 22:33:10.363
    STEP: Verifying replicas where modified for replication controller "e2e-rc-zqlrj" 04/17/23 22:33:10.368
    Apr 17 22:33:10.368: INFO: Get Replication Controller "e2e-rc-zqlrj" to confirm replicas
    Apr 17 22:33:11.370: INFO: Get Replication Controller "e2e-rc-zqlrj" to confirm replicas
    Apr 17 22:33:11.373: INFO: Found 2 replicas for "e2e-rc-zqlrj" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:33:11.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9661" for this suite. 04/17/23 22:33:11.377
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:33:11.385
Apr 17 22:33:11.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename aggregator 04/17/23 22:33:11.386
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:11.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:11.399
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Apr 17 22:33:11.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 04/17/23 22:33:11.402
Apr 17 22:33:11.960: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr 17 22:33:13.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:15.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:17.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:19.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:21.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:23.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:25.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:27.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:29.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:31.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:33.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:33:36.120: INFO: Waited 115.595394ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 04/17/23 22:33:36.718
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 04/17/23 22:33:36.76
STEP: List APIServices 04/17/23 22:33:36.813
Apr 17 22:33:36.864: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Apr 17 22:33:37.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-3175" for this suite. 04/17/23 22:33:37.713
------------------------------
• [SLOW TEST] [26.380 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:33:11.385
    Apr 17 22:33:11.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename aggregator 04/17/23 22:33:11.386
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:11.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:11.399
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Apr 17 22:33:11.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 04/17/23 22:33:11.402
    Apr 17 22:33:11.960: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Apr 17 22:33:13.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:15.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:17.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:19.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:21.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:23.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:25.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:27.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:29.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:31.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:33.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 33, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:33:36.120: INFO: Waited 115.595394ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 04/17/23 22:33:36.718
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 04/17/23 22:33:36.76
    STEP: List APIServices 04/17/23 22:33:36.813
    Apr 17 22:33:36.864: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:33:37.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-3175" for this suite. 04/17/23 22:33:37.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:33:37.766
Apr 17 22:33:37.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename emptydir 04/17/23 22:33:37.766
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:37.78
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:37.782
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 04/17/23 22:33:37.784
Apr 17 22:33:37.792: INFO: Waiting up to 5m0s for pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d" in namespace "emptydir-2512" to be "Succeeded or Failed"
Apr 17 22:33:37.794: INFO: Pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128495ms
Apr 17 22:33:39.798: INFO: Pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006225732s
Apr 17 22:33:41.799: INFO: Pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007251745s
STEP: Saw pod success 04/17/23 22:33:41.799
Apr 17 22:33:41.799: INFO: Pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d" satisfied condition "Succeeded or Failed"
Apr 17 22:33:41.801: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d container test-container: <nil>
STEP: delete the pod 04/17/23 22:33:41.813
Apr 17 22:33:41.858: INFO: Waiting for pod pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d to disappear
Apr 17 22:33:41.864: INFO: Pod pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Apr 17 22:33:41.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2512" for this suite. 04/17/23 22:33:41.879
------------------------------
• [4.147 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:33:37.766
    Apr 17 22:33:37.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename emptydir 04/17/23 22:33:37.766
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:37.78
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:37.782
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 04/17/23 22:33:37.784
    Apr 17 22:33:37.792: INFO: Waiting up to 5m0s for pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d" in namespace "emptydir-2512" to be "Succeeded or Failed"
    Apr 17 22:33:37.794: INFO: Pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.128495ms
    Apr 17 22:33:39.798: INFO: Pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006225732s
    Apr 17 22:33:41.799: INFO: Pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007251745s
    STEP: Saw pod success 04/17/23 22:33:41.799
    Apr 17 22:33:41.799: INFO: Pod "pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d" satisfied condition "Succeeded or Failed"
    Apr 17 22:33:41.801: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d container test-container: <nil>
    STEP: delete the pod 04/17/23 22:33:41.813
    Apr 17 22:33:41.858: INFO: Waiting for pod pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d to disappear
    Apr 17 22:33:41.864: INFO: Pod pod-9415cf3e-57e7-4f0d-964e-bbd23230e28d no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:33:41.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2512" for this suite. 04/17/23 22:33:41.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:33:41.914
Apr 17 22:33:41.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename deployment 04/17/23 22:33:41.914
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:41.944
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:41.949
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Apr 17 22:33:41.951: INFO: Creating deployment "test-recreate-deployment"
Apr 17 22:33:41.955: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr 17 22:33:41.960: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr 17 22:33:43.965: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr 17 22:33:43.967: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr 17 22:33:43.973: INFO: Updating deployment test-recreate-deployment
Apr 17 22:33:43.973: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 17 22:33:44.041: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-4929  661f06dd-d489-479f-b789-5b0b4de3fe34 72854 2 2023-04-17 22:33:41 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-17 22:33:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a9aa808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-17 22:33:44 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-04-17 22:33:44 +0000 UTC,LastTransitionTime:2023-04-17 22:33:41 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Apr 17 22:33:44.044: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4929  8f47c349-1134-4ad2-838c-d3d731a4ef3a 72851 1 2023-04-17 22:33:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 661f06dd-d489-479f-b789-5b0b4de3fe34 0xc005e3b3e0 0xc005e3b3e1}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"661f06dd-d489-479f-b789-5b0b4de3fe34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e3b478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:33:44.044: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr 17 22:33:44.044: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4929  fa00bbf6-d929-4139-b481-b53e7854dac5 72842 2 2023-04-17 22:33:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 661f06dd-d489-479f-b789-5b0b4de3fe34 0xc005e3b2b7 0xc005e3b2b8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:33:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"661f06dd-d489-479f-b789-5b0b4de3fe34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e3b378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:33:44.046: INFO: Pod "test-recreate-deployment-cff6dc657-c2qzd" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-c2qzd test-recreate-deployment-cff6dc657- deployment-4929  2c31272d-a764-4f5c-87df-c0dd88489eef 72853 0 2023-04-17 22:33:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 8f47c349-1134-4ad2-838c-d3d731a4ef3a 0xc002de72c0 0xc002de72c1}] [] [{kube-controller-manager Update v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f47c349-1134-4ad2-838c-d3d731a4ef3a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ppdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ppdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:33:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 17 22:33:44.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4929" for this suite. 04/17/23 22:33:44.05
------------------------------
• [2.141 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:33:41.914
    Apr 17 22:33:41.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename deployment 04/17/23 22:33:41.914
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:41.944
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:41.949
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Apr 17 22:33:41.951: INFO: Creating deployment "test-recreate-deployment"
    Apr 17 22:33:41.955: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Apr 17 22:33:41.960: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Apr 17 22:33:43.965: INFO: Waiting deployment "test-recreate-deployment" to complete
    Apr 17 22:33:43.967: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Apr 17 22:33:43.973: INFO: Updating deployment test-recreate-deployment
    Apr 17 22:33:43.973: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 17 22:33:44.041: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-4929  661f06dd-d489-479f-b789-5b0b4de3fe34 72854 2 2023-04-17 22:33:41 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-17 22:33:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a9aa808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-17 22:33:44 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-04-17 22:33:44 +0000 UTC,LastTransitionTime:2023-04-17 22:33:41 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Apr 17 22:33:44.044: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-4929  8f47c349-1134-4ad2-838c-d3d731a4ef3a 72851 1 2023-04-17 22:33:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 661f06dd-d489-479f-b789-5b0b4de3fe34 0xc005e3b3e0 0xc005e3b3e1}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"661f06dd-d489-479f-b789-5b0b4de3fe34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e3b478 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 22:33:44.044: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Apr 17 22:33:44.044: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-4929  fa00bbf6-d929-4139-b481-b53e7854dac5 72842 2 2023-04-17 22:33:41 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 661f06dd-d489-479f-b789-5b0b4de3fe34 0xc005e3b2b7 0xc005e3b2b8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:33:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"661f06dd-d489-479f-b789-5b0b4de3fe34\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005e3b378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 22:33:44.046: INFO: Pod "test-recreate-deployment-cff6dc657-c2qzd" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-c2qzd test-recreate-deployment-cff6dc657- deployment-4929  2c31272d-a764-4f5c-87df-c0dd88489eef 72853 0 2023-04-17 22:33:44 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 8f47c349-1134-4ad2-838c-d3d731a4ef3a 0xc002de72c0 0xc002de72c1}] [] [{kube-controller-manager Update v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8f47c349-1134-4ad2-838c-d3d731a4ef3a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:33:44 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7ppdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7ppdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:33:44 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:33:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:33:44.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4929" for this suite. 04/17/23 22:33:44.05
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:33:44.055
Apr 17 22:33:44.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename ingressclass 04/17/23 22:33:44.055
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:44.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:44.069
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 04/17/23 22:33:44.071
STEP: getting /apis/networking.k8s.io 04/17/23 22:33:44.073
STEP: getting /apis/networking.k8s.iov1 04/17/23 22:33:44.074
STEP: creating 04/17/23 22:33:44.075
STEP: getting 04/17/23 22:33:44.086
STEP: listing 04/17/23 22:33:44.088
STEP: watching 04/17/23 22:33:44.09
Apr 17 22:33:44.090: INFO: starting watch
STEP: patching 04/17/23 22:33:44.091
STEP: updating 04/17/23 22:33:44.095
Apr 17 22:33:44.098: INFO: waiting for watch events with expected annotations
Apr 17 22:33:44.098: INFO: saw patched and updated annotations
STEP: deleting 04/17/23 22:33:44.098
STEP: deleting a collection 04/17/23 22:33:44.106
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Apr 17 22:33:44.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-1397" for this suite. 04/17/23 22:33:44.12
------------------------------
• [0.070 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:33:44.055
    Apr 17 22:33:44.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename ingressclass 04/17/23 22:33:44.055
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:44.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:44.069
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 04/17/23 22:33:44.071
    STEP: getting /apis/networking.k8s.io 04/17/23 22:33:44.073
    STEP: getting /apis/networking.k8s.iov1 04/17/23 22:33:44.074
    STEP: creating 04/17/23 22:33:44.075
    STEP: getting 04/17/23 22:33:44.086
    STEP: listing 04/17/23 22:33:44.088
    STEP: watching 04/17/23 22:33:44.09
    Apr 17 22:33:44.090: INFO: starting watch
    STEP: patching 04/17/23 22:33:44.091
    STEP: updating 04/17/23 22:33:44.095
    Apr 17 22:33:44.098: INFO: waiting for watch events with expected annotations
    Apr 17 22:33:44.098: INFO: saw patched and updated annotations
    STEP: deleting 04/17/23 22:33:44.098
    STEP: deleting a collection 04/17/23 22:33:44.106
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:33:44.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-1397" for this suite. 04/17/23 22:33:44.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:33:44.126
Apr 17 22:33:44.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:33:44.127
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:44.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:44.141
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:33:44.143
Apr 17 22:33:44.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89" in namespace "downward-api-505" to be "Succeeded or Failed"
Apr 17 22:33:44.231: INFO: Pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698864ms
Apr 17 22:33:46.235: INFO: Pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006732171s
Apr 17 22:33:48.234: INFO: Pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005882556s
STEP: Saw pod success 04/17/23 22:33:48.234
Apr 17 22:33:48.234: INFO: Pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89" satisfied condition "Succeeded or Failed"
Apr 17 22:33:48.237: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89 container client-container: <nil>
STEP: delete the pod 04/17/23 22:33:48.241
Apr 17 22:33:48.252: INFO: Waiting for pod downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89 to disappear
Apr 17 22:33:48.254: INFO: Pod downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 22:33:48.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-505" for this suite. 04/17/23 22:33:48.258
------------------------------
• [4.136 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:33:44.126
    Apr 17 22:33:44.127: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:33:44.127
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:44.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:44.141
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:33:44.143
    Apr 17 22:33:44.228: INFO: Waiting up to 5m0s for pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89" in namespace "downward-api-505" to be "Succeeded or Failed"
    Apr 17 22:33:44.231: INFO: Pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.698864ms
    Apr 17 22:33:46.235: INFO: Pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006732171s
    Apr 17 22:33:48.234: INFO: Pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005882556s
    STEP: Saw pod success 04/17/23 22:33:48.234
    Apr 17 22:33:48.234: INFO: Pod "downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89" satisfied condition "Succeeded or Failed"
    Apr 17 22:33:48.237: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89 container client-container: <nil>
    STEP: delete the pod 04/17/23 22:33:48.241
    Apr 17 22:33:48.252: INFO: Waiting for pod downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89 to disappear
    Apr 17 22:33:48.254: INFO: Pod downwardapi-volume-68eaa926-e022-424c-89c2-2c60905e5d89 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:33:48.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-505" for this suite. 04/17/23 22:33:48.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:33:48.263
Apr 17 22:33:48.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:33:48.264
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:48.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:48.279
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-a0edcfe5-c506-438f-8feb-c896711c2949 04/17/23 22:33:48.285
STEP: Creating configMap with name cm-test-opt-upd-781f01fa-e0e8-4e92-a87f-e7f9b8ff4d9a 04/17/23 22:33:48.289
STEP: Creating the pod 04/17/23 22:33:48.293
Apr 17 22:33:48.298: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a" in namespace "projected-2553" to be "running and ready"
Apr 17 22:33:48.301: INFO: Pod "pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389242ms
Apr 17 22:33:48.301: INFO: The phase of Pod pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:33:50.305: INFO: Pod "pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006152706s
Apr 17 22:33:50.305: INFO: The phase of Pod pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a is Running (Ready = true)
Apr 17 22:33:50.305: INFO: Pod "pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-a0edcfe5-c506-438f-8feb-c896711c2949 04/17/23 22:33:50.32
STEP: Updating configmap cm-test-opt-upd-781f01fa-e0e8-4e92-a87f-e7f9b8ff4d9a 04/17/23 22:33:50.324
STEP: Creating configMap with name cm-test-opt-create-31302ab3-cb9b-412d-8b47-007b3701a5ec 04/17/23 22:33:50.328
STEP: waiting to observe update in volume 04/17/23 22:33:50.331
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:33:52.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2553" for this suite. 04/17/23 22:33:52.354
------------------------------
• [4.095 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:33:48.263
    Apr 17 22:33:48.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:33:48.264
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:48.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:48.279
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-a0edcfe5-c506-438f-8feb-c896711c2949 04/17/23 22:33:48.285
    STEP: Creating configMap with name cm-test-opt-upd-781f01fa-e0e8-4e92-a87f-e7f9b8ff4d9a 04/17/23 22:33:48.289
    STEP: Creating the pod 04/17/23 22:33:48.293
    Apr 17 22:33:48.298: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a" in namespace "projected-2553" to be "running and ready"
    Apr 17 22:33:48.301: INFO: Pod "pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.389242ms
    Apr 17 22:33:48.301: INFO: The phase of Pod pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:33:50.305: INFO: Pod "pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a": Phase="Running", Reason="", readiness=true. Elapsed: 2.006152706s
    Apr 17 22:33:50.305: INFO: The phase of Pod pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a is Running (Ready = true)
    Apr 17 22:33:50.305: INFO: Pod "pod-projected-configmaps-bff1569e-119d-45b6-9560-48086683bb2a" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-a0edcfe5-c506-438f-8feb-c896711c2949 04/17/23 22:33:50.32
    STEP: Updating configmap cm-test-opt-upd-781f01fa-e0e8-4e92-a87f-e7f9b8ff4d9a 04/17/23 22:33:50.324
    STEP: Creating configMap with name cm-test-opt-create-31302ab3-cb9b-412d-8b47-007b3701a5ec 04/17/23 22:33:50.328
    STEP: waiting to observe update in volume 04/17/23 22:33:50.331
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:33:52.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2553" for this suite. 04/17/23 22:33:52.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:33:52.359
Apr 17 22:33:52.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename daemonsets 04/17/23 22:33:52.36
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:52.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:52.375
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Apr 17 22:33:52.398: INFO: Create a RollingUpdate DaemonSet
Apr 17 22:33:52.403: INFO: Check that daemon pods launch on every node of the cluster
Apr 17 22:33:52.407: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:52.407: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:52.407: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:52.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:33:52.409: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:33:53.414: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:53.414: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:53.414: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:53.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Apr 17 22:33:53.417: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:33:54.414: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:54.414: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:54.414: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:54.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr 17 22:33:54.417: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
Apr 17 22:33:54.417: INFO: Update the DaemonSet to trigger a rollout
Apr 17 22:33:54.423: INFO: Updating DaemonSet daemon-set
Apr 17 22:33:57.436: INFO: Roll back the DaemonSet before rollout is complete
Apr 17 22:33:57.444: INFO: Updating DaemonSet daemon-set
Apr 17 22:33:57.444: INFO: Make sure DaemonSet rollback is complete
Apr 17 22:33:57.446: INFO: Wrong image for pod: daemon-set-2lwbx. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Apr 17 22:33:57.446: INFO: Pod daemon-set-2lwbx is not available
Apr 17 22:33:57.450: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:57.450: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:57.450: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:58.458: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:58.458: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:58.458: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:59.457: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:59.457: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:33:59.457: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:00.454: INFO: Pod daemon-set-jkp4x is not available
Apr 17 22:34:00.459: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:00.459: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:00.459: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/17/23 22:34:00.464
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2452, will wait for the garbage collector to delete the pods 04/17/23 22:34:00.464
Apr 17 22:34:00.590: INFO: Deleting DaemonSet.extensions daemon-set took: 73.669653ms
Apr 17 22:34:00.691: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.001867ms
Apr 17 22:34:01.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:34:01.994: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 17 22:34:01.996: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73287"},"items":null}

Apr 17 22:34:01.998: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73287"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:34:02.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2452" for this suite. 04/17/23 22:34:02.017
------------------------------
• [SLOW TEST] [9.663 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:33:52.359
    Apr 17 22:33:52.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename daemonsets 04/17/23 22:33:52.36
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:33:52.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:33:52.375
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Apr 17 22:33:52.398: INFO: Create a RollingUpdate DaemonSet
    Apr 17 22:33:52.403: INFO: Check that daemon pods launch on every node of the cluster
    Apr 17 22:33:52.407: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:52.407: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:52.407: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:52.409: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:33:52.409: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:33:53.414: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:53.414: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:53.414: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:53.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Apr 17 22:33:53.417: INFO: Node ip-10-0-64-189.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:33:54.414: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:54.414: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:54.414: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:54.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr 17 22:33:54.417: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    Apr 17 22:33:54.417: INFO: Update the DaemonSet to trigger a rollout
    Apr 17 22:33:54.423: INFO: Updating DaemonSet daemon-set
    Apr 17 22:33:57.436: INFO: Roll back the DaemonSet before rollout is complete
    Apr 17 22:33:57.444: INFO: Updating DaemonSet daemon-set
    Apr 17 22:33:57.444: INFO: Make sure DaemonSet rollback is complete
    Apr 17 22:33:57.446: INFO: Wrong image for pod: daemon-set-2lwbx. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Apr 17 22:33:57.446: INFO: Pod daemon-set-2lwbx is not available
    Apr 17 22:33:57.450: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:57.450: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:57.450: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:58.458: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:58.458: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:58.458: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:59.457: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:59.457: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:33:59.457: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:00.454: INFO: Pod daemon-set-jkp4x is not available
    Apr 17 22:34:00.459: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:00.459: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:00.459: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/17/23 22:34:00.464
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2452, will wait for the garbage collector to delete the pods 04/17/23 22:34:00.464
    Apr 17 22:34:00.590: INFO: Deleting DaemonSet.extensions daemon-set took: 73.669653ms
    Apr 17 22:34:00.691: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.001867ms
    Apr 17 22:34:01.994: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:34:01.994: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 17 22:34:01.996: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73287"},"items":null}

    Apr 17 22:34:01.998: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73287"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:34:02.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2452" for this suite. 04/17/23 22:34:02.017
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:34:02.022
Apr 17 22:34:02.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename daemonsets 04/17/23 22:34:02.023
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:02.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:02.04
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 04/17/23 22:34:02.059
STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 22:34:02.063
Apr 17 22:34:02.068: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:02.068: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:02.068: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:02.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:34:02.070: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:34:03.076: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:03.076: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:03.076: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:03.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Apr 17 22:34:03.080: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:34:04.075: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:04.075: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:04.075: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:04.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr 17 22:34:04.078: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 04/17/23 22:34:04.08
Apr 17 22:34:04.094: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:04.094: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:04.094: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:34:04.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Apr 17 22:34:04.099: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 04/17/23 22:34:04.099
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 04/17/23 22:34:04.103
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9612, will wait for the garbage collector to delete the pods 04/17/23 22:34:04.104
Apr 17 22:34:04.162: INFO: Deleting DaemonSet.extensions daemon-set took: 4.75984ms
Apr 17 22:34:04.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.520525ms
Apr 17 22:34:06.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Apr 17 22:34:06.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Apr 17 22:34:06.868: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73452"},"items":null}

Apr 17 22:34:06.870: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73452"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:34:06.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9612" for this suite. 04/17/23 22:34:06.888
------------------------------
• [4.870 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:34:02.022
    Apr 17 22:34:02.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename daemonsets 04/17/23 22:34:02.023
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:02.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:02.04
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 04/17/23 22:34:02.059
    STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 22:34:02.063
    Apr 17 22:34:02.068: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:02.068: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:02.068: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:02.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:34:02.070: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:34:03.076: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:03.076: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:03.076: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:03.080: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Apr 17 22:34:03.080: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:34:04.075: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:04.075: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:04.075: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:04.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr 17 22:34:04.078: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 04/17/23 22:34:04.08
    Apr 17 22:34:04.094: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:04.094: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:04.094: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:34:04.099: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Apr 17 22:34:04.099: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 04/17/23 22:34:04.099
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 04/17/23 22:34:04.103
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9612, will wait for the garbage collector to delete the pods 04/17/23 22:34:04.104
    Apr 17 22:34:04.162: INFO: Deleting DaemonSet.extensions daemon-set took: 4.75984ms
    Apr 17 22:34:04.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.520525ms
    Apr 17 22:34:06.866: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Apr 17 22:34:06.866: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Apr 17 22:34:06.868: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73452"},"items":null}

    Apr 17 22:34:06.870: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73452"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:34:06.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9612" for this suite. 04/17/23 22:34:06.888
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:34:06.893
Apr 17 22:34:06.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 22:34:06.894
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:06.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:06.908
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 04/17/23 22:34:06.912
STEP: waiting for available Endpoint 04/17/23 22:34:06.915
STEP: listing all Endpoints 04/17/23 22:34:06.916
STEP: updating the Endpoint 04/17/23 22:34:06.919
STEP: fetching the Endpoint 04/17/23 22:34:06.924
STEP: patching the Endpoint 04/17/23 22:34:06.926
STEP: fetching the Endpoint 04/17/23 22:34:06.933
STEP: deleting the Endpoint by Collection 04/17/23 22:34:06.935
STEP: waiting for Endpoint deletion 04/17/23 22:34:06.94
STEP: fetching the Endpoint 04/17/23 22:34:06.941
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 22:34:06.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-109" for this suite. 04/17/23 22:34:06.947
------------------------------
• [0.059 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:34:06.893
    Apr 17 22:34:06.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 22:34:06.894
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:06.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:06.908
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 04/17/23 22:34:06.912
    STEP: waiting for available Endpoint 04/17/23 22:34:06.915
    STEP: listing all Endpoints 04/17/23 22:34:06.916
    STEP: updating the Endpoint 04/17/23 22:34:06.919
    STEP: fetching the Endpoint 04/17/23 22:34:06.924
    STEP: patching the Endpoint 04/17/23 22:34:06.926
    STEP: fetching the Endpoint 04/17/23 22:34:06.933
    STEP: deleting the Endpoint by Collection 04/17/23 22:34:06.935
    STEP: waiting for Endpoint deletion 04/17/23 22:34:06.94
    STEP: fetching the Endpoint 04/17/23 22:34:06.941
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:34:06.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-109" for this suite. 04/17/23 22:34:06.947
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:34:06.952
Apr 17 22:34:06.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:34:06.953
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:06.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:06.966
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Apr 17 22:34:06.979: INFO: created pod
Apr 17 22:34:06.979: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8922" to be "Succeeded or Failed"
Apr 17 22:34:06.981: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368699ms
Apr 17 22:34:08.985: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006404468s
Apr 17 22:34:10.986: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007188303s
STEP: Saw pod success 04/17/23 22:34:10.986
Apr 17 22:34:10.986: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Apr 17 22:34:40.986: INFO: polling logs
Apr 17 22:34:40.998: INFO: Pod logs: 
I0417 22:34:07.735573       1 log.go:198] OK: Got token
I0417 22:34:07.735604       1 log.go:198] validating with in-cluster discovery
I0417 22:34:07.735833       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0417 22:34:07.735861       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8922:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1681771447, NotBefore:1681770847, IssuedAt:1681770847, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8922", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9e20ca10-eff2-411e-8e0f-2d8364a9d469"}}}
I0417 22:34:07.746242       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0417 22:34:07.751331       1 log.go:198] OK: Validated signature on JWT
I0417 22:34:07.751397       1 log.go:198] OK: Got valid claims from token!
I0417 22:34:07.751415       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8922:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1681771447, NotBefore:1681770847, IssuedAt:1681770847, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8922", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9e20ca10-eff2-411e-8e0f-2d8364a9d469"}}}

Apr 17 22:34:40.998: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 17 22:34:41.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8922" for this suite. 04/17/23 22:34:41.006
------------------------------
• [SLOW TEST] [34.061 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:34:06.952
    Apr 17 22:34:06.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:34:06.953
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:06.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:06.966
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Apr 17 22:34:06.979: INFO: created pod
    Apr 17 22:34:06.979: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-8922" to be "Succeeded or Failed"
    Apr 17 22:34:06.981: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368699ms
    Apr 17 22:34:08.985: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006404468s
    Apr 17 22:34:10.986: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007188303s
    STEP: Saw pod success 04/17/23 22:34:10.986
    Apr 17 22:34:10.986: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Apr 17 22:34:40.986: INFO: polling logs
    Apr 17 22:34:40.998: INFO: Pod logs: 
    I0417 22:34:07.735573       1 log.go:198] OK: Got token
    I0417 22:34:07.735604       1 log.go:198] validating with in-cluster discovery
    I0417 22:34:07.735833       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0417 22:34:07.735861       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8922:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1681771447, NotBefore:1681770847, IssuedAt:1681770847, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8922", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9e20ca10-eff2-411e-8e0f-2d8364a9d469"}}}
    I0417 22:34:07.746242       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0417 22:34:07.751331       1 log.go:198] OK: Validated signature on JWT
    I0417 22:34:07.751397       1 log.go:198] OK: Got valid claims from token!
    I0417 22:34:07.751415       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8922:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1681771447, NotBefore:1681770847, IssuedAt:1681770847, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8922", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"9e20ca10-eff2-411e-8e0f-2d8364a9d469"}}}

    Apr 17 22:34:40.998: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:34:41.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8922" for this suite. 04/17/23 22:34:41.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:34:41.014
Apr 17 22:34:41.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:34:41.014
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:41.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:41.029
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-53096101-2d44-442e-a31c-961db33b8f30 04/17/23 22:34:41.031
STEP: Creating a pod to test consume configMaps 04/17/23 22:34:41.034
Apr 17 22:34:41.041: INFO: Waiting up to 5m0s for pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f" in namespace "configmap-9986" to be "Succeeded or Failed"
Apr 17 22:34:41.043: INFO: Pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583764ms
Apr 17 22:34:43.046: INFO: Pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f": Phase="Running", Reason="", readiness=false. Elapsed: 2.00578789s
Apr 17 22:34:45.047: INFO: Pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006685293s
STEP: Saw pod success 04/17/23 22:34:45.047
Apr 17 22:34:45.047: INFO: Pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f" satisfied condition "Succeeded or Failed"
Apr 17 22:34:45.050: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:34:45.062
Apr 17 22:34:45.072: INFO: Waiting for pod pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f to disappear
Apr 17 22:34:45.074: INFO: Pod pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:34:45.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-9986" for this suite. 04/17/23 22:34:45.078
------------------------------
• [4.069 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:34:41.014
    Apr 17 22:34:41.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:34:41.014
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:41.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:41.029
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-53096101-2d44-442e-a31c-961db33b8f30 04/17/23 22:34:41.031
    STEP: Creating a pod to test consume configMaps 04/17/23 22:34:41.034
    Apr 17 22:34:41.041: INFO: Waiting up to 5m0s for pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f" in namespace "configmap-9986" to be "Succeeded or Failed"
    Apr 17 22:34:41.043: INFO: Pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.583764ms
    Apr 17 22:34:43.046: INFO: Pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f": Phase="Running", Reason="", readiness=false. Elapsed: 2.00578789s
    Apr 17 22:34:45.047: INFO: Pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006685293s
    STEP: Saw pod success 04/17/23 22:34:45.047
    Apr 17 22:34:45.047: INFO: Pod "pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f" satisfied condition "Succeeded or Failed"
    Apr 17 22:34:45.050: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:34:45.062
    Apr 17 22:34:45.072: INFO: Waiting for pod pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f to disappear
    Apr 17 22:34:45.074: INFO: Pod pod-configmaps-bea944f5-8917-4c47-91a7-d60fdfd97a4f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:34:45.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-9986" for this suite. 04/17/23 22:34:45.078
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:34:45.084
Apr 17 22:34:45.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 22:34:45.085
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:45.096
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:45.099
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-f52f8a05-56c0-45b6-ab14-9648ae4741a1 04/17/23 22:34:45.101
STEP: Creating a pod to test consume secrets 04/17/23 22:34:45.105
Apr 17 22:34:45.111: INFO: Waiting up to 5m0s for pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d" in namespace "secrets-3096" to be "Succeeded or Failed"
Apr 17 22:34:45.114: INFO: Pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.515475ms
Apr 17 22:34:47.118: INFO: Pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006340319s
Apr 17 22:34:49.117: INFO: Pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006058129s
STEP: Saw pod success 04/17/23 22:34:49.117
Apr 17 22:34:49.117: INFO: Pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d" satisfied condition "Succeeded or Failed"
Apr 17 22:34:49.120: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-a2700467-1311-43ad-974c-2469a233a86d container secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:34:49.126
Apr 17 22:34:49.142: INFO: Waiting for pod pod-secrets-a2700467-1311-43ad-974c-2469a233a86d to disappear
Apr 17 22:34:49.148: INFO: Pod pod-secrets-a2700467-1311-43ad-974c-2469a233a86d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 22:34:49.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3096" for this suite. 04/17/23 22:34:49.153
------------------------------
• [4.073 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:34:45.084
    Apr 17 22:34:45.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 22:34:45.085
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:45.096
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:45.099
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-f52f8a05-56c0-45b6-ab14-9648ae4741a1 04/17/23 22:34:45.101
    STEP: Creating a pod to test consume secrets 04/17/23 22:34:45.105
    Apr 17 22:34:45.111: INFO: Waiting up to 5m0s for pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d" in namespace "secrets-3096" to be "Succeeded or Failed"
    Apr 17 22:34:45.114: INFO: Pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.515475ms
    Apr 17 22:34:47.118: INFO: Pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006340319s
    Apr 17 22:34:49.117: INFO: Pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006058129s
    STEP: Saw pod success 04/17/23 22:34:49.117
    Apr 17 22:34:49.117: INFO: Pod "pod-secrets-a2700467-1311-43ad-974c-2469a233a86d" satisfied condition "Succeeded or Failed"
    Apr 17 22:34:49.120: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-secrets-a2700467-1311-43ad-974c-2469a233a86d container secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:34:49.126
    Apr 17 22:34:49.142: INFO: Waiting for pod pod-secrets-a2700467-1311-43ad-974c-2469a233a86d to disappear
    Apr 17 22:34:49.148: INFO: Pod pod-secrets-a2700467-1311-43ad-974c-2469a233a86d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:34:49.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3096" for this suite. 04/17/23 22:34:49.153
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:34:49.158
Apr 17 22:34:49.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:34:49.158
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:49.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:49.174
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:34:49.177
Apr 17 22:34:49.184: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be" in namespace "projected-2446" to be "Succeeded or Failed"
Apr 17 22:34:49.189: INFO: Pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438653ms
Apr 17 22:34:51.193: INFO: Pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be": Phase="Running", Reason="", readiness=false. Elapsed: 2.009052254s
Apr 17 22:34:53.193: INFO: Pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008793379s
STEP: Saw pod success 04/17/23 22:34:53.193
Apr 17 22:34:53.193: INFO: Pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be" satisfied condition "Succeeded or Failed"
Apr 17 22:34:53.195: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be container client-container: <nil>
STEP: delete the pod 04/17/23 22:34:53.2
Apr 17 22:34:53.210: INFO: Waiting for pod downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be to disappear
Apr 17 22:34:53.212: INFO: Pod downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 22:34:53.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2446" for this suite. 04/17/23 22:34:53.217
------------------------------
• [4.063 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:34:49.158
    Apr 17 22:34:49.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:34:49.158
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:49.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:49.174
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:34:49.177
    Apr 17 22:34:49.184: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be" in namespace "projected-2446" to be "Succeeded or Failed"
    Apr 17 22:34:49.189: INFO: Pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be": Phase="Pending", Reason="", readiness=false. Elapsed: 5.438653ms
    Apr 17 22:34:51.193: INFO: Pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be": Phase="Running", Reason="", readiness=false. Elapsed: 2.009052254s
    Apr 17 22:34:53.193: INFO: Pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008793379s
    STEP: Saw pod success 04/17/23 22:34:53.193
    Apr 17 22:34:53.193: INFO: Pod "downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be" satisfied condition "Succeeded or Failed"
    Apr 17 22:34:53.195: INFO: Trying to get logs from node ip-10-0-64-189.us-west-2.compute.internal pod downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be container client-container: <nil>
    STEP: delete the pod 04/17/23 22:34:53.2
    Apr 17 22:34:53.210: INFO: Waiting for pod downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be to disappear
    Apr 17 22:34:53.212: INFO: Pod downwardapi-volume-42114020-c02d-4506-90d7-bdecf05b95be no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:34:53.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2446" for this suite. 04/17/23 22:34:53.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:34:53.222
Apr 17 22:34:53.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:34:53.223
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:53.314
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:53.317
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-06bbbebb-295d-4681-8658-c101e69a5d22 04/17/23 22:34:53.319
STEP: Creating a pod to test consume secrets 04/17/23 22:34:53.323
Apr 17 22:34:53.331: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5" in namespace "projected-1008" to be "Succeeded or Failed"
Apr 17 22:34:53.336: INFO: Pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.45946ms
Apr 17 22:34:55.340: INFO: Pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009269318s
Apr 17 22:34:57.340: INFO: Pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009878549s
STEP: Saw pod success 04/17/23 22:34:57.341
Apr 17 22:34:57.341: INFO: Pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5" satisfied condition "Succeeded or Failed"
Apr 17 22:34:57.343: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5 container projected-secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:34:57.348
Apr 17 22:34:57.358: INFO: Waiting for pod pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5 to disappear
Apr 17 22:34:57.360: INFO: Pod pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 17 22:34:57.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1008" for this suite. 04/17/23 22:34:57.364
------------------------------
• [4.147 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:34:53.222
    Apr 17 22:34:53.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:34:53.223
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:53.314
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:53.317
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-06bbbebb-295d-4681-8658-c101e69a5d22 04/17/23 22:34:53.319
    STEP: Creating a pod to test consume secrets 04/17/23 22:34:53.323
    Apr 17 22:34:53.331: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5" in namespace "projected-1008" to be "Succeeded or Failed"
    Apr 17 22:34:53.336: INFO: Pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.45946ms
    Apr 17 22:34:55.340: INFO: Pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009269318s
    Apr 17 22:34:57.340: INFO: Pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009878549s
    STEP: Saw pod success 04/17/23 22:34:57.341
    Apr 17 22:34:57.341: INFO: Pod "pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5" satisfied condition "Succeeded or Failed"
    Apr 17 22:34:57.343: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5 container projected-secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:34:57.348
    Apr 17 22:34:57.358: INFO: Waiting for pod pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5 to disappear
    Apr 17 22:34:57.360: INFO: Pod pod-projected-secrets-3d003007-8701-41b4-95bd-8a611df81bc5 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:34:57.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1008" for this suite. 04/17/23 22:34:57.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:34:57.369
Apr 17 22:34:57.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 22:34:57.37
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:57.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:57.386
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 04/17/23 22:35:14.391
STEP: Creating a ResourceQuota 04/17/23 22:35:19.395
STEP: Ensuring resource quota status is calculated 04/17/23 22:35:19.399
STEP: Creating a ConfigMap 04/17/23 22:35:21.402
STEP: Ensuring resource quota status captures configMap creation 04/17/23 22:35:21.412
STEP: Deleting a ConfigMap 04/17/23 22:35:23.417
STEP: Ensuring resource quota status released usage 04/17/23 22:35:23.424
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 22:35:25.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7079" for this suite. 04/17/23 22:35:25.432
------------------------------
• [SLOW TEST] [28.068 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:34:57.369
    Apr 17 22:34:57.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 22:34:57.37
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:34:57.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:34:57.386
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 04/17/23 22:35:14.391
    STEP: Creating a ResourceQuota 04/17/23 22:35:19.395
    STEP: Ensuring resource quota status is calculated 04/17/23 22:35:19.399
    STEP: Creating a ConfigMap 04/17/23 22:35:21.402
    STEP: Ensuring resource quota status captures configMap creation 04/17/23 22:35:21.412
    STEP: Deleting a ConfigMap 04/17/23 22:35:23.417
    STEP: Ensuring resource quota status released usage 04/17/23 22:35:23.424
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:35:25.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7079" for this suite. 04/17/23 22:35:25.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:35:25.438
Apr 17 22:35:25.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename sched-preemption 04/17/23 22:35:25.439
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:35:25.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:35:25.457
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Apr 17 22:35:25.469: INFO: Waiting up to 1m0s for all nodes to be ready
Apr 17 22:36:25.518: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 04/17/23 22:36:25.521
Apr 17 22:36:25.538: INFO: Created pod: pod0-0-sched-preemption-low-priority
Apr 17 22:36:25.544: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Apr 17 22:36:25.562: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Apr 17 22:36:25.569: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Apr 17 22:36:25.605: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Apr 17 22:36:25.622: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Apr 17 22:36:25.681: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Apr 17 22:36:25.701: INFO: Created pod: pod3-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 04/17/23 22:36:25.701
Apr 17 22:36:25.702: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9857" to be "running"
Apr 17 22:36:25.728: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 26.781901ms
Apr 17 22:36:27.732: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.03080183s
Apr 17 22:36:27.732: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Apr 17 22:36:27.732: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
Apr 17 22:36:27.735: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.367539ms
Apr 17 22:36:27.735: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 22:36:27.735: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
Apr 17 22:36:27.737: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.402303ms
Apr 17 22:36:27.737: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 22:36:27.737: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
Apr 17 22:36:27.739: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.207023ms
Apr 17 22:36:27.739: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 22:36:27.739: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
Apr 17 22:36:27.741: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.039918ms
Apr 17 22:36:27.741: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 22:36:27.741: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
Apr 17 22:36:27.744: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.019794ms
Apr 17 22:36:27.744: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 22:36:27.744: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
Apr 17 22:36:27.746: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.001815ms
Apr 17 22:36:27.746: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Apr 17 22:36:27.746: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
Apr 17 22:36:27.748: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.034207ms
Apr 17 22:36:27.748: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 04/17/23 22:36:27.748
Apr 17 22:36:27.755: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Apr 17 22:36:27.760: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789615ms
Apr 17 22:36:29.764: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008855639s
Apr 17 22:36:31.763: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008408802s
Apr 17 22:36:33.763: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00821526s
Apr 17 22:36:33.763: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:36:33.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9857" for this suite. 04/17/23 22:36:33.868
------------------------------
• [SLOW TEST] [68.439 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:35:25.438
    Apr 17 22:35:25.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename sched-preemption 04/17/23 22:35:25.439
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:35:25.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:35:25.457
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Apr 17 22:35:25.469: INFO: Waiting up to 1m0s for all nodes to be ready
    Apr 17 22:36:25.518: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 04/17/23 22:36:25.521
    Apr 17 22:36:25.538: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Apr 17 22:36:25.544: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Apr 17 22:36:25.562: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Apr 17 22:36:25.569: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Apr 17 22:36:25.605: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Apr 17 22:36:25.622: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Apr 17 22:36:25.681: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Apr 17 22:36:25.701: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 04/17/23 22:36:25.701
    Apr 17 22:36:25.702: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9857" to be "running"
    Apr 17 22:36:25.728: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 26.781901ms
    Apr 17 22:36:27.732: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.03080183s
    Apr 17 22:36:27.732: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Apr 17 22:36:27.732: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
    Apr 17 22:36:27.735: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.367539ms
    Apr 17 22:36:27.735: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 22:36:27.735: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
    Apr 17 22:36:27.737: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.402303ms
    Apr 17 22:36:27.737: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 22:36:27.737: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
    Apr 17 22:36:27.739: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.207023ms
    Apr 17 22:36:27.739: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 22:36:27.739: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
    Apr 17 22:36:27.741: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.039918ms
    Apr 17 22:36:27.741: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 22:36:27.741: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
    Apr 17 22:36:27.744: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.019794ms
    Apr 17 22:36:27.744: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 22:36:27.744: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
    Apr 17 22:36:27.746: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.001815ms
    Apr 17 22:36:27.746: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Apr 17 22:36:27.746: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-9857" to be "running"
    Apr 17 22:36:27.748: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.034207ms
    Apr 17 22:36:27.748: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 04/17/23 22:36:27.748
    Apr 17 22:36:27.755: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Apr 17 22:36:27.760: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789615ms
    Apr 17 22:36:29.764: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008855639s
    Apr 17 22:36:31.763: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008408802s
    Apr 17 22:36:33.763: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.00821526s
    Apr 17 22:36:33.763: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:36:33.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9857" for this suite. 04/17/23 22:36:33.868
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:36:33.878
Apr 17 22:36:33.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename statefulset 04/17/23 22:36:33.878
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:36:33.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:36:33.892
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-831 04/17/23 22:36:33.895
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 04/17/23 22:36:33.899
STEP: Creating pod with conflicting port in namespace statefulset-831 04/17/23 22:36:33.904
STEP: Waiting until pod test-pod will start running in namespace statefulset-831 04/17/23 22:36:33.91
Apr 17 22:36:33.910: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-831" to be "running"
Apr 17 22:36:33.912: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120953ms
Apr 17 22:36:35.915: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005346438s
Apr 17 22:36:35.915: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-831 04/17/23 22:36:35.915
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-831 04/17/23 22:36:35.92
Apr 17 22:36:35.932: INFO: Observed stateful pod in namespace: statefulset-831, name: ss-0, uid: 19c217bd-1f42-4dd4-b285-5bd0262aeca4, status phase: Pending. Waiting for statefulset controller to delete.
Apr 17 22:36:35.944: INFO: Observed stateful pod in namespace: statefulset-831, name: ss-0, uid: 19c217bd-1f42-4dd4-b285-5bd0262aeca4, status phase: Failed. Waiting for statefulset controller to delete.
Apr 17 22:36:35.951: INFO: Observed stateful pod in namespace: statefulset-831, name: ss-0, uid: 19c217bd-1f42-4dd4-b285-5bd0262aeca4, status phase: Failed. Waiting for statefulset controller to delete.
Apr 17 22:36:35.956: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-831
STEP: Removing pod with conflicting port in namespace statefulset-831 04/17/23 22:36:35.956
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-831 and will be in running state 04/17/23 22:36:35.968
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 17 22:36:37.975: INFO: Deleting all statefulset in ns statefulset-831
Apr 17 22:36:37.978: INFO: Scaling statefulset ss to 0
Apr 17 22:36:47.991: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:36:47.993: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:36:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-831" for this suite. 04/17/23 22:36:48.007
------------------------------
• [SLOW TEST] [14.134 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:36:33.878
    Apr 17 22:36:33.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename statefulset 04/17/23 22:36:33.878
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:36:33.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:36:33.892
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-831 04/17/23 22:36:33.895
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 04/17/23 22:36:33.899
    STEP: Creating pod with conflicting port in namespace statefulset-831 04/17/23 22:36:33.904
    STEP: Waiting until pod test-pod will start running in namespace statefulset-831 04/17/23 22:36:33.91
    Apr 17 22:36:33.910: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-831" to be "running"
    Apr 17 22:36:33.912: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.120953ms
    Apr 17 22:36:35.915: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005346438s
    Apr 17 22:36:35.915: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-831 04/17/23 22:36:35.915
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-831 04/17/23 22:36:35.92
    Apr 17 22:36:35.932: INFO: Observed stateful pod in namespace: statefulset-831, name: ss-0, uid: 19c217bd-1f42-4dd4-b285-5bd0262aeca4, status phase: Pending. Waiting for statefulset controller to delete.
    Apr 17 22:36:35.944: INFO: Observed stateful pod in namespace: statefulset-831, name: ss-0, uid: 19c217bd-1f42-4dd4-b285-5bd0262aeca4, status phase: Failed. Waiting for statefulset controller to delete.
    Apr 17 22:36:35.951: INFO: Observed stateful pod in namespace: statefulset-831, name: ss-0, uid: 19c217bd-1f42-4dd4-b285-5bd0262aeca4, status phase: Failed. Waiting for statefulset controller to delete.
    Apr 17 22:36:35.956: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-831
    STEP: Removing pod with conflicting port in namespace statefulset-831 04/17/23 22:36:35.956
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-831 and will be in running state 04/17/23 22:36:35.968
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 17 22:36:37.975: INFO: Deleting all statefulset in ns statefulset-831
    Apr 17 22:36:37.978: INFO: Scaling statefulset ss to 0
    Apr 17 22:36:47.991: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 22:36:47.993: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:36:48.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-831" for this suite. 04/17/23 22:36:48.007
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:36:48.012
Apr 17 22:36:48.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename controllerrevisions 04/17/23 22:36:48.013
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:36:48.029
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:36:48.032
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-kw5vr-daemon-set" 04/17/23 22:36:48.05
STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 22:36:48.054
Apr 17 22:36:48.058: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:36:48.058: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:36:48.058: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:36:48.060: INFO: Number of nodes with available pods controlled by daemonset e2e-kw5vr-daemon-set: 0
Apr 17 22:36:48.060: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:36:49.067: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:36:49.067: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:36:49.067: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:36:49.071: INFO: Number of nodes with available pods controlled by daemonset e2e-kw5vr-daemon-set: 0
Apr 17 22:36:49.071: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
Apr 17 22:36:50.065: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:36:50.065: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:36:50.065: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Apr 17 22:36:50.068: INFO: Number of nodes with available pods controlled by daemonset e2e-kw5vr-daemon-set: 4
Apr 17 22:36:50.068: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-kw5vr-daemon-set
STEP: Confirm DaemonSet "e2e-kw5vr-daemon-set" successfully created with "daemonset-name=e2e-kw5vr-daemon-set" label 04/17/23 22:36:50.07
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kw5vr-daemon-set" 04/17/23 22:36:50.075
Apr 17 22:36:50.078: INFO: Located ControllerRevision: "e2e-kw5vr-daemon-set-7899696dfd"
STEP: Patching ControllerRevision "e2e-kw5vr-daemon-set-7899696dfd" 04/17/23 22:36:50.08
Apr 17 22:36:50.085: INFO: e2e-kw5vr-daemon-set-7899696dfd has been patched
STEP: Create a new ControllerRevision 04/17/23 22:36:50.085
Apr 17 22:36:50.089: INFO: Created ControllerRevision: e2e-kw5vr-daemon-set-8599d554c6
STEP: Confirm that there are two ControllerRevisions 04/17/23 22:36:50.089
Apr 17 22:36:50.090: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 17 22:36:50.092: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-kw5vr-daemon-set-7899696dfd" 04/17/23 22:36:50.092
STEP: Confirm that there is only one ControllerRevision 04/17/23 22:36:50.096
Apr 17 22:36:50.096: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 17 22:36:50.098: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-kw5vr-daemon-set-8599d554c6" 04/17/23 22:36:50.102
Apr 17 22:36:50.108: INFO: e2e-kw5vr-daemon-set-8599d554c6 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 04/17/23 22:36:50.108
W0417 22:36:50.114349      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 04/17/23 22:36:50.114
Apr 17 22:36:50.114: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 17 22:36:51.117: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 17 22:36:51.120: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kw5vr-daemon-set-8599d554c6=updated" 04/17/23 22:36:51.12
STEP: Confirm that there is only one ControllerRevision 04/17/23 22:36:51.126
Apr 17 22:36:51.126: INFO: Requesting list of ControllerRevisions to confirm quantity
Apr 17 22:36:51.128: INFO: Found 1 ControllerRevisions
Apr 17 22:36:51.130: INFO: ControllerRevision "e2e-kw5vr-daemon-set-7b99c49b77" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-kw5vr-daemon-set" 04/17/23 22:36:51.132
STEP: deleting DaemonSet.extensions e2e-kw5vr-daemon-set in namespace controllerrevisions-2950, will wait for the garbage collector to delete the pods 04/17/23 22:36:51.132
Apr 17 22:36:51.190: INFO: Deleting DaemonSet.extensions e2e-kw5vr-daemon-set took: 5.219169ms
Apr 17 22:36:51.290: INFO: Terminating DaemonSet.extensions e2e-kw5vr-daemon-set pods took: 100.303078ms
Apr 17 22:36:52.694: INFO: Number of nodes with available pods controlled by daemonset e2e-kw5vr-daemon-set: 0
Apr 17 22:36:52.694: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kw5vr-daemon-set
Apr 17 22:36:52.696: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75550"},"items":null}

Apr 17 22:36:52.698: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75551"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:36:52.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-2950" for this suite. 04/17/23 22:36:52.715
------------------------------
• [4.706 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:36:48.012
    Apr 17 22:36:48.013: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename controllerrevisions 04/17/23 22:36:48.013
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:36:48.029
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:36:48.032
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-kw5vr-daemon-set" 04/17/23 22:36:48.05
    STEP: Check that daemon pods launch on every node of the cluster. 04/17/23 22:36:48.054
    Apr 17 22:36:48.058: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:36:48.058: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:36:48.058: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:36:48.060: INFO: Number of nodes with available pods controlled by daemonset e2e-kw5vr-daemon-set: 0
    Apr 17 22:36:48.060: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:36:49.067: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:36:49.067: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:36:49.067: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:36:49.071: INFO: Number of nodes with available pods controlled by daemonset e2e-kw5vr-daemon-set: 0
    Apr 17 22:36:49.071: INFO: Node ip-10-0-106-231.us-west-2.compute.internal is running 0 daemon pod, expected 1
    Apr 17 22:36:50.065: INFO: DaemonSet pods can't tolerate node ip-10-0-132-180.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:36:50.065: INFO: DaemonSet pods can't tolerate node ip-10-0-198-248.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:36:50.065: INFO: DaemonSet pods can't tolerate node ip-10-0-86-140.us-west-2.compute.internal with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Apr 17 22:36:50.068: INFO: Number of nodes with available pods controlled by daemonset e2e-kw5vr-daemon-set: 4
    Apr 17 22:36:50.068: INFO: Number of running nodes: 4, number of available pods: 4 in daemonset e2e-kw5vr-daemon-set
    STEP: Confirm DaemonSet "e2e-kw5vr-daemon-set" successfully created with "daemonset-name=e2e-kw5vr-daemon-set" label 04/17/23 22:36:50.07
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-kw5vr-daemon-set" 04/17/23 22:36:50.075
    Apr 17 22:36:50.078: INFO: Located ControllerRevision: "e2e-kw5vr-daemon-set-7899696dfd"
    STEP: Patching ControllerRevision "e2e-kw5vr-daemon-set-7899696dfd" 04/17/23 22:36:50.08
    Apr 17 22:36:50.085: INFO: e2e-kw5vr-daemon-set-7899696dfd has been patched
    STEP: Create a new ControllerRevision 04/17/23 22:36:50.085
    Apr 17 22:36:50.089: INFO: Created ControllerRevision: e2e-kw5vr-daemon-set-8599d554c6
    STEP: Confirm that there are two ControllerRevisions 04/17/23 22:36:50.089
    Apr 17 22:36:50.090: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 17 22:36:50.092: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-kw5vr-daemon-set-7899696dfd" 04/17/23 22:36:50.092
    STEP: Confirm that there is only one ControllerRevision 04/17/23 22:36:50.096
    Apr 17 22:36:50.096: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 17 22:36:50.098: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-kw5vr-daemon-set-8599d554c6" 04/17/23 22:36:50.102
    Apr 17 22:36:50.108: INFO: e2e-kw5vr-daemon-set-8599d554c6 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 04/17/23 22:36:50.108
    W0417 22:36:50.114349      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 04/17/23 22:36:50.114
    Apr 17 22:36:50.114: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 17 22:36:51.117: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 17 22:36:51.120: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-kw5vr-daemon-set-8599d554c6=updated" 04/17/23 22:36:51.12
    STEP: Confirm that there is only one ControllerRevision 04/17/23 22:36:51.126
    Apr 17 22:36:51.126: INFO: Requesting list of ControllerRevisions to confirm quantity
    Apr 17 22:36:51.128: INFO: Found 1 ControllerRevisions
    Apr 17 22:36:51.130: INFO: ControllerRevision "e2e-kw5vr-daemon-set-7b99c49b77" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-kw5vr-daemon-set" 04/17/23 22:36:51.132
    STEP: deleting DaemonSet.extensions e2e-kw5vr-daemon-set in namespace controllerrevisions-2950, will wait for the garbage collector to delete the pods 04/17/23 22:36:51.132
    Apr 17 22:36:51.190: INFO: Deleting DaemonSet.extensions e2e-kw5vr-daemon-set took: 5.219169ms
    Apr 17 22:36:51.290: INFO: Terminating DaemonSet.extensions e2e-kw5vr-daemon-set pods took: 100.303078ms
    Apr 17 22:36:52.694: INFO: Number of nodes with available pods controlled by daemonset e2e-kw5vr-daemon-set: 0
    Apr 17 22:36:52.694: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-kw5vr-daemon-set
    Apr 17 22:36:52.696: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"75550"},"items":null}

    Apr 17 22:36:52.698: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"75551"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:36:52.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-2950" for this suite. 04/17/23 22:36:52.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:36:52.719
Apr 17 22:36:52.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 22:36:52.72
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:36:52.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:36:52.735
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 04/17/23 22:36:52.738
STEP: Ensuring ResourceQuota status is calculated 04/17/23 22:36:52.741
STEP: Creating a ResourceQuota with not best effort scope 04/17/23 22:36:54.744
STEP: Ensuring ResourceQuota status is calculated 04/17/23 22:36:54.748
STEP: Creating a best-effort pod 04/17/23 22:36:56.751
STEP: Ensuring resource quota with best effort scope captures the pod usage 04/17/23 22:36:56.763
STEP: Ensuring resource quota with not best effort ignored the pod usage 04/17/23 22:36:58.766
STEP: Deleting the pod 04/17/23 22:37:00.769
STEP: Ensuring resource quota status released the pod usage 04/17/23 22:37:00.78
STEP: Creating a not best-effort pod 04/17/23 22:37:02.783
STEP: Ensuring resource quota with not best effort scope captures the pod usage 04/17/23 22:37:02.791
STEP: Ensuring resource quota with best effort scope ignored the pod usage 04/17/23 22:37:04.795
STEP: Deleting the pod 04/17/23 22:37:06.799
STEP: Ensuring resource quota status released the pod usage 04/17/23 22:37:06.81
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:08.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4193" for this suite. 04/17/23 22:37:08.817
------------------------------
• [SLOW TEST] [16.104 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:36:52.719
    Apr 17 22:36:52.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 22:36:52.72
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:36:52.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:36:52.735
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 04/17/23 22:36:52.738
    STEP: Ensuring ResourceQuota status is calculated 04/17/23 22:36:52.741
    STEP: Creating a ResourceQuota with not best effort scope 04/17/23 22:36:54.744
    STEP: Ensuring ResourceQuota status is calculated 04/17/23 22:36:54.748
    STEP: Creating a best-effort pod 04/17/23 22:36:56.751
    STEP: Ensuring resource quota with best effort scope captures the pod usage 04/17/23 22:36:56.763
    STEP: Ensuring resource quota with not best effort ignored the pod usage 04/17/23 22:36:58.766
    STEP: Deleting the pod 04/17/23 22:37:00.769
    STEP: Ensuring resource quota status released the pod usage 04/17/23 22:37:00.78
    STEP: Creating a not best-effort pod 04/17/23 22:37:02.783
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 04/17/23 22:37:02.791
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 04/17/23 22:37:04.795
    STEP: Deleting the pod 04/17/23 22:37:06.799
    STEP: Ensuring resource quota status released the pod usage 04/17/23 22:37:06.81
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:08.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4193" for this suite. 04/17/23 22:37:08.817
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:08.823
Apr 17 22:37:08.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:37:08.824
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:08.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:08.837
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 04/17/23 22:37:08.84
Apr 17 22:37:08.840: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-66 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 04/17/23 22:37:08.879
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:08.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-66" for this suite. 04/17/23 22:37:08.89
------------------------------
• [0.073 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:08.823
    Apr 17 22:37:08.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:37:08.824
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:08.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:08.837
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 04/17/23 22:37:08.84
    Apr 17 22:37:08.840: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-66 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 04/17/23 22:37:08.879
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:08.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-66" for this suite. 04/17/23 22:37:08.89
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:08.897
Apr 17 22:37:08.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:37:08.898
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:08.91
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:08.912
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 04/17/23 22:37:08.914
Apr 17 22:37:08.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Apr 17 22:37:08.979: INFO: stderr: ""
Apr 17 22:37:08.979: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 04/17/23 22:37:08.979
Apr 17 22:37:08.979: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Apr 17 22:37:08.979: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1455" to be "running and ready, or succeeded"
Apr 17 22:37:08.983: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.524771ms
Apr 17 22:37:08.983: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-93-18.us-west-2.compute.internal' to be 'Running' but was 'Pending'
Apr 17 22:37:10.987: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007552551s
Apr 17 22:37:10.987: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Apr 17 22:37:10.987: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 04/17/23 22:37:10.987
Apr 17 22:37:10.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator'
Apr 17 22:37:11.053: INFO: stderr: ""
Apr 17 22:37:11.053: INFO: stdout: "I0417 22:37:09.749528       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/jwq 366\nI0417 22:37:09.949871       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/r856 565\nI0417 22:37:10.150173       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/ckp 369\nI0417 22:37:10.350476       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vvk 395\nI0417 22:37:10.549718       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/drpq 373\nI0417 22:37:10.750008       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/9vhx 216\nI0417 22:37:10.950306       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/clv 316\n"
STEP: limiting log lines 04/17/23 22:37:11.053
Apr 17 22:37:11.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --tail=1'
Apr 17 22:37:11.123: INFO: stderr: ""
Apr 17 22:37:11.123: INFO: stdout: "I0417 22:37:10.950306       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/clv 316\n"
Apr 17 22:37:11.123: INFO: got output "I0417 22:37:10.950306       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/clv 316\n"
STEP: limiting log bytes 04/17/23 22:37:11.123
Apr 17 22:37:11.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --limit-bytes=1'
Apr 17 22:37:11.183: INFO: stderr: ""
Apr 17 22:37:11.183: INFO: stdout: "I"
Apr 17 22:37:11.183: INFO: got output "I"
STEP: exposing timestamps 04/17/23 22:37:11.183
Apr 17 22:37:11.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --tail=1 --timestamps'
Apr 17 22:37:11.246: INFO: stderr: ""
Apr 17 22:37:11.246: INFO: stdout: "2023-04-17T22:37:11.149627780Z I0417 22:37:11.149548       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/vln 305\n"
Apr 17 22:37:11.246: INFO: got output "2023-04-17T22:37:11.149627780Z I0417 22:37:11.149548       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/vln 305\n"
STEP: restricting to a time range 04/17/23 22:37:11.246
Apr 17 22:37:13.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --since=1s'
Apr 17 22:37:13.806: INFO: stderr: ""
Apr 17 22:37:13.806: INFO: stdout: "I0417 22:37:12.950138       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/2sq 201\nI0417 22:37:13.150434       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/wt7 515\nI0417 22:37:13.349679       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/z7r5 229\nI0417 22:37:13.549966       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/t67t 551\nI0417 22:37:13.750272       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/7994 213\n"
Apr 17 22:37:13.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --since=24h'
Apr 17 22:37:13.874: INFO: stderr: ""
Apr 17 22:37:13.874: INFO: stdout: "I0417 22:37:09.749528       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/jwq 366\nI0417 22:37:09.949871       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/r856 565\nI0417 22:37:10.150173       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/ckp 369\nI0417 22:37:10.350476       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vvk 395\nI0417 22:37:10.549718       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/drpq 373\nI0417 22:37:10.750008       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/9vhx 216\nI0417 22:37:10.950306       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/clv 316\nI0417 22:37:11.149548       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/vln 305\nI0417 22:37:11.349845       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/l6r 259\nI0417 22:37:11.550142       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/22v2 253\nI0417 22:37:11.750437       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/brf9 351\nI0417 22:37:11.949671       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/msmz 492\nI0417 22:37:12.149967       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/kpv 232\nI0417 22:37:12.350261       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/sn2x 270\nI0417 22:37:12.550549       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/7kgv 286\nI0417 22:37:12.749839       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/hpk7 575\nI0417 22:37:12.950138       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/2sq 201\nI0417 22:37:13.150434       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/wt7 515\nI0417 22:37:13.349679       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/z7r5 229\nI0417 22:37:13.549966       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/t67t 551\nI0417 22:37:13.750272       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/7994 213\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Apr 17 22:37:13.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 delete pod logs-generator'
Apr 17 22:37:15.179: INFO: stderr: ""
Apr 17 22:37:15.179: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:15.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1455" for this suite. 04/17/23 22:37:15.183
------------------------------
• [SLOW TEST] [6.290 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:08.897
    Apr 17 22:37:08.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:37:08.898
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:08.91
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:08.912
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 04/17/23 22:37:08.914
    Apr 17 22:37:08.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Apr 17 22:37:08.979: INFO: stderr: ""
    Apr 17 22:37:08.979: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 04/17/23 22:37:08.979
    Apr 17 22:37:08.979: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Apr 17 22:37:08.979: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1455" to be "running and ready, or succeeded"
    Apr 17 22:37:08.983: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.524771ms
    Apr 17 22:37:08.983: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'ip-10-0-93-18.us-west-2.compute.internal' to be 'Running' but was 'Pending'
    Apr 17 22:37:10.987: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.007552551s
    Apr 17 22:37:10.987: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Apr 17 22:37:10.987: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 04/17/23 22:37:10.987
    Apr 17 22:37:10.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator'
    Apr 17 22:37:11.053: INFO: stderr: ""
    Apr 17 22:37:11.053: INFO: stdout: "I0417 22:37:09.749528       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/jwq 366\nI0417 22:37:09.949871       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/r856 565\nI0417 22:37:10.150173       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/ckp 369\nI0417 22:37:10.350476       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vvk 395\nI0417 22:37:10.549718       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/drpq 373\nI0417 22:37:10.750008       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/9vhx 216\nI0417 22:37:10.950306       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/clv 316\n"
    STEP: limiting log lines 04/17/23 22:37:11.053
    Apr 17 22:37:11.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --tail=1'
    Apr 17 22:37:11.123: INFO: stderr: ""
    Apr 17 22:37:11.123: INFO: stdout: "I0417 22:37:10.950306       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/clv 316\n"
    Apr 17 22:37:11.123: INFO: got output "I0417 22:37:10.950306       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/clv 316\n"
    STEP: limiting log bytes 04/17/23 22:37:11.123
    Apr 17 22:37:11.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --limit-bytes=1'
    Apr 17 22:37:11.183: INFO: stderr: ""
    Apr 17 22:37:11.183: INFO: stdout: "I"
    Apr 17 22:37:11.183: INFO: got output "I"
    STEP: exposing timestamps 04/17/23 22:37:11.183
    Apr 17 22:37:11.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --tail=1 --timestamps'
    Apr 17 22:37:11.246: INFO: stderr: ""
    Apr 17 22:37:11.246: INFO: stdout: "2023-04-17T22:37:11.149627780Z I0417 22:37:11.149548       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/vln 305\n"
    Apr 17 22:37:11.246: INFO: got output "2023-04-17T22:37:11.149627780Z I0417 22:37:11.149548       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/vln 305\n"
    STEP: restricting to a time range 04/17/23 22:37:11.246
    Apr 17 22:37:13.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --since=1s'
    Apr 17 22:37:13.806: INFO: stderr: ""
    Apr 17 22:37:13.806: INFO: stdout: "I0417 22:37:12.950138       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/2sq 201\nI0417 22:37:13.150434       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/wt7 515\nI0417 22:37:13.349679       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/z7r5 229\nI0417 22:37:13.549966       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/t67t 551\nI0417 22:37:13.750272       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/7994 213\n"
    Apr 17 22:37:13.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 logs logs-generator logs-generator --since=24h'
    Apr 17 22:37:13.874: INFO: stderr: ""
    Apr 17 22:37:13.874: INFO: stdout: "I0417 22:37:09.749528       1 logs_generator.go:76] 0 GET /api/v1/namespaces/kube-system/pods/jwq 366\nI0417 22:37:09.949871       1 logs_generator.go:76] 1 POST /api/v1/namespaces/default/pods/r856 565\nI0417 22:37:10.150173       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/ckp 369\nI0417 22:37:10.350476       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/vvk 395\nI0417 22:37:10.549718       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/drpq 373\nI0417 22:37:10.750008       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/9vhx 216\nI0417 22:37:10.950306       1 logs_generator.go:76] 6 POST /api/v1/namespaces/default/pods/clv 316\nI0417 22:37:11.149548       1 logs_generator.go:76] 7 POST /api/v1/namespaces/default/pods/vln 305\nI0417 22:37:11.349845       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/l6r 259\nI0417 22:37:11.550142       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/22v2 253\nI0417 22:37:11.750437       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/brf9 351\nI0417 22:37:11.949671       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/msmz 492\nI0417 22:37:12.149967       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/kpv 232\nI0417 22:37:12.350261       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/sn2x 270\nI0417 22:37:12.550549       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/7kgv 286\nI0417 22:37:12.749839       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/hpk7 575\nI0417 22:37:12.950138       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/2sq 201\nI0417 22:37:13.150434       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/wt7 515\nI0417 22:37:13.349679       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/z7r5 229\nI0417 22:37:13.549966       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/kube-system/pods/t67t 551\nI0417 22:37:13.750272       1 logs_generator.go:76] 20 GET /api/v1/namespaces/ns/pods/7994 213\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Apr 17 22:37:13.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-1455 delete pod logs-generator'
    Apr 17 22:37:15.179: INFO: stderr: ""
    Apr 17 22:37:15.179: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:15.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1455" for this suite. 04/17/23 22:37:15.183
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:15.188
Apr 17 22:37:15.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename proxy 04/17/23 22:37:15.188
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:15.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:15.203
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Apr 17 22:37:15.205: INFO: Creating pod...
Apr 17 22:37:15.210: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2420" to be "running"
Apr 17 22:37:15.213: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.347491ms
Apr 17 22:37:17.215: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005096929s
Apr 17 22:37:17.215: INFO: Pod "agnhost" satisfied condition "running"
Apr 17 22:37:17.215: INFO: Creating service...
Apr 17 22:37:17.226: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/DELETE
Apr 17 22:37:17.239: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 17 22:37:17.239: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/GET
Apr 17 22:37:17.242: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr 17 22:37:17.242: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/HEAD
Apr 17 22:37:17.245: INFO: http.Client request:HEAD | StatusCode:200
Apr 17 22:37:17.245: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/OPTIONS
Apr 17 22:37:17.248: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 17 22:37:17.248: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/PATCH
Apr 17 22:37:17.250: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 17 22:37:17.250: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/POST
Apr 17 22:37:17.253: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 17 22:37:17.253: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/PUT
Apr 17 22:37:17.256: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Apr 17 22:37:17.256: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/DELETE
Apr 17 22:37:17.259: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Apr 17 22:37:17.259: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/GET
Apr 17 22:37:17.263: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Apr 17 22:37:17.263: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/HEAD
Apr 17 22:37:17.266: INFO: http.Client request:HEAD | StatusCode:200
Apr 17 22:37:17.266: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/OPTIONS
Apr 17 22:37:17.270: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Apr 17 22:37:17.270: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/PATCH
Apr 17 22:37:17.274: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Apr 17 22:37:17.274: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/POST
Apr 17 22:37:17.278: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Apr 17 22:37:17.278: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/PUT
Apr 17 22:37:17.281: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:17.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2420" for this suite. 04/17/23 22:37:17.286
------------------------------
• [2.103 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:15.188
    Apr 17 22:37:15.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename proxy 04/17/23 22:37:15.188
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:15.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:15.203
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Apr 17 22:37:15.205: INFO: Creating pod...
    Apr 17 22:37:15.210: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2420" to be "running"
    Apr 17 22:37:15.213: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.347491ms
    Apr 17 22:37:17.215: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.005096929s
    Apr 17 22:37:17.215: INFO: Pod "agnhost" satisfied condition "running"
    Apr 17 22:37:17.215: INFO: Creating service...
    Apr 17 22:37:17.226: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/DELETE
    Apr 17 22:37:17.239: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 17 22:37:17.239: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/GET
    Apr 17 22:37:17.242: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Apr 17 22:37:17.242: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/HEAD
    Apr 17 22:37:17.245: INFO: http.Client request:HEAD | StatusCode:200
    Apr 17 22:37:17.245: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/OPTIONS
    Apr 17 22:37:17.248: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 17 22:37:17.248: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/PATCH
    Apr 17 22:37:17.250: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 17 22:37:17.250: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/POST
    Apr 17 22:37:17.253: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 17 22:37:17.253: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/pods/agnhost/proxy/some/path/with/PUT
    Apr 17 22:37:17.256: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Apr 17 22:37:17.256: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/DELETE
    Apr 17 22:37:17.259: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Apr 17 22:37:17.259: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/GET
    Apr 17 22:37:17.263: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Apr 17 22:37:17.263: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/HEAD
    Apr 17 22:37:17.266: INFO: http.Client request:HEAD | StatusCode:200
    Apr 17 22:37:17.266: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/OPTIONS
    Apr 17 22:37:17.270: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Apr 17 22:37:17.270: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/PATCH
    Apr 17 22:37:17.274: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Apr 17 22:37:17.274: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/POST
    Apr 17 22:37:17.278: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Apr 17 22:37:17.278: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2420/services/test-service/proxy/some/path/with/PUT
    Apr 17 22:37:17.281: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:17.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2420" for this suite. 04/17/23 22:37:17.286
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:17.291
Apr 17 22:37:17.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:37:17.292
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:17.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:17.308
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 04/17/23 22:37:17.31
Apr 17 22:37:17.318: INFO: Waiting up to 5m0s for pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4" in namespace "downward-api-865" to be "running and ready"
Apr 17 22:37:17.320: INFO: Pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.392172ms
Apr 17 22:37:17.320: INFO: The phase of Pod labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:37:19.324: INFO: Pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005836663s
Apr 17 22:37:19.324: INFO: The phase of Pod labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4 is Running (Ready = true)
Apr 17 22:37:19.324: INFO: Pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4" satisfied condition "running and ready"
Apr 17 22:37:19.849: INFO: Successfully updated pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:23.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-865" for this suite. 04/17/23 22:37:23.872
------------------------------
• [SLOW TEST] [6.585 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:17.291
    Apr 17 22:37:17.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:37:17.292
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:17.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:17.308
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 04/17/23 22:37:17.31
    Apr 17 22:37:17.318: INFO: Waiting up to 5m0s for pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4" in namespace "downward-api-865" to be "running and ready"
    Apr 17 22:37:17.320: INFO: Pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.392172ms
    Apr 17 22:37:17.320: INFO: The phase of Pod labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:37:19.324: INFO: Pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.005836663s
    Apr 17 22:37:19.324: INFO: The phase of Pod labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4 is Running (Ready = true)
    Apr 17 22:37:19.324: INFO: Pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4" satisfied condition "running and ready"
    Apr 17 22:37:19.849: INFO: Successfully updated pod "labelsupdatefa00d312-e977-4cfb-9066-16046e063fd4"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:23.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-865" for this suite. 04/17/23 22:37:23.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:23.876
Apr 17 22:37:23.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename resourcequota 04/17/23 22:37:23.877
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:23.89
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:23.893
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 04/17/23 22:37:23.895
STEP: Ensuring ResourceQuota status is calculated 04/17/23 22:37:23.898
STEP: Creating a ResourceQuota with not terminating scope 04/17/23 22:37:25.901
STEP: Ensuring ResourceQuota status is calculated 04/17/23 22:37:25.906
STEP: Creating a long running pod 04/17/23 22:37:27.91
STEP: Ensuring resource quota with not terminating scope captures the pod usage 04/17/23 22:37:27.922
STEP: Ensuring resource quota with terminating scope ignored the pod usage 04/17/23 22:37:29.926
STEP: Deleting the pod 04/17/23 22:37:31.93
STEP: Ensuring resource quota status released the pod usage 04/17/23 22:37:31.943
STEP: Creating a terminating pod 04/17/23 22:37:33.946
STEP: Ensuring resource quota with terminating scope captures the pod usage 04/17/23 22:37:33.955
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 04/17/23 22:37:35.959
STEP: Deleting the pod 04/17/23 22:37:37.963
STEP: Ensuring resource quota status released the pod usage 04/17/23 22:37:37.976
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:39.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5497" for this suite. 04/17/23 22:37:39.984
------------------------------
• [SLOW TEST] [16.112 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:23.876
    Apr 17 22:37:23.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename resourcequota 04/17/23 22:37:23.877
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:23.89
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:23.893
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 04/17/23 22:37:23.895
    STEP: Ensuring ResourceQuota status is calculated 04/17/23 22:37:23.898
    STEP: Creating a ResourceQuota with not terminating scope 04/17/23 22:37:25.901
    STEP: Ensuring ResourceQuota status is calculated 04/17/23 22:37:25.906
    STEP: Creating a long running pod 04/17/23 22:37:27.91
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 04/17/23 22:37:27.922
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 04/17/23 22:37:29.926
    STEP: Deleting the pod 04/17/23 22:37:31.93
    STEP: Ensuring resource quota status released the pod usage 04/17/23 22:37:31.943
    STEP: Creating a terminating pod 04/17/23 22:37:33.946
    STEP: Ensuring resource quota with terminating scope captures the pod usage 04/17/23 22:37:33.955
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 04/17/23 22:37:35.959
    STEP: Deleting the pod 04/17/23 22:37:37.963
    STEP: Ensuring resource quota status released the pod usage 04/17/23 22:37:37.976
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:39.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5497" for this suite. 04/17/23 22:37:39.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:39.989
Apr 17 22:37:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename csistoragecapacity 04/17/23 22:37:39.99
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:40.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:40.007
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 04/17/23 22:37:40.009
STEP: getting /apis/storage.k8s.io 04/17/23 22:37:40.011
STEP: getting /apis/storage.k8s.io/v1 04/17/23 22:37:40.012
STEP: creating 04/17/23 22:37:40.012
STEP: watching 04/17/23 22:37:40.024
Apr 17 22:37:40.024: INFO: starting watch
STEP: getting 04/17/23 22:37:40.029
STEP: listing in namespace 04/17/23 22:37:40.031
STEP: listing across namespaces 04/17/23 22:37:40.034
STEP: patching 04/17/23 22:37:40.036
STEP: updating 04/17/23 22:37:40.039
Apr 17 22:37:40.043: INFO: waiting for watch events with expected annotations in namespace
Apr 17 22:37:40.043: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 04/17/23 22:37:40.043
STEP: deleting a collection 04/17/23 22:37:40.051
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:40.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-7954" for this suite. 04/17/23 22:37:40.064
------------------------------
• [0.081 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:39.989
    Apr 17 22:37:39.989: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename csistoragecapacity 04/17/23 22:37:39.99
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:40.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:40.007
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 04/17/23 22:37:40.009
    STEP: getting /apis/storage.k8s.io 04/17/23 22:37:40.011
    STEP: getting /apis/storage.k8s.io/v1 04/17/23 22:37:40.012
    STEP: creating 04/17/23 22:37:40.012
    STEP: watching 04/17/23 22:37:40.024
    Apr 17 22:37:40.024: INFO: starting watch
    STEP: getting 04/17/23 22:37:40.029
    STEP: listing in namespace 04/17/23 22:37:40.031
    STEP: listing across namespaces 04/17/23 22:37:40.034
    STEP: patching 04/17/23 22:37:40.036
    STEP: updating 04/17/23 22:37:40.039
    Apr 17 22:37:40.043: INFO: waiting for watch events with expected annotations in namespace
    Apr 17 22:37:40.043: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 04/17/23 22:37:40.043
    STEP: deleting a collection 04/17/23 22:37:40.051
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:40.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-7954" for this suite. 04/17/23 22:37:40.064
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:40.071
Apr 17 22:37:40.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:37:40.071
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:40.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:40.085
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-7017d68e-446b-486d-a456-05f2a468b14d 04/17/23 22:37:40.087
STEP: Creating secret with name secret-projected-all-test-volume-f51bd273-279c-43d1-b064-62a9622fdc34 04/17/23 22:37:40.091
STEP: Creating a pod to test Check all projections for projected volume plugin 04/17/23 22:37:40.094
Apr 17 22:37:40.100: INFO: Waiting up to 5m0s for pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d" in namespace "projected-6842" to be "Succeeded or Failed"
Apr 17 22:37:40.102: INFO: Pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.441822ms
Apr 17 22:37:42.106: INFO: Pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006312769s
Apr 17 22:37:44.106: INFO: Pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006253684s
STEP: Saw pod success 04/17/23 22:37:44.106
Apr 17 22:37:44.106: INFO: Pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d" satisfied condition "Succeeded or Failed"
Apr 17 22:37:44.109: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d container projected-all-volume-test: <nil>
STEP: delete the pod 04/17/23 22:37:44.118
Apr 17 22:37:44.126: INFO: Waiting for pod projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d to disappear
Apr 17 22:37:44.128: INFO: Pod projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:44.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6842" for this suite. 04/17/23 22:37:44.133
------------------------------
• [4.068 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:40.071
    Apr 17 22:37:40.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:37:40.071
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:40.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:40.085
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-7017d68e-446b-486d-a456-05f2a468b14d 04/17/23 22:37:40.087
    STEP: Creating secret with name secret-projected-all-test-volume-f51bd273-279c-43d1-b064-62a9622fdc34 04/17/23 22:37:40.091
    STEP: Creating a pod to test Check all projections for projected volume plugin 04/17/23 22:37:40.094
    Apr 17 22:37:40.100: INFO: Waiting up to 5m0s for pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d" in namespace "projected-6842" to be "Succeeded or Failed"
    Apr 17 22:37:40.102: INFO: Pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.441822ms
    Apr 17 22:37:42.106: INFO: Pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006312769s
    Apr 17 22:37:44.106: INFO: Pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006253684s
    STEP: Saw pod success 04/17/23 22:37:44.106
    Apr 17 22:37:44.106: INFO: Pod "projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d" satisfied condition "Succeeded or Failed"
    Apr 17 22:37:44.109: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d container projected-all-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:37:44.118
    Apr 17 22:37:44.126: INFO: Waiting for pod projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d to disappear
    Apr 17 22:37:44.128: INFO: Pod projected-volume-4277b276-ee27-48ce-af55-1ba23a4e862d no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:44.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6842" for this suite. 04/17/23 22:37:44.133
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:44.139
Apr 17 22:37:44.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:37:44.14
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:44.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:44.156
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 04/17/23 22:37:44.158
STEP: watching for the ServiceAccount to be added 04/17/23 22:37:44.164
STEP: patching the ServiceAccount 04/17/23 22:37:44.165
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 04/17/23 22:37:44.17
STEP: deleting the ServiceAccount 04/17/23 22:37:44.172
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:44.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-7132" for this suite. 04/17/23 22:37:44.185
------------------------------
• [0.050 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:44.139
    Apr 17 22:37:44.139: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:37:44.14
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:44.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:44.156
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 04/17/23 22:37:44.158
    STEP: watching for the ServiceAccount to be added 04/17/23 22:37:44.164
    STEP: patching the ServiceAccount 04/17/23 22:37:44.165
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 04/17/23 22:37:44.17
    STEP: deleting the ServiceAccount 04/17/23 22:37:44.172
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:44.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-7132" for this suite. 04/17/23 22:37:44.185
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:44.191
Apr 17 22:37:44.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename configmap 04/17/23 22:37:44.192
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:44.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:44.206
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-3dd8e021-eb1d-4248-b088-4408a9c810c9 04/17/23 22:37:44.211
STEP: Creating configMap with name cm-test-opt-upd-1d335b00-1fab-4041-ad78-297b73d14810 04/17/23 22:37:44.215
STEP: Creating the pod 04/17/23 22:37:44.218
Apr 17 22:37:44.225: INFO: Waiting up to 5m0s for pod "pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0" in namespace "configmap-3461" to be "running and ready"
Apr 17 22:37:44.227: INFO: Pod "pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.206254ms
Apr 17 22:37:44.227: INFO: The phase of Pod pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:37:46.231: INFO: Pod "pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0": Phase="Running", Reason="", readiness=true. Elapsed: 2.006123916s
Apr 17 22:37:46.231: INFO: The phase of Pod pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0 is Running (Ready = true)
Apr 17 22:37:46.231: INFO: Pod "pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-3dd8e021-eb1d-4248-b088-4408a9c810c9 04/17/23 22:37:46.245
STEP: Updating configmap cm-test-opt-upd-1d335b00-1fab-4041-ad78-297b73d14810 04/17/23 22:37:46.25
STEP: Creating configMap with name cm-test-opt-create-41802e67-be33-4bec-ab98-6ac869f3459d 04/17/23 22:37:46.253
STEP: waiting to observe update in volume 04/17/23 22:37:46.257
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:48.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3461" for this suite. 04/17/23 22:37:48.279
------------------------------
• [4.093 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:44.191
    Apr 17 22:37:44.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename configmap 04/17/23 22:37:44.192
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:44.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:44.206
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-3dd8e021-eb1d-4248-b088-4408a9c810c9 04/17/23 22:37:44.211
    STEP: Creating configMap with name cm-test-opt-upd-1d335b00-1fab-4041-ad78-297b73d14810 04/17/23 22:37:44.215
    STEP: Creating the pod 04/17/23 22:37:44.218
    Apr 17 22:37:44.225: INFO: Waiting up to 5m0s for pod "pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0" in namespace "configmap-3461" to be "running and ready"
    Apr 17 22:37:44.227: INFO: Pod "pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.206254ms
    Apr 17 22:37:44.227: INFO: The phase of Pod pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:37:46.231: INFO: Pod "pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0": Phase="Running", Reason="", readiness=true. Elapsed: 2.006123916s
    Apr 17 22:37:46.231: INFO: The phase of Pod pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0 is Running (Ready = true)
    Apr 17 22:37:46.231: INFO: Pod "pod-configmaps-4cd867b6-45e2-43df-9a0a-b77bda1f8ab0" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-3dd8e021-eb1d-4248-b088-4408a9c810c9 04/17/23 22:37:46.245
    STEP: Updating configmap cm-test-opt-upd-1d335b00-1fab-4041-ad78-297b73d14810 04/17/23 22:37:46.25
    STEP: Creating configMap with name cm-test-opt-create-41802e67-be33-4bec-ab98-6ac869f3459d 04/17/23 22:37:46.253
    STEP: waiting to observe update in volume 04/17/23 22:37:46.257
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:48.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3461" for this suite. 04/17/23 22:37:48.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:48.284
Apr 17 22:37:48.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:37:48.285
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:48.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:48.299
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:37:48.311
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:37:48.595
STEP: Deploying the webhook pod 04/17/23 22:37:48.601
STEP: Wait for the deployment to be ready 04/17/23 22:37:48.614
Apr 17 22:37:48.621: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:37:50.63
STEP: Verifying the service has paired with the endpoint 04/17/23 22:37:50.709
Apr 17 22:37:51.710: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 04/17/23 22:37:51.764
STEP: Creating a configMap that should be mutated 04/17/23 22:37:51.774
STEP: Deleting the collection of validation webhooks 04/17/23 22:37:51.793
STEP: Creating a configMap that should not be mutated 04/17/23 22:37:51.829
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:51.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1987" for this suite. 04/17/23 22:37:51.87
STEP: Destroying namespace "webhook-1987-markers" for this suite. 04/17/23 22:37:51.876
------------------------------
• [3.598 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:48.284
    Apr 17 22:37:48.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:37:48.285
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:48.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:48.299
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:37:48.311
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:37:48.595
    STEP: Deploying the webhook pod 04/17/23 22:37:48.601
    STEP: Wait for the deployment to be ready 04/17/23 22:37:48.614
    Apr 17 22:37:48.621: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:37:50.63
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:37:50.709
    Apr 17 22:37:51.710: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 04/17/23 22:37:51.764
    STEP: Creating a configMap that should be mutated 04/17/23 22:37:51.774
    STEP: Deleting the collection of validation webhooks 04/17/23 22:37:51.793
    STEP: Creating a configMap that should not be mutated 04/17/23 22:37:51.829
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:51.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1987" for this suite. 04/17/23 22:37:51.87
    STEP: Destroying namespace "webhook-1987-markers" for this suite. 04/17/23 22:37:51.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:51.883
Apr 17 22:37:51.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename deployment 04/17/23 22:37:51.884
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:51.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:51.899
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Apr 17 22:37:51.902: INFO: Creating deployment "webserver-deployment"
Apr 17 22:37:51.907: INFO: Waiting for observed generation 1
Apr 17 22:37:53.913: INFO: Waiting for all required pods to come up
Apr 17 22:37:53.917: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 04/17/23 22:37:53.917
Apr 17 22:37:53.917: INFO: Waiting for deployment "webserver-deployment" to complete
Apr 17 22:37:53.921: INFO: Updating deployment "webserver-deployment" with a non-existent image
Apr 17 22:37:53.929: INFO: Updating deployment webserver-deployment
Apr 17 22:37:53.929: INFO: Waiting for observed generation 2
Apr 17 22:37:55.934: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr 17 22:37:55.936: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr 17 22:37:55.938: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 17 22:37:55.945: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr 17 22:37:55.945: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr 17 22:37:55.947: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Apr 17 22:37:55.952: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Apr 17 22:37:55.952: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Apr 17 22:37:55.961: INFO: Updating deployment webserver-deployment
Apr 17 22:37:55.961: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Apr 17 22:37:55.967: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr 17 22:37:55.969: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 17 22:37:57.981: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-3565  ec8edf1d-e559-477d-8931-4cb8ae91ff9c 77032 3 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004018ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:10,UnavailableReplicas:23,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-17 22:37:55 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-04-17 22:37:57 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,},},ReadyReplicas:10,CollisionCount:nil,},}

Apr 17 22:37:57.984: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-3565  90136ef6-68f7-4165-b42f-3ed33f18cbd8 76861 3 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ec8edf1d-e559-477d-8931-4cb8ae91ff9c 0xc0040193f7 0xc0040193f8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec8edf1d-e559-477d-8931-4cb8ae91ff9c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004019498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:37:57.984: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Apr 17 22:37:57.984: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-3565  b1892abd-9105-4898-aca3-edab71bed74e 77031 3 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ec8edf1d-e559-477d-8931-4cb8ae91ff9c 0xc004019307 0xc004019308}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec8edf1d-e559-477d-8931-4cb8ae91ff9c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004019398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:10,AvailableReplicas:10,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:37:57.991: INFO: Pod "webserver-deployment-7f5969cbc7-24dgt" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-24dgt webserver-deployment-7f5969cbc7- deployment-3565  79dc8001-8745-4f76-9b90-89828aef037e 76982 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:805332b7e113a7bf7deba87447d7b12a5a6142473d2839efb8bbdca60d5eecb6 cni.projectcalico.org/podIP:192.168.208.162/32 cni.projectcalico.org/podIPs:192.168.208.162/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0d3e7 0xc003f0d3e8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhvmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhvmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.991: INFO: Pod "webserver-deployment-7f5969cbc7-2qhzn" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2qhzn webserver-deployment-7f5969cbc7- deployment-3565  d61c1e4f-41cc-4688-8cd2-b88a1315b19c 76944 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c21034aea396f3801d1999ea751c45568e703936342df824eb037df9c3f2e1ab cni.projectcalico.org/podIP:192.168.200.181/32 cni.projectcalico.org/podIPs:192.168.200.181/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0d5f0 0xc003f0d5f1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vv65v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vv65v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.991: INFO: Pod "webserver-deployment-7f5969cbc7-5gt7n" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5gt7n webserver-deployment-7f5969cbc7- deployment-3565  883704ad-450f-4106-9104-f8afe9741925 76986 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:538d18f49012c256cd5a8df43a69b6e8285a583230be179cde0d3c7d80ebf310 cni.projectcalico.org/podIP:192.168.200.183/32 cni.projectcalico.org/podIPs:192.168.200.183/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0d7c7 0xc003f0d7c8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftfqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftfqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.991: INFO: Pod "webserver-deployment-7f5969cbc7-5zwqj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5zwqj webserver-deployment-7f5969cbc7- deployment-3565  7a05a51b-def7-4856-954c-0da844a25c18 76973 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ab9ccfa9516ebab13cc267afeb9db3352abda36ee99499105bedacaa4a57dbbd cni.projectcalico.org/podIP:192.168.200.182/32 cni.projectcalico.org/podIPs:192.168.200.182/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0d9a7 0xc003f0d9a8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rkg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rkg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-9xszg" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9xszg webserver-deployment-7f5969cbc7- deployment-3565  a3c9ea63-ea47-46ca-b88b-2cc285a5e722 77022 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:64d4d42613fa1545a0db399e1c63e5556d9ef42245099eab9d367fa8df3465bc cni.projectcalico.org/podIP:192.168.208.159/32 cni.projectcalico.org/podIPs:192.168.208.159/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0db87 0xc003f0db88}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zcrpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zcrpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-cgpw8" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cgpw8 webserver-deployment-7f5969cbc7- deployment-3565  f67d4840-f705-4961-bb35-ecc1421156be 76915 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5ae53072f6df8c69780c77604a1e2633bc6f8686a2d4ccdfba72d9cc633ba04b cni.projectcalico.org/podIP:192.168.213.12/32 cni.projectcalico.org/podIPs:192.168.213.12/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0dd60 0xc003f0dd61}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tp6w2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tp6w2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-dm822" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dm822 webserver-deployment-7f5969cbc7- deployment-3565  52b483bc-1be9-404d-bfd5-29ef468fda26 76598 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:87f0f51a6b169a628d9b50ca908db56b113498b588dca35b1452a501469a8903 cni.projectcalico.org/podIP:192.168.208.137/32 cni.projectcalico.org/podIPs:192.168.208.137/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0df30 0xc003f0df31}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cbf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cbf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.137,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1afb4c7399479d7d857e441e8a7a027debb741c31607ae765187641446c840ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-hlx9g" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hlx9g webserver-deployment-7f5969cbc7- deployment-3565  91c6eb4d-2b04-4043-8868-46553ac86a2e 76942 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5d16ac8e865815e6a04df08db6ffd15bcc483db8d08d33a3d9838585e250ea69 cni.projectcalico.org/podIP:192.168.208.149/32 cni.projectcalico.org/podIPs:192.168.208.149/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f2120 0xc0044f2121}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wgt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wgt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-jh725" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jh725 webserver-deployment-7f5969cbc7- deployment-3565  1abe7bd7-733a-46c0-a156-6a270b8491b9 76985 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:81c7b83d5d80330844244e3d965212cd5b5530800d550c1703a068bbe08e22e5 cni.projectcalico.org/podIP:192.168.213.20/32 cni.projectcalico.org/podIPs:192.168.213.20/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f22f0 0xc0044f22f1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wf8xf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wf8xf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-jqr58" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jqr58 webserver-deployment-7f5969cbc7- deployment-3565  10728e24-ebba-47b3-bf15-6f0d0b610a7e 76608 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:29092a4325d93343f544d35330ffa2d859998d51193b28a03d8046a75693e3a7 cni.projectcalico.org/podIP:192.168.213.9/32 cni.projectcalico.org/podIPs:192.168.213.9/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f24e0 0xc0044f24e1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8wk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8wk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.9,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://614e05dec9f9b751cdce20dc1cc12a8a52f7b55c2a6b6e53727bbb1d9134bb83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-lzg84" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lzg84 webserver-deployment-7f5969cbc7- deployment-3565  18b95624-0a47-4712-90c1-e476e70a0ec6 76641 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:00aa5a52ea8be2339f671b2e29d0a51929b2514551d879e4d58b8f7ee6096050 cni.projectcalico.org/podIP:192.168.163.216/32 cni.projectcalico.org/podIPs:192.168.163.216/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f26d0 0xc0044f26d1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8chcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8chcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.216,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://752efb64c176a6006dd92fb2e466894a300fbae0af6d0aca4143897d7c4e197e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-ncr6l" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ncr6l webserver-deployment-7f5969cbc7- deployment-3565  eb096b17-1ea3-4dff-8c8d-cd4a8a95483b 77029 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:efa6e10c377657df6f7dd5738a2aaba6c974537d4be31eec36ee2edd4a64a086 cni.projectcalico.org/podIP:192.168.163.215/32 cni.projectcalico.org/podIPs:192.168.163.215/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f28c7 0xc0044f28c8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8b4lr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8b4lr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.215,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fb74ae2b55bc5a8055bce8009ba0e5883d586e2ee9413857bf66a4b450753422,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-pm28p" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pm28p webserver-deployment-7f5969cbc7- deployment-3565  0f7c68e8-6b18-4057-8096-e9aa2635fb01 76578 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2bd2ded5e7097d94111e7a67a978bc6f7871fa2cf51affb8ca107a62a125f46a cni.projectcalico.org/podIP:192.168.163.208/32 cni.projectcalico.org/podIPs:192.168.163.208/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f2ac7 0xc0044f2ac8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwgjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwgjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.208,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://18c75cebb130c5256bc0057dda023c532c19680617de220cce658c0424e403ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-q67fc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q67fc webserver-deployment-7f5969cbc7- deployment-3565  8ae5fee9-0b05-4e0f-9601-4223133e9c48 76604 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e7954e2930f88e113e1e9eb807e6ca78b3339e05ec6a35634398b84a383bd676 cni.projectcalico.org/podIP:192.168.213.6/32 cni.projectcalico.org/podIPs:192.168.213.6/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f2ce7 0xc0044f2ce8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxmmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxmmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.6,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f75081f838705ccacbb36467304fe3dda6d5278e86acc1ca536a5eb3f6101956,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-srsqj" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-srsqj webserver-deployment-7f5969cbc7- deployment-3565  1b849821-5a0e-4ab8-886a-7e365bb7022e 76612 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7265ba1c91673582ed122646b137ab23d4a5078d22941c606aa83d100124a8da cni.projectcalico.org/podIP:192.168.200.177/32 cni.projectcalico.org/podIPs:192.168.200.177/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f2ee0 0xc0044f2ee1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.200.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6ptf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6ptf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:192.168.200.177,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://740d35d69172bd0453c4822963db4e90f41676c8dbe0c534aad884423fa0363d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.200.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-v428b" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v428b webserver-deployment-7f5969cbc7- deployment-3565  d7686479-b6f5-439f-9a35-3767ad7d526e 77025 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7639cf964777e41932273ce880defc27a22e9ce700bc1ed11b9ea1d06ec3042b cni.projectcalico.org/podIP:192.168.163.202/32 cni.projectcalico.org/podIPs:192.168.163.202/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f30d7 0xc0044f30d8}] [] [{calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d222p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d222p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.202,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bdaabf43c525fb6b126f77ed4189bfb4ca1750d4e347307650a656fe9c15657e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-v6khs" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v6khs webserver-deployment-7f5969cbc7- deployment-3565  75811220-3d28-48eb-87f7-d6046c514863 76596 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:889eb4d00dfb59e50b28aa6cc0c7407f73d861350e08bceac514b24295918ebf cni.projectcalico.org/podIP:192.168.208.152/32 cni.projectcalico.org/podIPs:192.168.208.152/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f32c7 0xc0044f32c8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlgcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlgcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.152,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d7703ef0e3582c814723f4854355e6ce3a5cb8fc8d4c1f3581081ae4eef2cba2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-wg2hj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wg2hj webserver-deployment-7f5969cbc7- deployment-3565  f116fb4a-2fbc-4dc4-8698-a95b22301c08 76952 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cd7e9fdf819fa0b063f200e19832af4aa6de338e5cca73ef53e1d35652c4903b cni.projectcalico.org/podIP:192.168.213.15/32 cni.projectcalico.org/podIPs:192.168.213.15/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f34c0 0xc0044f34c1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kdq9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kdq9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-7f5969cbc7-xm56p" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xm56p webserver-deployment-7f5969cbc7- deployment-3565  8e943fae-abeb-4f31-a934-8e810c50f9cb 76914 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b83bea129225774bf0c9debdcf30dc5ce191693923cc612852c2803c6f9f41ae cni.projectcalico.org/podIP:192.168.200.180/32 cni.projectcalico.org/podIPs:192.168.200.180/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f36a0 0xc0044f36a1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qkzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qkzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-7f5969cbc7-zvtqb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zvtqb webserver-deployment-7f5969cbc7- deployment-3565  bb8b7909-d8c7-4e0c-afec-2c02dd648481 76640 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f029a913c51c78471c6a8d731084a6862770073dd8998d75405ca0882c19b807 cni.projectcalico.org/podIP:192.168.163.203/32 cni.projectcalico.org/podIPs:192.168.163.203/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f3897 0xc0044f3898}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjt75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjt75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.203,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://248918ef9936c27b8b4ac6e8bddb7faee5f3073efffa3cc32e4981ad344bd0e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-282q6" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-282q6 webserver-deployment-d9f79cb5- deployment-3565  34cdff6c-5648-4a92-9c33-16331758a847 76726 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6656aa96e729421c1bab14f22bfbbe2a2e320eb475209054dedea791e2c6f1a8 cni.projectcalico.org/podIP:192.168.213.17/32 cni.projectcalico.org/podIPs:192.168.213.17/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc0044f3a97 0xc0044f3a98}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5s8s5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5s8s5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-2gz4g" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2gz4g webserver-deployment-d9f79cb5- deployment-3565  d515992c-4632-40f4-b249-b64b3d1ca540 77024 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0729d64abc5753c5e417a2ed5ecdaefe1dfee386c39f6e76da3530d431b691e5 cni.projectcalico.org/podIP:192.168.213.16/32 cni.projectcalico.org/podIPs:192.168.213.16/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc0044f3caf 0xc0044f3cc0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76zgk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76zgk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-485pj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-485pj webserver-deployment-d9f79cb5- deployment-3565  b6eb5156-ede9-4e8d-9852-d914845115d8 76920 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f61b693064aeed1b35510538fee9e70c6e99861da950746c0a47ef7c84743056 cni.projectcalico.org/podIP:192.168.208.140/32 cni.projectcalico.org/podIPs:192.168.208.140/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc0044f3e9f 0xc0044f3eb0}] [] [{calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rjkdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rjkdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-62bc7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-62bc7 webserver-deployment-d9f79cb5- deployment-3565  3d52b706-0a13-4ddf-8c1f-9d3630ea8e0d 76857 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee608f 0xc003ee60a0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stvbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stvbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-8b7nx" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8b7nx webserver-deployment-d9f79cb5- deployment-3565  b3251a43-29bb-4c87-840c-68f5ba9045b0 76886 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d22b6b9c94d767e5aaf481df02a050b39ad9f5207ab9a66470b8850317a73936 cni.projectcalico.org/podIP:192.168.163.220/32 cni.projectcalico.org/podIPs:192.168.163.220/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee625f 0xc003ee6270}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmg2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmg2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-cdsjk" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cdsjk webserver-deployment-d9f79cb5- deployment-3565  c7f54406-bdd1-469f-a456-efd3a3eb1863 76987 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fb9e344d373014c9135c0ed24495ee8c8bfca8bb673459f8882021722b79ed4a cni.projectcalico.org/podIP:192.168.208.147/32 cni.projectcalico.org/podIPs:192.168.208.147/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee6477 0xc003ee6478}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ct5kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ct5kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-dpprw" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dpprw webserver-deployment-d9f79cb5- deployment-3565  5dea926b-77bd-4092-9d52-dadcd22780c7 77021 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f63594be7382dc2b4d7afce60e9c235e40f422ecd8f0be0ec88a80b171728788 cni.projectcalico.org/podIP:192.168.200.184/32 cni.projectcalico.org/podIPs:192.168.200.184/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee665f 0xc003ee6670}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hft7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hft7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-dshp7" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dshp7 webserver-deployment-d9f79cb5- deployment-3565  d61da424-4d08-47e4-b976-325fb1f0d16e 77033 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:5cc62e94aca87cabfe6987c2a740ec8a406c7b75e34bdb688976abfc55b07343 cni.projectcalico.org/podIP:192.168.163.207/32 cni.projectcalico.org/podIPs:192.168.163.207/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee684f 0xc003ee6860}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvv6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvv6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.207,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-f2vjd" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f2vjd webserver-deployment-d9f79cb5- deployment-3565  8f5b677b-21b8-4878-a200-340adac8a562 77020 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:af9fe2b6f31efb8c5510985eb0156c275187763d23de29813142229cdb046459 cni.projectcalico.org/podIP:192.168.213.37/32 cni.projectcalico.org/podIPs:192.168.213.37/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee6a87 0xc003ee6a88}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gft4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gft4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-fwtns" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fwtns webserver-deployment-d9f79cb5- deployment-3565  8aedada2-1b82-4dc8-8d0a-ceab6aed817d 76747 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:20f5f478c7e6dbcd1e19344ece15bec899a673f1c76fec9ed92caf790ce4ad4f cni.projectcalico.org/podIP:192.168.213.22/32 cni.projectcalico.org/podIPs:192.168.213.22/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee6c6f 0xc003ee6c80}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6dlzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6dlzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.22,StartTime:2023-04-17 22:37:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-gp8rr" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gp8rr webserver-deployment-d9f79cb5- deployment-3565  853c1fbd-440e-4f11-8b62-0363a6ba6334 76995 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:212984aa36eca22ebb978dd2e37d62cc9161e5bc01928dc16036fb75843f0058 cni.projectcalico.org/podIP:192.168.208.143/32 cni.projectcalico.org/podIPs:192.168.208.143/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee6e8f 0xc003ee6ea0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s7k9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s7k9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.143,StartTime:2023-04-17 22:37:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-h6jjj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-h6jjj webserver-deployment-d9f79cb5- deployment-3565  64395785-d594-44f6-861e-1a81474850d0 77044 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:14e071a7c8454d06effa80e79539cddccd6aab1a995f91a2d0f9cd2fad880c25 cni.projectcalico.org/podIP:192.168.200.178/32 cni.projectcalico.org/podIPs:192.168.200.178/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee70af 0xc003ee70c0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.200.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zl6dg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zl6dg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:192.168.200.178,StartTime:2023-04-17 22:37:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.200.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-s6dpz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s6dpz webserver-deployment-d9f79cb5- deployment-3565  5fa87514-b09f-42d3-aaf8-a30614737ed0 76767 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fa0cda0076a65f97acbced2d1af5b791498d551e9e2fc0bc311146faec7901a8 cni.projectcalico.org/podIP:192.168.163.213/32 cni.projectcalico.org/podIPs:192.168.163.213/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee72cf 0xc003ee72e0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-56n5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-56n5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.213,StartTime:2023-04-17 22:37:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:57.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3565" for this suite. 04/17/23 22:37:58
------------------------------
• [SLOW TEST] [6.122 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:51.883
    Apr 17 22:37:51.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename deployment 04/17/23 22:37:51.884
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:51.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:51.899
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Apr 17 22:37:51.902: INFO: Creating deployment "webserver-deployment"
    Apr 17 22:37:51.907: INFO: Waiting for observed generation 1
    Apr 17 22:37:53.913: INFO: Waiting for all required pods to come up
    Apr 17 22:37:53.917: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 04/17/23 22:37:53.917
    Apr 17 22:37:53.917: INFO: Waiting for deployment "webserver-deployment" to complete
    Apr 17 22:37:53.921: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Apr 17 22:37:53.929: INFO: Updating deployment webserver-deployment
    Apr 17 22:37:53.929: INFO: Waiting for observed generation 2
    Apr 17 22:37:55.934: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Apr 17 22:37:55.936: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Apr 17 22:37:55.938: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Apr 17 22:37:55.945: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Apr 17 22:37:55.945: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Apr 17 22:37:55.947: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Apr 17 22:37:55.952: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Apr 17 22:37:55.952: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Apr 17 22:37:55.961: INFO: Updating deployment webserver-deployment
    Apr 17 22:37:55.961: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Apr 17 22:37:55.967: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Apr 17 22:37:55.969: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 17 22:37:57.981: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-3565  ec8edf1d-e559-477d-8931-4cb8ae91ff9c 77032 3 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004018ee8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:10,UnavailableReplicas:23,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-04-17 22:37:55 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-04-17 22:37:57 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,},},ReadyReplicas:10,CollisionCount:nil,},}

    Apr 17 22:37:57.984: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-3565  90136ef6-68f7-4165-b42f-3ed33f18cbd8 76861 3 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ec8edf1d-e559-477d-8931-4cb8ae91ff9c 0xc0040193f7 0xc0040193f8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec8edf1d-e559-477d-8931-4cb8ae91ff9c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004019498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 22:37:57.984: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Apr 17 22:37:57.984: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-3565  b1892abd-9105-4898-aca3-edab71bed74e 77031 3 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ec8edf1d-e559-477d-8931-4cb8ae91ff9c 0xc004019307 0xc004019308}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec8edf1d-e559-477d-8931-4cb8ae91ff9c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004019398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:10,AvailableReplicas:10,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 22:37:57.991: INFO: Pod "webserver-deployment-7f5969cbc7-24dgt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-24dgt webserver-deployment-7f5969cbc7- deployment-3565  79dc8001-8745-4f76-9b90-89828aef037e 76982 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:805332b7e113a7bf7deba87447d7b12a5a6142473d2839efb8bbdca60d5eecb6 cni.projectcalico.org/podIP:192.168.208.162/32 cni.projectcalico.org/podIPs:192.168.208.162/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0d3e7 0xc003f0d3e8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xhvmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xhvmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.991: INFO: Pod "webserver-deployment-7f5969cbc7-2qhzn" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2qhzn webserver-deployment-7f5969cbc7- deployment-3565  d61c1e4f-41cc-4688-8cd2-b88a1315b19c 76944 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:c21034aea396f3801d1999ea751c45568e703936342df824eb037df9c3f2e1ab cni.projectcalico.org/podIP:192.168.200.181/32 cni.projectcalico.org/podIPs:192.168.200.181/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0d5f0 0xc003f0d5f1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vv65v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vv65v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.991: INFO: Pod "webserver-deployment-7f5969cbc7-5gt7n" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5gt7n webserver-deployment-7f5969cbc7- deployment-3565  883704ad-450f-4106-9104-f8afe9741925 76986 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:538d18f49012c256cd5a8df43a69b6e8285a583230be179cde0d3c7d80ebf310 cni.projectcalico.org/podIP:192.168.200.183/32 cni.projectcalico.org/podIPs:192.168.200.183/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0d7c7 0xc003f0d7c8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ftfqq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ftfqq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.991: INFO: Pod "webserver-deployment-7f5969cbc7-5zwqj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5zwqj webserver-deployment-7f5969cbc7- deployment-3565  7a05a51b-def7-4856-954c-0da844a25c18 76973 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ab9ccfa9516ebab13cc267afeb9db3352abda36ee99499105bedacaa4a57dbbd cni.projectcalico.org/podIP:192.168.200.182/32 cni.projectcalico.org/podIPs:192.168.200.182/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0d9a7 0xc003f0d9a8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8rkg4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8rkg4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-9xszg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9xszg webserver-deployment-7f5969cbc7- deployment-3565  a3c9ea63-ea47-46ca-b88b-2cc285a5e722 77022 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:64d4d42613fa1545a0db399e1c63e5556d9ef42245099eab9d367fa8df3465bc cni.projectcalico.org/podIP:192.168.208.159/32 cni.projectcalico.org/podIPs:192.168.208.159/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0db87 0xc003f0db88}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zcrpp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zcrpp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-cgpw8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-cgpw8 webserver-deployment-7f5969cbc7- deployment-3565  f67d4840-f705-4961-bb35-ecc1421156be 76915 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5ae53072f6df8c69780c77604a1e2633bc6f8686a2d4ccdfba72d9cc633ba04b cni.projectcalico.org/podIP:192.168.213.12/32 cni.projectcalico.org/podIPs:192.168.213.12/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0dd60 0xc003f0dd61}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tp6w2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tp6w2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-dm822" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-dm822 webserver-deployment-7f5969cbc7- deployment-3565  52b483bc-1be9-404d-bfd5-29ef468fda26 76598 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:87f0f51a6b169a628d9b50ca908db56b113498b588dca35b1452a501469a8903 cni.projectcalico.org/podIP:192.168.208.137/32 cni.projectcalico.org/podIPs:192.168.208.137/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc003f0df30 0xc003f0df31}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8cbf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8cbf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.137,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1afb4c7399479d7d857e441e8a7a027debb741c31607ae765187641446c840ba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.137,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-hlx9g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-hlx9g webserver-deployment-7f5969cbc7- deployment-3565  91c6eb4d-2b04-4043-8868-46553ac86a2e 76942 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5d16ac8e865815e6a04df08db6ffd15bcc483db8d08d33a3d9838585e250ea69 cni.projectcalico.org/podIP:192.168.208.149/32 cni.projectcalico.org/podIPs:192.168.208.149/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f2120 0xc0044f2121}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2wgt9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2wgt9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-jh725" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jh725 webserver-deployment-7f5969cbc7- deployment-3565  1abe7bd7-733a-46c0-a156-6a270b8491b9 76985 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:81c7b83d5d80330844244e3d965212cd5b5530800d550c1703a068bbe08e22e5 cni.projectcalico.org/podIP:192.168.213.20/32 cni.projectcalico.org/podIPs:192.168.213.20/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f22f0 0xc0044f22f1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wf8xf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wf8xf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-jqr58" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jqr58 webserver-deployment-7f5969cbc7- deployment-3565  10728e24-ebba-47b3-bf15-6f0d0b610a7e 76608 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:29092a4325d93343f544d35330ffa2d859998d51193b28a03d8046a75693e3a7 cni.projectcalico.org/podIP:192.168.213.9/32 cni.projectcalico.org/podIPs:192.168.213.9/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f24e0 0xc0044f24e1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.9\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v8wk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v8wk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.9,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://614e05dec9f9b751cdce20dc1cc12a8a52f7b55c2a6b6e53727bbb1d9134bb83,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.9,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.992: INFO: Pod "webserver-deployment-7f5969cbc7-lzg84" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-lzg84 webserver-deployment-7f5969cbc7- deployment-3565  18b95624-0a47-4712-90c1-e476e70a0ec6 76641 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:00aa5a52ea8be2339f671b2e29d0a51929b2514551d879e4d58b8f7ee6096050 cni.projectcalico.org/podIP:192.168.163.216/32 cni.projectcalico.org/podIPs:192.168.163.216/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f26d0 0xc0044f26d1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8chcd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8chcd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.216,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://752efb64c176a6006dd92fb2e466894a300fbae0af6d0aca4143897d7c4e197e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-ncr6l" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-ncr6l webserver-deployment-7f5969cbc7- deployment-3565  eb096b17-1ea3-4dff-8c8d-cd4a8a95483b 77029 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:efa6e10c377657df6f7dd5738a2aaba6c974537d4be31eec36ee2edd4a64a086 cni.projectcalico.org/podIP:192.168.163.215/32 cni.projectcalico.org/podIPs:192.168.163.215/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f28c7 0xc0044f28c8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8b4lr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8b4lr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.215,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fb74ae2b55bc5a8055bce8009ba0e5883d586e2ee9413857bf66a4b450753422,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-pm28p" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-pm28p webserver-deployment-7f5969cbc7- deployment-3565  0f7c68e8-6b18-4057-8096-e9aa2635fb01 76578 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:2bd2ded5e7097d94111e7a67a978bc6f7871fa2cf51affb8ca107a62a125f46a cni.projectcalico.org/podIP:192.168.163.208/32 cni.projectcalico.org/podIPs:192.168.163.208/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f2ac7 0xc0044f2ac8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwgjk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwgjk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.208,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://18c75cebb130c5256bc0057dda023c532c19680617de220cce658c0424e403ff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-q67fc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q67fc webserver-deployment-7f5969cbc7- deployment-3565  8ae5fee9-0b05-4e0f-9601-4223133e9c48 76604 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:e7954e2930f88e113e1e9eb807e6ca78b3339e05ec6a35634398b84a383bd676 cni.projectcalico.org/podIP:192.168.213.6/32 cni.projectcalico.org/podIPs:192.168.213.6/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f2ce7 0xc0044f2ce8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sxmmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sxmmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.6,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f75081f838705ccacbb36467304fe3dda6d5278e86acc1ca536a5eb3f6101956,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.6,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-srsqj" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-srsqj webserver-deployment-7f5969cbc7- deployment-3565  1b849821-5a0e-4ab8-886a-7e365bb7022e 76612 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7265ba1c91673582ed122646b137ab23d4a5078d22941c606aa83d100124a8da cni.projectcalico.org/podIP:192.168.200.177/32 cni.projectcalico.org/podIPs:192.168.200.177/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f2ee0 0xc0044f2ee1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.200.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6ptf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6ptf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:192.168.200.177,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://740d35d69172bd0453c4822963db4e90f41676c8dbe0c534aad884423fa0363d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.200.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-v428b" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v428b webserver-deployment-7f5969cbc7- deployment-3565  d7686479-b6f5-439f-9a35-3767ad7d526e 77025 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:7639cf964777e41932273ce880defc27a22e9ce700bc1ed11b9ea1d06ec3042b cni.projectcalico.org/podIP:192.168.163.202/32 cni.projectcalico.org/podIPs:192.168.163.202/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f30d7 0xc0044f30d8}] [] [{calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.202\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d222p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d222p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.202,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://bdaabf43c525fb6b126f77ed4189bfb4ca1750d4e347307650a656fe9c15657e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.202,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-v6khs" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-v6khs webserver-deployment-7f5969cbc7- deployment-3565  75811220-3d28-48eb-87f7-d6046c514863 76596 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:889eb4d00dfb59e50b28aa6cc0c7407f73d861350e08bceac514b24295918ebf cni.projectcalico.org/podIP:192.168.208.152/32 cni.projectcalico.org/podIPs:192.168.208.152/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f32c7 0xc0044f32c8}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.152\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zlgcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zlgcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.152,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d7703ef0e3582c814723f4854355e6ce3a5cb8fc8d4c1f3581081ae4eef2cba2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.152,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.993: INFO: Pod "webserver-deployment-7f5969cbc7-wg2hj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-wg2hj webserver-deployment-7f5969cbc7- deployment-3565  f116fb4a-2fbc-4dc4-8698-a95b22301c08 76952 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:cd7e9fdf819fa0b063f200e19832af4aa6de338e5cca73ef53e1d35652c4903b cni.projectcalico.org/podIP:192.168.213.15/32 cni.projectcalico.org/podIPs:192.168.213.15/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f34c0 0xc0044f34c1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kdq9k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kdq9k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-7f5969cbc7-xm56p" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xm56p webserver-deployment-7f5969cbc7- deployment-3565  8e943fae-abeb-4f31-a934-8e810c50f9cb 76914 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:b83bea129225774bf0c9debdcf30dc5ce191693923cc612852c2803c6f9f41ae cni.projectcalico.org/podIP:192.168.200.180/32 cni.projectcalico.org/podIPs:192.168.200.180/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f36a0 0xc0044f36a1}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8qkzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8qkzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-7f5969cbc7-zvtqb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zvtqb webserver-deployment-7f5969cbc7- deployment-3565  bb8b7909-d8c7-4e0c-afec-2c02dd648481 76640 0 2023-04-17 22:37:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f029a913c51c78471c6a8d731084a6862770073dd8998d75405ca0882c19b807 cni.projectcalico.org/podIP:192.168.163.203/32 cni.projectcalico.org/podIPs:192.168.163.203/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 b1892abd-9105-4898-aca3-edab71bed74e 0xc0044f3897 0xc0044f3898}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1892abd-9105-4898-aca3-edab71bed74e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cjt75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cjt75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.203,StartTime:2023-04-17 22:37:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:37:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://248918ef9936c27b8b4ac6e8bddb7faee5f3073efffa3cc32e4981ad344bd0e8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.203,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-282q6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-282q6 webserver-deployment-d9f79cb5- deployment-3565  34cdff6c-5648-4a92-9c33-16331758a847 76726 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:6656aa96e729421c1bab14f22bfbbe2a2e320eb475209054dedea791e2c6f1a8 cni.projectcalico.org/podIP:192.168.213.17/32 cni.projectcalico.org/podIPs:192.168.213.17/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc0044f3a97 0xc0044f3a98}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5s8s5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5s8s5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-2gz4g" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-2gz4g webserver-deployment-d9f79cb5- deployment-3565  d515992c-4632-40f4-b249-b64b3d1ca540 77024 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:0729d64abc5753c5e417a2ed5ecdaefe1dfee386c39f6e76da3530d431b691e5 cni.projectcalico.org/podIP:192.168.213.16/32 cni.projectcalico.org/podIPs:192.168.213.16/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc0044f3caf 0xc0044f3cc0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-76zgk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-76zgk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-485pj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-485pj webserver-deployment-d9f79cb5- deployment-3565  b6eb5156-ede9-4e8d-9852-d914845115d8 76920 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f61b693064aeed1b35510538fee9e70c6e99861da950746c0a47ef7c84743056 cni.projectcalico.org/podIP:192.168.208.140/32 cni.projectcalico.org/podIPs:192.168.208.140/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc0044f3e9f 0xc0044f3eb0}] [] [{calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rjkdt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rjkdt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-62bc7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-62bc7 webserver-deployment-d9f79cb5- deployment-3565  3d52b706-0a13-4ddf-8c1f-9d3630ea8e0d 76857 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee608f 0xc003ee60a0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-stvbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-stvbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-8b7nx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-8b7nx webserver-deployment-d9f79cb5- deployment-3565  b3251a43-29bb-4c87-840c-68f5ba9045b0 76886 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:d22b6b9c94d767e5aaf481df02a050b39ad9f5207ab9a66470b8850317a73936 cni.projectcalico.org/podIP:192.168.163.220/32 cni.projectcalico.org/podIPs:192.168.163.220/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee625f 0xc003ee6270}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmg2l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmg2l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.994: INFO: Pod "webserver-deployment-d9f79cb5-cdsjk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-cdsjk webserver-deployment-d9f79cb5- deployment-3565  c7f54406-bdd1-469f-a456-efd3a3eb1863 76987 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fb9e344d373014c9135c0ed24495ee8c8bfca8bb673459f8882021722b79ed4a cni.projectcalico.org/podIP:192.168.208.147/32 cni.projectcalico.org/podIPs:192.168.208.147/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee6477 0xc003ee6478}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ct5kk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ct5kk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-dpprw" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dpprw webserver-deployment-d9f79cb5- deployment-3565  5dea926b-77bd-4092-9d52-dadcd22780c7 77021 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:f63594be7382dc2b4d7afce60e9c235e40f422ecd8f0be0ec88a80b171728788 cni.projectcalico.org/podIP:192.168.200.184/32 cni.projectcalico.org/podIPs:192.168.200.184/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee665f 0xc003ee6670}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hft7h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hft7h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:,StartTime:2023-04-17 22:37:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-dshp7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-dshp7 webserver-deployment-d9f79cb5- deployment-3565  d61da424-4d08-47e4-b976-325fb1f0d16e 77033 0 2023-04-17 22:37:55 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:5cc62e94aca87cabfe6987c2a740ec8a406c7b75e34bdb688976abfc55b07343 cni.projectcalico.org/podIP:192.168.163.207/32 cni.projectcalico.org/podIPs:192.168.163.207/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee684f 0xc003ee6860}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.207\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gvv6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gvv6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.207,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.207,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-f2vjd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f2vjd webserver-deployment-d9f79cb5- deployment-3565  8f5b677b-21b8-4878-a200-340adac8a562 77020 0 2023-04-17 22:37:56 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:af9fe2b6f31efb8c5510985eb0156c275187763d23de29813142229cdb046459 cni.projectcalico.org/podIP:192.168.213.37/32 cni.projectcalico.org/podIPs:192.168.213.37/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee6a87 0xc003ee6a88}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gft4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gft4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:,StartTime:2023-04-17 22:37:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-fwtns" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fwtns webserver-deployment-d9f79cb5- deployment-3565  8aedada2-1b82-4dc8-8d0a-ceab6aed817d 76747 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:20f5f478c7e6dbcd1e19344ece15bec899a673f1c76fec9ed92caf790ce4ad4f cni.projectcalico.org/podIP:192.168.213.22/32 cni.projectcalico.org/podIPs:192.168.213.22/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee6c6f 0xc003ee6c80}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.213.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6dlzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6dlzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-93-18.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.93.18,PodIP:192.168.213.22,StartTime:2023-04-17 22:37:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.213.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-gp8rr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-gp8rr webserver-deployment-d9f79cb5- deployment-3565  853c1fbd-440e-4f11-8b62-0363a6ba6334 76995 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:212984aa36eca22ebb978dd2e37d62cc9161e5bc01928dc16036fb75843f0058 cni.projectcalico.org/podIP:192.168.208.143/32 cni.projectcalico.org/podIPs:192.168.208.143/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee6e8f 0xc003ee6ea0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s7k9v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s7k9v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.143,StartTime:2023-04-17 22:37:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-h6jjj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-h6jjj webserver-deployment-d9f79cb5- deployment-3565  64395785-d594-44f6-861e-1a81474850d0 77044 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:14e071a7c8454d06effa80e79539cddccd6aab1a995f91a2d0f9cd2fad880c25 cni.projectcalico.org/podIP:192.168.200.178/32 cni.projectcalico.org/podIPs:192.168.200.178/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee70af 0xc003ee70c0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.200.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zl6dg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zl6dg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-64-189.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.64.189,PodIP:192.168.200.178,StartTime:2023-04-17 22:37:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.200.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Apr 17 22:37:57.995: INFO: Pod "webserver-deployment-d9f79cb5-s6dpz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-s6dpz webserver-deployment-d9f79cb5- deployment-3565  5fa87514-b09f-42d3-aaf8-a30614737ed0 76767 0 2023-04-17 22:37:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:fa0cda0076a65f97acbced2d1af5b791498d551e9e2fc0bc311146faec7901a8 cni.projectcalico.org/podIP:192.168.163.213/32 cni.projectcalico.org/podIPs:192.168.163.213/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 90136ef6-68f7-4165-b42f-3ed33f18cbd8 0xc003ee72cf 0xc003ee72e0}] [] [{kube-controller-manager Update v1 2023-04-17 22:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"90136ef6-68f7-4165-b42f-3ed33f18cbd8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:37:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.163.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-56n5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-56n5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-106-231.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:37:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.106.231,PodIP:192.168.163.213,StartTime:2023-04-17 22:37:53 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.163.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:57.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3565" for this suite. 04/17/23 22:37:58
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:58.006
Apr 17 22:37:58.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename ingress 04/17/23 22:37:58.007
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:58.021
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:58.023
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 04/17/23 22:37:58.025
STEP: getting /apis/networking.k8s.io 04/17/23 22:37:58.028
STEP: getting /apis/networking.k8s.iov1 04/17/23 22:37:58.029
STEP: creating 04/17/23 22:37:58.03
STEP: getting 04/17/23 22:37:58.044
STEP: listing 04/17/23 22:37:58.046
STEP: watching 04/17/23 22:37:58.048
Apr 17 22:37:58.048: INFO: starting watch
STEP: cluster-wide listing 04/17/23 22:37:58.049
STEP: cluster-wide watching 04/17/23 22:37:58.051
Apr 17 22:37:58.051: INFO: starting watch
STEP: patching 04/17/23 22:37:58.052
STEP: updating 04/17/23 22:37:58.056
Apr 17 22:37:58.061: INFO: waiting for watch events with expected annotations
Apr 17 22:37:58.061: INFO: saw patched and updated annotations
STEP: patching /status 04/17/23 22:37:58.061
STEP: updating /status 04/17/23 22:37:58.066
STEP: get /status 04/17/23 22:37:58.072
STEP: deleting 04/17/23 22:37:58.074
STEP: deleting a collection 04/17/23 22:37:58.082
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Apr 17 22:37:58.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-2837" for this suite. 04/17/23 22:37:58.096
------------------------------
• [0.094 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:58.006
    Apr 17 22:37:58.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename ingress 04/17/23 22:37:58.007
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:58.021
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:58.023
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 04/17/23 22:37:58.025
    STEP: getting /apis/networking.k8s.io 04/17/23 22:37:58.028
    STEP: getting /apis/networking.k8s.iov1 04/17/23 22:37:58.029
    STEP: creating 04/17/23 22:37:58.03
    STEP: getting 04/17/23 22:37:58.044
    STEP: listing 04/17/23 22:37:58.046
    STEP: watching 04/17/23 22:37:58.048
    Apr 17 22:37:58.048: INFO: starting watch
    STEP: cluster-wide listing 04/17/23 22:37:58.049
    STEP: cluster-wide watching 04/17/23 22:37:58.051
    Apr 17 22:37:58.051: INFO: starting watch
    STEP: patching 04/17/23 22:37:58.052
    STEP: updating 04/17/23 22:37:58.056
    Apr 17 22:37:58.061: INFO: waiting for watch events with expected annotations
    Apr 17 22:37:58.061: INFO: saw patched and updated annotations
    STEP: patching /status 04/17/23 22:37:58.061
    STEP: updating /status 04/17/23 22:37:58.066
    STEP: get /status 04/17/23 22:37:58.072
    STEP: deleting 04/17/23 22:37:58.074
    STEP: deleting a collection 04/17/23 22:37:58.082
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:37:58.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-2837" for this suite. 04/17/23 22:37:58.096
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:37:58.101
Apr 17 22:37:58.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-probe 04/17/23 22:37:58.102
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:58.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:58.116
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Apr 17 22:37:58.124: INFO: Waiting up to 5m0s for pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60" in namespace "container-probe-7269" to be "running and ready"
Apr 17 22:37:58.126: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1348ms
Apr 17 22:37:58.126: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:38:00.130: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 2.006058705s
Apr 17 22:38:00.130: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:02.130: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 4.005936274s
Apr 17 22:38:02.130: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:04.141: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 6.017316778s
Apr 17 22:38:04.141: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:06.132: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 8.008118659s
Apr 17 22:38:06.132: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:08.129: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 10.005343332s
Apr 17 22:38:08.129: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:10.129: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 12.005318595s
Apr 17 22:38:10.129: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:12.129: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 14.005129972s
Apr 17 22:38:12.129: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:14.129: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 16.005251942s
Apr 17 22:38:14.129: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:16.130: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 18.005997541s
Apr 17 22:38:16.130: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:18.130: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 20.005819437s
Apr 17 22:38:18.130: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
Apr 17 22:38:20.131: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=true. Elapsed: 22.006684932s
Apr 17 22:38:20.131: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = true)
Apr 17 22:38:20.131: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60" satisfied condition "running and ready"
Apr 17 22:38:20.133: INFO: Container started at 2023-04-17 22:37:59 +0000 UTC, pod became ready at 2023-04-17 22:38:18 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 17 22:38:20.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7269" for this suite. 04/17/23 22:38:20.139
------------------------------
• [SLOW TEST] [22.044 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:37:58.101
    Apr 17 22:37:58.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-probe 04/17/23 22:37:58.102
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:37:58.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:37:58.116
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Apr 17 22:37:58.124: INFO: Waiting up to 5m0s for pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60" in namespace "container-probe-7269" to be "running and ready"
    Apr 17 22:37:58.126: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1348ms
    Apr 17 22:37:58.126: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:38:00.130: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 2.006058705s
    Apr 17 22:38:00.130: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:02.130: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 4.005936274s
    Apr 17 22:38:02.130: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:04.141: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 6.017316778s
    Apr 17 22:38:04.141: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:06.132: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 8.008118659s
    Apr 17 22:38:06.132: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:08.129: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 10.005343332s
    Apr 17 22:38:08.129: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:10.129: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 12.005318595s
    Apr 17 22:38:10.129: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:12.129: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 14.005129972s
    Apr 17 22:38:12.129: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:14.129: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 16.005251942s
    Apr 17 22:38:14.129: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:16.130: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 18.005997541s
    Apr 17 22:38:16.130: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:18.130: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=false. Elapsed: 20.005819437s
    Apr 17 22:38:18.130: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = false)
    Apr 17 22:38:20.131: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60": Phase="Running", Reason="", readiness=true. Elapsed: 22.006684932s
    Apr 17 22:38:20.131: INFO: The phase of Pod test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60 is Running (Ready = true)
    Apr 17 22:38:20.131: INFO: Pod "test-webserver-0ab3e04e-ec35-47fd-a6b6-f0a52f079c60" satisfied condition "running and ready"
    Apr 17 22:38:20.133: INFO: Container started at 2023-04-17 22:37:59 +0000 UTC, pod became ready at 2023-04-17 22:38:18 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:38:20.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7269" for this suite. 04/17/23 22:38:20.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:38:20.145
Apr 17 22:38:20.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename cronjob 04/17/23 22:38:20.146
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:20.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:20.16
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 04/17/23 22:38:20.162
STEP: creating 04/17/23 22:38:20.163
STEP: getting 04/17/23 22:38:20.167
STEP: listing 04/17/23 22:38:20.169
STEP: watching 04/17/23 22:38:20.171
Apr 17 22:38:20.171: INFO: starting watch
STEP: cluster-wide listing 04/17/23 22:38:20.172
STEP: cluster-wide watching 04/17/23 22:38:20.174
Apr 17 22:38:20.174: INFO: starting watch
STEP: patching 04/17/23 22:38:20.175
STEP: updating 04/17/23 22:38:20.179
Apr 17 22:38:20.185: INFO: waiting for watch events with expected annotations
Apr 17 22:38:20.185: INFO: saw patched and updated annotations
STEP: patching /status 04/17/23 22:38:20.185
STEP: updating /status 04/17/23 22:38:20.19
STEP: get /status 04/17/23 22:38:20.196
STEP: deleting 04/17/23 22:38:20.198
STEP: deleting a collection 04/17/23 22:38:20.21
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Apr 17 22:38:20.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8244" for this suite. 04/17/23 22:38:20.222
------------------------------
• [0.082 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:38:20.145
    Apr 17 22:38:20.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename cronjob 04/17/23 22:38:20.146
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:20.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:20.16
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 04/17/23 22:38:20.162
    STEP: creating 04/17/23 22:38:20.163
    STEP: getting 04/17/23 22:38:20.167
    STEP: listing 04/17/23 22:38:20.169
    STEP: watching 04/17/23 22:38:20.171
    Apr 17 22:38:20.171: INFO: starting watch
    STEP: cluster-wide listing 04/17/23 22:38:20.172
    STEP: cluster-wide watching 04/17/23 22:38:20.174
    Apr 17 22:38:20.174: INFO: starting watch
    STEP: patching 04/17/23 22:38:20.175
    STEP: updating 04/17/23 22:38:20.179
    Apr 17 22:38:20.185: INFO: waiting for watch events with expected annotations
    Apr 17 22:38:20.185: INFO: saw patched and updated annotations
    STEP: patching /status 04/17/23 22:38:20.185
    STEP: updating /status 04/17/23 22:38:20.19
    STEP: get /status 04/17/23 22:38:20.196
    STEP: deleting 04/17/23 22:38:20.198
    STEP: deleting a collection 04/17/23 22:38:20.21
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:38:20.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8244" for this suite. 04/17/23 22:38:20.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:38:20.228
Apr 17 22:38:20.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:38:20.229
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:20.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:20.248
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-033192e0-d3b2-425d-bb78-64911c029862 04/17/23 22:38:20.252
STEP: Creating a pod to test consume secrets 04/17/23 22:38:20.256
Apr 17 22:38:20.263: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558" in namespace "projected-9408" to be "Succeeded or Failed"
Apr 17 22:38:20.266: INFO: Pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558": Phase="Pending", Reason="", readiness=false. Elapsed: 3.095181ms
Apr 17 22:38:22.270: INFO: Pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00700213s
Apr 17 22:38:24.270: INFO: Pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006987347s
STEP: Saw pod success 04/17/23 22:38:24.27
Apr 17 22:38:24.270: INFO: Pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558" satisfied condition "Succeeded or Failed"
Apr 17 22:38:24.273: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558 container secret-volume-test: <nil>
STEP: delete the pod 04/17/23 22:38:24.277
Apr 17 22:38:24.287: INFO: Waiting for pod pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558 to disappear
Apr 17 22:38:24.289: INFO: Pod pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Apr 17 22:38:24.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9408" for this suite. 04/17/23 22:38:24.294
------------------------------
• [4.071 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:38:20.228
    Apr 17 22:38:20.228: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:38:20.229
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:20.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:20.248
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-033192e0-d3b2-425d-bb78-64911c029862 04/17/23 22:38:20.252
    STEP: Creating a pod to test consume secrets 04/17/23 22:38:20.256
    Apr 17 22:38:20.263: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558" in namespace "projected-9408" to be "Succeeded or Failed"
    Apr 17 22:38:20.266: INFO: Pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558": Phase="Pending", Reason="", readiness=false. Elapsed: 3.095181ms
    Apr 17 22:38:22.270: INFO: Pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00700213s
    Apr 17 22:38:24.270: INFO: Pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006987347s
    STEP: Saw pod success 04/17/23 22:38:24.27
    Apr 17 22:38:24.270: INFO: Pod "pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558" satisfied condition "Succeeded or Failed"
    Apr 17 22:38:24.273: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558 container secret-volume-test: <nil>
    STEP: delete the pod 04/17/23 22:38:24.277
    Apr 17 22:38:24.287: INFO: Waiting for pod pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558 to disappear
    Apr 17 22:38:24.289: INFO: Pod pod-projected-secrets-3138cb38-573a-4772-acee-369ab2bcd558 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:38:24.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9408" for this suite. 04/17/23 22:38:24.294
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:38:24.3
Apr 17 22:38:24.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename gc 04/17/23 22:38:24.3
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:24.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:24.314
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Apr 17 22:38:24.353: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1d265941-8613-43c6-87d9-f0c43afe9200", Controller:(*bool)(0xc00368c69e), BlockOwnerDeletion:(*bool)(0xc00368c69f)}}
Apr 17 22:38:24.359: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5bcf34d8-6ac4-4f1e-9ab6-aeacec9ad0b9", Controller:(*bool)(0xc000b3a12e), BlockOwnerDeletion:(*bool)(0xc000b3a12f)}}
Apr 17 22:38:24.368: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"738c4625-3866-4ff9-bbee-0bb7677e7bd5", Controller:(*bool)(0xc004da8526), BlockOwnerDeletion:(*bool)(0xc004da8527)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 17 22:38:29.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9547" for this suite. 04/17/23 22:38:29.382
------------------------------
• [SLOW TEST] [5.087 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:38:24.3
    Apr 17 22:38:24.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename gc 04/17/23 22:38:24.3
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:24.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:24.314
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Apr 17 22:38:24.353: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1d265941-8613-43c6-87d9-f0c43afe9200", Controller:(*bool)(0xc00368c69e), BlockOwnerDeletion:(*bool)(0xc00368c69f)}}
    Apr 17 22:38:24.359: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"5bcf34d8-6ac4-4f1e-9ab6-aeacec9ad0b9", Controller:(*bool)(0xc000b3a12e), BlockOwnerDeletion:(*bool)(0xc000b3a12f)}}
    Apr 17 22:38:24.368: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"738c4625-3866-4ff9-bbee-0bb7677e7bd5", Controller:(*bool)(0xc004da8526), BlockOwnerDeletion:(*bool)(0xc004da8527)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:38:29.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9547" for this suite. 04/17/23 22:38:29.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:38:29.387
Apr 17 22:38:29.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:38:29.388
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:29.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:29.404
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 04/17/23 22:38:29.407
Apr 17 22:38:29.407: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-3830 proxy --unix-socket=/tmp/kubectl-proxy-unix1984325157/test'
STEP: retrieving proxy /api/ output 04/17/23 22:38:29.447
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:38:29.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3830" for this suite. 04/17/23 22:38:29.453
------------------------------
• [0.070 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:38:29.387
    Apr 17 22:38:29.387: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:38:29.388
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:29.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:29.404
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 04/17/23 22:38:29.407
    Apr 17 22:38:29.407: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-3830 proxy --unix-socket=/tmp/kubectl-proxy-unix1984325157/test'
    STEP: retrieving proxy /api/ output 04/17/23 22:38:29.447
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:38:29.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3830" for this suite. 04/17/23 22:38:29.453
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:38:29.458
Apr 17 22:38:29.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename deployment 04/17/23 22:38:29.459
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:29.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:29.474
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Apr 17 22:38:29.483: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr 17 22:38:34.486: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 04/17/23 22:38:34.487
Apr 17 22:38:34.487: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr 17 22:38:36.491: INFO: Creating deployment "test-rollover-deployment"
Apr 17 22:38:36.497: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr 17 22:38:38.502: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr 17 22:38:38.507: INFO: Ensure that both replica sets have 1 created replica
Apr 17 22:38:38.511: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr 17 22:38:38.519: INFO: Updating deployment test-rollover-deployment
Apr 17 22:38:38.519: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr 17 22:38:40.525: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr 17 22:38:40.530: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr 17 22:38:40.534: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 22:38:40.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:38:42.541: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 22:38:42.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:38:44.540: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 22:38:44.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:38:46.541: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 22:38:46.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:38:48.540: INFO: all replica sets need to contain the pod-template-hash label
Apr 17 22:38:48.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr 17 22:38:50.540: INFO: 
Apr 17 22:38:50.540: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Apr 17 22:38:50.547: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-9856  797a67f3-f5c5-49c5-840f-18eb3565ac9e 78122 2 2023-04-17 22:38:36 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00874a358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-17 22:38:36 +0000 UTC,LastTransitionTime:2023-04-17 22:38:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-04-17 22:38:50 +0000 UTC,LastTransitionTime:2023-04-17 22:38:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Apr 17 22:38:50.549: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9856  ddeb9d12-3469-4b0f-bf6a-efae05b7d2fc 78112 2 2023-04-17 22:38:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 797a67f3-f5c5-49c5-840f-18eb3565ac9e 0xc006af34d7 0xc006af34d8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797a67f3-f5c5-49c5-840f-18eb3565ac9e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006af3588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:38:50.549: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr 17 22:38:50.549: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9856  74f04fa6-de35-4ac8-920b-83df955dced5 78121 2 2023-04-17 22:38:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 797a67f3-f5c5-49c5-840f-18eb3565ac9e 0xc006af33a7 0xc006af33a8}] [] [{e2e.test Update apps/v1 2023-04-17 22:38:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797a67f3-f5c5-49c5-840f-18eb3565ac9e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006af3468 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:38:50.549: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9856  d06bfdd1-eeae-48b4-bb93-48249aefe1ef 77998 2 2023-04-17 22:38:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 797a67f3-f5c5-49c5-840f-18eb3565ac9e 0xc006af35f7 0xc006af35f8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797a67f3-f5c5-49c5-840f-18eb3565ac9e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006af36a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Apr 17 22:38:50.552: INFO: Pod "test-rollover-deployment-6c6df9974f-9vvtt" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-9vvtt test-rollover-deployment-6c6df9974f- deployment-9856  0388ac53-4476-4ed3-a4f5-c4ac4a9be254 78023 0 2023-04-17 22:38:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:df19694cf3e2085bf394dde652d0f374678ec0078040e250665d238ca218f34a cni.projectcalico.org/podIP:192.168.208.160/32 cni.projectcalico.org/podIPs:192.168.208.160/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f ddeb9d12-3469-4b0f-bf6a-efae05b7d2fc 0xc00874a6e7 0xc00874a6e8}] [] [{kube-controller-manager Update v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddeb9d12-3469-4b0f-bf6a-efae05b7d2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:38:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:38:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sjdg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sjdg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:38:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:38:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.160,StartTime:2023-04-17 22:38:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:38:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://806a6f02b4d9f14b15c1ed2a16db4120b44cd182fd4a5856e0365c926e4cd3d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Apr 17 22:38:50.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9856" for this suite. 04/17/23 22:38:50.556
------------------------------
• [SLOW TEST] [21.105 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:38:29.458
    Apr 17 22:38:29.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename deployment 04/17/23 22:38:29.459
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:29.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:29.474
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Apr 17 22:38:29.483: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Apr 17 22:38:34.486: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 04/17/23 22:38:34.487
    Apr 17 22:38:34.487: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Apr 17 22:38:36.491: INFO: Creating deployment "test-rollover-deployment"
    Apr 17 22:38:36.497: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Apr 17 22:38:38.502: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Apr 17 22:38:38.507: INFO: Ensure that both replica sets have 1 created replica
    Apr 17 22:38:38.511: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Apr 17 22:38:38.519: INFO: Updating deployment test-rollover-deployment
    Apr 17 22:38:38.519: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Apr 17 22:38:40.525: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Apr 17 22:38:40.530: INFO: Make sure deployment "test-rollover-deployment" is complete
    Apr 17 22:38:40.534: INFO: all replica sets need to contain the pod-template-hash label
    Apr 17 22:38:40.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:38:42.541: INFO: all replica sets need to contain the pod-template-hash label
    Apr 17 22:38:42.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:38:44.540: INFO: all replica sets need to contain the pod-template-hash label
    Apr 17 22:38:44.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:38:46.541: INFO: all replica sets need to contain the pod-template-hash label
    Apr 17 22:38:46.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:38:48.540: INFO: all replica sets need to contain the pod-template-hash label
    Apr 17 22:38:48.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.April, 17, 22, 38, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.April, 17, 22, 38, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Apr 17 22:38:50.540: INFO: 
    Apr 17 22:38:50.540: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Apr 17 22:38:50.547: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-9856  797a67f3-f5c5-49c5-840f-18eb3565ac9e 78122 2 2023-04-17 22:38:36 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00874a358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-04-17 22:38:36 +0000 UTC,LastTransitionTime:2023-04-17 22:38:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-04-17 22:38:50 +0000 UTC,LastTransitionTime:2023-04-17 22:38:36 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Apr 17 22:38:50.549: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-9856  ddeb9d12-3469-4b0f-bf6a-efae05b7d2fc 78112 2 2023-04-17 22:38:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 797a67f3-f5c5-49c5-840f-18eb3565ac9e 0xc006af34d7 0xc006af34d8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797a67f3-f5c5-49c5-840f-18eb3565ac9e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006af3588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 22:38:50.549: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Apr 17 22:38:50.549: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-9856  74f04fa6-de35-4ac8-920b-83df955dced5 78121 2 2023-04-17 22:38:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 797a67f3-f5c5-49c5-840f-18eb3565ac9e 0xc006af33a7 0xc006af33a8}] [] [{e2e.test Update apps/v1 2023-04-17 22:38:29 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797a67f3-f5c5-49c5-840f-18eb3565ac9e\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:50 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006af3468 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 22:38:50.549: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-9856  d06bfdd1-eeae-48b4-bb93-48249aefe1ef 77998 2 2023-04-17 22:38:36 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 797a67f3-f5c5-49c5-840f-18eb3565ac9e 0xc006af35f7 0xc006af35f8}] [] [{kube-controller-manager Update apps/v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"797a67f3-f5c5-49c5-840f-18eb3565ac9e\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006af36a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Apr 17 22:38:50.552: INFO: Pod "test-rollover-deployment-6c6df9974f-9vvtt" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-9vvtt test-rollover-deployment-6c6df9974f- deployment-9856  0388ac53-4476-4ed3-a4f5-c4ac4a9be254 78023 0 2023-04-17 22:38:38 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:df19694cf3e2085bf394dde652d0f374678ec0078040e250665d238ca218f34a cni.projectcalico.org/podIP:192.168.208.160/32 cni.projectcalico.org/podIPs:192.168.208.160/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f ddeb9d12-3469-4b0f-bf6a-efae05b7d2fc 0xc00874a6e7 0xc00874a6e8}] [] [{kube-controller-manager Update v1 2023-04-17 22:38:38 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ddeb9d12-3469-4b0f-bf6a-efae05b7d2fc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-04-17 22:38:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-04-17 22:38:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.208.160\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sjdg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sjdg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-74-52.us-west-2.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:38:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:38:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-04-17 22:38:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.74.52,PodIP:192.168.208.160,StartTime:2023-04-17 22:38:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-04-17 22:38:39 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://806a6f02b4d9f14b15c1ed2a16db4120b44cd182fd4a5856e0365c926e4cd3d9,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.208.160,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:38:50.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9856" for this suite. 04/17/23 22:38:50.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:38:50.563
Apr 17 22:38:50.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:38:50.564
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:50.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:50.58
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  04/17/23 22:38:50.582
Apr 17 22:38:50.588: INFO: Waiting up to 5m0s for pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222" in namespace "svcaccounts-4291" to be "Succeeded or Failed"
Apr 17 22:38:50.590: INFO: Pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134176ms
Apr 17 22:38:52.594: INFO: Pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222": Phase="Running", Reason="", readiness=false. Elapsed: 2.005953299s
Apr 17 22:38:54.594: INFO: Pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00637851s
STEP: Saw pod success 04/17/23 22:38:54.594
Apr 17 22:38:54.594: INFO: Pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222" satisfied condition "Succeeded or Failed"
Apr 17 22:38:54.597: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222 container agnhost-container: <nil>
STEP: delete the pod 04/17/23 22:38:54.601
Apr 17 22:38:54.611: INFO: Waiting for pod test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222 to disappear
Apr 17 22:38:54.613: INFO: Pod test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Apr 17 22:38:54.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4291" for this suite. 04/17/23 22:38:54.617
------------------------------
• [4.059 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:38:50.563
    Apr 17 22:38:50.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename svcaccounts 04/17/23 22:38:50.564
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:50.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:50.58
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  04/17/23 22:38:50.582
    Apr 17 22:38:50.588: INFO: Waiting up to 5m0s for pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222" in namespace "svcaccounts-4291" to be "Succeeded or Failed"
    Apr 17 22:38:50.590: INFO: Pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222": Phase="Pending", Reason="", readiness=false. Elapsed: 2.134176ms
    Apr 17 22:38:52.594: INFO: Pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222": Phase="Running", Reason="", readiness=false. Elapsed: 2.005953299s
    Apr 17 22:38:54.594: INFO: Pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00637851s
    STEP: Saw pod success 04/17/23 22:38:54.594
    Apr 17 22:38:54.594: INFO: Pod "test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222" satisfied condition "Succeeded or Failed"
    Apr 17 22:38:54.597: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222 container agnhost-container: <nil>
    STEP: delete the pod 04/17/23 22:38:54.601
    Apr 17 22:38:54.611: INFO: Waiting for pod test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222 to disappear
    Apr 17 22:38:54.613: INFO: Pod test-pod-246e5bc8-f14c-4afe-a693-b60d93e12222 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:38:54.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4291" for this suite. 04/17/23 22:38:54.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:38:54.622
Apr 17 22:38:54.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:38:54.623
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:54.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:54.637
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:38:54.649
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:38:55.24
STEP: Deploying the webhook pod 04/17/23 22:38:55.246
STEP: Wait for the deployment to be ready 04/17/23 22:38:55.265
Apr 17 22:38:55.277: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:38:57.284
STEP: Verifying the service has paired with the endpoint 04/17/23 22:38:57.295
Apr 17 22:38:58.295: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 04/17/23 22:38:58.298
STEP: create a namespace for the webhook 04/17/23 22:38:58.31
STEP: create a configmap should be unconditionally rejected by the webhook 04/17/23 22:38:58.315
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:38:58.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7736" for this suite. 04/17/23 22:38:58.431
STEP: Destroying namespace "webhook-7736-markers" for this suite. 04/17/23 22:38:58.436
------------------------------
• [3.818 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:38:54.622
    Apr 17 22:38:54.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:38:54.623
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:54.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:54.637
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:38:54.649
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:38:55.24
    STEP: Deploying the webhook pod 04/17/23 22:38:55.246
    STEP: Wait for the deployment to be ready 04/17/23 22:38:55.265
    Apr 17 22:38:55.277: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:38:57.284
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:38:57.295
    Apr 17 22:38:58.295: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 04/17/23 22:38:58.298
    STEP: create a namespace for the webhook 04/17/23 22:38:58.31
    STEP: create a configmap should be unconditionally rejected by the webhook 04/17/23 22:38:58.315
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:38:58.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7736" for this suite. 04/17/23 22:38:58.431
    STEP: Destroying namespace "webhook-7736-markers" for this suite. 04/17/23 22:38:58.436
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:38:58.441
Apr 17 22:38:58.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 22:38:58.442
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:58.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:58.465
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 04/17/23 22:38:58.467
Apr 17 22:38:58.484: INFO: created test-pod-1
Apr 17 22:38:58.490: INFO: created test-pod-2
Apr 17 22:38:58.498: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 04/17/23 22:38:58.498
Apr 17 22:38:58.498: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6533' to be running and ready
Apr 17 22:38:58.510: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr 17 22:38:58.510: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr 17 22:38:58.510: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Apr 17 22:38:58.510: INFO: 0 / 3 pods in namespace 'pods-6533' are running and ready (0 seconds elapsed)
Apr 17 22:38:58.510: INFO: expected 0 pod replicas in namespace 'pods-6533', 0 are Running and Ready.
Apr 17 22:38:58.510: INFO: POD         NODE                                        PHASE    GRACE  CONDITIONS
Apr 17 22:38:58.510: INFO: test-pod-1  ip-10-0-106-231.us-west-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC  }]
Apr 17 22:38:58.510: INFO: test-pod-2  ip-10-0-106-231.us-west-2.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC  }]
Apr 17 22:38:58.510: INFO: test-pod-3  ip-10-0-74-52.us-west-2.compute.internal    Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC  }]
Apr 17 22:38:58.510: INFO: 
Apr 17 22:39:00.518: INFO: 3 / 3 pods in namespace 'pods-6533' are running and ready (2 seconds elapsed)
Apr 17 22:39:00.518: INFO: expected 0 pod replicas in namespace 'pods-6533', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 04/17/23 22:39:00.536
Apr 17 22:39:00.538: INFO: Pod quantity 3 is different from expected quantity 0
Apr 17 22:39:01.542: INFO: Pod quantity 3 is different from expected quantity 0
Apr 17 22:39:02.542: INFO: Pod quantity 2 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:03.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6533" for this suite. 04/17/23 22:39:03.545
------------------------------
• [SLOW TEST] [5.108 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:38:58.441
    Apr 17 22:38:58.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 22:38:58.442
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:38:58.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:38:58.465
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 04/17/23 22:38:58.467
    Apr 17 22:38:58.484: INFO: created test-pod-1
    Apr 17 22:38:58.490: INFO: created test-pod-2
    Apr 17 22:38:58.498: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 04/17/23 22:38:58.498
    Apr 17 22:38:58.498: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-6533' to be running and ready
    Apr 17 22:38:58.510: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr 17 22:38:58.510: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr 17 22:38:58.510: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Apr 17 22:38:58.510: INFO: 0 / 3 pods in namespace 'pods-6533' are running and ready (0 seconds elapsed)
    Apr 17 22:38:58.510: INFO: expected 0 pod replicas in namespace 'pods-6533', 0 are Running and Ready.
    Apr 17 22:38:58.510: INFO: POD         NODE                                        PHASE    GRACE  CONDITIONS
    Apr 17 22:38:58.510: INFO: test-pod-1  ip-10-0-106-231.us-west-2.compute.internal  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC  }]
    Apr 17 22:38:58.510: INFO: test-pod-2  ip-10-0-106-231.us-west-2.compute.internal  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC  }]
    Apr 17 22:38:58.510: INFO: test-pod-3  ip-10-0-74-52.us-west-2.compute.internal    Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-04-17 22:38:58 +0000 UTC  }]
    Apr 17 22:38:58.510: INFO: 
    Apr 17 22:39:00.518: INFO: 3 / 3 pods in namespace 'pods-6533' are running and ready (2 seconds elapsed)
    Apr 17 22:39:00.518: INFO: expected 0 pod replicas in namespace 'pods-6533', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 04/17/23 22:39:00.536
    Apr 17 22:39:00.538: INFO: Pod quantity 3 is different from expected quantity 0
    Apr 17 22:39:01.542: INFO: Pod quantity 3 is different from expected quantity 0
    Apr 17 22:39:02.542: INFO: Pod quantity 2 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:03.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6533" for this suite. 04/17/23 22:39:03.545
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:03.55
Apr 17 22:39:03.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 22:39:03.551
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:03.567
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:03.57
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 04/17/23 22:39:03.572
STEP: submitting the pod to kubernetes 04/17/23 22:39:03.572
Apr 17 22:39:03.579: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd" in namespace "pods-2232" to be "running and ready"
Apr 17 22:39:03.581: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21505ms
Apr 17 22:39:03.581: INFO: The phase of Pod pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:39:05.584: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005403045s
Apr 17 22:39:05.584: INFO: The phase of Pod pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd is Running (Ready = true)
Apr 17 22:39:05.584: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 04/17/23 22:39:05.586
STEP: updating the pod 04/17/23 22:39:05.589
Apr 17 22:39:06.100: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd"
Apr 17 22:39:06.100: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd" in namespace "pods-2232" to be "terminated with reason DeadlineExceeded"
Apr 17 22:39:06.104: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Running", Reason="", readiness=true. Elapsed: 4.080075ms
Apr 17 22:39:08.107: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Running", Reason="", readiness=false. Elapsed: 2.007402905s
Apr 17 22:39:10.108: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007968762s
Apr 17 22:39:10.108: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:10.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2232" for this suite. 04/17/23 22:39:10.112
------------------------------
• [SLOW TEST] [6.567 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:03.55
    Apr 17 22:39:03.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 22:39:03.551
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:03.567
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:03.57
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 04/17/23 22:39:03.572
    STEP: submitting the pod to kubernetes 04/17/23 22:39:03.572
    Apr 17 22:39:03.579: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd" in namespace "pods-2232" to be "running and ready"
    Apr 17 22:39:03.581: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21505ms
    Apr 17 22:39:03.581: INFO: The phase of Pod pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:39:05.584: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.005403045s
    Apr 17 22:39:05.584: INFO: The phase of Pod pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd is Running (Ready = true)
    Apr 17 22:39:05.584: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 04/17/23 22:39:05.586
    STEP: updating the pod 04/17/23 22:39:05.589
    Apr 17 22:39:06.100: INFO: Successfully updated pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd"
    Apr 17 22:39:06.100: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd" in namespace "pods-2232" to be "terminated with reason DeadlineExceeded"
    Apr 17 22:39:06.104: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Running", Reason="", readiness=true. Elapsed: 4.080075ms
    Apr 17 22:39:08.107: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Running", Reason="", readiness=false. Elapsed: 2.007402905s
    Apr 17 22:39:10.108: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007968762s
    Apr 17 22:39:10.108: INFO: Pod "pod-update-activedeadlineseconds-2414cc81-c567-4e6c-bf36-ecca1a94f6bd" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:10.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2232" for this suite. 04/17/23 22:39:10.112
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:10.118
Apr 17 22:39:10.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pods 04/17/23 22:39:10.119
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:10.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:10.134
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Apr 17 22:39:10.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: creating the pod 04/17/23 22:39:10.136
STEP: submitting the pod to kubernetes 04/17/23 22:39:10.136
Apr 17 22:39:10.142: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19" in namespace "pods-4774" to be "running and ready"
Apr 17 22:39:10.144: INFO: Pod "pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21319ms
Apr 17 22:39:10.144: INFO: The phase of Pod pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:39:12.147: INFO: Pod "pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19": Phase="Running", Reason="", readiness=true. Elapsed: 2.005095017s
Apr 17 22:39:12.147: INFO: The phase of Pod pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19 is Running (Ready = true)
Apr 17 22:39:12.147: INFO: Pod "pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:12.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4774" for this suite. 04/17/23 22:39:12.168
------------------------------
• [2.056 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:10.118
    Apr 17 22:39:10.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pods 04/17/23 22:39:10.119
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:10.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:10.134
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Apr 17 22:39:10.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: creating the pod 04/17/23 22:39:10.136
    STEP: submitting the pod to kubernetes 04/17/23 22:39:10.136
    Apr 17 22:39:10.142: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19" in namespace "pods-4774" to be "running and ready"
    Apr 17 22:39:10.144: INFO: Pod "pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.21319ms
    Apr 17 22:39:10.144: INFO: The phase of Pod pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:39:12.147: INFO: Pod "pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19": Phase="Running", Reason="", readiness=true. Elapsed: 2.005095017s
    Apr 17 22:39:12.147: INFO: The phase of Pod pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19 is Running (Ready = true)
    Apr 17 22:39:12.147: INFO: Pod "pod-logs-websocket-984c1a6c-e686-4f6a-acef-d5055da16e19" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:12.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4774" for this suite. 04/17/23 22:39:12.168
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:12.174
Apr 17 22:39:12.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:39:12.175
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:12.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:12.189
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Apr 17 22:39:12.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/17/23 22:39:17.352
Apr 17 22:39:17.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 --namespace=crd-publish-openapi-3290 create -f -'
Apr 17 22:39:18.851: INFO: stderr: ""
Apr 17 22:39:18.851: INFO: stdout: "e2e-test-crd-publish-openapi-3861-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 17 22:39:18.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 --namespace=crd-publish-openapi-3290 delete e2e-test-crd-publish-openapi-3861-crds test-cr'
Apr 17 22:39:18.912: INFO: stderr: ""
Apr 17 22:39:18.912: INFO: stdout: "e2e-test-crd-publish-openapi-3861-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Apr 17 22:39:18.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 --namespace=crd-publish-openapi-3290 apply -f -'
Apr 17 22:39:20.212: INFO: stderr: ""
Apr 17 22:39:20.212: INFO: stdout: "e2e-test-crd-publish-openapi-3861-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Apr 17 22:39:20.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 --namespace=crd-publish-openapi-3290 delete e2e-test-crd-publish-openapi-3861-crds test-cr'
Apr 17 22:39:20.306: INFO: stderr: ""
Apr 17 22:39:20.306: INFO: stdout: "e2e-test-crd-publish-openapi-3861-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 04/17/23 22:39:20.306
Apr 17 22:39:20.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 explain e2e-test-crd-publish-openapi-3861-crds'
Apr 17 22:39:20.686: INFO: stderr: ""
Apr 17 22:39:20.686: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3861-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:24.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3290" for this suite. 04/17/23 22:39:24.534
------------------------------
• [SLOW TEST] [12.366 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:12.174
    Apr 17 22:39:12.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:39:12.175
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:12.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:12.189
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Apr 17 22:39:12.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 04/17/23 22:39:17.352
    Apr 17 22:39:17.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 --namespace=crd-publish-openapi-3290 create -f -'
    Apr 17 22:39:18.851: INFO: stderr: ""
    Apr 17 22:39:18.851: INFO: stdout: "e2e-test-crd-publish-openapi-3861-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Apr 17 22:39:18.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 --namespace=crd-publish-openapi-3290 delete e2e-test-crd-publish-openapi-3861-crds test-cr'
    Apr 17 22:39:18.912: INFO: stderr: ""
    Apr 17 22:39:18.912: INFO: stdout: "e2e-test-crd-publish-openapi-3861-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Apr 17 22:39:18.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 --namespace=crd-publish-openapi-3290 apply -f -'
    Apr 17 22:39:20.212: INFO: stderr: ""
    Apr 17 22:39:20.212: INFO: stdout: "e2e-test-crd-publish-openapi-3861-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Apr 17 22:39:20.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 --namespace=crd-publish-openapi-3290 delete e2e-test-crd-publish-openapi-3861-crds test-cr'
    Apr 17 22:39:20.306: INFO: stderr: ""
    Apr 17 22:39:20.306: INFO: stdout: "e2e-test-crd-publish-openapi-3861-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 04/17/23 22:39:20.306
    Apr 17 22:39:20.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=crd-publish-openapi-3290 explain e2e-test-crd-publish-openapi-3861-crds'
    Apr 17 22:39:20.686: INFO: stderr: ""
    Apr 17 22:39:20.686: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3861-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:24.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3290" for this suite. 04/17/23 22:39:24.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:24.541
Apr 17 22:39:24.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:39:24.541
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:24.552
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:24.554
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:39:24.556
Apr 17 22:39:24.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5" in namespace "projected-2542" to be "Succeeded or Failed"
Apr 17 22:39:24.568: INFO: Pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650366ms
Apr 17 22:39:26.571: INFO: Pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5": Phase="Running", Reason="", readiness=false. Elapsed: 2.007631568s
Apr 17 22:39:28.571: INFO: Pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00778292s
STEP: Saw pod success 04/17/23 22:39:28.571
Apr 17 22:39:28.571: INFO: Pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5" satisfied condition "Succeeded or Failed"
Apr 17 22:39:28.574: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5 container client-container: <nil>
STEP: delete the pod 04/17/23 22:39:28.584
Apr 17 22:39:28.594: INFO: Waiting for pod downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5 to disappear
Apr 17 22:39:28.596: INFO: Pod downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:28.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2542" for this suite. 04/17/23 22:39:28.601
------------------------------
• [4.068 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:24.541
    Apr 17 22:39:24.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:39:24.541
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:24.552
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:24.554
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:39:24.556
    Apr 17 22:39:24.564: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5" in namespace "projected-2542" to be "Succeeded or Failed"
    Apr 17 22:39:24.568: INFO: Pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650366ms
    Apr 17 22:39:26.571: INFO: Pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5": Phase="Running", Reason="", readiness=false. Elapsed: 2.007631568s
    Apr 17 22:39:28.571: INFO: Pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00778292s
    STEP: Saw pod success 04/17/23 22:39:28.571
    Apr 17 22:39:28.571: INFO: Pod "downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5" satisfied condition "Succeeded or Failed"
    Apr 17 22:39:28.574: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5 container client-container: <nil>
    STEP: delete the pod 04/17/23 22:39:28.584
    Apr 17 22:39:28.594: INFO: Waiting for pod downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5 to disappear
    Apr 17 22:39:28.596: INFO: Pod downwardapi-volume-f3f4f6be-ffc2-4431-bb73-31937e91eec5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:28.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2542" for this suite. 04/17/23 22:39:28.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:28.609
Apr 17 22:39:28.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:39:28.61
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:28.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:28.622
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 04/17/23 22:39:28.624
Apr 17 22:39:28.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-6127 create -f -'
Apr 17 22:39:30.153: INFO: stderr: ""
Apr 17 22:39:30.154: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/17/23 22:39:30.154
Apr 17 22:39:31.157: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 17 22:39:31.157: INFO: Found 0 / 1
Apr 17 22:39:32.157: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 17 22:39:32.157: INFO: Found 1 / 1
Apr 17 22:39:32.157: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 04/17/23 22:39:32.157
Apr 17 22:39:32.159: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 17 22:39:32.159: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 17 22:39:32.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-6127 patch pod agnhost-primary-k7gcq -p {"metadata":{"annotations":{"x":"y"}}}'
Apr 17 22:39:32.223: INFO: stderr: ""
Apr 17 22:39:32.223: INFO: stdout: "pod/agnhost-primary-k7gcq patched\n"
STEP: checking annotations 04/17/23 22:39:32.223
Apr 17 22:39:32.226: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 17 22:39:32.226: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:32.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6127" for this suite. 04/17/23 22:39:32.23
------------------------------
• [3.626 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:28.609
    Apr 17 22:39:28.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:39:28.61
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:28.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:28.622
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 04/17/23 22:39:28.624
    Apr 17 22:39:28.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-6127 create -f -'
    Apr 17 22:39:30.153: INFO: stderr: ""
    Apr 17 22:39:30.154: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/17/23 22:39:30.154
    Apr 17 22:39:31.157: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 17 22:39:31.157: INFO: Found 0 / 1
    Apr 17 22:39:32.157: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 17 22:39:32.157: INFO: Found 1 / 1
    Apr 17 22:39:32.157: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 04/17/23 22:39:32.157
    Apr 17 22:39:32.159: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 17 22:39:32.159: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr 17 22:39:32.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-6127 patch pod agnhost-primary-k7gcq -p {"metadata":{"annotations":{"x":"y"}}}'
    Apr 17 22:39:32.223: INFO: stderr: ""
    Apr 17 22:39:32.223: INFO: stdout: "pod/agnhost-primary-k7gcq patched\n"
    STEP: checking annotations 04/17/23 22:39:32.223
    Apr 17 22:39:32.226: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 17 22:39:32.226: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:32.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6127" for this suite. 04/17/23 22:39:32.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:32.236
Apr 17 22:39:32.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:39:32.236
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:32.25
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:32.252
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:39:32.253
Apr 17 22:39:32.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca" in namespace "downward-api-5829" to be "Succeeded or Failed"
Apr 17 22:39:32.263: INFO: Pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187111ms
Apr 17 22:39:34.267: INFO: Pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00569924s
Apr 17 22:39:36.268: INFO: Pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006644596s
STEP: Saw pod success 04/17/23 22:39:36.268
Apr 17 22:39:36.268: INFO: Pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca" satisfied condition "Succeeded or Failed"
Apr 17 22:39:36.270: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca container client-container: <nil>
STEP: delete the pod 04/17/23 22:39:36.28
Apr 17 22:39:36.292: INFO: Waiting for pod downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca to disappear
Apr 17 22:39:36.294: INFO: Pod downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:36.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5829" for this suite. 04/17/23 22:39:36.298
------------------------------
• [4.069 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:32.236
    Apr 17 22:39:32.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:39:32.236
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:32.25
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:32.252
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:39:32.253
    Apr 17 22:39:32.261: INFO: Waiting up to 5m0s for pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca" in namespace "downward-api-5829" to be "Succeeded or Failed"
    Apr 17 22:39:32.263: INFO: Pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.187111ms
    Apr 17 22:39:34.267: INFO: Pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00569924s
    Apr 17 22:39:36.268: INFO: Pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006644596s
    STEP: Saw pod success 04/17/23 22:39:36.268
    Apr 17 22:39:36.268: INFO: Pod "downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca" satisfied condition "Succeeded or Failed"
    Apr 17 22:39:36.270: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca container client-container: <nil>
    STEP: delete the pod 04/17/23 22:39:36.28
    Apr 17 22:39:36.292: INFO: Waiting for pod downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca to disappear
    Apr 17 22:39:36.294: INFO: Pod downwardapi-volume-00c1a7ee-70a3-46f8-bb0a-a84b506a0fca no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:36.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5829" for this suite. 04/17/23 22:39:36.298
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:36.305
Apr 17 22:39:36.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 22:39:36.305
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:36.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:36.318
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-2079 04/17/23 22:39:36.32
STEP: creating service affinity-nodeport-transition in namespace services-2079 04/17/23 22:39:36.32
STEP: creating replication controller affinity-nodeport-transition in namespace services-2079 04/17/23 22:39:36.334
I0417 22:39:36.342555      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2079, replica count: 3
I0417 22:39:39.393154      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 22:39:39.402: INFO: Creating new exec pod
Apr 17 22:39:39.406: INFO: Waiting up to 5m0s for pod "execpod-affinitywwbbn" in namespace "services-2079" to be "running"
Apr 17 22:39:39.409: INFO: Pod "execpod-affinitywwbbn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.519156ms
Apr 17 22:39:41.413: INFO: Pod "execpod-affinitywwbbn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006810231s
Apr 17 22:39:41.413: INFO: Pod "execpod-affinitywwbbn" satisfied condition "running"
Apr 17 22:39:42.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Apr 17 22:39:42.562: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Apr 17 22:39:42.562: INFO: stdout: ""
Apr 17 22:39:42.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c nc -v -z -w 2 10.101.177.131 80'
Apr 17 22:39:42.677: INFO: stderr: "+ nc -v -z -w 2 10.101.177.131 80\nConnection to 10.101.177.131 80 port [tcp/http] succeeded!\n"
Apr 17 22:39:42.677: INFO: stdout: ""
Apr 17 22:39:42.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c nc -v -z -w 2 10.0.74.52 32736'
Apr 17 22:39:42.809: INFO: stderr: "+ nc -v -z -w 2 10.0.74.52 32736\nConnection to 10.0.74.52 32736 port [tcp/*] succeeded!\n"
Apr 17 22:39:42.809: INFO: stdout: ""
Apr 17 22:39:42.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c nc -v -z -w 2 10.0.106.231 32736'
Apr 17 22:39:42.945: INFO: stderr: "+ nc -v -z -w 2 10.0.106.231 32736\nConnection to 10.0.106.231 32736 port [tcp/*] succeeded!\n"
Apr 17 22:39:42.945: INFO: stdout: ""
Apr 17 22:39:42.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.106.231:32736/ ; done'
Apr 17 22:39:43.140: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n"
Apr 17 22:39:43.140: INFO: stdout: "\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-9zf2z\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-9zf2z\naffinity-nodeport-transition-9zf2z\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-9zf2z\naffinity-nodeport-transition-9zf2z"
Apr 17 22:39:43.140: INFO: Received response from host: affinity-nodeport-transition-d45kz
Apr 17 22:39:43.140: INFO: Received response from host: affinity-nodeport-transition-9zf2z
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-9zf2z
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-9zf2z
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-9zf2z
Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-9zf2z
Apr 17 22:39:43.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.106.231:32736/ ; done'
Apr 17 22:39:43.325: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n"
Apr 17 22:39:43.325: INFO: stdout: "\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl"
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
Apr 17 22:39:43.325: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2079, will wait for the garbage collector to delete the pods 04/17/23 22:39:43.341
Apr 17 22:39:43.400: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.259123ms
Apr 17 22:39:43.501: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.887584ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:46.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2079" for this suite. 04/17/23 22:39:46.046
------------------------------
• [SLOW TEST] [9.747 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:36.305
    Apr 17 22:39:36.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 22:39:36.305
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:36.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:36.318
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-2079 04/17/23 22:39:36.32
    STEP: creating service affinity-nodeport-transition in namespace services-2079 04/17/23 22:39:36.32
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2079 04/17/23 22:39:36.334
    I0417 22:39:36.342555      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2079, replica count: 3
    I0417 22:39:39.393154      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 17 22:39:39.402: INFO: Creating new exec pod
    Apr 17 22:39:39.406: INFO: Waiting up to 5m0s for pod "execpod-affinitywwbbn" in namespace "services-2079" to be "running"
    Apr 17 22:39:39.409: INFO: Pod "execpod-affinitywwbbn": Phase="Pending", Reason="", readiness=false. Elapsed: 2.519156ms
    Apr 17 22:39:41.413: INFO: Pod "execpod-affinitywwbbn": Phase="Running", Reason="", readiness=true. Elapsed: 2.006810231s
    Apr 17 22:39:41.413: INFO: Pod "execpod-affinitywwbbn" satisfied condition "running"
    Apr 17 22:39:42.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Apr 17 22:39:42.562: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Apr 17 22:39:42.562: INFO: stdout: ""
    Apr 17 22:39:42.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c nc -v -z -w 2 10.101.177.131 80'
    Apr 17 22:39:42.677: INFO: stderr: "+ nc -v -z -w 2 10.101.177.131 80\nConnection to 10.101.177.131 80 port [tcp/http] succeeded!\n"
    Apr 17 22:39:42.677: INFO: stdout: ""
    Apr 17 22:39:42.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c nc -v -z -w 2 10.0.74.52 32736'
    Apr 17 22:39:42.809: INFO: stderr: "+ nc -v -z -w 2 10.0.74.52 32736\nConnection to 10.0.74.52 32736 port [tcp/*] succeeded!\n"
    Apr 17 22:39:42.809: INFO: stdout: ""
    Apr 17 22:39:42.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c nc -v -z -w 2 10.0.106.231 32736'
    Apr 17 22:39:42.945: INFO: stderr: "+ nc -v -z -w 2 10.0.106.231 32736\nConnection to 10.0.106.231 32736 port [tcp/*] succeeded!\n"
    Apr 17 22:39:42.945: INFO: stdout: ""
    Apr 17 22:39:42.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.106.231:32736/ ; done'
    Apr 17 22:39:43.140: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n"
    Apr 17 22:39:43.140: INFO: stdout: "\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-9zf2z\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-9zf2z\naffinity-nodeport-transition-9zf2z\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-d45kz\naffinity-nodeport-transition-9zf2z\naffinity-nodeport-transition-9zf2z"
    Apr 17 22:39:43.140: INFO: Received response from host: affinity-nodeport-transition-d45kz
    Apr 17 22:39:43.140: INFO: Received response from host: affinity-nodeport-transition-9zf2z
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-9zf2z
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-9zf2z
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-d45kz
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-9zf2z
    Apr 17 22:39:43.141: INFO: Received response from host: affinity-nodeport-transition-9zf2z
    Apr 17 22:39:43.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-2079 exec execpod-affinitywwbbn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.0.106.231:32736/ ; done'
    Apr 17 22:39:43.325: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.0.106.231:32736/\n"
    Apr 17 22:39:43.325: INFO: stdout: "\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl\naffinity-nodeport-transition-kn5pl"
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Received response from host: affinity-nodeport-transition-kn5pl
    Apr 17 22:39:43.325: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2079, will wait for the garbage collector to delete the pods 04/17/23 22:39:43.341
    Apr 17 22:39:43.400: INFO: Deleting ReplicationController affinity-nodeport-transition took: 5.259123ms
    Apr 17 22:39:43.501: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.887584ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:46.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2079" for this suite. 04/17/23 22:39:46.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:46.053
Apr 17 22:39:46.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:39:46.054
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:46.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:46.069
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:39:46.071
Apr 17 22:39:46.078: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e" in namespace "downward-api-1320" to be "Succeeded or Failed"
Apr 17 22:39:46.080: INFO: Pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162861ms
Apr 17 22:39:48.082: INFO: Pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004844569s
Apr 17 22:39:50.084: INFO: Pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006681543s
STEP: Saw pod success 04/17/23 22:39:50.084
Apr 17 22:39:50.084: INFO: Pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e" satisfied condition "Succeeded or Failed"
Apr 17 22:39:50.087: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e container client-container: <nil>
STEP: delete the pod 04/17/23 22:39:50.094
Apr 17 22:39:50.103: INFO: Waiting for pod downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e to disappear
Apr 17 22:39:50.106: INFO: Pod downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1320" for this suite. 04/17/23 22:39:50.11
------------------------------
• [4.062 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:46.053
    Apr 17 22:39:46.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:39:46.054
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:46.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:46.069
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:39:46.071
    Apr 17 22:39:46.078: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e" in namespace "downward-api-1320" to be "Succeeded or Failed"
    Apr 17 22:39:46.080: INFO: Pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.162861ms
    Apr 17 22:39:48.082: INFO: Pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004844569s
    Apr 17 22:39:50.084: INFO: Pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006681543s
    STEP: Saw pod success 04/17/23 22:39:50.084
    Apr 17 22:39:50.084: INFO: Pod "downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e" satisfied condition "Succeeded or Failed"
    Apr 17 22:39:50.087: INFO: Trying to get logs from node ip-10-0-74-52.us-west-2.compute.internal pod downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e container client-container: <nil>
    STEP: delete the pod 04/17/23 22:39:50.094
    Apr 17 22:39:50.103: INFO: Waiting for pod downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e to disappear
    Apr 17 22:39:50.106: INFO: Pod downwardapi-volume-dd45dde8-1458-44ec-bdc8-c9f5bfadfa2e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1320" for this suite. 04/17/23 22:39:50.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:50.115
Apr 17 22:39:50.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename downward-api 04/17/23 22:39:50.115
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:50.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:50.129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:39:50.131
Apr 17 22:39:50.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12" in namespace "downward-api-445" to be "Succeeded or Failed"
Apr 17 22:39:50.140: INFO: Pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.304302ms
Apr 17 22:39:52.143: INFO: Pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005549188s
Apr 17 22:39:54.144: INFO: Pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006030494s
STEP: Saw pod success 04/17/23 22:39:54.144
Apr 17 22:39:54.144: INFO: Pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12" satisfied condition "Succeeded or Failed"
Apr 17 22:39:54.147: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12 container client-container: <nil>
STEP: delete the pod 04/17/23 22:39:54.151
Apr 17 22:39:54.164: INFO: Waiting for pod downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12 to disappear
Apr 17 22:39:54.166: INFO: Pod downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:54.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-445" for this suite. 04/17/23 22:39:54.17
------------------------------
• [4.060 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:50.115
    Apr 17 22:39:50.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename downward-api 04/17/23 22:39:50.115
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:50.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:50.129
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:39:50.131
    Apr 17 22:39:50.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12" in namespace "downward-api-445" to be "Succeeded or Failed"
    Apr 17 22:39:50.140: INFO: Pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.304302ms
    Apr 17 22:39:52.143: INFO: Pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005549188s
    Apr 17 22:39:54.144: INFO: Pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006030494s
    STEP: Saw pod success 04/17/23 22:39:54.144
    Apr 17 22:39:54.144: INFO: Pod "downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12" satisfied condition "Succeeded or Failed"
    Apr 17 22:39:54.147: INFO: Trying to get logs from node ip-10-0-93-18.us-west-2.compute.internal pod downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12 container client-container: <nil>
    STEP: delete the pod 04/17/23 22:39:54.151
    Apr 17 22:39:54.164: INFO: Waiting for pod downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12 to disappear
    Apr 17 22:39:54.166: INFO: Pod downwardapi-volume-a4530444-2aca-4d7b-bfed-c668a24b5a12 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:54.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-445" for this suite. 04/17/23 22:39:54.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:54.175
Apr 17 22:39:54.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename gc 04/17/23 22:39:54.176
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:54.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:54.19
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 04/17/23 22:39:54.191
STEP: Wait for the Deployment to create new ReplicaSet 04/17/23 22:39:54.195
STEP: delete the deployment 04/17/23 22:39:54.7
STEP: wait for all rs to be garbage collected 04/17/23 22:39:54.705
STEP: expected 0 rs, got 1 rs 04/17/23 22:39:54.71
STEP: expected 0 pods, got 2 pods 04/17/23 22:39:54.713
STEP: Gathering metrics 04/17/23 22:39:55.22
Apr 17 22:39:55.241: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
Apr 17 22:39:55.244: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.348797ms
Apr 17 22:39:55.244: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
Apr 17 22:39:55.244: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
Apr 17 22:39:55.301: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:55.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9087" for this suite. 04/17/23 22:39:55.306
------------------------------
• [1.140 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:54.175
    Apr 17 22:39:54.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename gc 04/17/23 22:39:54.176
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:54.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:54.19
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 04/17/23 22:39:54.191
    STEP: Wait for the Deployment to create new ReplicaSet 04/17/23 22:39:54.195
    STEP: delete the deployment 04/17/23 22:39:54.7
    STEP: wait for all rs to be garbage collected 04/17/23 22:39:54.705
    STEP: expected 0 rs, got 1 rs 04/17/23 22:39:54.71
    STEP: expected 0 pods, got 2 pods 04/17/23 22:39:54.713
    STEP: Gathering metrics 04/17/23 22:39:55.22
    Apr 17 22:39:55.241: INFO: Waiting up to 5m0s for pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" in namespace "kube-system" to be "running and ready"
    Apr 17 22:39:55.244: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal": Phase="Running", Reason="", readiness=true. Elapsed: 2.348797ms
    Apr 17 22:39:55.244: INFO: The phase of Pod kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal is Running (Ready = true)
    Apr 17 22:39:55.244: INFO: Pod "kube-controller-manager-ip-10-0-86-140.us-west-2.compute.internal" satisfied condition "running and ready"
    Apr 17 22:39:55.301: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:55.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9087" for this suite. 04/17/23 22:39:55.306
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:55.315
Apr 17 22:39:55.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 22:39:55.316
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:55.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:55.33
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 04/17/23 22:39:55.331
Apr 17 22:39:55.332: INFO: Creating e2e-svc-a-lp2pl
Apr 17 22:39:55.343: INFO: Creating e2e-svc-b-n9lrv
Apr 17 22:39:55.359: INFO: Creating e2e-svc-c-q7qsp
STEP: deleting service collection 04/17/23 22:39:55.378
Apr 17 22:39:55.590: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 22:39:55.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4073" for this suite. 04/17/23 22:39:55.596
------------------------------
• [0.289 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:55.315
    Apr 17 22:39:55.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 22:39:55.316
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:55.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:55.33
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 04/17/23 22:39:55.331
    Apr 17 22:39:55.332: INFO: Creating e2e-svc-a-lp2pl
    Apr 17 22:39:55.343: INFO: Creating e2e-svc-b-n9lrv
    Apr 17 22:39:55.359: INFO: Creating e2e-svc-c-q7qsp
    STEP: deleting service collection 04/17/23 22:39:55.378
    Apr 17 22:39:55.590: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:39:55.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4073" for this suite. 04/17/23 22:39:55.596
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:39:55.605
Apr 17 22:39:55.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename statefulset 04/17/23 22:39:55.605
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:55.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:55.619
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-7013 04/17/23 22:39:55.62
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 04/17/23 22:39:55.624
Apr 17 22:39:55.637: INFO: Found 0 stateful pods, waiting for 3
Apr 17 22:40:05.642: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:40:05.642: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:40:05.642: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/17/23 22:40:05.66
Apr 17 22:40:05.679: INFO: Updating stateful set ss2
STEP: Creating a new revision 04/17/23 22:40:05.679
STEP: Not applying an update when the partition is greater than the number of replicas 04/17/23 22:40:15.693
STEP: Performing a canary update 04/17/23 22:40:15.693
Apr 17 22:40:15.712: INFO: Updating stateful set ss2
Apr 17 22:40:15.717: INFO: Waiting for Pod statefulset-7013/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 04/17/23 22:40:25.723
Apr 17 22:40:25.764: INFO: Found 2 stateful pods, waiting for 3
Apr 17 22:40:35.768: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:40:35.768: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr 17 22:40:35.768: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 04/17/23 22:40:35.772
Apr 17 22:40:35.791: INFO: Updating stateful set ss2
Apr 17 22:40:35.796: INFO: Waiting for Pod statefulset-7013/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Apr 17 22:40:45.821: INFO: Updating stateful set ss2
Apr 17 22:40:45.826: INFO: Waiting for StatefulSet statefulset-7013/ss2 to complete update
Apr 17 22:40:45.826: INFO: Waiting for Pod statefulset-7013/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Apr 17 22:40:55.832: INFO: Deleting all statefulset in ns statefulset-7013
Apr 17 22:40:55.834: INFO: Scaling statefulset ss2 to 0
Apr 17 22:41:05.848: INFO: Waiting for statefulset status.replicas updated to 0
Apr 17 22:41:05.850: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Apr 17 22:41:05.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-7013" for this suite. 04/17/23 22:41:05.872
------------------------------
• [SLOW TEST] [70.273 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:39:55.605
    Apr 17 22:39:55.605: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename statefulset 04/17/23 22:39:55.605
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:39:55.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:39:55.619
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-7013 04/17/23 22:39:55.62
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 04/17/23 22:39:55.624
    Apr 17 22:39:55.637: INFO: Found 0 stateful pods, waiting for 3
    Apr 17 22:40:05.642: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:40:05.642: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:40:05.642: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 04/17/23 22:40:05.66
    Apr 17 22:40:05.679: INFO: Updating stateful set ss2
    STEP: Creating a new revision 04/17/23 22:40:05.679
    STEP: Not applying an update when the partition is greater than the number of replicas 04/17/23 22:40:15.693
    STEP: Performing a canary update 04/17/23 22:40:15.693
    Apr 17 22:40:15.712: INFO: Updating stateful set ss2
    Apr 17 22:40:15.717: INFO: Waiting for Pod statefulset-7013/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 04/17/23 22:40:25.723
    Apr 17 22:40:25.764: INFO: Found 2 stateful pods, waiting for 3
    Apr 17 22:40:35.768: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:40:35.768: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Apr 17 22:40:35.768: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 04/17/23 22:40:35.772
    Apr 17 22:40:35.791: INFO: Updating stateful set ss2
    Apr 17 22:40:35.796: INFO: Waiting for Pod statefulset-7013/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Apr 17 22:40:45.821: INFO: Updating stateful set ss2
    Apr 17 22:40:45.826: INFO: Waiting for StatefulSet statefulset-7013/ss2 to complete update
    Apr 17 22:40:45.826: INFO: Waiting for Pod statefulset-7013/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Apr 17 22:40:55.832: INFO: Deleting all statefulset in ns statefulset-7013
    Apr 17 22:40:55.834: INFO: Scaling statefulset ss2 to 0
    Apr 17 22:41:05.848: INFO: Waiting for statefulset status.replicas updated to 0
    Apr 17 22:41:05.850: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:41:05.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-7013" for this suite. 04/17/23 22:41:05.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:41:05.878
Apr 17 22:41:05.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename webhook 04/17/23 22:41:05.879
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:41:05.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:41:05.893
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 04/17/23 22:41:05.905
STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:41:06.115
STEP: Deploying the webhook pod 04/17/23 22:41:06.121
STEP: Wait for the deployment to be ready 04/17/23 22:41:06.132
Apr 17 22:41:06.138: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 04/17/23 22:41:08.146
STEP: Verifying the service has paired with the endpoint 04/17/23 22:41:08.292
Apr 17 22:41:09.292: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 04/17/23 22:41:09.295
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 04/17/23 22:41:09.296
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 04/17/23 22:41:09.296
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 04/17/23 22:41:09.296
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 04/17/23 22:41:09.297
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 04/17/23 22:41:09.297
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 04/17/23 22:41:09.298
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:41:09.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-4152" for this suite. 04/17/23 22:41:09.341
STEP: Destroying namespace "webhook-4152-markers" for this suite. 04/17/23 22:41:09.349
------------------------------
• [3.480 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:41:05.878
    Apr 17 22:41:05.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename webhook 04/17/23 22:41:05.879
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:41:05.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:41:05.893
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 04/17/23 22:41:05.905
    STEP: Create role binding to let webhook read extension-apiserver-authentication 04/17/23 22:41:06.115
    STEP: Deploying the webhook pod 04/17/23 22:41:06.121
    STEP: Wait for the deployment to be ready 04/17/23 22:41:06.132
    Apr 17 22:41:06.138: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 04/17/23 22:41:08.146
    STEP: Verifying the service has paired with the endpoint 04/17/23 22:41:08.292
    Apr 17 22:41:09.292: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 04/17/23 22:41:09.295
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 04/17/23 22:41:09.296
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 04/17/23 22:41:09.296
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 04/17/23 22:41:09.296
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 04/17/23 22:41:09.297
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 04/17/23 22:41:09.297
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 04/17/23 22:41:09.298
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:41:09.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-4152" for this suite. 04/17/23 22:41:09.341
    STEP: Destroying namespace "webhook-4152-markers" for this suite. 04/17/23 22:41:09.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:41:09.358
Apr 17 22:41:09.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename pod-network-test 04/17/23 22:41:09.359
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:41:09.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:41:09.374
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5597 04/17/23 22:41:09.376
STEP: creating a selector 04/17/23 22:41:09.376
STEP: Creating the service pods in kubernetes 04/17/23 22:41:09.376
Apr 17 22:41:09.376: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Apr 17 22:41:09.411: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5597" to be "running and ready"
Apr 17 22:41:09.418: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.962981ms
Apr 17 22:41:09.418: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:41:11.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00993099s
Apr 17 22:41:11.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:13.422: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01111623s
Apr 17 22:41:13.422: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:15.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010152871s
Apr 17 22:41:15.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:17.422: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010996829s
Apr 17 22:41:17.422: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:19.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010013445s
Apr 17 22:41:19.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:21.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009926573s
Apr 17 22:41:21.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:23.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014359242s
Apr 17 22:41:23.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:25.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010768437s
Apr 17 22:41:25.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:27.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010704825s
Apr 17 22:41:27.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:29.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010356479s
Apr 17 22:41:29.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Apr 17 22:41:31.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010756008s
Apr 17 22:41:31.421: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Apr 17 22:41:31.421: INFO: Pod "netserver-0" satisfied condition "running and ready"
Apr 17 22:41:31.424: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5597" to be "running and ready"
Apr 17 22:41:31.426: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.210545ms
Apr 17 22:41:31.426: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Apr 17 22:41:31.426: INFO: Pod "netserver-1" satisfied condition "running and ready"
Apr 17 22:41:31.428: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5597" to be "running and ready"
Apr 17 22:41:31.430: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.156381ms
Apr 17 22:41:31.430: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Apr 17 22:41:31.430: INFO: Pod "netserver-2" satisfied condition "running and ready"
Apr 17 22:41:31.432: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5597" to be "running and ready"
Apr 17 22:41:31.434: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 1.967134ms
Apr 17 22:41:31.434: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Apr 17 22:41:31.434: INFO: Pod "netserver-3" satisfied condition "running and ready"
STEP: Creating test pods 04/17/23 22:41:31.437
Apr 17 22:41:31.448: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5597" to be "running"
Apr 17 22:41:31.454: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.642237ms
Apr 17 22:41:33.459: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01083685s
Apr 17 22:41:33.459: INFO: Pod "test-container-pod" satisfied condition "running"
Apr 17 22:41:33.462: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5597" to be "running"
Apr 17 22:41:33.465: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.269844ms
Apr 17 22:41:33.465: INFO: Pod "host-test-container-pod" satisfied condition "running"
Apr 17 22:41:33.467: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
Apr 17 22:41:33.467: INFO: Going to poll 192.168.163.226 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Apr 17 22:41:33.469: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.163.226 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:41:33.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:41:33.469: INFO: ExecWithOptions: Clientset creation
Apr 17 22:41:33.469: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.163.226+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 17 22:41:34.558: INFO: Found all 1 expected endpoints: [netserver-0]
Apr 17 22:41:34.558: INFO: Going to poll 192.168.200.136 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Apr 17 22:41:34.560: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.200.136 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:41:34.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:41:34.561: INFO: ExecWithOptions: Clientset creation
Apr 17 22:41:34.561: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.200.136+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 17 22:41:35.628: INFO: Found all 1 expected endpoints: [netserver-1]
Apr 17 22:41:35.628: INFO: Going to poll 192.168.208.148 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Apr 17 22:41:35.630: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.208.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:41:35.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:41:35.631: INFO: ExecWithOptions: Clientset creation
Apr 17 22:41:35.631: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.208.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 17 22:41:36.724: INFO: Found all 1 expected endpoints: [netserver-2]
Apr 17 22:41:36.724: INFO: Going to poll 192.168.213.36 on port 8081 at least 0 times, with a maximum of 46 tries before failing
Apr 17 22:41:36.727: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.213.36 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:41:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:41:36.727: INFO: ExecWithOptions: Clientset creation
Apr 17 22:41:36.727: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.213.36+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Apr 17 22:41:37.809: INFO: Found all 1 expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Apr 17 22:41:37.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5597" for this suite. 04/17/23 22:41:37.813
------------------------------
• [SLOW TEST] [28.461 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:41:09.358
    Apr 17 22:41:09.358: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename pod-network-test 04/17/23 22:41:09.359
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:41:09.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:41:09.374
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5597 04/17/23 22:41:09.376
    STEP: creating a selector 04/17/23 22:41:09.376
    STEP: Creating the service pods in kubernetes 04/17/23 22:41:09.376
    Apr 17 22:41:09.376: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Apr 17 22:41:09.411: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5597" to be "running and ready"
    Apr 17 22:41:09.418: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.962981ms
    Apr 17 22:41:09.418: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:41:11.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00993099s
    Apr 17 22:41:11.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:13.422: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01111623s
    Apr 17 22:41:13.422: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:15.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010152871s
    Apr 17 22:41:15.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:17.422: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010996829s
    Apr 17 22:41:17.422: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:19.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010013445s
    Apr 17 22:41:19.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:21.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.009926573s
    Apr 17 22:41:21.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:23.425: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.014359242s
    Apr 17 22:41:23.425: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:25.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010768437s
    Apr 17 22:41:25.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:27.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010704825s
    Apr 17 22:41:27.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:29.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010356479s
    Apr 17 22:41:29.421: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Apr 17 22:41:31.421: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010756008s
    Apr 17 22:41:31.421: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Apr 17 22:41:31.421: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Apr 17 22:41:31.424: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5597" to be "running and ready"
    Apr 17 22:41:31.426: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.210545ms
    Apr 17 22:41:31.426: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Apr 17 22:41:31.426: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Apr 17 22:41:31.428: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5597" to be "running and ready"
    Apr 17 22:41:31.430: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.156381ms
    Apr 17 22:41:31.430: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Apr 17 22:41:31.430: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Apr 17 22:41:31.432: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5597" to be "running and ready"
    Apr 17 22:41:31.434: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 1.967134ms
    Apr 17 22:41:31.434: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Apr 17 22:41:31.434: INFO: Pod "netserver-3" satisfied condition "running and ready"
    STEP: Creating test pods 04/17/23 22:41:31.437
    Apr 17 22:41:31.448: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5597" to be "running"
    Apr 17 22:41:31.454: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.642237ms
    Apr 17 22:41:33.459: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01083685s
    Apr 17 22:41:33.459: INFO: Pod "test-container-pod" satisfied condition "running"
    Apr 17 22:41:33.462: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5597" to be "running"
    Apr 17 22:41:33.465: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.269844ms
    Apr 17 22:41:33.465: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Apr 17 22:41:33.467: INFO: Setting MaxTries for pod polling to 46 for networking test based on endpoint count 4
    Apr 17 22:41:33.467: INFO: Going to poll 192.168.163.226 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Apr 17 22:41:33.469: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.163.226 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:41:33.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:41:33.469: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:41:33.469: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.163.226+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 17 22:41:34.558: INFO: Found all 1 expected endpoints: [netserver-0]
    Apr 17 22:41:34.558: INFO: Going to poll 192.168.200.136 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Apr 17 22:41:34.560: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.200.136 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:41:34.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:41:34.561: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:41:34.561: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.200.136+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 17 22:41:35.628: INFO: Found all 1 expected endpoints: [netserver-1]
    Apr 17 22:41:35.628: INFO: Going to poll 192.168.208.148 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Apr 17 22:41:35.630: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.208.148 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:41:35.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:41:35.631: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:41:35.631: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.208.148+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 17 22:41:36.724: INFO: Found all 1 expected endpoints: [netserver-2]
    Apr 17 22:41:36.724: INFO: Going to poll 192.168.213.36 on port 8081 at least 0 times, with a maximum of 46 tries before failing
    Apr 17 22:41:36.727: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.213.36 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5597 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:41:36.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:41:36.727: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:41:36.727: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5597/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.213.36+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Apr 17 22:41:37.809: INFO: Found all 1 expected endpoints: [netserver-3]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:41:37.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5597" for this suite. 04/17/23 22:41:37.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:41:37.819
Apr 17 22:41:37.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:41:37.82
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:41:37.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:41:37.833
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 04/17/23 22:41:37.835
Apr 17 22:41:37.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:41:41.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Apr 17 22:41:55.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8148" for this suite. 04/17/23 22:41:55.671
------------------------------
• [SLOW TEST] [17.856 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:41:37.819
    Apr 17 22:41:37.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename crd-publish-openapi 04/17/23 22:41:37.82
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:41:37.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:41:37.833
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 04/17/23 22:41:37.835
    Apr 17 22:41:37.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:41:41.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:41:55.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8148" for this suite. 04/17/23 22:41:55.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:41:55.677
Apr 17 22:41:55.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename kubectl 04/17/23 22:41:55.678
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:41:55.692
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:41:55.694
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 04/17/23 22:41:55.696
Apr 17 22:41:55.696: INFO: namespace kubectl-7392
Apr 17 22:41:55.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-7392 create -f -'
Apr 17 22:41:57.171: INFO: stderr: ""
Apr 17 22:41:57.171: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 04/17/23 22:41:57.171
Apr 17 22:41:58.175: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 17 22:41:58.175: INFO: Found 0 / 1
Apr 17 22:41:59.174: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 17 22:41:59.174: INFO: Found 1 / 1
Apr 17 22:41:59.174: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr 17 22:41:59.177: INFO: Selector matched 1 pods for map[app:agnhost]
Apr 17 22:41:59.177: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr 17 22:41:59.177: INFO: wait on agnhost-primary startup in kubectl-7392 
Apr 17 22:41:59.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-7392 logs agnhost-primary-96z8c agnhost-primary'
Apr 17 22:41:59.242: INFO: stderr: ""
Apr 17 22:41:59.242: INFO: stdout: "Paused\n"
STEP: exposing RC 04/17/23 22:41:59.242
Apr 17 22:41:59.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-7392 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Apr 17 22:41:59.311: INFO: stderr: ""
Apr 17 22:41:59.311: INFO: stdout: "service/rm2 exposed\n"
Apr 17 22:41:59.314: INFO: Service rm2 in namespace kubectl-7392 found.
STEP: exposing service 04/17/23 22:42:01.319
Apr 17 22:42:01.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-7392 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Apr 17 22:42:01.394: INFO: stderr: ""
Apr 17 22:42:01.394: INFO: stdout: "service/rm3 exposed\n"
Apr 17 22:42:01.398: INFO: Service rm3 in namespace kubectl-7392 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Apr 17 22:42:03.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7392" for this suite. 04/17/23 22:42:03.41
------------------------------
• [SLOW TEST] [7.739 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:41:55.677
    Apr 17 22:41:55.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename kubectl 04/17/23 22:41:55.678
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:41:55.692
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:41:55.694
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 04/17/23 22:41:55.696
    Apr 17 22:41:55.696: INFO: namespace kubectl-7392
    Apr 17 22:41:55.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-7392 create -f -'
    Apr 17 22:41:57.171: INFO: stderr: ""
    Apr 17 22:41:57.171: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 04/17/23 22:41:57.171
    Apr 17 22:41:58.175: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 17 22:41:58.175: INFO: Found 0 / 1
    Apr 17 22:41:59.174: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 17 22:41:59.174: INFO: Found 1 / 1
    Apr 17 22:41:59.174: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Apr 17 22:41:59.177: INFO: Selector matched 1 pods for map[app:agnhost]
    Apr 17 22:41:59.177: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Apr 17 22:41:59.177: INFO: wait on agnhost-primary startup in kubectl-7392 
    Apr 17 22:41:59.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-7392 logs agnhost-primary-96z8c agnhost-primary'
    Apr 17 22:41:59.242: INFO: stderr: ""
    Apr 17 22:41:59.242: INFO: stdout: "Paused\n"
    STEP: exposing RC 04/17/23 22:41:59.242
    Apr 17 22:41:59.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-7392 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Apr 17 22:41:59.311: INFO: stderr: ""
    Apr 17 22:41:59.311: INFO: stdout: "service/rm2 exposed\n"
    Apr 17 22:41:59.314: INFO: Service rm2 in namespace kubectl-7392 found.
    STEP: exposing service 04/17/23 22:42:01.319
    Apr 17 22:42:01.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=kubectl-7392 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Apr 17 22:42:01.394: INFO: stderr: ""
    Apr 17 22:42:01.394: INFO: stdout: "service/rm3 exposed\n"
    Apr 17 22:42:01.398: INFO: Service rm3 in namespace kubectl-7392 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:42:03.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7392" for this suite. 04/17/23 22:42:03.41
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:42:03.422
Apr 17 22:42:03.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename container-probe 04/17/23 22:42:03.422
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:03.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:03.44
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4 in namespace container-probe-529 04/17/23 22:42:03.442
Apr 17 22:42:03.448: INFO: Waiting up to 5m0s for pod "liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4" in namespace "container-probe-529" to be "not pending"
Apr 17 22:42:03.450: INFO: Pod "liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.194418ms
Apr 17 22:42:05.454: INFO: Pod "liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006159996s
Apr 17 22:42:05.454: INFO: Pod "liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4" satisfied condition "not pending"
Apr 17 22:42:05.454: INFO: Started pod liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4 in namespace container-probe-529
STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 22:42:05.454
Apr 17 22:42:05.456: INFO: Initial restart count of pod liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4 is 0
Apr 17 22:42:25.497: INFO: Restart count of pod container-probe-529/liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4 is now 1 (20.040515811s elapsed)
STEP: deleting the pod 04/17/23 22:42:25.497
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Apr 17 22:42:25.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-529" for this suite. 04/17/23 22:42:25.514
------------------------------
• [SLOW TEST] [22.096 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:42:03.422
    Apr 17 22:42:03.422: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename container-probe 04/17/23 22:42:03.422
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:03.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:03.44
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4 in namespace container-probe-529 04/17/23 22:42:03.442
    Apr 17 22:42:03.448: INFO: Waiting up to 5m0s for pod "liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4" in namespace "container-probe-529" to be "not pending"
    Apr 17 22:42:03.450: INFO: Pod "liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.194418ms
    Apr 17 22:42:05.454: INFO: Pod "liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4": Phase="Running", Reason="", readiness=true. Elapsed: 2.006159996s
    Apr 17 22:42:05.454: INFO: Pod "liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4" satisfied condition "not pending"
    Apr 17 22:42:05.454: INFO: Started pod liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4 in namespace container-probe-529
    STEP: checking the pod's current state and verifying that restartCount is present 04/17/23 22:42:05.454
    Apr 17 22:42:05.456: INFO: Initial restart count of pod liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4 is 0
    Apr 17 22:42:25.497: INFO: Restart count of pod container-probe-529/liveness-5c517b81-cf8d-4d3f-8ac1-1c5879b080d4 is now 1 (20.040515811s elapsed)
    STEP: deleting the pod 04/17/23 22:42:25.497
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:42:25.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-529" for this suite. 04/17/23 22:42:25.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:42:25.518
Apr 17 22:42:25.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename services 04/17/23 22:42:25.519
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:25.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:25.536
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-6900 04/17/23 22:42:25.538
STEP: creating service affinity-clusterip in namespace services-6900 04/17/23 22:42:25.538
STEP: creating replication controller affinity-clusterip in namespace services-6900 04/17/23 22:42:25.549
I0417 22:42:25.556358      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6900, replica count: 3
I0417 22:42:28.607124      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr 17 22:42:28.612: INFO: Creating new exec pod
Apr 17 22:42:28.630: INFO: Waiting up to 5m0s for pod "execpod-affinity64vt9" in namespace "services-6900" to be "running"
Apr 17 22:42:28.633: INFO: Pod "execpod-affinity64vt9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.714099ms
Apr 17 22:42:30.636: INFO: Pod "execpod-affinity64vt9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006384268s
Apr 17 22:42:30.636: INFO: Pod "execpod-affinity64vt9" satisfied condition "running"
Apr 17 22:42:31.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6900 exec execpod-affinity64vt9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Apr 17 22:42:31.770: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Apr 17 22:42:31.770: INFO: stdout: ""
Apr 17 22:42:31.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6900 exec execpod-affinity64vt9 -- /bin/sh -x -c nc -v -z -w 2 10.103.61.188 80'
Apr 17 22:42:31.893: INFO: stderr: "+ nc -v -z -w 2 10.103.61.188 80\nConnection to 10.103.61.188 80 port [tcp/http] succeeded!\n"
Apr 17 22:42:31.893: INFO: stdout: ""
Apr 17 22:42:31.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6900 exec execpod-affinity64vt9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.61.188:80/ ; done'
Apr 17 22:42:32.077: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n"
Apr 17 22:42:32.077: INFO: stdout: "\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66"
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.078: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.078: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.078: INFO: Received response from host: affinity-clusterip-vfk66
Apr 17 22:42:32.078: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-6900, will wait for the garbage collector to delete the pods 04/17/23 22:42:32.088
Apr 17 22:42:32.145: INFO: Deleting ReplicationController affinity-clusterip took: 4.569349ms
Apr 17 22:42:32.246: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.388359ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Apr 17 22:42:34.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6900" for this suite. 04/17/23 22:42:34.367
------------------------------
• [SLOW TEST] [8.853 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:42:25.518
    Apr 17 22:42:25.518: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename services 04/17/23 22:42:25.519
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:25.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:25.536
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-6900 04/17/23 22:42:25.538
    STEP: creating service affinity-clusterip in namespace services-6900 04/17/23 22:42:25.538
    STEP: creating replication controller affinity-clusterip in namespace services-6900 04/17/23 22:42:25.549
    I0417 22:42:25.556358      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-6900, replica count: 3
    I0417 22:42:28.607124      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Apr 17 22:42:28.612: INFO: Creating new exec pod
    Apr 17 22:42:28.630: INFO: Waiting up to 5m0s for pod "execpod-affinity64vt9" in namespace "services-6900" to be "running"
    Apr 17 22:42:28.633: INFO: Pod "execpod-affinity64vt9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.714099ms
    Apr 17 22:42:30.636: INFO: Pod "execpod-affinity64vt9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006384268s
    Apr 17 22:42:30.636: INFO: Pod "execpod-affinity64vt9" satisfied condition "running"
    Apr 17 22:42:31.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6900 exec execpod-affinity64vt9 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Apr 17 22:42:31.770: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Apr 17 22:42:31.770: INFO: stdout: ""
    Apr 17 22:42:31.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6900 exec execpod-affinity64vt9 -- /bin/sh -x -c nc -v -z -w 2 10.103.61.188 80'
    Apr 17 22:42:31.893: INFO: stderr: "+ nc -v -z -w 2 10.103.61.188 80\nConnection to 10.103.61.188 80 port [tcp/http] succeeded!\n"
    Apr 17 22:42:31.893: INFO: stdout: ""
    Apr 17 22:42:31.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2794059258 --namespace=services-6900 exec execpod-affinity64vt9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.103.61.188:80/ ; done'
    Apr 17 22:42:32.077: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.103.61.188:80/\n"
    Apr 17 22:42:32.077: INFO: stdout: "\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66\naffinity-clusterip-vfk66"
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.077: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.078: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.078: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.078: INFO: Received response from host: affinity-clusterip-vfk66
    Apr 17 22:42:32.078: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-6900, will wait for the garbage collector to delete the pods 04/17/23 22:42:32.088
    Apr 17 22:42:32.145: INFO: Deleting ReplicationController affinity-clusterip took: 4.569349ms
    Apr 17 22:42:32.246: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.388359ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:42:34.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6900" for this suite. 04/17/23 22:42:34.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:42:34.372
Apr 17 22:42:34.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 04/17/23 22:42:34.373
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:34.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:34.389
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 04/17/23 22:42:34.391
STEP: Creating hostNetwork=false pod 04/17/23 22:42:34.391
Apr 17 22:42:34.399: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3957" to be "running and ready"
Apr 17 22:42:34.401: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.145079ms
Apr 17 22:42:34.401: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:42:36.404: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005748876s
Apr 17 22:42:36.404: INFO: The phase of Pod test-pod is Running (Ready = true)
Apr 17 22:42:36.404: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 04/17/23 22:42:36.407
Apr 17 22:42:36.412: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3957" to be "running and ready"
Apr 17 22:42:36.414: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446302ms
Apr 17 22:42:36.414: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Apr 17 22:42:38.417: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00538057s
Apr 17 22:42:38.417: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Apr 17 22:42:38.417: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 04/17/23 22:42:38.419
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 04/17/23 22:42:38.419
Apr 17 22:42:38.419: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:38.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:38.420: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:38.420: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 17 22:42:38.499: INFO: Exec stderr: ""
Apr 17 22:42:38.499: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:38.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:38.500: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:38.500: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 17 22:42:38.558: INFO: Exec stderr: ""
Apr 17 22:42:38.558: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:38.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:38.558: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:38.559: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 17 22:42:38.644: INFO: Exec stderr: ""
Apr 17 22:42:38.644: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:38.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:38.644: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:38.644: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 17 22:42:38.727: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 04/17/23 22:42:38.727
Apr 17 22:42:38.727: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:38.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:38.728: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:38.728: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Apr 17 22:42:38.803: INFO: Exec stderr: ""
Apr 17 22:42:38.803: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:38.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:38.804: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:38.804: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Apr 17 22:42:38.887: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 04/17/23 22:42:38.887
Apr 17 22:42:38.887: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:38.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:38.888: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:38.888: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 17 22:42:38.966: INFO: Exec stderr: ""
Apr 17 22:42:38.966: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:38.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:38.967: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:38.967: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Apr 17 22:42:39.046: INFO: Exec stderr: ""
Apr 17 22:42:39.046: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:39.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:39.047: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:39.047: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 17 22:42:39.097: INFO: Exec stderr: ""
Apr 17 22:42:39.097: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Apr 17 22:42:39.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
Apr 17 22:42:39.097: INFO: ExecWithOptions: Clientset creation
Apr 17 22:42:39.097: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Apr 17 22:42:39.154: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Apr 17 22:42:39.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3957" for this suite. 04/17/23 22:42:39.159
------------------------------
• [4.793 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:42:34.372
    Apr 17 22:42:34.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 04/17/23 22:42:34.373
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:34.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:34.389
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 04/17/23 22:42:34.391
    STEP: Creating hostNetwork=false pod 04/17/23 22:42:34.391
    Apr 17 22:42:34.399: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-3957" to be "running and ready"
    Apr 17 22:42:34.401: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.145079ms
    Apr 17 22:42:34.401: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:42:36.404: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.005748876s
    Apr 17 22:42:36.404: INFO: The phase of Pod test-pod is Running (Ready = true)
    Apr 17 22:42:36.404: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 04/17/23 22:42:36.407
    Apr 17 22:42:36.412: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-3957" to be "running and ready"
    Apr 17 22:42:36.414: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.446302ms
    Apr 17 22:42:36.414: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Apr 17 22:42:38.417: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00538057s
    Apr 17 22:42:38.417: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Apr 17 22:42:38.417: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 04/17/23 22:42:38.419
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 04/17/23 22:42:38.419
    Apr 17 22:42:38.419: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:38.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:38.420: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:38.420: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 17 22:42:38.499: INFO: Exec stderr: ""
    Apr 17 22:42:38.499: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:38.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:38.500: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:38.500: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 17 22:42:38.558: INFO: Exec stderr: ""
    Apr 17 22:42:38.558: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:38.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:38.558: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:38.559: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 17 22:42:38.644: INFO: Exec stderr: ""
    Apr 17 22:42:38.644: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:38.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:38.644: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:38.644: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 17 22:42:38.727: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 04/17/23 22:42:38.727
    Apr 17 22:42:38.727: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:38.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:38.728: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:38.728: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Apr 17 22:42:38.803: INFO: Exec stderr: ""
    Apr 17 22:42:38.803: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:38.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:38.804: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:38.804: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Apr 17 22:42:38.887: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 04/17/23 22:42:38.887
    Apr 17 22:42:38.887: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:38.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:38.888: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:38.888: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 17 22:42:38.966: INFO: Exec stderr: ""
    Apr 17 22:42:38.966: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:38.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:38.967: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:38.967: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Apr 17 22:42:39.046: INFO: Exec stderr: ""
    Apr 17 22:42:39.046: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:39.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:39.047: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:39.047: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 17 22:42:39.097: INFO: Exec stderr: ""
    Apr 17 22:42:39.097: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3957 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Apr 17 22:42:39.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    Apr 17 22:42:39.097: INFO: ExecWithOptions: Clientset creation
    Apr 17 22:42:39.097: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3957/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Apr 17 22:42:39.154: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:42:39.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-3957" for this suite. 04/17/23 22:42:39.159
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:42:39.165
Apr 17 22:42:39.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename secrets 04/17/23 22:42:39.166
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:39.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:39.18
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Apr 17 22:42:39.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6789" for this suite. 04/17/23 22:42:39.211
------------------------------
• [0.050 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:42:39.165
    Apr 17 22:42:39.165: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename secrets 04/17/23 22:42:39.166
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:39.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:39.18
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:42:39.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6789" for this suite. 04/17/23 22:42:39.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 04/17/23 22:42:39.216
Apr 17 22:42:39.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
STEP: Building a namespace api object, basename projected 04/17/23 22:42:39.217
STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:39.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:39.231
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 04/17/23 22:42:39.233
Apr 17 22:42:39.239: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e" in namespace "projected-9876" to be "Succeeded or Failed"
Apr 17 22:42:39.242: INFO: Pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.401119ms
Apr 17 22:42:41.245: INFO: Pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005484983s
Apr 17 22:42:43.245: INFO: Pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005775068s
STEP: Saw pod success 04/17/23 22:42:43.245
Apr 17 22:42:43.245: INFO: Pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e" satisfied condition "Succeeded or Failed"
Apr 17 22:42:43.247: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e container client-container: <nil>
STEP: delete the pod 04/17/23 22:42:43.259
Apr 17 22:42:43.271: INFO: Waiting for pod downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e to disappear
Apr 17 22:42:43.273: INFO: Pod downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Apr 17 22:42:43.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9876" for this suite. 04/17/23 22:42:43.277
------------------------------
• [4.067 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 04/17/23 22:42:39.216
    Apr 17 22:42:39.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2794059258
    STEP: Building a namespace api object, basename projected 04/17/23 22:42:39.217
    STEP: Waiting for a default service account to be provisioned in namespace 04/17/23 22:42:39.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 04/17/23 22:42:39.231
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 04/17/23 22:42:39.233
    Apr 17 22:42:39.239: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e" in namespace "projected-9876" to be "Succeeded or Failed"
    Apr 17 22:42:39.242: INFO: Pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.401119ms
    Apr 17 22:42:41.245: INFO: Pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005484983s
    Apr 17 22:42:43.245: INFO: Pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005775068s
    STEP: Saw pod success 04/17/23 22:42:43.245
    Apr 17 22:42:43.245: INFO: Pod "downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e" satisfied condition "Succeeded or Failed"
    Apr 17 22:42:43.247: INFO: Trying to get logs from node ip-10-0-106-231.us-west-2.compute.internal pod downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e container client-container: <nil>
    STEP: delete the pod 04/17/23 22:42:43.259
    Apr 17 22:42:43.271: INFO: Waiting for pod downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e to disappear
    Apr 17 22:42:43.273: INFO: Pod downwardapi-volume-a343cb33-2f2e-4a47-a64e-245ed73bc66e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Apr 17 22:42:43.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9876" for this suite. 04/17/23 22:42:43.277
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Apr 17 22:42:43.284: INFO: Running AfterSuite actions on node 1
Apr 17 22:42:43.284: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Apr 17 22:42:43.284: INFO: Running AfterSuite actions on node 1
    Apr 17 22:42:43.284: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.078 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5600.198 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h33m20.472702009s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

